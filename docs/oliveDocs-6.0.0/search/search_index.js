const local_index = {"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Open Language Interface for Voice Exploitation (OLIVE) 6.0.0 OLIVE is a suite of audio processing software tools to enable a wide range of audio analysis for several mission types, including large scale audio triage, targeted close forensic analysis, detection of speech, speakers, languages, and keywords of interest. OLIVE 6.0.0 Changelog Highlights Stateless Enrollment Support has been added to allow client-side management of enrollments. Enhanced Job Cancellation support - now allows specifying individual job(s) to cancel, rather than forcing all jobs to be canceled. Runtime updated to CUDA 12.4. Updated minimum Nvidia GPU driver requirements OLIVE Martini base image has been updated from Ubuntu 20.04 to 22.04 for improved longevity. Runtime updated to support new Wav2Vec2-BERT ASR model architecture. OLIVE Runtime has been updated to address various security vulnerabilities in third party libraries. Scans show 0 Critical or High vulnerabilities as of March 2025. Raven Batch UI has been updated with a revamped visual design, and the addition of features like direct plugin enrollment (no longer requires a custom enrollment workflow). Various bug fixes and stability improvements Several new or updated plugin releases : asr-end2end-v4.0.0 dfa-end2end-v1.0.0 ldd-embed-v2.0.0 lid-hdplda-v2.0.2 qbe-ftdnn-v2.0.0 sdd-embed-v2.0.0 sid-dplda-v3.3.0 tmt-ctranslate-v1.3.2 For more information about the OLIVE 6.0.0 plugins that are currently released and their capabilities, refer to the OLIVE 6.0.0 Release Plugins list, that links to more information about each. There are many facets to OLIVE, several of which are documented here. Feel free to navigate through the documentation using the bar along the left side of each page. The links along the right side of each page provide navigation within that particular page. If you're unsure of the best place to look, this may help you choose where to get started: Administration and General Setup Documentation Installation and Setup OLIVE Martini Docker-based Installation - Details for installing and setting up the OLIVE software package for docker-container-based deliveries. OLIVE Martini includes a web-based Raven Batch GUI, and other utilities such as a web broker that exposes the new OLIVE REST API. This is the standard OLIVE deliverable format. Windows Native OLIVE Installation - Details for installing and using the OLIVE software package for Windows, released with OLIVE 5.7.1. OLIVE GPU Plugin Configuration - Information on how to properly configure OLIVE plugins and domains to use an available Nvidia GPU. OLIVE Server Guide - For reference for setting up the OLIVE environment and firing up an OLIVE server instance, along with important details about how the Server works with the OLIVE Runtime and the OLIVE Runtime contents. OLIVE System Requirements - Speed and memory benchmarking results for some plugins, and overall resource requirement information. Usage Documentation User Interfaces OLIVE Nightingale GUI - Details for launching and interacting with the OLIVE tools through the the optional OLIVE Nightingale GUI. Speaker Redaction GUI Task - Details how to step through the process of using the OLIVE GUI for Speaker Redaction. Raven Web Batch GUI - Details how to launch and interact with the Raven web-based batch GUI for OLIVE, available in OLIVE Martini-based deliveries. Java and Python Client Utilities - How to install and get started with running the example Java and Python command-line client utilities. Command Line Interface Documentation (Legacy) - If you're interested in exploring the capabilities of OLIVE and its plugins as an evaluation tool just using the command line. This is recommended for light testing and evaluation only, when using the native linux OLIVE, not for integration or for use with docker-based OLIVE deliveries. Supported Audio Formats - Outlines the audio formats OLIVE can currently process and which situations these restrictions apply Integration Specific Documentation Workflow Integration - Details for getting started submitting enrollment and analysis requests using OLIVE workflows, a powerful new tool allowing you to leverage several plugins with a single server request/API message. Workflow integration is recommended for most integrations, unless very specific functionality is required that's not yet supported by workflows. OLIVE REST API Documentation - Swagger documentation covering the OLIVE REST API exposed by the OLIVE Web Broker. If this documentation is being hosted by the OLIVE Martini container, this documentation will be interactive, allowing you to submit REST API messages straight from the documentation for experimentation. OLIVE Enterprise API Integration: Enterprise API Primer - If interested in details about the mechanisms that allow integrating the OLIVE Server and backend audio processing capabilities to an existing system or GUI. Creating an API Reference Implementation - Contains information to help guide the creation of a new API Reference Implementation if Java is not the target language for client integration. Enterprise API Message Definitions - Provides more of the low-level details on the available API messages, their structures and contents. OlivePy Python API Client Documentation - Provides auto-generated PyDoc style documentation for the OlivePy Python Client API (brings you out of the normal OLIVE documentation - press browser's 'back' button to return). Integrating the (Java) Client API - Provides code examples (currently only in Java) for how to perform several OLIVE tasks and integrate the OLIVE Java Reference Implementation to create a new client or augment a client with OLIVE functionality. OLIVE Plugin Traits - Information regarding plugins including terminology definitions for different plugin types and other types Plugin Information Release Plugins - List of the plugins released with this version of OLIVE, with links to more information on the specific plugins themselves. General Plugin Info - Information regarding the actual technologies and capabilities provided by OLIVE (such as speech activity detection, language identification, etc.), and details on implementing these technologies. OLIVE Plugin Resource Requirements - Information about the processing speed and memory requirements for a selection of OLIVE plugins. Additional Info Glossary - Definitions of important terms and concepts. Contact Us - Information for how to reach out to the OLIVE team Care was made to provide links to important contextual information regarding important terms, acronyms, API messages, etc. throughout the documentation. If you find any section to be unclear or lacking in important details, please let us know which area(s) require improvement. For any questions, comments, or concerns about this documentation contact olive-support@sri.com .","title":"Overview"},{"location":"index.html#open-language-interface-for-voice-exploitation-olive-600","text":"OLIVE is a suite of audio processing software tools to enable a wide range of audio analysis for several mission types, including large scale audio triage, targeted close forensic analysis, detection of speech, speakers, languages, and keywords of interest.","title":"Open Language Interface for Voice Exploitation (OLIVE) 6.0.0"},{"location":"index.html#olive-600-changelog-highlights","text":"Stateless Enrollment Support has been added to allow client-side management of enrollments. Enhanced Job Cancellation support - now allows specifying individual job(s) to cancel, rather than forcing all jobs to be canceled. Runtime updated to CUDA 12.4. Updated minimum Nvidia GPU driver requirements OLIVE Martini base image has been updated from Ubuntu 20.04 to 22.04 for improved longevity. Runtime updated to support new Wav2Vec2-BERT ASR model architecture. OLIVE Runtime has been updated to address various security vulnerabilities in third party libraries. Scans show 0 Critical or High vulnerabilities as of March 2025. Raven Batch UI has been updated with a revamped visual design, and the addition of features like direct plugin enrollment (no longer requires a custom enrollment workflow). Various bug fixes and stability improvements Several new or updated plugin releases : asr-end2end-v4.0.0 dfa-end2end-v1.0.0 ldd-embed-v2.0.0 lid-hdplda-v2.0.2 qbe-ftdnn-v2.0.0 sdd-embed-v2.0.0 sid-dplda-v3.3.0 tmt-ctranslate-v1.3.2 For more information about the OLIVE 6.0.0 plugins that are currently released and their capabilities, refer to the OLIVE 6.0.0 Release Plugins list, that links to more information about each. There are many facets to OLIVE, several of which are documented here. Feel free to navigate through the documentation using the bar along the left side of each page. The links along the right side of each page provide navigation within that particular page. If you're unsure of the best place to look, this may help you choose where to get started:","title":"OLIVE 6.0.0 Changelog Highlights"},{"location":"index.html#administration-and-general-setup-documentation","text":"Installation and Setup OLIVE Martini Docker-based Installation - Details for installing and setting up the OLIVE software package for docker-container-based deliveries. OLIVE Martini includes a web-based Raven Batch GUI, and other utilities such as a web broker that exposes the new OLIVE REST API. This is the standard OLIVE deliverable format. Windows Native OLIVE Installation - Details for installing and using the OLIVE software package for Windows, released with OLIVE 5.7.1. OLIVE GPU Plugin Configuration - Information on how to properly configure OLIVE plugins and domains to use an available Nvidia GPU. OLIVE Server Guide - For reference for setting up the OLIVE environment and firing up an OLIVE server instance, along with important details about how the Server works with the OLIVE Runtime and the OLIVE Runtime contents. OLIVE System Requirements - Speed and memory benchmarking results for some plugins, and overall resource requirement information.","title":"Administration and General Setup Documentation"},{"location":"index.html#usage-documentation","text":"User Interfaces OLIVE Nightingale GUI - Details for launching and interacting with the OLIVE tools through the the optional OLIVE Nightingale GUI. Speaker Redaction GUI Task - Details how to step through the process of using the OLIVE GUI for Speaker Redaction. Raven Web Batch GUI - Details how to launch and interact with the Raven web-based batch GUI for OLIVE, available in OLIVE Martini-based deliveries. Java and Python Client Utilities - How to install and get started with running the example Java and Python command-line client utilities. Command Line Interface Documentation (Legacy) - If you're interested in exploring the capabilities of OLIVE and its plugins as an evaluation tool just using the command line. This is recommended for light testing and evaluation only, when using the native linux OLIVE, not for integration or for use with docker-based OLIVE deliveries. Supported Audio Formats - Outlines the audio formats OLIVE can currently process and which situations these restrictions apply","title":"Usage Documentation"},{"location":"index.html#integration-specific-documentation","text":"Workflow Integration - Details for getting started submitting enrollment and analysis requests using OLIVE workflows, a powerful new tool allowing you to leverage several plugins with a single server request/API message. Workflow integration is recommended for most integrations, unless very specific functionality is required that's not yet supported by workflows. OLIVE REST API Documentation - Swagger documentation covering the OLIVE REST API exposed by the OLIVE Web Broker. If this documentation is being hosted by the OLIVE Martini container, this documentation will be interactive, allowing you to submit REST API messages straight from the documentation for experimentation. OLIVE Enterprise API Integration: Enterprise API Primer - If interested in details about the mechanisms that allow integrating the OLIVE Server and backend audio processing capabilities to an existing system or GUI. Creating an API Reference Implementation - Contains information to help guide the creation of a new API Reference Implementation if Java is not the target language for client integration. Enterprise API Message Definitions - Provides more of the low-level details on the available API messages, their structures and contents. OlivePy Python API Client Documentation - Provides auto-generated PyDoc style documentation for the OlivePy Python Client API (brings you out of the normal OLIVE documentation - press browser's 'back' button to return). Integrating the (Java) Client API - Provides code examples (currently only in Java) for how to perform several OLIVE tasks and integrate the OLIVE Java Reference Implementation to create a new client or augment a client with OLIVE functionality. OLIVE Plugin Traits - Information regarding plugins including terminology definitions for different plugin types and other types","title":"Integration Specific Documentation"},{"location":"index.html#plugin-information","text":"Release Plugins - List of the plugins released with this version of OLIVE, with links to more information on the specific plugins themselves. General Plugin Info - Information regarding the actual technologies and capabilities provided by OLIVE (such as speech activity detection, language identification, etc.), and details on implementing these technologies. OLIVE Plugin Resource Requirements - Information about the processing speed and memory requirements for a selection of OLIVE plugins.","title":"Plugin Information"},{"location":"index.html#additional-info","text":"Glossary - Definitions of important terms and concepts. Contact Us - Information for how to reach out to the OLIVE team Care was made to provide links to important contextual information regarding important terms, acronyms, API messages, etc. throughout the documentation. If you find any section to be unclear or lacking in important details, please let us know which area(s) require improvement. For any questions, comments, or concerns about this documentation contact olive-support@sri.com .","title":"Additional Info"},{"location":"apiBuildReferenceImp.html","text":"Developing an OLIVE API Reference Implementation If SRI\u2019s Java client library does not meet your needs or you need to create an implementation in a language other than Java, then the following information is helpful for creating a new reference implementation of the OLIVE Enterprise API. Things to know before you start Before you start, it is a good idea to first develop an understanding of the current OLIVE Java Client API. Reading the API Primer page and reviewing the provided Java Client API source files are all good places to start. As an alternative, To enable users to quickly put together client codes, the OLIVE Java API hides many of the low level implementation details such as assemby of request messages, submission of requests over a port, etc. This section describes some of these implementation details you need to consider when creating a brand new OLIVE API. 1. Communicating with the OLIVE Server A messaging system called ZeroMQ , or ZMQ for short, is used as the backbone for communicating with the OLIVE Server. In order to communicate with a running server, you must initialize ZeroMQ (ZMQ) sockets in your client code. There are two sockets: request socket: This is the socket over which you send requests and receive replies, all of which are serialized protobuf messages. You must conect to this socket using the ZMQ.DEALER configuration. status socket: This socket provides a simple heartbeat broadcast from the server. You may optionally monitor this socket to determine the up/down status of the server. Connect to this socket using the ZMQ.PUB configuration. The code for initializing a ZMQ context and creating/connecting the necessary sockets can differ by language. Below is a ZMQ initialization example in Java. ZMQ . Context context = ZMQ . context (); ZMQ . Socket request_socket = context . socket ( ZMQ . DEALER ); ZMQ . Socket status_socket = context . socket ( ZMQ . SUB ); request_socket . connect ( \"tcp://myserver:6678\" ); status_socket . connect ( \"tcp://myserver:6679\" ); status_socket . subscribe ( \"\" ); // Don\u2019t forget this After connecting the sockets, you can begin sending requests and receiving responses over the request port. The specifics of how this operation is performed are described in the next section. 2. Serialization of messages to and from the OLIVE Server Messages are exchanged between the client and server in serialized form, over the request port. Serialization is provided by the Google Protobufs library. You should familiarize yourself with protobufs before beginning your integration. In order to utilize protobufs, you must first take the scenic.proto message definition file (provided in the example code package or available upon request ) and use protoc (or protobuf.net) to automatically generate classes that represent the OLIVE API messages. For more information see the Google Protocol Buffers Documentation . For example for the OLIVE Java Client API, protoc is used to produce the Java file Scenic.java, located in src/main/java/com/sri/scenic/api within the example code package. This file contains the class Scenic which is used in the Java API for all message related classes and definitions. Once you have compiled the OLIVE messages into your code base you can begin your integration. As you may have seen in the OLIVE Java Client API code, (Scenic.java), every OLIVE message is an instance of a class named Envelope . As its name implies, Envelope acts as a container for enclosing messages. Messages are instances of a class named ScenicMessage . An Envelope can contain multiple instances of ScenicMessage , allowing you to batch your communications to the server. Envelope and ScenicMessage are special because they are used for every communication across the request_port and are basically just wrappers. They\u2019re analogous to the envelope and paper when writing someone a letter. The remaining OLIVE messages comprise the actual API requests and responses. The remaining messages each have an entry in the MessageType enum, allowing you to request certain types when retrieving data from database as well as dynamically deserializing data returned by the server. Message Building Example To get started, your integration will probably need to retrieve some information from the server. For example, you may wish to know the list of available plugins. An example pseudocode excerpt accomplishing this is shown below. PluginDirectoryRequest . Builder req = PluginDirectoryRequest . newBuilder () String id = getUUIDString () ScenicMessage msg = ScenicMessage . newBuilder () . setMessageType ( MessageType . PLUGIN_DIRECTORY_REQUEST ) . addMessageData ( req . build (). toByteString ()) . setMessageId ( id ) . build (); Envelope env = Envelope . newBuilder () . setSenderId ( \u201c third - party - integration \u201d ) . addMessage ( msg ). build (); // Now send the message to the server request_socket . send ( env . toByteArray ()); Envelope resp = Envelope . parseFrom ( request_socket . recv ()); for ( ScenicMessage sm : resp . getMessageList ()) { // For purposes of this example, we assume the above message was // the first and only sent so we can assume things about the response // message, namely that it corresponds to our request. assert ( id == msg . getMessageId ()) \u2019 assert ( msg . getMessageType () == MessageType . PLUGIN_DIRECTORY_RESULT ); if ( sm . hasError ()) { System . out . println ( \u201c Dang : \u201c + sm . getError ()); continue ; } PluginDirectoryResult rep = PluginDirectoryResult . parseFrom ( sm . getMessageData ( 0 )); for ( Plugin p : rep . getPluginsList ()){ System . out . println ( p . getId () + \": \" + p . getDesc () ); } } Please note the following: We could have put other requests in the Envelope. Their responses may or may not have come back in the same envelope, but they would have come back in order. We are guaranteed that our messages are received in order by the server and responses sent in order. However, for messages such as scoring requests and class modification requests (enrollment), which are highly asynchronous, there is no guarantee about the order in which they will finish. The sure-fire way to ensure that you process a message from the server correctly is to base your actions on the message id (which you originally assigned in your request). To properly deserialize the data contained within a ScenicMessage , you must check or otherwise be sure of the MessageType . Some OLIVE plugins need to be preloaded by the server in order to fulfill a request. In such cases either the API or the client program must first send a Load Plugin Domain Request to have the targeted plugin preloaded. An Analysis (Scoring) Message Example Now let\u2019s assume we wish to perform language identification on an audio file. We can create a LID like request as follows: // Variable init String plugin = \u201c lid - embed - v2 \u201d String domain = \u201c multi - v1 \u201d String audioFilePath = \u201c / home / user / audio / file1 . wav \u201d // Build audio object Scenic . Audio . Builder audio = Scenic . Audio . newBuilder (). setPath ( audioFilePath . toAbsolutePath (). toString ; // Create LID request // If specifically processing stereo audio files and wish to score // both channels, please use the FrameScorerStereoRequest or // GlobalScorerStereoRequest messages, that will be responded to with // an FrameScoreStereo or GlobalScoreStereo message, containing score results // for both channels of the submitted audio. // If submitting a stereo audio file using the standard xScorerRequest functions, // and you don\u2019t desire to score both channels independently, there are two // options: // - Specify the channel you wish to be scored -> you will receive results // for that channel only. // - Do not specify a channel -> you will receive a single set of results // corresponding to the merged mono representation of the stereo file. Scenic . GlobalScorerRequest . Builder req = Scenic . GlobalScorerRequest . newBuilder () . setAudio ( audio ) . setPlugin ( plugin ) . setDomain ( domain ); Note that this example relies on code written in our Java Client API, but the general steps to perform the task are the same. Now we wrap the request in a ScenicMessage and Envelope like we did in the last example and send it across the request socket. Analysis requests as well as Enrollment requests take significant time to process on the server. It\u2019s likely you\u2019ll want your integration to be doing other things while it is waiting for the response, such as issuing further analysis requests. This is fully supported. However, you don\u2019t know when or in what order the responses to your analyze request will emerge from the server. Therefore, it\u2019s advantageous to track the message ids that you\u2019ve issued in a map of the form message_id -> request message , so that you know the request to which a newly received response pertains. Let\u2019s assume we\u2019ve received a GlobalScorerResult message and have deserialized it into a variable named res . We could process the result as follows: // Currently OLIVE (SCENIC) will only send back one score reply per score // request. Future releases may be able to send back multiple. // Because of this, we must iterate though all of the scores. List < Scenic . GlobalScore > scores = res . getScoreList (); for ( Scenic . GlobalScore gs : scores ){ system . out . println ( \"LID Score: class \" + gs . getClassId () \u201c = \u201c + gs . getScore ()); } For more details regarding the specifications and breadth of the possible requests, their replies, and the structure of each of these data objects, including how results are represented, please refer to the OLIVE API Message Reference documentation. \u2003 Request a new OLIVE API Reference Implementation If there is a need for an OLIVE API in another language, SRI International would be an ideal candidate to undertake the task because of its rich experience having already done it in Java. However, it will still be a complex software engineering undertaking which will definitely take up a significant amount of project resources. For more information about what this would entail, please reach out to olive-support@sri.com.","title":"Building an API Reference Implementation"},{"location":"apiBuildReferenceImp.html#developing-an-olive-api-reference-implementation","text":"If SRI\u2019s Java client library does not meet your needs or you need to create an implementation in a language other than Java, then the following information is helpful for creating a new reference implementation of the OLIVE Enterprise API.","title":"Developing an OLIVE API Reference Implementation"},{"location":"apiBuildReferenceImp.html#things-to-know-before-you-start","text":"Before you start, it is a good idea to first develop an understanding of the current OLIVE Java Client API. Reading the API Primer page and reviewing the provided Java Client API source files are all good places to start. As an alternative, To enable users to quickly put together client codes, the OLIVE Java API hides many of the low level implementation details such as assemby of request messages, submission of requests over a port, etc. This section describes some of these implementation details you need to consider when creating a brand new OLIVE API.","title":"Things to know before you start"},{"location":"apiBuildReferenceImp.html#1-communicating-with-the-olive-server","text":"A messaging system called ZeroMQ , or ZMQ for short, is used as the backbone for communicating with the OLIVE Server. In order to communicate with a running server, you must initialize ZeroMQ (ZMQ) sockets in your client code. There are two sockets: request socket: This is the socket over which you send requests and receive replies, all of which are serialized protobuf messages. You must conect to this socket using the ZMQ.DEALER configuration. status socket: This socket provides a simple heartbeat broadcast from the server. You may optionally monitor this socket to determine the up/down status of the server. Connect to this socket using the ZMQ.PUB configuration. The code for initializing a ZMQ context and creating/connecting the necessary sockets can differ by language. Below is a ZMQ initialization example in Java. ZMQ . Context context = ZMQ . context (); ZMQ . Socket request_socket = context . socket ( ZMQ . DEALER ); ZMQ . Socket status_socket = context . socket ( ZMQ . SUB ); request_socket . connect ( \"tcp://myserver:6678\" ); status_socket . connect ( \"tcp://myserver:6679\" ); status_socket . subscribe ( \"\" ); // Don\u2019t forget this After connecting the sockets, you can begin sending requests and receiving responses over the request port. The specifics of how this operation is performed are described in the next section.","title":"1. Communicating with the OLIVE Server"},{"location":"apiBuildReferenceImp.html#2-serialization-of-messages-to-and-from-the-olive-server","text":"Messages are exchanged between the client and server in serialized form, over the request port. Serialization is provided by the Google Protobufs library. You should familiarize yourself with protobufs before beginning your integration. In order to utilize protobufs, you must first take the scenic.proto message definition file (provided in the example code package or available upon request ) and use protoc (or protobuf.net) to automatically generate classes that represent the OLIVE API messages. For more information see the Google Protocol Buffers Documentation . For example for the OLIVE Java Client API, protoc is used to produce the Java file Scenic.java, located in src/main/java/com/sri/scenic/api within the example code package. This file contains the class Scenic which is used in the Java API for all message related classes and definitions. Once you have compiled the OLIVE messages into your code base you can begin your integration. As you may have seen in the OLIVE Java Client API code, (Scenic.java), every OLIVE message is an instance of a class named Envelope . As its name implies, Envelope acts as a container for enclosing messages. Messages are instances of a class named ScenicMessage . An Envelope can contain multiple instances of ScenicMessage , allowing you to batch your communications to the server. Envelope and ScenicMessage are special because they are used for every communication across the request_port and are basically just wrappers. They\u2019re analogous to the envelope and paper when writing someone a letter. The remaining OLIVE messages comprise the actual API requests and responses. The remaining messages each have an entry in the MessageType enum, allowing you to request certain types when retrieving data from database as well as dynamically deserializing data returned by the server.","title":"2.  Serialization of messages to and from the OLIVE Server"},{"location":"apiBuildReferenceImp.html#message-building-example","text":"To get started, your integration will probably need to retrieve some information from the server. For example, you may wish to know the list of available plugins. An example pseudocode excerpt accomplishing this is shown below. PluginDirectoryRequest . Builder req = PluginDirectoryRequest . newBuilder () String id = getUUIDString () ScenicMessage msg = ScenicMessage . newBuilder () . setMessageType ( MessageType . PLUGIN_DIRECTORY_REQUEST ) . addMessageData ( req . build (). toByteString ()) . setMessageId ( id ) . build (); Envelope env = Envelope . newBuilder () . setSenderId ( \u201c third - party - integration \u201d ) . addMessage ( msg ). build (); // Now send the message to the server request_socket . send ( env . toByteArray ()); Envelope resp = Envelope . parseFrom ( request_socket . recv ()); for ( ScenicMessage sm : resp . getMessageList ()) { // For purposes of this example, we assume the above message was // the first and only sent so we can assume things about the response // message, namely that it corresponds to our request. assert ( id == msg . getMessageId ()) \u2019 assert ( msg . getMessageType () == MessageType . PLUGIN_DIRECTORY_RESULT ); if ( sm . hasError ()) { System . out . println ( \u201c Dang : \u201c + sm . getError ()); continue ; } PluginDirectoryResult rep = PluginDirectoryResult . parseFrom ( sm . getMessageData ( 0 )); for ( Plugin p : rep . getPluginsList ()){ System . out . println ( p . getId () + \": \" + p . getDesc () ); } } Please note the following: We could have put other requests in the Envelope. Their responses may or may not have come back in the same envelope, but they would have come back in order. We are guaranteed that our messages are received in order by the server and responses sent in order. However, for messages such as scoring requests and class modification requests (enrollment), which are highly asynchronous, there is no guarantee about the order in which they will finish. The sure-fire way to ensure that you process a message from the server correctly is to base your actions on the message id (which you originally assigned in your request). To properly deserialize the data contained within a ScenicMessage , you must check or otherwise be sure of the MessageType . Some OLIVE plugins need to be preloaded by the server in order to fulfill a request. In such cases either the API or the client program must first send a Load Plugin Domain Request to have the targeted plugin preloaded.","title":"Message Building Example"},{"location":"apiBuildReferenceImp.html#an-analysis-scoring-message-example","text":"Now let\u2019s assume we wish to perform language identification on an audio file. We can create a LID like request as follows: // Variable init String plugin = \u201c lid - embed - v2 \u201d String domain = \u201c multi - v1 \u201d String audioFilePath = \u201c / home / user / audio / file1 . wav \u201d // Build audio object Scenic . Audio . Builder audio = Scenic . Audio . newBuilder (). setPath ( audioFilePath . toAbsolutePath (). toString ; // Create LID request // If specifically processing stereo audio files and wish to score // both channels, please use the FrameScorerStereoRequest or // GlobalScorerStereoRequest messages, that will be responded to with // an FrameScoreStereo or GlobalScoreStereo message, containing score results // for both channels of the submitted audio. // If submitting a stereo audio file using the standard xScorerRequest functions, // and you don\u2019t desire to score both channels independently, there are two // options: // - Specify the channel you wish to be scored -> you will receive results // for that channel only. // - Do not specify a channel -> you will receive a single set of results // corresponding to the merged mono representation of the stereo file. Scenic . GlobalScorerRequest . Builder req = Scenic . GlobalScorerRequest . newBuilder () . setAudio ( audio ) . setPlugin ( plugin ) . setDomain ( domain ); Note that this example relies on code written in our Java Client API, but the general steps to perform the task are the same. Now we wrap the request in a ScenicMessage and Envelope like we did in the last example and send it across the request socket. Analysis requests as well as Enrollment requests take significant time to process on the server. It\u2019s likely you\u2019ll want your integration to be doing other things while it is waiting for the response, such as issuing further analysis requests. This is fully supported. However, you don\u2019t know when or in what order the responses to your analyze request will emerge from the server. Therefore, it\u2019s advantageous to track the message ids that you\u2019ve issued in a map of the form message_id -> request message , so that you know the request to which a newly received response pertains. Let\u2019s assume we\u2019ve received a GlobalScorerResult message and have deserialized it into a variable named res . We could process the result as follows: // Currently OLIVE (SCENIC) will only send back one score reply per score // request. Future releases may be able to send back multiple. // Because of this, we must iterate though all of the scores. List < Scenic . GlobalScore > scores = res . getScoreList (); for ( Scenic . GlobalScore gs : scores ){ system . out . println ( \"LID Score: class \" + gs . getClassId () \u201c = \u201c + gs . getScore ()); } For more details regarding the specifications and breadth of the possible requests, their replies, and the structure of each of these data objects, including how results are represented, please refer to the OLIVE API Message Reference documentation.","title":"An Analysis (Scoring) Message Example"},{"location":"apiBuildReferenceImp.html#request-a-new-olive-api-reference-implementation","text":"If there is a need for an OLIVE API in another language, SRI International would be an ideal candidate to undertake the task because of its rich experience having already done it in Java. However, it will still be a complex software engineering undertaking which will definitely take up a significant amount of project resources. For more information about what this would entail, please reach out to olive-support@sri.com.","title":"Request a new OLIVE API Reference Implementation"},{"location":"apiCode.html","text":"Integrating with the OLIVE Java API (JOlive) While the OLIVE Server allows direct client integration via Protobuf messages\u2014from a variety of languages such as Java, Python, C++ and C# and operating systems such as Windows, Linux, and MacOS\u2014a Java native API is also provided as a convenience. This page includes guidance for integrating a client application using the OLIVE Java API. The sections below cover establishing a connection to an OLIVE Server, requesting available plugins, various ways to build and submit scoring and enrollment requests, and model adaptation. All instructions and code examples presented in this section are for the OLIVE Java API. Fundamentally, all OLIVE API implementations are based on exchanging Protobuf messages between the client and OLIVE Server. These messages are defined in the API Message Reference Documentation. It's recommended that new integrators review the Enterprise API Primer for an overview of key concepts that also apply to the OLIVE Java API. Distribution The OLIVE Java API is distributed as a JAR file. It is located in the OLIVE delivery at the path api/java/repo/sri/speech/olive/api/olive-api/<version>/olive-api-<version>.jar . Dependencies The OLIVE Java API dependencies include: com.google.protobuf:protobuf-java:3.8.0 com.google.protobuf:protobuf-java-util:3.8.0 com.googlecode.json-simple:json-simple:1.1.1 org.json:json:20220320 org.zeromq:jeromq:0.5.2 org.slf4j:slf4j-api:1.7.30 ch.qos.logback:logback-core:1.2.3 ch.qos.logback:logback-classic:1.2.3 commons-lang:commons-lang:2.6 commons-io:commons-io:2.4 commons-cli:commons-cli:1.4 All dependencies are bundled with the OLIVE delivery in the directory api/java/repo . Using api/java/repo as a maven remote repository for offline support Using api/java/repo as maven remote repository lets integrators set up their project offline without pulling dependencies from Maven Central. Add the following to the project's pom.xml (replacing the stub local_path_to_jolive with the actual path): <repositories> <repository> <id> repo_id </id> <url> file:///local_path_to_jolive/repo </url> </repository> </repositories> Next, add the OLIVE Java API and it's dependencies to the project's pom.xml . <dependencies> ... <dependency> <groupId> com.sri.speech.olive.api </groupId> <artifactId> olive-api </artifactId> <version> api version provided </version> </dependency> ... </dependencies> For example, com.google.protobuf:protobuf-java:3.8.0 should be added as: <dependency> <groupId> com.google.protobuf </groupId> <artifactId> protobuf-java </artifactId> <version> 3.8.0 </version> </dependency> You should now be able to develop with the OLIVE Java API within your maven project. Integration Quickstart Here's a complete example for those in a hurry: import java.util.ArrayList ; import java.util.List ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.ClientUtils ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.utils.Pair ; public class Quickstart { private static Logger log = LoggerFactory . getLogger ( Quickstart . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; // Create a callback to handle LID results from the server private static final Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > rc = new Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > () { @Override public void call ( Server . Result < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > r ) { if ( ! r . hasError ()) { Olive . GlobalScore topScore = null ; for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { if ( topScore == null ) { topScore = gs ; } else { if ( gs . getScore () > topScore . getScore ()) { topScore = gs ; } } } log . info ( \"Top scoring: {} = {}\" , topScore . getClassId (), topScore . getScore ()); } else { log . info ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // exit if cannot connect if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); System . exit ( 1 ); } // find a Language ID (LID) plugin List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); Pair < Olive . Plugin , Olive . Domain > pd = null ; for ( Pair < Olive . Plugin , Olive . Domain > pair : pluginList ) { if ( \"LID\" . equalsIgnoreCase ( pair . getFirst (). getTask ())) { pd = pair ; break ; } } // run Language ID (LID) on serialized audio file if ( pd != null ) { ClientUtils . requestGlobalScore ( server , pd , Olive . TraitType . GLOBAL_SCORER , audioFileName , 1 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } else { log . error ( \"Could not retrieve a LID plugin!\" ); System . exit ( 1 ); } } } The code above accepts a filepath command line argument, connects to an OLIVE Server, programmatically locates a Language Identification (LID) plugin, submits a serialized audio file for LID analysis, and finally prints out the top scoring language. Establish Server Connection Before making any request, a client must establish a connection with the server. By default, the OLIVE server listens on ports 5588 (request port) and 5589 (status port) for client connection and status requests. These ports are configurable, but if the server has not been instructed to change its listening ports, the code below should establish a connection. A connection to the server can be established with a call to com.sri.speech.olive.api.Server#connect() , as shown below: Server server = new Server (); server . connect ( \"exampleClient\" , //client-id \"localhost\" , //address of server 5588 , //request-port 5589 , //status-port 10000 //timeout for failed connection request ); The request port (5588 by default) is used for request and response messages (Protobuf messages). Each request message sent to this port is guaranteed a response from the server. Messages in the API Message Reference are often suffixed with 'Request' and 'Result' to denote whether it's a request or result message. There is no need to poll the server for information on a submitted request, as the result/response for the a request is returned to the client as soon as it is available. The status port (5589 by default) is used by the Server to publish health and status messages (Heartbeat) to client(s). Clients can not send requests on this port. TLS Encrypted Connections As of 5.6.0, the OLIVE Java API supports TLS encrypted communication with the OLIVE Server. Encrypted connections require the OLIVE Server to be configured and started with TLS. For example, OLIVE Martini must be started with the flag --tls_server_only or --tls_server_and_client . By default, OLIVE listens on port 5588 for encrypted connections. To set up an encrypted connection using the OLIVE Java API, connect using the com.sri.speech.olive.api.Server#secureConnect() method and provide the required PKCS12 ( *.p12 ) certificate arguments: Server server = new Server (); server . secureConnect ( \"exampleClient\" , //client-id \"localhost\" , //address of server 5588 , //secure port \"path/to/certificate.p12\" , //certificate path \"certificatePassword\" , //certificate password \"path/to/cert/authority.p12\" //certificate authority path \"authorityPassword\" , //certificate authority password 10000 //timeout for failed connection request ); Connecting to the OLIVE Server using TLS encryption takes slightly longer than a standard non-encrypted connection; however, after the initial connection is made, all subsequent requests will run over TLS with no time penalty. Request Available Plugins In order to submit most server requests, the client must specify the plugin and domain pair to use for the request (i.e. Pair<Olive.Plugin, Olive.Domain> ). The function requestPlugins() provided by the ClientUtils class can be used to get all the plugin/domain pairs available: // ask the server for a list of currently available plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); The pairs can be iterated through looking for specific criteria of interest. // iterate through and find a plugin and domain to use for SAD Pair < Olive . Plugin , Olive . Domain > pd = null ; for ( Pair < Olive . Plugin , Olive . Domain > pair : pluginList ) { if ( \"SAD\" . equals ( pair . getFirst (). getTask ())) { pd = pair ; break ; } } if ( pd != null ) { log . info ( \"{}/{} supports SAD!\" , pd . getFirst (). getId (), pd . getSecond (). getLabel ()); } else { log . info ( \"No SAD plugin found!\" ); } Alternatively, if the client already knows the desired plugin ID and domain name, the pair reference can be obtained using the findPluginDomain() provided by the ClientUtils class. String pluginName = \"sad-dnn-v8.0.0\" ; String domainName = \"multi-v1\" ; // Look up a specific plugin by ID and domain pd = ClientUtils . findPluginDomain ( pluginName , domainName , pluginList ); if ( pd != null ) { log . info ( \"{}/{} was found!\" , pd . getFirst (). getId (), pd . getSecond (). getLabel ()); } else { log . info ( \"{}/{} was NOT found!\" , pluginName , domainName ); } Audio Submission Guidelines One of the core client activities is submitting Audio with a request. In the OLIVE Java API, three ways are provided for a client to send audio data to the OLIVE server: file path buffer of raw audio sample data serialized file buffer object The enum ClientUtils.AudioTransferType is used to specify which of the three transfer mechanisms to use. In almost every case, the OLIVE Java API handles packaging up audio appropriately based on the passed in AudioTransferType so the client only needs to understand a few basic related to the transfer type, and they are explained below. When the client and the OLIVE server share the same file system, the easiest way for the client to send audio data to the server is by specifying the audio's file path on disk. The OLIVE Java API provides the utility below to package audio files which are accessible to the server locally: AudioTransferType . SEND_AS_PATH When the client and the server don't share the same file system, as in the case of a client making a remote connection to the OLIVE server, it is necessary to send the client's local audio files as a file buffer. To help package the client's audio data in a raw buffer, the OLIVE Java API provides the utility below: AudioTransferType . SEND_SAMPLES_BUFFER When the client wants to ensure that all audio header information is provided intact to the server, the OLIVE Java API provides the utility below: AudioTransferType . SEND_SERIALIZED_BUFFER Note on 'serialized buffer' usage. This transfer mechanism passes the original file to the server in its entirety in one contiguous buffer, leaving the audio file header intact. This allows the server to properly decode and process the audio once its received, since it can directly access the bit depth, encoding type, sample rate and other necessary information from the header itself. The tradeoff with serialized files is that there may be additional overhead needed to process the audio into a consumable form. If the client and server reside on the same hardware and file system, it is advisable to simply pass filepaths when possible. This saves the memory overhead burden of both the client and server loading audio into memory. If using common audio types, like 16-bit PCM .wav files, it may also be possible to simply pass a non-serialized file buffer. Synchronous vs. Asynchronous Message Submission The OLIVE Java API allows the client to choose between processing a task request synchronously or asynchronously. Processing a task request synchronously means the client will block and wait for the task result to return before proceeding to other task requests. On the other hand, asynchronous processing means the client will not wait for the result to come back before moving on, allowing several jobs to be submitted in parallel. The examples below generally show submitting requests asynchronously. The argument async is used in many ClientUtils functions ( requestFrameScore() , requestGlobalScore() , requestRegionScores() , requestEnrollClass() , etc.), and can be used to choose whether the client intends to wait for the task result to return or not. When async is set to true , the client will not block when a request is sent to the server, so other task requests can be made before the results are received asynchronously and handled by the callback. OLIVE Java API Code Samples The OLIVE Java API includes functionality to accommodate many of the available request messages. Utilities such as requestFrameScore() , requestRegionScores() , requestEnrollClass() , etc. not only do the packaging of request messages, but also take care of sending the request messages to the server, all in one call. They are available from the class com.sri.speech.olive.api.utils.ClientUtils . The required parameters to send many of these requests include: server handle (server - see here ) the plugin handle (pd - see here ) the name of the audio file to submit to the server (filename) channel number of the audio to be processed when audio has more than 1 channel (channelNumber) a callback function for handling results returned either asynchronously or synchronously (rc) whether the client will block for task result to return (async - see here ) an enum of how to submit audio to the server (transferType) optional lists of annotations of the submitted audio (regions) optional list of parameters for customizing plugin behavior (options) optional list of class IDs for filtering the results (classIDs) Performing an enrollment request adds an additional parameter: the ID of the class to be enrolled The primary requests covered below are: Frame score requests - used to make some SAD requests (note that some SAD plugins return regions, not frame scores - most are capable of performing both) Global score requests - used to make LID, SID, or GID score requests Region score requests - used to make ASR, SDD, LDD, QBE, and often SAD score requests Enrollment requests - used to enroll speakers or other class types for plugins that support the ClassEnroller trait Frame Score Request The example below provides sample code for a function handleMySADFrameScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Frame scoring request to the connected server. public static boolean handleMySADFrameScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > rc = new Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > () { @Override public void call ( Server . Result < Olive . FrameScorerRequest , Olive . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Olive . FrameScores fs : r . getRep (). getResultList ()) { log . info ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { if ( scores [ i ] > 0.0 ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); log . info ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pd , filename , 1 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } This code passes the audio to the server using a serialized buffer. It is also possible to perform this request using buffered audio samples or a file path. Only a plugin that support the FrameScorer trait can handle this request. All SAD plugins support FrameScorer, while some also SAD plugins also support the RegionScorer trait. The method signature for ClientUtils.requestFrameScore is: public static boolean requestFrameScore ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename , int channelNumber , Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) throws ClientException , IOException , UnsupportedAudioFileException For a complete example of how to call this code with a specific plugin, refer to the SAD Scoring Request code example below. It contains the full Java code file this example was pulled from, showing the process of establishing a server connection, polling the server for available plugins to retrieve the appropriate plugin/domain handle, and making the request. Global Score Request The example below provides sample code for a function handleMyLIDGlobalScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Global scoring request to the connected server. public static boolean handleMyLIDGlobalScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle LID results from the server Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > rc = new Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > () { @Override public void call ( Server . Result < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > r ) { // output LID global scores if ( ! r . hasError ()) { log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { log . info ( String . format ( \"%s = %f\" , gs . getClassId (), gs . getScore ())); } } else { log . info ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; return ClientUtils . requestGlobalScore ( server , pd , Olive . TraitType . GLOBAL_SCORER , filename , 1 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } This code passes the audio to the server using a serialized buffer. It is also possible to perform this request using buffered audio samples or a file path. The code required to submit a GlobalScorerRequest message doesn't care what type of plugin is going to be doing the scoring, as long as the plugin implements the GlobalScorer Trait. This means that the exact same code can be used for submitting audio to global scoring LID plugins, SID plugins, or any other global scoring plugin. The method signature for ClientUtils.requestGlobalScore is: public static boolean requestGlobalScore ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , Olive . TraitType trait , String filename , int channelNumber , Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) throws ClientException , IOException Region Score Request The example below provides sample code for a function handleMyRegionScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Region scoring request to the connected server. public static boolean handleMyRegionScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle results from the server Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > rc = new Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > () { @Override public void call ( Server . Result < Olive . RegionScorerRequest , Olive . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Olive . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"{} ({}-{}secs, score={})\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; return ClientUtils . requestRegionScores ( server , pd , filename , 0 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } This code passes the audio to the server using a serialized buffer. It is also possible to perform this request using file path or a serialized file. The method signature for ClientUtils.requestRegionScore is: public static boolean requestRegionScores ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) throws ClientException , IOException For a complete example of how to call this code with a specific plugin, refer to the ASR Scoring Request code example below. It contains the full Java code file this example was pulled from, showing the process of establishing a server connection, polling the server to retrieve the appropriate plugin/domain handle, and making the request. Enrollment Request The example below provides sample code for a function handleMyEnrollmentRequest that takes a server connection, a plugin/domain handle, the name of the class (speaker) to enroll, and a path to an audio file as arguments, and uses this information to build and submit a Region Scoring request to the connected server. public static boolean handleMyEnrollmentRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String speakerName , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // create a callback that handles the enrollment result from the server Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > () { @Override public void call ( Server . Result < Olive . ClassModificationRequest , Olive . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()) { log . info ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make it a synchronized call, so we know the speaker is enrolled before we boolean enrolled = ClientUtils . requestEnrollClass ( server , pd , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , ClientUtils . DataType . AUDIO_DATA , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); return enrolled ; } The method signature for ClientUtils.requestEnrollClass is: public static boolean requestEnrollClass ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String id , String wavePath , int channelNumber , Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > rc , boolean async , DataType dataType , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options ) throws ClientException Vectorization and Stateless Score Request See the appropriate section of the OLIVE Plugin Overview documentation for a description of Vectorization and Stateless Scoring. Vectorization The example below provides sample code for a function generateAudioVectors that creates audio vectors that can be used in future stateless scoring reequests. public static boolean generateAudioVectors ( Server server , Pair < Olive . Plugin , Olive . Domain > pp , String speakerName , String enrollmentFileName , List < AudioVector > audioVectors ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // create a callback that handles the enrollment result from the server Server . ResultCallback < Olive . PluginAudioVectorRequest , Olive . PluginAudioVectorResult > enrollmentCallback = new Server . ResultCallback <> () { @Override public void call ( Server . Result < Olive . PluginAudioVectorRequest , Olive . PluginAudioVectorResult > r ) { // examine enrollment result if ( ! r . hasError ()) { for ( VectorResult vr : r . getRep (). getVectorResultList ()) { if ( vr . getSuccessful ()) { audioVectors . add ( vr . getAudioVector ()); } else { log . error ( \"Vectorization failed: {}\" , vr . getMessage ()); } } } else { log . error ( \"Vectorization request failed: {}\" , r . getError ()); } } }; // make it a synchronized call so we can collect the vector before trying to score boolean enrolled = ClientUtils . requestAudioVector ( server , pp , enrollmentFileName , 0 , enrollmentCallback , false , AudioTransferType . SEND_SERIALIZED_BUFFER , Collections . emptyList ()); return enrolled ; } The method signature for ClientUtils.requestAudioVector is: public static boolean requestAudioVector ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Olive . PluginAudioVectorRequest , Olive . PluginAudioVectorResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions ) throws IOException , UnsupportedAudioFileException { Serialization and Deserialization The code below provides an example of how to serialize and deserialize an audio vector: Path audioVectorSaveDirPath = Files . createTempDirectory ( speakerName ); audioVectorSaveDirPath . toFile (). deleteOnExit (); // serialize and save audio vector(s) to disk (for demo purposes) for ( AudioVector audioVector : generatedAudioVectors ) { byte [] serializedAudioVector = MessageUtils . serializeMessage ( audioVector ); Path audioVectorSavePath = audioVectorSaveDirPath . resolve ( UUID . randomUUID (). toString ()); try ( FileOutputStream fos = new FileOutputStream ( audioVectorSavePath . toFile ())) { fos . write ( serializedAudioVector ); } } // reload audio vector(s) from disk (for demo purposes) List < AudioVector > reloadedAudioVectors = new ArrayList <> (); try ( Stream < Path > pathStream = Files . walk ( audioVectorSaveDirPath )) { List < Path > audioVectorPaths = pathStream . filter ( Files :: isRegularFile ). collect ( Collectors . toList ()); for ( Path audioVectorPath : audioVectorPaths ) { try ( FileInputStream fis = new FileInputStream ( audioVectorPath . toString ())) { AudioVector audioVector = MessageUtils . deserializeMessage ( fis . readAllBytes (), AudioVector . parser ()); reloadedAudioVectors . add ( audioVector ); } } } Warning Saving to disk is FOR DEMO PURPOSES ONLY . A production system should consider using SQLite or similar technology which allows associating a speaker label String to binary data blobs rather than storing on disk if they require long-term persistent storage. The examples are to illustrate serialization and deserialization of audio vectors. The method signature for MessageUtils.serializeMessage is: public static byte [] serializeMessage ( Message message ) The method signature for MessageUtils.deserializeMessage is: public static < T extends Message > T deserializeMessage ( byte [] bytes , Parser < T > parser ) throws IOException Stateless Score Request The example below provides sample code for a function scoreUsingAudioVectors that sends a Stateless Score Request using the in-memory audio vectors. public static void scoreUsingAudioVectors ( Server server , Pair < Olive . Plugin , Olive . Domain > pp , String speakerName , String scoreWaveFileName , List < AudioVector > audioVectors ) throws ClientException , IOException , UnsupportedAudioFileException { // Create a call back to handle the stateless scoring request Server . ResultCallback < Olive . StatelessGlobalScorerRequest , Olive . GlobalScorerResult > scoreCallback = new Server . ResultCallback <> () { @Override public void call ( Server . Result < Olive . StatelessGlobalScorerRequest , Olive . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { log . info ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { log . error ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; List < ClassVector > classVectors = new ArrayList <> ( List . of ( ClientUtils . createClassVector ( speakerName , audioVectors ))); ClientUtils . requestGlobalScoreStateless ( server , pp , scoreWaveFileName , 0 , scoreCallback , false , AudioTransferType . SEND_SERIALIZED_BUFFER , Collections . emptyList (), Collections . emptyList (), classVectors ); } The method signature for ClientUtils.requestGlobalScoreStateless is: public static boolean requestGlobalScoreStateless ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Olive . StatelessGlobalScorerRequest , Olive . GlobalScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < ClassVector > classVectors ) throws ClientException , IOException The method signature for ClientUtils.requestRegionScoreStateless is: public static boolean requestRegionScoresStateless ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Olive . StatelessRegionScorerRequest , Olive . RegionScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < ClassVector > classVectors ) throws ClientException , IOException Plugin Specific Code Examples This section shows examples of using the functions just outlined to make calls to specific plugins, and demonstrate how the same code can be reused for several purposes - for example, requestGlobalScore is valid to request scoring from both SID and LID plugins. SAD Scoring Example SID Enrollment and Scoring Example LID Enrollment and Scoring Example ASR Scoring Example TMT Scoring Example SAD Adaptation Example SAD Scoring Request This shows a full implementation of a client program which sends a frame scoring request to a SAD plugin. Upon return of the result, it outputs the received frame scores. Included also is a second version where a threshold is used to filter out frame scores which are higher. import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.client.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.util.* ; public class MySADFrameScorer { private static Logger log = LoggerFactory . getLogger ( MySADFrameScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"sad-dnn-v8.0.0\" ; public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // from the list of plugins, find the targeted plugin for the task Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Olive . TraitType . FRAME_SCORER , pluginList ); // formulate SAD frame scoring request and send to server handleMySADFrameScorerRequest ( server , pd , audioFileName ); } public static boolean handleMySADFrameScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > rc = new Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > () { @Override public void call ( Server . Result < Olive . FrameScorerRequest , Olive . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Olive . FrameScores fs : r . getRep (). getResultList ()) { log . info ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); log . info ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pd , filename , 1 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } } The code above outputs all frame scores of the input audio, this can generate massive amount of output, especially when the audio is long. One good way to trim down the output is to filter out regions with frame scores higher than a preset threshold value. The following shows how this can be done using a threshold of 0.0. ... for ( int i = 0 ; i < scores . length ; i ++ ) { if ( scores [ i ] > 0.0 ) { // only print with score 0.0 or greater!!! int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); log . info ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } ... SID Enrollment and Scoring Request This example is a full implementation of a client program which sends an enrollment request to a SID plugin, followed by a scoring request to the same SID plugin. import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.utils.parser.RegionParser ; import com.sri.speech.olive.api.client.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.util.* ; public class MySIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"sid-dplda-v3.0.0\" ; private static String speakerName = \"EDMUND_YAO\" ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // obtain SID plugin handle Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Olive . TraitType . CLASS_ENROLLER , pluginList ); // perform SID enrollment task boolean enrolled = handleMyEnrollmentRequest ( server , pd , speakerName , enrollmentFileName ); if ( enrolled ) { // make a region score request handleMyRegionScoreRequest ( server , pd , speakerName , scoreWaveFileName ); } } public static boolean handleMyEnrollmentRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String speakerName , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // create a callback that handles the enrollment result from the server Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > () { @Override public void call ( Server . Result < Olive . ClassModificationRequest , Olive . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()) { log . info ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make it a synchronized call, so we know the speaker is enrolled before we boolean enrolled = ClientUtils . requestEnrollClass ( server , pd , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , ClientUtils . DataType . AUDIO_DATA , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); return enrolled ; } public static void handleMyRegionScoreRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String speakerName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { // Create a call back to handle the SID scoring request Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > () { @Override public void call ( Server . Result < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { log . info ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { log . error ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // SID is a global scorer, so make a global score reqeust: ClientUtils . requestGlobalScore ( server , pd , Olive . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , AudioTransferType . SEND_SERIALIZED_BUFFER , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } } LID Enrollment and Scoring Request This example is a full implementation of a client program which sends an enrollment request to a LID plugin, followed by a scoring request to the same LID plugin. import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.client.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.util.* ; public class MyLIDEnrollmentAndScore { private static final Logger log = LoggerFactory . getLogger ( MyLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v4.0.0\" ; private static String languageName = \"Esperanto\" ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // obtain LID plugin handle Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Olive . TraitType . CLASS_ENROLLER , pluginList ); // perform LID enrollment task boolean enrolled = handleMyEnrollmentRequest ( server , pd , languageName , enrollmentFileName ); if ( enrolled ) { // make a region score request handleMyRegionScoreRequest ( server , pd , languageName , scoreWaveFileName ); } } public static boolean handleMyEnrollmentRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String languageName , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // create a callback that handles the enrollment result from the server Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > () { @Override public void call ( Server . Result < Olive . ClassModificationRequest , Olive . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()) { log . info ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make it a synchronized call, so we know the language is enrolled before we boolean enrolled = ClientUtils . requestEnrollClass ( server , pd , languageName , enrollmentFileName , 0 , enrollmentCallback , false , ClientUtils . DataType . AUDIO_DATA , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); return enrolled ; } public static void handleMyRegionScoreRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String languageName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { // Create a call back to handle the LID scoring request Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > () { @Override public void call ( Server . Result < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { log . info ( String . format ( \"language{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { log . error ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: ClientUtils . requestGlobalScore ( server , pd , Olive . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } } ASR Scoring Request The following example shows a full implementation of a ASR scoring request. It sends a RegionScorerRequest and receives a RegionScorerResult . import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.client.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.util.* ; public class MyASRRegionScorer { private static Logger log = LoggerFactory . getLogger ( MyASRRegionScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"english-tdnnLookaheadRnnlm-tel-v2\" ; private static String pluginName = \"asr-dynapy-v4.1.0\" ; public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // formulate the frame scoring task request Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"ASR\" , Olive . TraitType . REGION_SCORER , pluginList ); // Perform ASR frame scoring task handleMyRegionScorerRequest ( server , pd , audioFileName ); } public static boolean handleMyRegionScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle results from the server Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > rc = new Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > () { @Override public void call ( Server . Result < Olive . RegionScorerRequest , Olive . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Olive . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"{} ({}-{}secs, score={})\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; return ClientUtils . requestRegionScores ( server , pd , filename , 0 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } } TMT Request The following code shows a full implementation of a text machine translation request made to a TMT plugin. The request in this client is a TextTransformationRequest . import java.util.List ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.client.ClientException ; import com.sri.speech.olive.api.utils.ClientUtils ; import com.sri.speech.olive.api.utils.Pair ; public class MyTMTRequest { private static final Logger log = LoggerFactory . getLogger ( MyTMTRequest . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String pluginName = \"tmt-neural-v1.1.1\" ; private static String domainName = \"cmn-eng-nmt-v1\" ; public static void main ( String [] args ) throws ClientException { // input text is passed as an argument String inputText = args [ 0 ] ; // establish server connection Server server = new Server (); server . connect ( \"exampleClient\" , //client-id DEFAULT_SERVERNAME , //address of server DEFAULT_PORT , //request-port DEFAULT_PORT + 1 , //status-port TIMEOUT //timeout for failed connection request ); List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); Pair < Olive . Plugin , Olive . Domain > pluginDomainPair = ClientUtils . findPluginDomain ( pluginName , domainName , pluginList ); // build the request Olive . TextTransformationRequest . Builder req = Olive . TextTransformationRequest . newBuilder () . setText ( inputText ) . setPlugin ( pluginDomainPair . getFirst (). getId ()) . setDomain ( pluginDomainPair . getSecond (). getId ()); // submit request log . info ( String . format ( \"Submitting %s for translation with plugin %s and domain %s\" , inputText , pluginDomainPair . getFirst (). getId (), pluginDomainPair . getSecond (). getId ())); Server . Result < Olive . TextTransformationRequest , Olive . TextTransformationResult > result = server . synchRequest ( req . build ()); // handle response if ( ! result . hasError ()) { for ( Olive . TextTransformation transformation : result . getRep (). getTransformationList ()) { log . info ( transformation . getTransformedText ()); } } else { log . error ( String . format ( \"Translation error\" , result . getError ())); } // disconnect server . disconnect (); } } SAD Adaptation Request Below is a full implementation of a SAD request to adapt a new domain from an existing domain. The list of adaptation training files is passed into the client as a file. The code handles both supervised (speech regions specified) and unsupervised (speech regions not specified) SAD adaptations. However, some SAD plugins may not have the unsupervised adaptation capability, in which case the client will exit with a failure message. import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.Olive.AnnotationRegion ; import com.sri.speech.olive.api.Olive.AudioAnnotation ; import com.sri.speech.olive.api.Olive.Domain ; import com.sri.speech.olive.api.Olive.Plugin ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.parser.LearningParser ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import java.util.* ; public class MySADAdaptation { private static Logger log = LoggerFactory . getLogger ( MySADAdaptation . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"sad-dnn-v8.0.0\" ; private static String newDomainName = \"custom-v1\" ; private static LearningParser learningParser = new LearningParser (); public static void main ( String [] args ) throws Exception { // audio file list is passed as an argument String audioFileList = args [ 0 ] ; // parse files in list learningParser . parse ( audioFileList ); if ( ! learningParser . isValid ()) { log . error ( \"Invalid input file: \" + audioFileList ); System . exit ( - 1 ); } // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // from the list of plugins, find the targeted plugin for the task Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , learningParser . isUnsupervised () ? Olive . TraitType . UNSUPERVISED_ADAPTER : Olive . TraitType . SUPERVISED_ADAPTER , pluginList ); // Preproces audio - doesn't matter if supervised or unsupervised String adaptID = UUID . randomUUID (). toString (); Plugin plugin = pd . getFirst (); Domain domain = pd . getSecond (); // optional annotations, generated if found in the parser (supervised // adaptation) Map < String , List < AudioAnnotation >> annotations = new HashMap <> (); int numPreprocessed = 0 ; for ( String filename : learningParser . getFilenames ()) { try { // build the audio Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( filename , - 1 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); // build up request metadata String id = null ; Collection < String > classIDs = learningParser . getAnnotations ( filename ). keySet (); if ( classIDs . size () > 0 ) { id = \"supervised\" ; } // Prepare the request Olive . PreprocessAudioAdaptRequest . Builder req = Olive . PreprocessAudioAdaptRequest . newBuilder () . setPlugin ( plugin . getId ()) . setDomain ( domain . getId ()) . setAdaptSpace ( adaptID ) // We don't set the optional start/end regions... those are used later when // we finalize . setAudio ( audio . build ()); if ( id != null ) { req . setClassId ( id ); } // send the request Server . Result < Olive . PreprocessAudioAdaptRequest , Olive . PreprocessAudioAdaptResult > result = server . synchRequest ( req . build ()); if ( result . hasError ()) { log . error ( String . format ( \"Error preprocessing audio %s because: %s\" , filename , result . getError ())); } else { numPreprocessed += 1 ; log . info ( String . format ( \"Audio file %s successfully preprocessed\" , filename )); String audioId = result . getRep (). getAudioId (); // Set the audio ID for any classID(s) associated with this audio for ( String classIDName : learningParser . getAnnotations ( filename ). keySet ()) { // Add the class/audio id mapping , and optionally add annotation regions List < AudioAnnotation > audioAnnots ; if ( annotations . containsKey ( classIDName )) { audioAnnots = annotations . get ( classIDName ); } else { audioAnnots = new ArrayList <> (); annotations . put ( classIDName , audioAnnots ); } Olive . AudioAnnotation . Builder aaBuilder = Olive . AudioAnnotation . newBuilder () . setAudioId ( audioId ); for ( RegionWord word : learningParser . getAnnotations ( filename ). get ( classIDName )) { AnnotationRegion . Builder ab = AnnotationRegion . newBuilder () . setStartT ( word . getStartTimeSeconds ()). setEndT ( word . getEndTimeSeconds ()); aaBuilder . addRegions ( ab . build ()); } audioAnnots . add ( aaBuilder . build ()); } } } catch ( Exception /* | UnsupportedAudioFileException */ e ) { log . error ( \"Unable to preprocess file: \" + filename ); log . debug ( \"File preprocess error: \" , e ); } } // perform adaptation if ( learningParser . isUnsupervised ()) { if ( numPreprocessed > 0 ) { // Prepare the request Olive . UnsupervisedAdaptationRequest . Builder req = Olive . UnsupervisedAdaptationRequest . newBuilder () . setPlugin ( plugin . getId ()) . setDomain ( domain . getId ()) . setAdaptSpace ( adaptID ) . setNewDomain ( newDomainName ); // Now send the finalize request Server . Result < Olive . UnsupervisedAdaptationRequest , Olive . UnsupervisedAdaptationResult > result = server . synchRequest ( req . build ()); if ( result . hasError ()) { log . error ( String . format ( \"Unsupervised adaptation failed for new domain '%s' because: %s\" , newDomainName , result . getError ())); } else { log . info ( String . format ( \"New Domain '%s' Adapted\" , newDomainName )); } } else { log . error ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } else { // supervised adaptation if ( numPreprocessed > 0 ) { List < Olive . ClassAnnotation > classAnnotations = new ArrayList <> (); for ( String id : annotations . keySet ()) { Olive . ClassAnnotation . Builder caBuilder = Olive . ClassAnnotation . newBuilder (). setClassId ( id ) . addAllAnnotations ( annotations . get ( id )); classAnnotations . add ( caBuilder . build ()); } // Prepare the request Olive . SupervisedAdaptationRequest . Builder req = Olive . SupervisedAdaptationRequest . newBuilder () . setPlugin ( plugin . getId ()) . setDomain ( domain . getId ()) . setAdaptSpace ( adaptID ) . setNewDomain ( newDomainName ) . addAllClassAnnotations ( classAnnotations ); // Now send the finalize request Server . Result < Olive . SupervisedAdaptationRequest , Olive . SupervisedAdaptationResult > result = server . synchRequest ( req . build ()); if ( result . hasError ()) { log . error ( String . format ( \"Failed to adapt new Domain '%s' because: %s\" , newDomainName , result . getError ())); } else { log . info ( String . format ( \"New Domain '%s' Adapted\" , newDomainName )); } } else { log . error ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } log . info ( \"\" ); log . info ( \"Learning finished. Exiting...\" ); System . exit ( 0 ); } } Workflow Integration In addition to the basic API integration mechanisms described above, JOlive also supports OLIVE Workflows. See Workflows for a primer on OLIVE Workflows and then continue reading below for JOlive specific details. Quickstart Here's a complete workflow example for those in a hurry: import java.util.ArrayList ; import java.util.Collections ; import java.util.List ; import java.util.Map ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.Workflow.WorkflowAnalysisRequest ; import com.sri.speech.olive.api.Workflow.WorkflowAnalysisResult ; import com.sri.speech.olive.api.Workflow.WorkflowDataRequest ; import com.sri.speech.olive.api.utils.ClientUtils ; import com.sri.speech.olive.api.utils.Pair ; import com.sri.speech.olive.api.workflow.ActivatedWorkflow ; import com.sri.speech.olive.api.workflow.OliveWorkflowDefinition ; import com.sri.speech.olive.api.workflow.WorkflowUtils ; import com.sri.speech.olive.api.workflow.wrapper.JobResult ; import com.sri.speech.olive.api.workflow.wrapper.TaskResult ; public class WorkflowQuickstart { private static Logger log = LoggerFactory . getLogger ( WorkflowQuickstart . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; // Create a callback to handle workflow results from the server private static final Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > wc = new Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > () { @Override public void call ( Server . Result < WorkflowAnalysisRequest , WorkflowAnalysisResult > r ) { if ( ! r . hasError ()) { Map < String , JobResult > jobResults = WorkflowUtils . extractWorkflowAnalysis ( r . getRep ()); for ( String jobName : jobResults . keySet ()) { JobResult jr = jobResults . get ( jobName ); if ( ! jr . isError ()) { for ( String taskName : jr . getTasks (). keySet ()) { log . info ( \"--{} Results --\" , taskName ); List < TaskResult > trs = jr . getTasks (). get ( taskName ); for ( TaskResult tr : trs ) { if ( ! tr . isError ()) { if ( tr . getTraitType () == Olive . TraitType . REGION_SCORER ) { Olive . RegionScorerResult rsr = ( Olive . RegionScorerResult ) tr . getTaskMessage (); for ( Olive . RegionScore rs : rsr . getRegionList ()) { log . info ( \"{} ({}-{}secs, score={})\" , rs . getClassId (), rs . getStartT (), rs . getEndT (), rs . getScore ()); } } else if ( tr . getTraitType () == Olive . TraitType . GLOBAL_SCORER ) { Olive . GlobalScorerResult gsr = ( Olive . GlobalScorerResult ) tr . getTaskMessage (); for ( Olive . GlobalScore gs : gsr . getScoreList ()) { log . info ( \"{} = {}\" , gs . getClassId (), gs . getScore ()); } } else if ( tr . getTraitType () == Olive . TraitType . FRAME_SCORER ) { Olive . FrameScorerResult fsr = ( Olive . FrameScorerResult ) tr . getTaskMessage (); for ( Olive . FrameScores fs : fsr . getResultList ()) { for ( int i = 0 ; i < fs . getScoreCount (); i ++ ) { log . info ( \"frame#{} {}={}\" , i , fs . getClassId (), fs . getScore ( i )); } } } else if ( tr . getTraitType () == Olive . TraitType . TEXT_TRANSFORMER ) { Olive . TextTransformationResult ttr = ( Olive . TextTransformationResult ) tr . getTaskMessage (); for ( Olive . TextTransformation transformation : ttr . getTransformationList ()) { System . out . println ( transformation . getTransformedText ()); } } else if ( tr . getTraitType () == Olive . TraitType . CLASS_MODIFIER ) { Olive . ClassModificationResult cmr = ( Olive . ClassModificationResult ) tr . getTaskMessage (); System . out . println ( cmr ); } } else { log . error ( \"Workflow task {} failed: {}\" , taskName , tr . getErrMsg ()); } } } } else { log . error ( \"Workflow job failed: {}\" , jr . getErrMsg ()); } } } else { log . error ( \"Workflow request failed: {}\" , r . getError ()); } System . exit ( 0 ); } }; public static void main ( String [] args ) throws Exception { String audioPath = args [ 0 ] ; String workflowPath = args [ 1 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleWorkflowClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // exit if cannot connect if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); System . exit ( 1 ); } // load the workflow definition to get an activated workflow OliveWorkflowDefinition workflowDefinition = new OliveWorkflowDefinition ( workflowPath ); ActivatedWorkflow workflow = workflowDefinition . createWorkflow ( server ); Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio = workflow . packageAudio ( audio . build (), \"audio\" ); workflow . analyze ( Collections . singletonList ( packagedAudio ), wc , new ArrayList < Pair < String , String >> ()); } } Initializing a Workflow As described in Workflows , workflow logic is encapsulated in a Workflow Definition file distributed as either binary (i.e. *.workflow - deprecated) or JSON (i.e. *.workflow.json ). Workflows are preconfigured to perform tasks such as Speech Activity Detection (SAD), Language Identification (LID), Speaker Identification (SID), etc. with a single call to the OLIVE server. These Workflow Definition files must be initialized (aka 'created') with the JOlive server before the workflow can be used. The snippet below initializes a workflow with the server : OliveWorkflowDefinition workflowDefinition = new OliveWorkflowDefinition ( workflowPath ); ActivatedWorkflow workflow = workflowDefinition . createWorkflow ( server ); A workflow 'helper' object ( ActivatedWorkflow ) is returned and is used to submit audio files directly to that workflow for analysis, enrollment, or unenrollment. In the snippet above, workflow is the 'helper'. Packaging Audio The same Audio Submission Guidelines discussed earlier in this guide apply to workflows and each of the following are supported for workflows: AudioTransferType.SEND_AS_PATH AudioTransferType.SEND_SAMPLES_BUFFER AudioTransferType.SEND_SERIALIZED_BUFFER The difference with workflows is that the workflow 'helper' is used to package audios rather than JOlive server directly; the audio is wrapped in a WorkflowDataRequest that is submitted to OLIVE for processing: // package audio as a serialized buffer using the workflow 'helper' Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio = workflow . packageAudio ( audio . build (), audioLabel ); Multi-channel Audio The default workflow behavior is to merge multi-channel audio into a single channel, which is known as MONO mode. To perform analysis on each channel individually instead of a merged channel, the Workflow Definition must be authored with a mode of SPLIT . When using the split mode, each channel in a multi-channel audio input is \"split\" into a job. Here is a mode within a workflow definition file that merges multi-channel audio into a single channel audio input: \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, ... and one that handles each channel individually: \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"SPLIT\" }, Audio Annotations The audio submitted for analysis (or enrollment) can be annotated with start/end regions when packaging audio using the createAudioFromFile() function. The snippet below specifies two regions within a file: // Provide annotations for two regions: 1.0 to 3.0 seconds, and 6.0 to 10.0 seconds in audio List < RegionWord > regions = new ArrayList < RegionWord > (); regions . add ( new RegionWord ( 1000 , 3000 )); regions . add ( new RegionWord ( 6000 , 10000 )); Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , regions ); Submitting Audio Submitting audio for analysis, enrollment, and unenrollment using workflows is supported. Analysis The snippet below packages and submits an audio file to the workflow 'helper' for analysis: // package audio as a serialized buffer Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , regions ); WorkflowDataRequest packagedAudio = workflow . packageAudio ( audio . build (), audioLabel ); // submit workflow request on serialized audio file workflow . analyze ( Collections . singletonList ( packagedAudio ), wc , new ArrayList < Pair < String , String >> ()); Batch request The analyze function accepts a List of audio files to so multiple files can be analyzed as a complete 'batch' request: // package audio as a serialized buffer Olive . Audio . Builder audio1 = ClientUtils . createAudioFromFile ( audioPath1 , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio1 = workflow . packageAudio ( audio1 . build (), audioLabel1 ); Olive . Audio . Builder audio2 = ClientUtils . createAudioFromFile ( audioPath2 , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio2 = workflow . packageAudio ( audio2 . build (), audioLabel2 ); List < WorkflowDataRequest > list = new ArrayList <> (); list . add ( packagedAudio1 ); list . add ( packagedAudio2 ); // submit workflow request on multiple serialized audio files workflow . analyze ( list , wc , new ArrayList < Pair < String , String >> ()); Enrollments Enroll Some workflows support enrollment for one or more jobs. To list the jobs in a workflow that support enrollment, use the getEnrollmentJobNames() function: System . out . println ( \"Enrollment Jobs: \" + sidEnrollWorkflow . getEnrollmentJobNames ()); // Enrollment Jobs: [SID Enrollment] To enroll a speaker via this workflow, use the workflow 'helper's enroll function: audioLabel = \"EDMUND_YEO\" ; // find a Speaker Identification (SID) enrollment job name String sidEnrollmentJobName = \"\" ; for ( String jobName : sidEnrollWorkflow . getEnrollmentJobNames ()){ if ( jobName . contains ( \"SID\" )){ sidEnrollmentJobName = jobName ; break ; } } Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio = sidEnrollWorkflow . packageAudio ( audio . build (), audioLabel ); sidEnrollWorkflow . enroll ( Collections . singletonList ( packagedAudio ), audioLabel , Collections . singletonList ( sidEnrollmentJobName ), wc , new ArrayList < Pair < String , String >> ()); Note also that not all workflows support enrollment. Please check that the workflow being used supports this before submitting an enrollment request. To confirm the new speaker name was added: private static final Server . ResultCallback < WorkflowClassStatusRequest , WorkflowClassStatusResult > cc = new Server . ResultCallback < WorkflowClassStatusRequest , WorkflowClassStatusResult > (){ @Override public void call ( Server . Result < WorkflowClassStatusRequest , WorkflowClassStatusResult > r ){ if ( ! r . hasError ()){ WorkflowClassStatusResult result = r . getRep (); System . out . println ( result ); } } }; ... sidEnrollWorkflow . currentClasses ( cc ); Which produces the following output: job_class { job_ na me : \"SID analysis\" tas k { tas k_ na me : \"SID\" class_id : \"EDMUND_YEO\" } } 'Class ID' is a general term used to describe 'labels' that apply to the specific plugin. Here, class_id represents all the speaker labels that are enrolled. Unenroll Workflow definitions that support enrollment often also support unenrollment. Similar to enrollment, use the getUnenrollmentJobNames() to get a list of jobs that support unenrollment, send unenrollment requests to unenroll , and use currentClasses() to list enrollments. Here is an example snippet of all three: private static final Server . ResultCallback < WorkflowClassStatusRequest , WorkflowClassStatusResult > cc = new Server . ResultCallback < WorkflowClassStatusRequest , WorkflowClassStatusResult > (){ @Override public void call ( Server . Result < WorkflowClassStatusRequest , WorkflowClassStatusResult > r ){ if ( ! r . hasError ()){ WorkflowClassStatusResult result = r . getRep (); System . out . println ( result ); } } }; ... audioLabel = \"EDMUND_YEO\" ; // find a Speaker Identification (SID) enrollment job name String sidUnenrollmentJobName = \"\" ; for ( String jobName : sidUnenrollWorkflow . getUnenrollmentJobNames ()){ if ( jobName . contains ( \"SID\" )){ sidUnenrollmentJobName = jobName ; break ; } } // submit unenrollment (i.e. Class Modification Request) for Speaker ID plugin sidUnenrollWorkflow . unenroll ( audioLabel , Collections . singletonList ( sidUnenrollmentJobName ), wc , new ArrayList < Pair < String , String >> ()); sidUnenrollWorkflow . currentClasses ( cc ); Which produces the following output: job_class { job_ na me : \"SID analysis\" tas k { tas k_ na me : \"SID\" } } There is no class_id attribute because the speaker was successfully unenrolled and there aren't any others enrolled in the system. Parsing Workflow Responses A successful workflow request produces a response that includes information about the results. Information is grouped into one or more 'jobs', where a job is includes the name, tasks that were performed as part of the job, the results of each task, and potentially information about the input audio. For example the following code snippet: private static final Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > wc = new Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > () { @Override public void call ( Server . Result < WorkflowAnalysisRequest , WorkflowAnalysisResult > r ) { System . out . println ( r . getRep ()); } }; ... // package audio files for analysis Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio = workflow . packageAudio ( audio . build (), audioLabel ); workflow . analyze ( Collections . singletonList ( packagedAudio ), wc , new ArrayList < Pair < String , String >> ()); would produce the following output: Example Workflow Output (click to expand) job_resul t { job_ na me : \"SAD, LID, SID analysis\" tas k_resul ts { tas k_ na me : \"SAD\" tas k_ tra i t : REGION_SCORER tas k_ t ype : \"SAD\" message_ t ype : REGION_SCORER_RESULT message_da ta : \"\\n\\027\\r\\000\\000\\000\\000\\025\\232\\231\\347A\\032\\006speech%\\000\\000\\000\\000\" plugi n : \"sad-dnn-v8.0.1\" domai n : \"multi-v1\" } tas k_resul ts { tas k_ na me : \"LID\" tas k_ tra i t : GLOBAL_SCORER tas k_ t ype : \"LID\" message_ t ype : GLOBAL_SCORER_RESULT message_da ta : \"\\n\\016\\n\\aAmharic\\025\\366\\375N\\301\\n\\016\\n\\aEnglish\\025\\333\\222\\232\\300\\n\\020\\n\\tEsperanto\\025\\347\\332!\\300\\n\\r\\n\\006French\\025Ga\\\"\\301\\n\\026\\n\\017Iranian Persian\\025\\354\\372:\\301\\n\\023\\n\\fIraqi Arabic\\025_\\017\\t\\301\\n\\017\\n\\bJapanese\\025\\230!\\325\\300\\n\\r\\n\\006Korean\\025\\030\\357\\362\\277\\n\\027\\n\\020Levantine Arabic\\025t\\373\\016\\301\\n\\017\\n\\bMandarin\\025!!\\252@\\n\\035\\n\\026Modern Standard Arabic\\025\\315\\030*\\301\\n\\r\\n\\006Pashto\\025\\361\\372t\\301\\n\\021\\n\\nPortuguese\\025\\265\\207r\\301\\n\\016\\n\\aRussian\\025\\353\\231u\\301\\n\\016\\n\\aSpanish\\025$;k\\301\\n\\016\\n\\aTagalog\\025\\002[\\023\\301\\n\\021\\n\\nVietnamese\\025W\\266\\250\\300\" plugi n : \"lid-embedplda-v4.0.1\" domai n : \"multi-v1\" } tas k_resul ts { tas k_ na me : \"SID\" tas k_ tra i t : GLOBAL_SCORER tas k_ t ype : \"SID\" message_ t ype : GLOBAL_SCORER_RESULT message_da ta : \"\\n\\021\\n\\nEdmund_Yeo\\025if\\364@\\n\\v\\n\\004huh?\\025KE(A\" plugi n : \"sid-dplda-v3.0.1\" domai n : \"multi-v1\" } da ta _resul ts { da ta _id : \"Edmund_Yeo_voice_ch.wav\" msg_ t ype : PREPROCESSED_AUDIO_RESULT resul t _da ta : \"\\b\\001\\020\\000\\030\\300>%\\217B\\352A(\\0012\\027Edmund_Yeo_voice_ch.wav:@cbc95af3f693de48654a72ec288adb8ad182a8f86993a3d0d42e3e2a5b4d5548\" } } Each object contains information about job including the job_name that identifies which job was run, the data attribute that contains information about the input audio, and the tasks attribute has holds results of each task performed. Each task object with tasks has a message_type attribute identifies the type of output produced by the task which; it can be used to determine which fields will be available in the analysis attribute. See the Enterprise API Message Reference for a reference on what data will be available for each message type (ex. GlobalScorerResult and RegionScorerResult ) The json result can be parsed and iterated through to provide a simplified output: private static final Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > wc = new Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > () { @Override public void call ( Server . Result < WorkflowAnalysisRequest , WorkflowAnalysisResult > r ) { if ( ! r . hasError ()) { Map < String , JobResult > jobResults = WorkflowUtils . extractWorkflowAnalysis ( r . getRep ()); for ( String jobName : jobResults . keySet ()) { JobResult jr = jobResults . get ( jobName ); if ( ! jr . isError ()) { for ( String taskName : jr . getTasks (). keySet ()) { log . info ( \"--{} Results --\" , taskName ); List < TaskResult > trs = jr . getTasks (). get ( taskName ); for ( TaskResult tr : trs ) { if ( ! tr . isError ()) { if ( tr . getTraitType () == Olive . TraitType . REGION_SCORER ) { Olive . RegionScorerResult rsr = ( Olive . RegionScorerResult ) tr . getTaskMessage (); for ( Olive . RegionScore rs : rsr . getRegionList ()) { log . info ( \"{} ({}-{}secs, score={})\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else if ( tr . getTraitType () == Olive . TraitType . GLOBAL_SCORER ) { Olive . GlobalScorerResult gsr = ( Olive . GlobalScorerResult ) tr . getTaskMessage (); for ( Olive . GlobalScore gs : gsr . getScoreList ()) { log . info ( \"{} = {}\" , gs . getClassId (), gs . getScore ()); } } else if ( tr . getTraitType () == Olive . TraitType . FRAME_SCORER ) { Olive . FrameScorerResult fsr = ( Olive . FrameScorerResult ) tr . getTaskMessage (); for ( Olive . FrameScores fs : fsr . getResultList ()) { for ( int i = 0 ; i < fs . getScoreCount (); i ++ ) { log . info ( \"frame#{} {}={}\" , i , fs . getClassId (), fs . getScore ( i )); } } } else if ( tr . getTraitType () == Olive . TraitType . TEXT_TRANSFORMER ) { Olive . TextTransformationResult ttr = ( Olive . TextTransformationResult ) tr . getTaskMessage (); for ( Olive . TextTransformation transformation : ttr . getTransformationList ()) { System . out . println ( transformation . getTransformedText ()); } } else if ( tr . getTraitType () == Olive . TraitType . CLASS_MODIFIER ) { Olive . ClassModificationResult cmr = ( Olive . ClassModificationResult ) tr . getTaskMessage (); System . out . println ( cmr ); } } else { log . error ( \"Workflow task {} failed: {}\" , taskName , tr . getErrMsg ()); } } } } else { log . error ( \"Workflow job failed: {}\" , jr . getErrMsg ()); } } } else { log . error ( \"Workflow request failed: {}\" , r . getError ()); } System . exit ( 0 ); } }; Which produces an output in the following format: --LID Results -- Amharic = -12.937002 English = -4.8304267 Esperanto = -2.5289857 French = -10.148749 Iranian Persian = -11.68626 Iraqi Arabic = -8.566253 Japanese = -6.660351 Korean = -1.8979216 Levantine Arabic = -8.93639 Mandarin = 5.316544 Modern Standard Arabic = -10.631055 Pashto = -15.311265 Portuguese = -15.158132 Russian = -15.350078 Spanish = -14.701939 Tagalog = -9.209719 Vietnamese = -5.2722583 --SAD Results -- speech (0.0-28.95secs, score=0.0) --SID Results -- EDMUND_YEO = 7.6375012","title":"JOlive Primer"},{"location":"apiCode.html#integrating-with-the-olive-java-api-jolive","text":"While the OLIVE Server allows direct client integration via Protobuf messages\u2014from a variety of languages such as Java, Python, C++ and C# and operating systems such as Windows, Linux, and MacOS\u2014a Java native API is also provided as a convenience. This page includes guidance for integrating a client application using the OLIVE Java API. The sections below cover establishing a connection to an OLIVE Server, requesting available plugins, various ways to build and submit scoring and enrollment requests, and model adaptation. All instructions and code examples presented in this section are for the OLIVE Java API. Fundamentally, all OLIVE API implementations are based on exchanging Protobuf messages between the client and OLIVE Server. These messages are defined in the API Message Reference Documentation. It's recommended that new integrators review the Enterprise API Primer for an overview of key concepts that also apply to the OLIVE Java API.","title":"Integrating with the OLIVE Java API (JOlive)"},{"location":"apiCode.html#distribution","text":"The OLIVE Java API is distributed as a JAR file. It is located in the OLIVE delivery at the path api/java/repo/sri/speech/olive/api/olive-api/<version>/olive-api-<version>.jar .","title":"Distribution"},{"location":"apiCode.html#dependencies","text":"The OLIVE Java API dependencies include: com.google.protobuf:protobuf-java:3.8.0 com.google.protobuf:protobuf-java-util:3.8.0 com.googlecode.json-simple:json-simple:1.1.1 org.json:json:20220320 org.zeromq:jeromq:0.5.2 org.slf4j:slf4j-api:1.7.30 ch.qos.logback:logback-core:1.2.3 ch.qos.logback:logback-classic:1.2.3 commons-lang:commons-lang:2.6 commons-io:commons-io:2.4 commons-cli:commons-cli:1.4 All dependencies are bundled with the OLIVE delivery in the directory api/java/repo . Using api/java/repo as a maven remote repository for offline support Using api/java/repo as maven remote repository lets integrators set up their project offline without pulling dependencies from Maven Central. Add the following to the project's pom.xml (replacing the stub local_path_to_jolive with the actual path): <repositories> <repository> <id> repo_id </id> <url> file:///local_path_to_jolive/repo </url> </repository> </repositories> Next, add the OLIVE Java API and it's dependencies to the project's pom.xml . <dependencies> ... <dependency> <groupId> com.sri.speech.olive.api </groupId> <artifactId> olive-api </artifactId> <version> api version provided </version> </dependency> ... </dependencies> For example, com.google.protobuf:protobuf-java:3.8.0 should be added as: <dependency> <groupId> com.google.protobuf </groupId> <artifactId> protobuf-java </artifactId> <version> 3.8.0 </version> </dependency> You should now be able to develop with the OLIVE Java API within your maven project.","title":"Dependencies"},{"location":"apiCode.html#integration","text":"","title":"Integration"},{"location":"apiCode.html#quickstart","text":"Here's a complete example for those in a hurry: import java.util.ArrayList ; import java.util.List ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.ClientUtils ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.utils.Pair ; public class Quickstart { private static Logger log = LoggerFactory . getLogger ( Quickstart . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; // Create a callback to handle LID results from the server private static final Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > rc = new Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > () { @Override public void call ( Server . Result < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > r ) { if ( ! r . hasError ()) { Olive . GlobalScore topScore = null ; for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { if ( topScore == null ) { topScore = gs ; } else { if ( gs . getScore () > topScore . getScore ()) { topScore = gs ; } } } log . info ( \"Top scoring: {} = {}\" , topScore . getClassId (), topScore . getScore ()); } else { log . info ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // exit if cannot connect if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); System . exit ( 1 ); } // find a Language ID (LID) plugin List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); Pair < Olive . Plugin , Olive . Domain > pd = null ; for ( Pair < Olive . Plugin , Olive . Domain > pair : pluginList ) { if ( \"LID\" . equalsIgnoreCase ( pair . getFirst (). getTask ())) { pd = pair ; break ; } } // run Language ID (LID) on serialized audio file if ( pd != null ) { ClientUtils . requestGlobalScore ( server , pd , Olive . TraitType . GLOBAL_SCORER , audioFileName , 1 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } else { log . error ( \"Could not retrieve a LID plugin!\" ); System . exit ( 1 ); } } } The code above accepts a filepath command line argument, connects to an OLIVE Server, programmatically locates a Language Identification (LID) plugin, submits a serialized audio file for LID analysis, and finally prints out the top scoring language.","title":"Quickstart"},{"location":"apiCode.html#establish-server-connection","text":"Before making any request, a client must establish a connection with the server. By default, the OLIVE server listens on ports 5588 (request port) and 5589 (status port) for client connection and status requests. These ports are configurable, but if the server has not been instructed to change its listening ports, the code below should establish a connection. A connection to the server can be established with a call to com.sri.speech.olive.api.Server#connect() , as shown below: Server server = new Server (); server . connect ( \"exampleClient\" , //client-id \"localhost\" , //address of server 5588 , //request-port 5589 , //status-port 10000 //timeout for failed connection request ); The request port (5588 by default) is used for request and response messages (Protobuf messages). Each request message sent to this port is guaranteed a response from the server. Messages in the API Message Reference are often suffixed with 'Request' and 'Result' to denote whether it's a request or result message. There is no need to poll the server for information on a submitted request, as the result/response for the a request is returned to the client as soon as it is available. The status port (5589 by default) is used by the Server to publish health and status messages (Heartbeat) to client(s). Clients can not send requests on this port.","title":"Establish Server Connection"},{"location":"apiCode.html#tls-encrypted-connections","text":"As of 5.6.0, the OLIVE Java API supports TLS encrypted communication with the OLIVE Server. Encrypted connections require the OLIVE Server to be configured and started with TLS. For example, OLIVE Martini must be started with the flag --tls_server_only or --tls_server_and_client . By default, OLIVE listens on port 5588 for encrypted connections. To set up an encrypted connection using the OLIVE Java API, connect using the com.sri.speech.olive.api.Server#secureConnect() method and provide the required PKCS12 ( *.p12 ) certificate arguments: Server server = new Server (); server . secureConnect ( \"exampleClient\" , //client-id \"localhost\" , //address of server 5588 , //secure port \"path/to/certificate.p12\" , //certificate path \"certificatePassword\" , //certificate password \"path/to/cert/authority.p12\" //certificate authority path \"authorityPassword\" , //certificate authority password 10000 //timeout for failed connection request ); Connecting to the OLIVE Server using TLS encryption takes slightly longer than a standard non-encrypted connection; however, after the initial connection is made, all subsequent requests will run over TLS with no time penalty.","title":"TLS Encrypted Connections"},{"location":"apiCode.html#request-available-plugins","text":"In order to submit most server requests, the client must specify the plugin and domain pair to use for the request (i.e. Pair<Olive.Plugin, Olive.Domain> ). The function requestPlugins() provided by the ClientUtils class can be used to get all the plugin/domain pairs available: // ask the server for a list of currently available plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); The pairs can be iterated through looking for specific criteria of interest. // iterate through and find a plugin and domain to use for SAD Pair < Olive . Plugin , Olive . Domain > pd = null ; for ( Pair < Olive . Plugin , Olive . Domain > pair : pluginList ) { if ( \"SAD\" . equals ( pair . getFirst (). getTask ())) { pd = pair ; break ; } } if ( pd != null ) { log . info ( \"{}/{} supports SAD!\" , pd . getFirst (). getId (), pd . getSecond (). getLabel ()); } else { log . info ( \"No SAD plugin found!\" ); } Alternatively, if the client already knows the desired plugin ID and domain name, the pair reference can be obtained using the findPluginDomain() provided by the ClientUtils class. String pluginName = \"sad-dnn-v8.0.0\" ; String domainName = \"multi-v1\" ; // Look up a specific plugin by ID and domain pd = ClientUtils . findPluginDomain ( pluginName , domainName , pluginList ); if ( pd != null ) { log . info ( \"{}/{} was found!\" , pd . getFirst (). getId (), pd . getSecond (). getLabel ()); } else { log . info ( \"{}/{} was NOT found!\" , pluginName , domainName ); }","title":"Request Available Plugins"},{"location":"apiCode.html#audio-submission-guidelines","text":"One of the core client activities is submitting Audio with a request. In the OLIVE Java API, three ways are provided for a client to send audio data to the OLIVE server: file path buffer of raw audio sample data serialized file buffer object The enum ClientUtils.AudioTransferType is used to specify which of the three transfer mechanisms to use. In almost every case, the OLIVE Java API handles packaging up audio appropriately based on the passed in AudioTransferType so the client only needs to understand a few basic related to the transfer type, and they are explained below. When the client and the OLIVE server share the same file system, the easiest way for the client to send audio data to the server is by specifying the audio's file path on disk. The OLIVE Java API provides the utility below to package audio files which are accessible to the server locally: AudioTransferType . SEND_AS_PATH When the client and the server don't share the same file system, as in the case of a client making a remote connection to the OLIVE server, it is necessary to send the client's local audio files as a file buffer. To help package the client's audio data in a raw buffer, the OLIVE Java API provides the utility below: AudioTransferType . SEND_SAMPLES_BUFFER When the client wants to ensure that all audio header information is provided intact to the server, the OLIVE Java API provides the utility below: AudioTransferType . SEND_SERIALIZED_BUFFER Note on 'serialized buffer' usage. This transfer mechanism passes the original file to the server in its entirety in one contiguous buffer, leaving the audio file header intact. This allows the server to properly decode and process the audio once its received, since it can directly access the bit depth, encoding type, sample rate and other necessary information from the header itself. The tradeoff with serialized files is that there may be additional overhead needed to process the audio into a consumable form. If the client and server reside on the same hardware and file system, it is advisable to simply pass filepaths when possible. This saves the memory overhead burden of both the client and server loading audio into memory. If using common audio types, like 16-bit PCM .wav files, it may also be possible to simply pass a non-serialized file buffer.","title":"Audio Submission Guidelines"},{"location":"apiCode.html#synchronous-vs-asynchronous-message-submission","text":"The OLIVE Java API allows the client to choose between processing a task request synchronously or asynchronously. Processing a task request synchronously means the client will block and wait for the task result to return before proceeding to other task requests. On the other hand, asynchronous processing means the client will not wait for the result to come back before moving on, allowing several jobs to be submitted in parallel. The examples below generally show submitting requests asynchronously. The argument async is used in many ClientUtils functions ( requestFrameScore() , requestGlobalScore() , requestRegionScores() , requestEnrollClass() , etc.), and can be used to choose whether the client intends to wait for the task result to return or not. When async is set to true , the client will not block when a request is sent to the server, so other task requests can be made before the results are received asynchronously and handled by the callback.","title":"Synchronous vs. Asynchronous Message Submission"},{"location":"apiCode.html#olive-java-api-code-samples","text":"The OLIVE Java API includes functionality to accommodate many of the available request messages. Utilities such as requestFrameScore() , requestRegionScores() , requestEnrollClass() , etc. not only do the packaging of request messages, but also take care of sending the request messages to the server, all in one call. They are available from the class com.sri.speech.olive.api.utils.ClientUtils . The required parameters to send many of these requests include: server handle (server - see here ) the plugin handle (pd - see here ) the name of the audio file to submit to the server (filename) channel number of the audio to be processed when audio has more than 1 channel (channelNumber) a callback function for handling results returned either asynchronously or synchronously (rc) whether the client will block for task result to return (async - see here ) an enum of how to submit audio to the server (transferType) optional lists of annotations of the submitted audio (regions) optional list of parameters for customizing plugin behavior (options) optional list of class IDs for filtering the results (classIDs) Performing an enrollment request adds an additional parameter: the ID of the class to be enrolled The primary requests covered below are: Frame score requests - used to make some SAD requests (note that some SAD plugins return regions, not frame scores - most are capable of performing both) Global score requests - used to make LID, SID, or GID score requests Region score requests - used to make ASR, SDD, LDD, QBE, and often SAD score requests Enrollment requests - used to enroll speakers or other class types for plugins that support the ClassEnroller trait","title":"OLIVE Java API Code Samples"},{"location":"apiCode.html#frame-score-request","text":"The example below provides sample code for a function handleMySADFrameScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Frame scoring request to the connected server. public static boolean handleMySADFrameScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > rc = new Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > () { @Override public void call ( Server . Result < Olive . FrameScorerRequest , Olive . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Olive . FrameScores fs : r . getRep (). getResultList ()) { log . info ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { if ( scores [ i ] > 0.0 ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); log . info ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pd , filename , 1 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } This code passes the audio to the server using a serialized buffer. It is also possible to perform this request using buffered audio samples or a file path. Only a plugin that support the FrameScorer trait can handle this request. All SAD plugins support FrameScorer, while some also SAD plugins also support the RegionScorer trait. The method signature for ClientUtils.requestFrameScore is: public static boolean requestFrameScore ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename , int channelNumber , Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) throws ClientException , IOException , UnsupportedAudioFileException For a complete example of how to call this code with a specific plugin, refer to the SAD Scoring Request code example below. It contains the full Java code file this example was pulled from, showing the process of establishing a server connection, polling the server for available plugins to retrieve the appropriate plugin/domain handle, and making the request.","title":"Frame Score Request"},{"location":"apiCode.html#global-score-request","text":"The example below provides sample code for a function handleMyLIDGlobalScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Global scoring request to the connected server. public static boolean handleMyLIDGlobalScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle LID results from the server Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > rc = new Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > () { @Override public void call ( Server . Result < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > r ) { // output LID global scores if ( ! r . hasError ()) { log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { log . info ( String . format ( \"%s = %f\" , gs . getClassId (), gs . getScore ())); } } else { log . info ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; return ClientUtils . requestGlobalScore ( server , pd , Olive . TraitType . GLOBAL_SCORER , filename , 1 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } This code passes the audio to the server using a serialized buffer. It is also possible to perform this request using buffered audio samples or a file path. The code required to submit a GlobalScorerRequest message doesn't care what type of plugin is going to be doing the scoring, as long as the plugin implements the GlobalScorer Trait. This means that the exact same code can be used for submitting audio to global scoring LID plugins, SID plugins, or any other global scoring plugin. The method signature for ClientUtils.requestGlobalScore is: public static boolean requestGlobalScore ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , Olive . TraitType trait , String filename , int channelNumber , Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) throws ClientException , IOException","title":"Global Score Request"},{"location":"apiCode.html#region-score-request","text":"The example below provides sample code for a function handleMyRegionScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Region scoring request to the connected server. public static boolean handleMyRegionScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle results from the server Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > rc = new Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > () { @Override public void call ( Server . Result < Olive . RegionScorerRequest , Olive . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Olive . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"{} ({}-{}secs, score={})\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; return ClientUtils . requestRegionScores ( server , pd , filename , 0 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } This code passes the audio to the server using a serialized buffer. It is also possible to perform this request using file path or a serialized file. The method signature for ClientUtils.requestRegionScore is: public static boolean requestRegionScores ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) throws ClientException , IOException For a complete example of how to call this code with a specific plugin, refer to the ASR Scoring Request code example below. It contains the full Java code file this example was pulled from, showing the process of establishing a server connection, polling the server to retrieve the appropriate plugin/domain handle, and making the request.","title":"Region Score Request"},{"location":"apiCode.html#enrollment-request","text":"The example below provides sample code for a function handleMyEnrollmentRequest that takes a server connection, a plugin/domain handle, the name of the class (speaker) to enroll, and a path to an audio file as arguments, and uses this information to build and submit a Region Scoring request to the connected server. public static boolean handleMyEnrollmentRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String speakerName , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // create a callback that handles the enrollment result from the server Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > () { @Override public void call ( Server . Result < Olive . ClassModificationRequest , Olive . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()) { log . info ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make it a synchronized call, so we know the speaker is enrolled before we boolean enrolled = ClientUtils . requestEnrollClass ( server , pd , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , ClientUtils . DataType . AUDIO_DATA , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); return enrolled ; } The method signature for ClientUtils.requestEnrollClass is: public static boolean requestEnrollClass ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String id , String wavePath , int channelNumber , Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > rc , boolean async , DataType dataType , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options ) throws ClientException","title":"Enrollment Request"},{"location":"apiCode.html#vectorization-and-stateless-score-request","text":"See the appropriate section of the OLIVE Plugin Overview documentation for a description of Vectorization and Stateless Scoring.","title":"Vectorization and Stateless Score Request"},{"location":"apiCode.html#vectorization","text":"The example below provides sample code for a function generateAudioVectors that creates audio vectors that can be used in future stateless scoring reequests. public static boolean generateAudioVectors ( Server server , Pair < Olive . Plugin , Olive . Domain > pp , String speakerName , String enrollmentFileName , List < AudioVector > audioVectors ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // create a callback that handles the enrollment result from the server Server . ResultCallback < Olive . PluginAudioVectorRequest , Olive . PluginAudioVectorResult > enrollmentCallback = new Server . ResultCallback <> () { @Override public void call ( Server . Result < Olive . PluginAudioVectorRequest , Olive . PluginAudioVectorResult > r ) { // examine enrollment result if ( ! r . hasError ()) { for ( VectorResult vr : r . getRep (). getVectorResultList ()) { if ( vr . getSuccessful ()) { audioVectors . add ( vr . getAudioVector ()); } else { log . error ( \"Vectorization failed: {}\" , vr . getMessage ()); } } } else { log . error ( \"Vectorization request failed: {}\" , r . getError ()); } } }; // make it a synchronized call so we can collect the vector before trying to score boolean enrolled = ClientUtils . requestAudioVector ( server , pp , enrollmentFileName , 0 , enrollmentCallback , false , AudioTransferType . SEND_SERIALIZED_BUFFER , Collections . emptyList ()); return enrolled ; } The method signature for ClientUtils.requestAudioVector is: public static boolean requestAudioVector ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Olive . PluginAudioVectorRequest , Olive . PluginAudioVectorResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions ) throws IOException , UnsupportedAudioFileException {","title":"Vectorization"},{"location":"apiCode.html#serialization-and-deserialization","text":"The code below provides an example of how to serialize and deserialize an audio vector: Path audioVectorSaveDirPath = Files . createTempDirectory ( speakerName ); audioVectorSaveDirPath . toFile (). deleteOnExit (); // serialize and save audio vector(s) to disk (for demo purposes) for ( AudioVector audioVector : generatedAudioVectors ) { byte [] serializedAudioVector = MessageUtils . serializeMessage ( audioVector ); Path audioVectorSavePath = audioVectorSaveDirPath . resolve ( UUID . randomUUID (). toString ()); try ( FileOutputStream fos = new FileOutputStream ( audioVectorSavePath . toFile ())) { fos . write ( serializedAudioVector ); } } // reload audio vector(s) from disk (for demo purposes) List < AudioVector > reloadedAudioVectors = new ArrayList <> (); try ( Stream < Path > pathStream = Files . walk ( audioVectorSaveDirPath )) { List < Path > audioVectorPaths = pathStream . filter ( Files :: isRegularFile ). collect ( Collectors . toList ()); for ( Path audioVectorPath : audioVectorPaths ) { try ( FileInputStream fis = new FileInputStream ( audioVectorPath . toString ())) { AudioVector audioVector = MessageUtils . deserializeMessage ( fis . readAllBytes (), AudioVector . parser ()); reloadedAudioVectors . add ( audioVector ); } } } Warning Saving to disk is FOR DEMO PURPOSES ONLY . A production system should consider using SQLite or similar technology which allows associating a speaker label String to binary data blobs rather than storing on disk if they require long-term persistent storage. The examples are to illustrate serialization and deserialization of audio vectors. The method signature for MessageUtils.serializeMessage is: public static byte [] serializeMessage ( Message message ) The method signature for MessageUtils.deserializeMessage is: public static < T extends Message > T deserializeMessage ( byte [] bytes , Parser < T > parser ) throws IOException","title":"Serialization and Deserialization"},{"location":"apiCode.html#stateless-score-request","text":"The example below provides sample code for a function scoreUsingAudioVectors that sends a Stateless Score Request using the in-memory audio vectors. public static void scoreUsingAudioVectors ( Server server , Pair < Olive . Plugin , Olive . Domain > pp , String speakerName , String scoreWaveFileName , List < AudioVector > audioVectors ) throws ClientException , IOException , UnsupportedAudioFileException { // Create a call back to handle the stateless scoring request Server . ResultCallback < Olive . StatelessGlobalScorerRequest , Olive . GlobalScorerResult > scoreCallback = new Server . ResultCallback <> () { @Override public void call ( Server . Result < Olive . StatelessGlobalScorerRequest , Olive . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { log . info ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { log . error ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; List < ClassVector > classVectors = new ArrayList <> ( List . of ( ClientUtils . createClassVector ( speakerName , audioVectors ))); ClientUtils . requestGlobalScoreStateless ( server , pp , scoreWaveFileName , 0 , scoreCallback , false , AudioTransferType . SEND_SERIALIZED_BUFFER , Collections . emptyList (), Collections . emptyList (), classVectors ); } The method signature for ClientUtils.requestGlobalScoreStateless is: public static boolean requestGlobalScoreStateless ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Olive . StatelessGlobalScorerRequest , Olive . GlobalScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < ClassVector > classVectors ) throws ClientException , IOException The method signature for ClientUtils.requestRegionScoreStateless is: public static boolean requestRegionScoresStateless ( Server server , Pair < Olive . Plugin , Olive . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Olive . StatelessRegionScorerRequest , Olive . RegionScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < ClassVector > classVectors ) throws ClientException , IOException","title":"Stateless Score Request"},{"location":"apiCode.html#plugin-specific-code-examples","text":"This section shows examples of using the functions just outlined to make calls to specific plugins, and demonstrate how the same code can be reused for several purposes - for example, requestGlobalScore is valid to request scoring from both SID and LID plugins. SAD Scoring Example SID Enrollment and Scoring Example LID Enrollment and Scoring Example ASR Scoring Example TMT Scoring Example SAD Adaptation Example","title":"Plugin Specific Code Examples"},{"location":"apiCode.html#sad-scoring-request","text":"This shows a full implementation of a client program which sends a frame scoring request to a SAD plugin. Upon return of the result, it outputs the received frame scores. Included also is a second version where a threshold is used to filter out frame scores which are higher. import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.client.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.util.* ; public class MySADFrameScorer { private static Logger log = LoggerFactory . getLogger ( MySADFrameScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"sad-dnn-v8.0.0\" ; public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // from the list of plugins, find the targeted plugin for the task Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Olive . TraitType . FRAME_SCORER , pluginList ); // formulate SAD frame scoring request and send to server handleMySADFrameScorerRequest ( server , pd , audioFileName ); } public static boolean handleMySADFrameScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > rc = new Server . ResultCallback < Olive . FrameScorerRequest , Olive . FrameScorerResult > () { @Override public void call ( Server . Result < Olive . FrameScorerRequest , Olive . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Olive . FrameScores fs : r . getRep (). getResultList ()) { log . info ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); log . info ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pd , filename , 1 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } } The code above outputs all frame scores of the input audio, this can generate massive amount of output, especially when the audio is long. One good way to trim down the output is to filter out regions with frame scores higher than a preset threshold value. The following shows how this can be done using a threshold of 0.0. ... for ( int i = 0 ; i < scores . length ; i ++ ) { if ( scores [ i ] > 0.0 ) { // only print with score 0.0 or greater!!! int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); log . info ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } ...","title":"SAD Scoring Request"},{"location":"apiCode.html#sid-enrollment-and-scoring-request","text":"This example is a full implementation of a client program which sends an enrollment request to a SID plugin, followed by a scoring request to the same SID plugin. import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.utils.parser.RegionParser ; import com.sri.speech.olive.api.client.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.util.* ; public class MySIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"sid-dplda-v3.0.0\" ; private static String speakerName = \"EDMUND_YAO\" ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // obtain SID plugin handle Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Olive . TraitType . CLASS_ENROLLER , pluginList ); // perform SID enrollment task boolean enrolled = handleMyEnrollmentRequest ( server , pd , speakerName , enrollmentFileName ); if ( enrolled ) { // make a region score request handleMyRegionScoreRequest ( server , pd , speakerName , scoreWaveFileName ); } } public static boolean handleMyEnrollmentRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String speakerName , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // create a callback that handles the enrollment result from the server Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > () { @Override public void call ( Server . Result < Olive . ClassModificationRequest , Olive . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()) { log . info ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make it a synchronized call, so we know the speaker is enrolled before we boolean enrolled = ClientUtils . requestEnrollClass ( server , pd , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , ClientUtils . DataType . AUDIO_DATA , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); return enrolled ; } public static void handleMyRegionScoreRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String speakerName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { // Create a call back to handle the SID scoring request Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > () { @Override public void call ( Server . Result < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { log . info ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { log . error ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // SID is a global scorer, so make a global score reqeust: ClientUtils . requestGlobalScore ( server , pd , Olive . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , AudioTransferType . SEND_SERIALIZED_BUFFER , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } }","title":"SID Enrollment and Scoring Request"},{"location":"apiCode.html#lid-enrollment-and-scoring-request","text":"This example is a full implementation of a client program which sends an enrollment request to a LID plugin, followed by a scoring request to the same LID plugin. import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.client.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.util.* ; public class MyLIDEnrollmentAndScore { private static final Logger log = LoggerFactory . getLogger ( MyLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v4.0.0\" ; private static String languageName = \"Esperanto\" ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // obtain LID plugin handle Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Olive . TraitType . CLASS_ENROLLER , pluginList ); // perform LID enrollment task boolean enrolled = handleMyEnrollmentRequest ( server , pd , languageName , enrollmentFileName ); if ( enrolled ) { // make a region score request handleMyRegionScoreRequest ( server , pd , languageName , scoreWaveFileName ); } } public static boolean handleMyEnrollmentRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String languageName , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // create a callback that handles the enrollment result from the server Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Olive . ClassModificationRequest , Olive . ClassModificationResult > () { @Override public void call ( Server . Result < Olive . ClassModificationRequest , Olive . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()) { log . info ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make it a synchronized call, so we know the language is enrolled before we boolean enrolled = ClientUtils . requestEnrollClass ( server , pd , languageName , enrollmentFileName , 0 , enrollmentCallback , false , ClientUtils . DataType . AUDIO_DATA , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); return enrolled ; } public static void handleMyRegionScoreRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String languageName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { // Create a call back to handle the LID scoring request Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > () { @Override public void call ( Server . Result < Olive . GlobalScorerRequest , Olive . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Olive . GlobalScore gs : r . getRep (). getScoreList ()) { log . info ( String . format ( \"language{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { log . error ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: ClientUtils . requestGlobalScore ( server , pd , Olive . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } }","title":"LID Enrollment and Scoring Request"},{"location":"apiCode.html#asr-scoring-request","text":"The following example shows a full implementation of a ASR scoring request. It sends a RegionScorerRequest and receives a RegionScorerResult . import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.ClientUtils.AudioTransferType ; import com.sri.speech.olive.api.client.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.util.* ; public class MyASRRegionScorer { private static Logger log = LoggerFactory . getLogger ( MyASRRegionScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"english-tdnnLookaheadRnnlm-tel-v2\" ; private static String pluginName = \"asr-dynapy-v4.1.0\" ; public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // formulate the frame scoring task request Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"ASR\" , Olive . TraitType . REGION_SCORER , pluginList ); // Perform ASR frame scoring task handleMyRegionScorerRequest ( server , pd , audioFileName ); } public static boolean handleMyRegionScorerRequest ( Server server , Pair < Olive . Plugin , Olive . Domain > pd , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pd ) { return false ; } // Create a callback to handle results from the server Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > rc = new Server . ResultCallback < Olive . RegionScorerRequest , Olive . RegionScorerResult > () { @Override public void call ( Server . Result < Olive . RegionScorerRequest , Olive . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Olive . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"{} ({}-{}secs, score={})\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; return ClientUtils . requestRegionScores ( server , pd , filename , 0 , rc , true , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), new ArrayList <> (), new ArrayList <> ()); } }","title":"ASR Scoring Request"},{"location":"apiCode.html#tmt-request","text":"The following code shows a full implementation of a text machine translation request made to a TMT plugin. The request in this client is a TextTransformationRequest . import java.util.List ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.client.ClientException ; import com.sri.speech.olive.api.utils.ClientUtils ; import com.sri.speech.olive.api.utils.Pair ; public class MyTMTRequest { private static final Logger log = LoggerFactory . getLogger ( MyTMTRequest . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String pluginName = \"tmt-neural-v1.1.1\" ; private static String domainName = \"cmn-eng-nmt-v1\" ; public static void main ( String [] args ) throws ClientException { // input text is passed as an argument String inputText = args [ 0 ] ; // establish server connection Server server = new Server (); server . connect ( \"exampleClient\" , //client-id DEFAULT_SERVERNAME , //address of server DEFAULT_PORT , //request-port DEFAULT_PORT + 1 , //status-port TIMEOUT //timeout for failed connection request ); List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); Pair < Olive . Plugin , Olive . Domain > pluginDomainPair = ClientUtils . findPluginDomain ( pluginName , domainName , pluginList ); // build the request Olive . TextTransformationRequest . Builder req = Olive . TextTransformationRequest . newBuilder () . setText ( inputText ) . setPlugin ( pluginDomainPair . getFirst (). getId ()) . setDomain ( pluginDomainPair . getSecond (). getId ()); // submit request log . info ( String . format ( \"Submitting %s for translation with plugin %s and domain %s\" , inputText , pluginDomainPair . getFirst (). getId (), pluginDomainPair . getSecond (). getId ())); Server . Result < Olive . TextTransformationRequest , Olive . TextTransformationResult > result = server . synchRequest ( req . build ()); // handle response if ( ! result . hasError ()) { for ( Olive . TextTransformation transformation : result . getRep (). getTransformationList ()) { log . info ( transformation . getTransformedText ()); } } else { log . error ( String . format ( \"Translation error\" , result . getError ())); } // disconnect server . disconnect (); } }","title":"TMT Request"},{"location":"apiCode.html#sad-adaptation-request","text":"Below is a full implementation of a SAD request to adapt a new domain from an existing domain. The list of adaptation training files is passed into the client as a file. The code handles both supervised (speech regions specified) and unsupervised (speech regions not specified) SAD adaptations. However, some SAD plugins may not have the unsupervised adaptation capability, in which case the client will exit with a failure message. import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.Olive.AnnotationRegion ; import com.sri.speech.olive.api.Olive.AudioAnnotation ; import com.sri.speech.olive.api.Olive.Domain ; import com.sri.speech.olive.api.Olive.Plugin ; import com.sri.speech.olive.api.utils.* ; import com.sri.speech.olive.api.utils.parser.LearningParser ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import java.util.* ; public class MySADAdaptation { private static Logger log = LoggerFactory . getLogger ( MySADAdaptation . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"sad-dnn-v8.0.0\" ; private static String newDomainName = \"custom-v1\" ; private static LearningParser learningParser = new LearningParser (); public static void main ( String [] args ) throws Exception { // audio file list is passed as an argument String audioFileList = args [ 0 ] ; // parse files in list learningParser . parse ( audioFileList ); if ( ! learningParser . isValid ()) { log . error ( \"Invalid input file: \" + audioFileList ); System . exit ( - 1 ); } // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Olive . Plugin , Olive . Domain >> pluginList = ClientUtils . requestPlugins ( server ); // from the list of plugins, find the targeted plugin for the task Pair < Olive . Plugin , Olive . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , learningParser . isUnsupervised () ? Olive . TraitType . UNSUPERVISED_ADAPTER : Olive . TraitType . SUPERVISED_ADAPTER , pluginList ); // Preproces audio - doesn't matter if supervised or unsupervised String adaptID = UUID . randomUUID (). toString (); Plugin plugin = pd . getFirst (); Domain domain = pd . getSecond (); // optional annotations, generated if found in the parser (supervised // adaptation) Map < String , List < AudioAnnotation >> annotations = new HashMap <> (); int numPreprocessed = 0 ; for ( String filename : learningParser . getFilenames ()) { try { // build the audio Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( filename , - 1 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); // build up request metadata String id = null ; Collection < String > classIDs = learningParser . getAnnotations ( filename ). keySet (); if ( classIDs . size () > 0 ) { id = \"supervised\" ; } // Prepare the request Olive . PreprocessAudioAdaptRequest . Builder req = Olive . PreprocessAudioAdaptRequest . newBuilder () . setPlugin ( plugin . getId ()) . setDomain ( domain . getId ()) . setAdaptSpace ( adaptID ) // We don't set the optional start/end regions... those are used later when // we finalize . setAudio ( audio . build ()); if ( id != null ) { req . setClassId ( id ); } // send the request Server . Result < Olive . PreprocessAudioAdaptRequest , Olive . PreprocessAudioAdaptResult > result = server . synchRequest ( req . build ()); if ( result . hasError ()) { log . error ( String . format ( \"Error preprocessing audio %s because: %s\" , filename , result . getError ())); } else { numPreprocessed += 1 ; log . info ( String . format ( \"Audio file %s successfully preprocessed\" , filename )); String audioId = result . getRep (). getAudioId (); // Set the audio ID for any classID(s) associated with this audio for ( String classIDName : learningParser . getAnnotations ( filename ). keySet ()) { // Add the class/audio id mapping , and optionally add annotation regions List < AudioAnnotation > audioAnnots ; if ( annotations . containsKey ( classIDName )) { audioAnnots = annotations . get ( classIDName ); } else { audioAnnots = new ArrayList <> (); annotations . put ( classIDName , audioAnnots ); } Olive . AudioAnnotation . Builder aaBuilder = Olive . AudioAnnotation . newBuilder () . setAudioId ( audioId ); for ( RegionWord word : learningParser . getAnnotations ( filename ). get ( classIDName )) { AnnotationRegion . Builder ab = AnnotationRegion . newBuilder () . setStartT ( word . getStartTimeSeconds ()). setEndT ( word . getEndTimeSeconds ()); aaBuilder . addRegions ( ab . build ()); } audioAnnots . add ( aaBuilder . build ()); } } } catch ( Exception /* | UnsupportedAudioFileException */ e ) { log . error ( \"Unable to preprocess file: \" + filename ); log . debug ( \"File preprocess error: \" , e ); } } // perform adaptation if ( learningParser . isUnsupervised ()) { if ( numPreprocessed > 0 ) { // Prepare the request Olive . UnsupervisedAdaptationRequest . Builder req = Olive . UnsupervisedAdaptationRequest . newBuilder () . setPlugin ( plugin . getId ()) . setDomain ( domain . getId ()) . setAdaptSpace ( adaptID ) . setNewDomain ( newDomainName ); // Now send the finalize request Server . Result < Olive . UnsupervisedAdaptationRequest , Olive . UnsupervisedAdaptationResult > result = server . synchRequest ( req . build ()); if ( result . hasError ()) { log . error ( String . format ( \"Unsupervised adaptation failed for new domain '%s' because: %s\" , newDomainName , result . getError ())); } else { log . info ( String . format ( \"New Domain '%s' Adapted\" , newDomainName )); } } else { log . error ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } else { // supervised adaptation if ( numPreprocessed > 0 ) { List < Olive . ClassAnnotation > classAnnotations = new ArrayList <> (); for ( String id : annotations . keySet ()) { Olive . ClassAnnotation . Builder caBuilder = Olive . ClassAnnotation . newBuilder (). setClassId ( id ) . addAllAnnotations ( annotations . get ( id )); classAnnotations . add ( caBuilder . build ()); } // Prepare the request Olive . SupervisedAdaptationRequest . Builder req = Olive . SupervisedAdaptationRequest . newBuilder () . setPlugin ( plugin . getId ()) . setDomain ( domain . getId ()) . setAdaptSpace ( adaptID ) . setNewDomain ( newDomainName ) . addAllClassAnnotations ( classAnnotations ); // Now send the finalize request Server . Result < Olive . SupervisedAdaptationRequest , Olive . SupervisedAdaptationResult > result = server . synchRequest ( req . build ()); if ( result . hasError ()) { log . error ( String . format ( \"Failed to adapt new Domain '%s' because: %s\" , newDomainName , result . getError ())); } else { log . info ( String . format ( \"New Domain '%s' Adapted\" , newDomainName )); } } else { log . error ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } log . info ( \"\" ); log . info ( \"Learning finished. Exiting...\" ); System . exit ( 0 ); } }","title":"SAD Adaptation Request"},{"location":"apiCode.html#workflow-integration","text":"In addition to the basic API integration mechanisms described above, JOlive also supports OLIVE Workflows. See Workflows for a primer on OLIVE Workflows and then continue reading below for JOlive specific details.","title":"Workflow Integration"},{"location":"apiCode.html#quickstart_1","text":"Here's a complete workflow example for those in a hurry: import java.util.ArrayList ; import java.util.Collections ; import java.util.List ; import java.util.Map ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import com.sri.speech.olive.api.Olive ; import com.sri.speech.olive.api.Server ; import com.sri.speech.olive.api.Workflow.WorkflowAnalysisRequest ; import com.sri.speech.olive.api.Workflow.WorkflowAnalysisResult ; import com.sri.speech.olive.api.Workflow.WorkflowDataRequest ; import com.sri.speech.olive.api.utils.ClientUtils ; import com.sri.speech.olive.api.utils.Pair ; import com.sri.speech.olive.api.workflow.ActivatedWorkflow ; import com.sri.speech.olive.api.workflow.OliveWorkflowDefinition ; import com.sri.speech.olive.api.workflow.WorkflowUtils ; import com.sri.speech.olive.api.workflow.wrapper.JobResult ; import com.sri.speech.olive.api.workflow.wrapper.TaskResult ; public class WorkflowQuickstart { private static Logger log = LoggerFactory . getLogger ( WorkflowQuickstart . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; // Create a callback to handle workflow results from the server private static final Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > wc = new Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > () { @Override public void call ( Server . Result < WorkflowAnalysisRequest , WorkflowAnalysisResult > r ) { if ( ! r . hasError ()) { Map < String , JobResult > jobResults = WorkflowUtils . extractWorkflowAnalysis ( r . getRep ()); for ( String jobName : jobResults . keySet ()) { JobResult jr = jobResults . get ( jobName ); if ( ! jr . isError ()) { for ( String taskName : jr . getTasks (). keySet ()) { log . info ( \"--{} Results --\" , taskName ); List < TaskResult > trs = jr . getTasks (). get ( taskName ); for ( TaskResult tr : trs ) { if ( ! tr . isError ()) { if ( tr . getTraitType () == Olive . TraitType . REGION_SCORER ) { Olive . RegionScorerResult rsr = ( Olive . RegionScorerResult ) tr . getTaskMessage (); for ( Olive . RegionScore rs : rsr . getRegionList ()) { log . info ( \"{} ({}-{}secs, score={})\" , rs . getClassId (), rs . getStartT (), rs . getEndT (), rs . getScore ()); } } else if ( tr . getTraitType () == Olive . TraitType . GLOBAL_SCORER ) { Olive . GlobalScorerResult gsr = ( Olive . GlobalScorerResult ) tr . getTaskMessage (); for ( Olive . GlobalScore gs : gsr . getScoreList ()) { log . info ( \"{} = {}\" , gs . getClassId (), gs . getScore ()); } } else if ( tr . getTraitType () == Olive . TraitType . FRAME_SCORER ) { Olive . FrameScorerResult fsr = ( Olive . FrameScorerResult ) tr . getTaskMessage (); for ( Olive . FrameScores fs : fsr . getResultList ()) { for ( int i = 0 ; i < fs . getScoreCount (); i ++ ) { log . info ( \"frame#{} {}={}\" , i , fs . getClassId (), fs . getScore ( i )); } } } else if ( tr . getTraitType () == Olive . TraitType . TEXT_TRANSFORMER ) { Olive . TextTransformationResult ttr = ( Olive . TextTransformationResult ) tr . getTaskMessage (); for ( Olive . TextTransformation transformation : ttr . getTransformationList ()) { System . out . println ( transformation . getTransformedText ()); } } else if ( tr . getTraitType () == Olive . TraitType . CLASS_MODIFIER ) { Olive . ClassModificationResult cmr = ( Olive . ClassModificationResult ) tr . getTaskMessage (); System . out . println ( cmr ); } } else { log . error ( \"Workflow task {} failed: {}\" , taskName , tr . getErrMsg ()); } } } } else { log . error ( \"Workflow job failed: {}\" , jr . getErrMsg ()); } } } else { log . error ( \"Workflow request failed: {}\" , r . getError ()); } System . exit ( 0 ); } }; public static void main ( String [] args ) throws Exception { String audioPath = args [ 0 ] ; String workflowPath = args [ 1 ] ; // Setup the connection to the OLIVE server Server server = new Server (); server . connect ( \"exampleWorkflowClient\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // exit if cannot connect if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the OLIVE server: {}\" , DEFAULT_SERVERNAME ); System . exit ( 1 ); } // load the workflow definition to get an activated workflow OliveWorkflowDefinition workflowDefinition = new OliveWorkflowDefinition ( workflowPath ); ActivatedWorkflow workflow = workflowDefinition . createWorkflow ( server ); Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio = workflow . packageAudio ( audio . build (), \"audio\" ); workflow . analyze ( Collections . singletonList ( packagedAudio ), wc , new ArrayList < Pair < String , String >> ()); } }","title":"Quickstart"},{"location":"apiCode.html#initializing-a-workflow","text":"As described in Workflows , workflow logic is encapsulated in a Workflow Definition file distributed as either binary (i.e. *.workflow - deprecated) or JSON (i.e. *.workflow.json ). Workflows are preconfigured to perform tasks such as Speech Activity Detection (SAD), Language Identification (LID), Speaker Identification (SID), etc. with a single call to the OLIVE server. These Workflow Definition files must be initialized (aka 'created') with the JOlive server before the workflow can be used. The snippet below initializes a workflow with the server : OliveWorkflowDefinition workflowDefinition = new OliveWorkflowDefinition ( workflowPath ); ActivatedWorkflow workflow = workflowDefinition . createWorkflow ( server ); A workflow 'helper' object ( ActivatedWorkflow ) is returned and is used to submit audio files directly to that workflow for analysis, enrollment, or unenrollment. In the snippet above, workflow is the 'helper'.","title":"Initializing a Workflow"},{"location":"apiCode.html#packaging-audio","text":"The same Audio Submission Guidelines discussed earlier in this guide apply to workflows and each of the following are supported for workflows: AudioTransferType.SEND_AS_PATH AudioTransferType.SEND_SAMPLES_BUFFER AudioTransferType.SEND_SERIALIZED_BUFFER The difference with workflows is that the workflow 'helper' is used to package audios rather than JOlive server directly; the audio is wrapped in a WorkflowDataRequest that is submitted to OLIVE for processing: // package audio as a serialized buffer using the workflow 'helper' Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio = workflow . packageAudio ( audio . build (), audioLabel );","title":"Packaging Audio"},{"location":"apiCode.html#multi-channel-audio","text":"The default workflow behavior is to merge multi-channel audio into a single channel, which is known as MONO mode. To perform analysis on each channel individually instead of a merged channel, the Workflow Definition must be authored with a mode of SPLIT . When using the split mode, each channel in a multi-channel audio input is \"split\" into a job. Here is a mode within a workflow definition file that merges multi-channel audio into a single channel audio input: \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, ... and one that handles each channel individually: \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"SPLIT\" },","title":"Multi-channel Audio"},{"location":"apiCode.html#audio-annotations","text":"The audio submitted for analysis (or enrollment) can be annotated with start/end regions when packaging audio using the createAudioFromFile() function. The snippet below specifies two regions within a file: // Provide annotations for two regions: 1.0 to 3.0 seconds, and 6.0 to 10.0 seconds in audio List < RegionWord > regions = new ArrayList < RegionWord > (); regions . add ( new RegionWord ( 1000 , 3000 )); regions . add ( new RegionWord ( 6000 , 10000 )); Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , regions );","title":"Audio Annotations"},{"location":"apiCode.html#submitting-audio","text":"Submitting audio for analysis, enrollment, and unenrollment using workflows is supported.","title":"Submitting Audio"},{"location":"apiCode.html#analysis","text":"The snippet below packages and submits an audio file to the workflow 'helper' for analysis: // package audio as a serialized buffer Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , regions ); WorkflowDataRequest packagedAudio = workflow . packageAudio ( audio . build (), audioLabel ); // submit workflow request on serialized audio file workflow . analyze ( Collections . singletonList ( packagedAudio ), wc , new ArrayList < Pair < String , String >> ());","title":"Analysis"},{"location":"apiCode.html#batch-request","text":"The analyze function accepts a List of audio files to so multiple files can be analyzed as a complete 'batch' request: // package audio as a serialized buffer Olive . Audio . Builder audio1 = ClientUtils . createAudioFromFile ( audioPath1 , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio1 = workflow . packageAudio ( audio1 . build (), audioLabel1 ); Olive . Audio . Builder audio2 = ClientUtils . createAudioFromFile ( audioPath2 , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio2 = workflow . packageAudio ( audio2 . build (), audioLabel2 ); List < WorkflowDataRequest > list = new ArrayList <> (); list . add ( packagedAudio1 ); list . add ( packagedAudio2 ); // submit workflow request on multiple serialized audio files workflow . analyze ( list , wc , new ArrayList < Pair < String , String >> ());","title":"Batch request"},{"location":"apiCode.html#enrollments","text":"","title":"Enrollments"},{"location":"apiCode.html#enroll","text":"Some workflows support enrollment for one or more jobs. To list the jobs in a workflow that support enrollment, use the getEnrollmentJobNames() function: System . out . println ( \"Enrollment Jobs: \" + sidEnrollWorkflow . getEnrollmentJobNames ()); // Enrollment Jobs: [SID Enrollment] To enroll a speaker via this workflow, use the workflow 'helper's enroll function: audioLabel = \"EDMUND_YEO\" ; // find a Speaker Identification (SID) enrollment job name String sidEnrollmentJobName = \"\" ; for ( String jobName : sidEnrollWorkflow . getEnrollmentJobNames ()){ if ( jobName . contains ( \"SID\" )){ sidEnrollmentJobName = jobName ; break ; } } Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio = sidEnrollWorkflow . packageAudio ( audio . build (), audioLabel ); sidEnrollWorkflow . enroll ( Collections . singletonList ( packagedAudio ), audioLabel , Collections . singletonList ( sidEnrollmentJobName ), wc , new ArrayList < Pair < String , String >> ()); Note also that not all workflows support enrollment. Please check that the workflow being used supports this before submitting an enrollment request. To confirm the new speaker name was added: private static final Server . ResultCallback < WorkflowClassStatusRequest , WorkflowClassStatusResult > cc = new Server . ResultCallback < WorkflowClassStatusRequest , WorkflowClassStatusResult > (){ @Override public void call ( Server . Result < WorkflowClassStatusRequest , WorkflowClassStatusResult > r ){ if ( ! r . hasError ()){ WorkflowClassStatusResult result = r . getRep (); System . out . println ( result ); } } }; ... sidEnrollWorkflow . currentClasses ( cc ); Which produces the following output: job_class { job_ na me : \"SID analysis\" tas k { tas k_ na me : \"SID\" class_id : \"EDMUND_YEO\" } } 'Class ID' is a general term used to describe 'labels' that apply to the specific plugin. Here, class_id represents all the speaker labels that are enrolled.","title":"Enroll"},{"location":"apiCode.html#unenroll","text":"Workflow definitions that support enrollment often also support unenrollment. Similar to enrollment, use the getUnenrollmentJobNames() to get a list of jobs that support unenrollment, send unenrollment requests to unenroll , and use currentClasses() to list enrollments. Here is an example snippet of all three: private static final Server . ResultCallback < WorkflowClassStatusRequest , WorkflowClassStatusResult > cc = new Server . ResultCallback < WorkflowClassStatusRequest , WorkflowClassStatusResult > (){ @Override public void call ( Server . Result < WorkflowClassStatusRequest , WorkflowClassStatusResult > r ){ if ( ! r . hasError ()){ WorkflowClassStatusResult result = r . getRep (); System . out . println ( result ); } } }; ... audioLabel = \"EDMUND_YEO\" ; // find a Speaker Identification (SID) enrollment job name String sidUnenrollmentJobName = \"\" ; for ( String jobName : sidUnenrollWorkflow . getUnenrollmentJobNames ()){ if ( jobName . contains ( \"SID\" )){ sidUnenrollmentJobName = jobName ; break ; } } // submit unenrollment (i.e. Class Modification Request) for Speaker ID plugin sidUnenrollWorkflow . unenroll ( audioLabel , Collections . singletonList ( sidUnenrollmentJobName ), wc , new ArrayList < Pair < String , String >> ()); sidUnenrollWorkflow . currentClasses ( cc ); Which produces the following output: job_class { job_ na me : \"SID analysis\" tas k { tas k_ na me : \"SID\" } } There is no class_id attribute because the speaker was successfully unenrolled and there aren't any others enrolled in the system.","title":"Unenroll"},{"location":"apiCode.html#parsing-workflow-responses","text":"A successful workflow request produces a response that includes information about the results. Information is grouped into one or more 'jobs', where a job is includes the name, tasks that were performed as part of the job, the results of each task, and potentially information about the input audio. For example the following code snippet: private static final Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > wc = new Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > () { @Override public void call ( Server . Result < WorkflowAnalysisRequest , WorkflowAnalysisResult > r ) { System . out . println ( r . getRep ()); } }; ... // package audio files for analysis Olive . Audio . Builder audio = ClientUtils . createAudioFromFile ( audioPath , 0 , ClientUtils . AudioTransferType . SEND_SERIALIZED_BUFFER , null ); WorkflowDataRequest packagedAudio = workflow . packageAudio ( audio . build (), audioLabel ); workflow . analyze ( Collections . singletonList ( packagedAudio ), wc , new ArrayList < Pair < String , String >> ()); would produce the following output: Example Workflow Output (click to expand) job_resul t { job_ na me : \"SAD, LID, SID analysis\" tas k_resul ts { tas k_ na me : \"SAD\" tas k_ tra i t : REGION_SCORER tas k_ t ype : \"SAD\" message_ t ype : REGION_SCORER_RESULT message_da ta : \"\\n\\027\\r\\000\\000\\000\\000\\025\\232\\231\\347A\\032\\006speech%\\000\\000\\000\\000\" plugi n : \"sad-dnn-v8.0.1\" domai n : \"multi-v1\" } tas k_resul ts { tas k_ na me : \"LID\" tas k_ tra i t : GLOBAL_SCORER tas k_ t ype : \"LID\" message_ t ype : GLOBAL_SCORER_RESULT message_da ta : \"\\n\\016\\n\\aAmharic\\025\\366\\375N\\301\\n\\016\\n\\aEnglish\\025\\333\\222\\232\\300\\n\\020\\n\\tEsperanto\\025\\347\\332!\\300\\n\\r\\n\\006French\\025Ga\\\"\\301\\n\\026\\n\\017Iranian Persian\\025\\354\\372:\\301\\n\\023\\n\\fIraqi Arabic\\025_\\017\\t\\301\\n\\017\\n\\bJapanese\\025\\230!\\325\\300\\n\\r\\n\\006Korean\\025\\030\\357\\362\\277\\n\\027\\n\\020Levantine Arabic\\025t\\373\\016\\301\\n\\017\\n\\bMandarin\\025!!\\252@\\n\\035\\n\\026Modern Standard Arabic\\025\\315\\030*\\301\\n\\r\\n\\006Pashto\\025\\361\\372t\\301\\n\\021\\n\\nPortuguese\\025\\265\\207r\\301\\n\\016\\n\\aRussian\\025\\353\\231u\\301\\n\\016\\n\\aSpanish\\025$;k\\301\\n\\016\\n\\aTagalog\\025\\002[\\023\\301\\n\\021\\n\\nVietnamese\\025W\\266\\250\\300\" plugi n : \"lid-embedplda-v4.0.1\" domai n : \"multi-v1\" } tas k_resul ts { tas k_ na me : \"SID\" tas k_ tra i t : GLOBAL_SCORER tas k_ t ype : \"SID\" message_ t ype : GLOBAL_SCORER_RESULT message_da ta : \"\\n\\021\\n\\nEdmund_Yeo\\025if\\364@\\n\\v\\n\\004huh?\\025KE(A\" plugi n : \"sid-dplda-v3.0.1\" domai n : \"multi-v1\" } da ta _resul ts { da ta _id : \"Edmund_Yeo_voice_ch.wav\" msg_ t ype : PREPROCESSED_AUDIO_RESULT resul t _da ta : \"\\b\\001\\020\\000\\030\\300>%\\217B\\352A(\\0012\\027Edmund_Yeo_voice_ch.wav:@cbc95af3f693de48654a72ec288adb8ad182a8f86993a3d0d42e3e2a5b4d5548\" } } Each object contains information about job including the job_name that identifies which job was run, the data attribute that contains information about the input audio, and the tasks attribute has holds results of each task performed. Each task object with tasks has a message_type attribute identifies the type of output produced by the task which; it can be used to determine which fields will be available in the analysis attribute. See the Enterprise API Message Reference for a reference on what data will be available for each message type (ex. GlobalScorerResult and RegionScorerResult ) The json result can be parsed and iterated through to provide a simplified output: private static final Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > wc = new Server . ResultCallback < WorkflowAnalysisRequest , WorkflowAnalysisResult > () { @Override public void call ( Server . Result < WorkflowAnalysisRequest , WorkflowAnalysisResult > r ) { if ( ! r . hasError ()) { Map < String , JobResult > jobResults = WorkflowUtils . extractWorkflowAnalysis ( r . getRep ()); for ( String jobName : jobResults . keySet ()) { JobResult jr = jobResults . get ( jobName ); if ( ! jr . isError ()) { for ( String taskName : jr . getTasks (). keySet ()) { log . info ( \"--{} Results --\" , taskName ); List < TaskResult > trs = jr . getTasks (). get ( taskName ); for ( TaskResult tr : trs ) { if ( ! tr . isError ()) { if ( tr . getTraitType () == Olive . TraitType . REGION_SCORER ) { Olive . RegionScorerResult rsr = ( Olive . RegionScorerResult ) tr . getTaskMessage (); for ( Olive . RegionScore rs : rsr . getRegionList ()) { log . info ( \"{} ({}-{}secs, score={})\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else if ( tr . getTraitType () == Olive . TraitType . GLOBAL_SCORER ) { Olive . GlobalScorerResult gsr = ( Olive . GlobalScorerResult ) tr . getTaskMessage (); for ( Olive . GlobalScore gs : gsr . getScoreList ()) { log . info ( \"{} = {}\" , gs . getClassId (), gs . getScore ()); } } else if ( tr . getTraitType () == Olive . TraitType . FRAME_SCORER ) { Olive . FrameScorerResult fsr = ( Olive . FrameScorerResult ) tr . getTaskMessage (); for ( Olive . FrameScores fs : fsr . getResultList ()) { for ( int i = 0 ; i < fs . getScoreCount (); i ++ ) { log . info ( \"frame#{} {}={}\" , i , fs . getClassId (), fs . getScore ( i )); } } } else if ( tr . getTraitType () == Olive . TraitType . TEXT_TRANSFORMER ) { Olive . TextTransformationResult ttr = ( Olive . TextTransformationResult ) tr . getTaskMessage (); for ( Olive . TextTransformation transformation : ttr . getTransformationList ()) { System . out . println ( transformation . getTransformedText ()); } } else if ( tr . getTraitType () == Olive . TraitType . CLASS_MODIFIER ) { Olive . ClassModificationResult cmr = ( Olive . ClassModificationResult ) tr . getTaskMessage (); System . out . println ( cmr ); } } else { log . error ( \"Workflow task {} failed: {}\" , taskName , tr . getErrMsg ()); } } } } else { log . error ( \"Workflow job failed: {}\" , jr . getErrMsg ()); } } } else { log . error ( \"Workflow request failed: {}\" , r . getError ()); } System . exit ( 0 ); } }; Which produces an output in the following format: --LID Results -- Amharic = -12.937002 English = -4.8304267 Esperanto = -2.5289857 French = -10.148749 Iranian Persian = -11.68626 Iraqi Arabic = -8.566253 Japanese = -6.660351 Korean = -1.8979216 Levantine Arabic = -8.93639 Mandarin = 5.316544 Modern Standard Arabic = -10.631055 Pashto = -15.311265 Portuguese = -15.158132 Russian = -15.350078 Spanish = -14.701939 Tagalog = -9.209719 Vietnamese = -5.2722583 --SAD Results -- speech (0.0-28.95secs, score=0.0) --SID Results -- EDMUND_YEO = 7.6375012","title":"Parsing Workflow Responses"},{"location":"apiInfo.html","text":"OLIVE Enterprise API Primer Introduction The OLIVE Enterprise API enables third-party tools and existing workflows to interface with the OLIVE backend system. This page provides an introduction to the concepts and information needed to begin implementing OLIVE\u2019s speech processing tools and capabilities within a client application by integrating through the OLIVE Enterprise API (previously known as the SCENIC Enterprise API). In general, the three main components of an OLIVE-based audio processing system are: The OLIVE Server The OLIVE Plugins An OLIVE-enabled Client Links are provided above and throughout this page for more information about the OLIVE Server and the Plugins. The main focus of this page is covering how to help implement or create an OLIVE-enabled Client. SRI offers a Java-based Reference API implementation for OLIVE to allow integrators relying on Java to quickly start folding OLIVE functionality into their new or current project. This code is also available as example code for integrators using different programming languages to use as a general reference for accomplishing certain tasks. This guide covers both implementing the existing Java-based API reference implementation, as well as some information that you will need to build your own reference implentation, if desired. If you're interested in having a reference implementation built and provided in a language other than Java, please refer to this section below. The next section starts by introducing general operating concepts of the OLIVE system, with links to resources that expand on this information if you'd like to dig a bit deeper. Following that is a section that describes the steps a client uses to connect to the OLIVE server, submit requests for processing, and unpack results returned from the server. If you are interested in full Java examples of clients requesting various tasks to be performed using this API, continue on to the Java API Client Integration Guide . Although the current reference OLIVE API is Java based, this message based API can be implemented in different language. Should you have a need for a non-Java OLIVE API, this page of the documentation contains useful information for building an entirely new OLIVE API in a language other than Java. Useful Concepts to Know Before you start using the OLIVE API to put together client programs, it is useful to have some background information about how the OLIVE system operates. The OLIVE system is client/server based, using Google Protocol Buffers (Protobuf) transported over ZMQ sockets to communicate between the client and server. Therefore any API implementation that allows client access to the backend OLIVE system must also follow the same client/server model of exchanging Google Protocol Buffer based messages over ZMQ sockets. The OLIVE server accepts client connections at pre-selected but configurable ports. Once a connection is established between a client and the server, task requests and results (implemented as Protobuf messages) can be sent and received over this connection. Via the server, client requests are directed to and performed by OLIVE speech processing agents (plugins). Also via the server, results of completed requests are returned to the client. Task requests and results are sent in the form of OLIVE messages which contain information pertinent to the type of request and the type of plugin performing the request. The contemts and details of all of the possible messages, both requests and responses, are detailed in the API Message Reference page, but more details about which of these will be important to you and how to actually use these messages follow below. If you are new to the OLIVE Enterprise API then you will quickly discover that the API does not offer explicit API calls for making \u201cSAD\u201d, \u201cSID\u201d, or \u201cLID\u201d requests. The reason for this is the API is based on the OLIVE Plugin Framework used by the OLIVE server, which abstracts speech systems such as SAD, SID, and LID into \u201ctraits\u201d. It is these \u201ctraits\u201d that are expressed in this API, so score and class modification requests are based on the plugins traits and not the type of speech system. The OLIVE server identifies plugins by the task types (SAD, SID, LID, etc.) they claim they can perform, and the details of how these plugins can go about those tasks are defined by the Traits that they implement. For example, a plugin with a Speaker Identification task often implements the GlobalScorer Trait. From the table below or from the Traits Info Page , you can see the messages that accompany implementing this Trait, which define the functionality associated with being a GlobalScorer, and let us see the types of output we can expect to retrieve from such a plugin. The following subsections describe each of these basic concepts in more details. OLIVE Plugin Tasks and Traits The OLIVE server identifies plugins by the type of tasks they can perform. For example, a plugin designed to recognize and identify the voice of a pre-enrolled speaker carries the Task of Speaker Identification , or SID. Likewise, a plugin that is designed to label speech regions within an audio clip has the Task of Speech Activity Detection , or SAD. In order to perform these Tasks, plugins require some abilities - for example, a SID task plugin requires the ability to perform enrollment of speakers and scoring of audio against those enrollments; a SAD type plugin must be able to perform a scoring task to calculate the likelihood of speech throughout an audio clip, and sometimes adaptation tasks; etc. These abilities are defined by what OLIVE calls Traits . More detailed information on the available OLIVE Traits can be found on the respective info page, but a quick primer is also included below. Continuing with the previous example, a SID type plugin needs to be able to perform the scoring task. Historically, most SID plugins assume that the audio being passed to it is homogenous, and consists of a single speaker, and it is therefore possible to score all of the audio and return a single score for each enrolled speaker model representing whether or not the candidate speech is likely from this speaker. This type of score is called a global score, and the Trait that the plugin implements to gain this functionality is called GlobalScorer . \"Implementing\" this Trait means that the plugin contains routine definitions that allow it to receive and appropriately respond to the the OLIVE API Messages associated with that Trait. In order to have speaker models to score against, a SID plugin also needs the ability to enroll speaker models as classes. The ability to modify enrolled classes comes from the ClassModifier Trait and its associated API messages. For a complete list of the available Traits and their associated request messages, including the appropriate reply message to each one, refer to the Plugin Traits info page and/or the API Message Reference page. For a high level overview of what this means in terms of available plugins, continue below. The following table shows the likely traits for the scoring functionality of selected OLIVE plugin task types: Plugin Type Scoring Trait Speech Activity Detection (SAD) FrameScorer and/or RegionScorer Language Identification (LID) GlobalScorer Language and Dialect Detection (LDD) RegionScorer Speaker Identification (SID) GlobalScorer Speaker Diarization and Detection (SDD) RegionScorer Keyword Spotting (KWS) RegionScorer Query By Example KWS (QBE) RegionScorer Speech Enhancement (ENH) AudioConverter Voice Type Discrimination (VTD) FrameScorer and/or RegionScorer For plugins that allow or require enrollment functionality, the associated Trait is ClassModifier . The following plugin types may currently have this Trait for the enrollment task: Plugin Type Enrollment Trait Language Identification (LID) * ClassModifier Speaker Identification (SID) ClassModifier Speaker Diarization and Detection (SDD) ClassModifier Query By Example KWS (QBE) ClassModifier *Note that not all LID plugins allow or support language/class enrollment. When in doubt, refer to individual plugin documentation, or check the plugin's implemented Traits. Please remember, these tables may not be true for all plugins and some Plugins may support additional Traits. This mapping is only intended to help introduce the OLIVE Enterprise API and its underlying Plugin Framework to new developers. Some SAD plugins also allow the end user to perform domain adaptation to improve plugin performance in certain audio conditions. The Trait listed below is associated with this task. Plugin Type Adaptation Trait Speech Activity Detection (SAD) SupervisedAdapter OLIVE Message Requests / Results By Plugin Traits Now that you know a bit about the available Plugin Tasks and the Traits they're likely to implement, we will discuss the Messages that actually allow for requests to be made to the plugins, and for information to be passed back from the plugins to the client. A client connected to the OLIVE server can submit message to the server to request information from plugins. The table below shows what requests are generally available for selected plugin types. Note that it is possible to create plugins that may stray from this list and may implement a different Trait than what is shown below. Scoring Traits Plugin Trait Task Request Message Result Message Global Scorer LID , SID GlobalScorerRequest GlobalScorerStereoRequest GlobalScorerResult GlobalScorerStereoResult RegionScorer LDD , SDD , SAD *, KWS , QBE RegionScorerRequest RegionScorerStereoRequest RegionScorerResult RegionScorerStereoResult FrameScorer SAD * FrameScorerRequest FrameScorerStereoRequest FrameScorerResult FrameScorerStereoResult *Note that not all SAD plugins support FrameScorer and/or RegionScorer. Please refer to specific plugin documentation or consult with SRI if unsure. As you can see from this table, the same few API messages are reused for most scoring requests, meaning the actual code implementation for these tasks can be kept simple. Other Traits Plugin Trait Task Functionality Request Message Result Message AudioConverter ENH Audio Modification, Speech Enhancement AudioModificationRequest AudioModificationResult GlobalComparer FOR Forensic Audio Comparison GlobalComparerRequest GlobalComparerResult LearningTrait / SupervisedAdapter SAD Audio Condition Domain Adaptation SupervisedAdaptationRequest PreprocessAudioAdaptRequest SupervisedAdaptationResult PreprocessAudioAdaptResult Other Useful OLIVE Message Types Besides the messages related to plugin tasking and interaction mentioned in the two sections above, there are several additional messages which are useful to know for server management and other non-plugin-specific tasks. PluginDirectoryRequest and PluginDirectoryResult The client sends a PluginDirectoryRequest to the server to request a list of available plugins, and the server sends back to the client a PluginDirectoryResult . The PluginDirectoryResult response includes supported plugins, their traits, and optional parameters they support. A comprehensive list of OLIVE API Messages is available in OLIVE API Message Reference . Information Persistence As of OLIVE 4.0 the backend OLIVE server and API no longer support persistence. It is the responsibility of the client to store, manage, and reference results from the OLIVE server. The OLIVE server does persist enrolled class models and some collected adaptation information. Dependencies The OLIVE Enterprise API utilizes the following dependencies: Google Protocol Buffers 3.4: Used to define the messages that comprise the OLIVE API. Most messages are in the form of request/reply. ZeroMQ 3.2.3: Provides inter-process communication over several possible mechanisms including TCP. Protobuf-net : Optional - needed if you wish to integrate from a .NET/Mono application. You will need versions of these software dependencies appropriate for your system architecture/operating system in order to communicate with the OLIVE server. Supported Languages Given the dependencies described in the previous section it is possible to utilize the OLIVE API from the following programming languages/runtimes: Java (or other JVM languages that provide Java interoperability) C# (or other .NET language, via the protobuf.net library. This is an extra dependency) Python C++ Note that because the Java-based OLIVE UI utilizes the API, SRI has already developed a Java client library to facilitate use of the API from Java. For more information see the \u201cJava Client Library\u201d section of this document.","title":"Enterprise API Primer"},{"location":"apiInfo.html#olive-enterprise-api-primer","text":"","title":"OLIVE Enterprise API Primer"},{"location":"apiInfo.html#introduction","text":"The OLIVE Enterprise API enables third-party tools and existing workflows to interface with the OLIVE backend system. This page provides an introduction to the concepts and information needed to begin implementing OLIVE\u2019s speech processing tools and capabilities within a client application by integrating through the OLIVE Enterprise API (previously known as the SCENIC Enterprise API). In general, the three main components of an OLIVE-based audio processing system are: The OLIVE Server The OLIVE Plugins An OLIVE-enabled Client Links are provided above and throughout this page for more information about the OLIVE Server and the Plugins. The main focus of this page is covering how to help implement or create an OLIVE-enabled Client. SRI offers a Java-based Reference API implementation for OLIVE to allow integrators relying on Java to quickly start folding OLIVE functionality into their new or current project. This code is also available as example code for integrators using different programming languages to use as a general reference for accomplishing certain tasks. This guide covers both implementing the existing Java-based API reference implementation, as well as some information that you will need to build your own reference implentation, if desired. If you're interested in having a reference implementation built and provided in a language other than Java, please refer to this section below. The next section starts by introducing general operating concepts of the OLIVE system, with links to resources that expand on this information if you'd like to dig a bit deeper. Following that is a section that describes the steps a client uses to connect to the OLIVE server, submit requests for processing, and unpack results returned from the server. If you are interested in full Java examples of clients requesting various tasks to be performed using this API, continue on to the Java API Client Integration Guide . Although the current reference OLIVE API is Java based, this message based API can be implemented in different language. Should you have a need for a non-Java OLIVE API, this page of the documentation contains useful information for building an entirely new OLIVE API in a language other than Java.","title":"Introduction"},{"location":"apiInfo.html#useful-concepts-to-know","text":"Before you start using the OLIVE API to put together client programs, it is useful to have some background information about how the OLIVE system operates. The OLIVE system is client/server based, using Google Protocol Buffers (Protobuf) transported over ZMQ sockets to communicate between the client and server. Therefore any API implementation that allows client access to the backend OLIVE system must also follow the same client/server model of exchanging Google Protocol Buffer based messages over ZMQ sockets. The OLIVE server accepts client connections at pre-selected but configurable ports. Once a connection is established between a client and the server, task requests and results (implemented as Protobuf messages) can be sent and received over this connection. Via the server, client requests are directed to and performed by OLIVE speech processing agents (plugins). Also via the server, results of completed requests are returned to the client. Task requests and results are sent in the form of OLIVE messages which contain information pertinent to the type of request and the type of plugin performing the request. The contemts and details of all of the possible messages, both requests and responses, are detailed in the API Message Reference page, but more details about which of these will be important to you and how to actually use these messages follow below. If you are new to the OLIVE Enterprise API then you will quickly discover that the API does not offer explicit API calls for making \u201cSAD\u201d, \u201cSID\u201d, or \u201cLID\u201d requests. The reason for this is the API is based on the OLIVE Plugin Framework used by the OLIVE server, which abstracts speech systems such as SAD, SID, and LID into \u201ctraits\u201d. It is these \u201ctraits\u201d that are expressed in this API, so score and class modification requests are based on the plugins traits and not the type of speech system. The OLIVE server identifies plugins by the task types (SAD, SID, LID, etc.) they claim they can perform, and the details of how these plugins can go about those tasks are defined by the Traits that they implement. For example, a plugin with a Speaker Identification task often implements the GlobalScorer Trait. From the table below or from the Traits Info Page , you can see the messages that accompany implementing this Trait, which define the functionality associated with being a GlobalScorer, and let us see the types of output we can expect to retrieve from such a plugin. The following subsections describe each of these basic concepts in more details.","title":"Useful Concepts to Know"},{"location":"apiInfo.html#olive-plugin-tasks-and-traits","text":"The OLIVE server identifies plugins by the type of tasks they can perform. For example, a plugin designed to recognize and identify the voice of a pre-enrolled speaker carries the Task of Speaker Identification , or SID. Likewise, a plugin that is designed to label speech regions within an audio clip has the Task of Speech Activity Detection , or SAD. In order to perform these Tasks, plugins require some abilities - for example, a SID task plugin requires the ability to perform enrollment of speakers and scoring of audio against those enrollments; a SAD type plugin must be able to perform a scoring task to calculate the likelihood of speech throughout an audio clip, and sometimes adaptation tasks; etc. These abilities are defined by what OLIVE calls Traits . More detailed information on the available OLIVE Traits can be found on the respective info page, but a quick primer is also included below. Continuing with the previous example, a SID type plugin needs to be able to perform the scoring task. Historically, most SID plugins assume that the audio being passed to it is homogenous, and consists of a single speaker, and it is therefore possible to score all of the audio and return a single score for each enrolled speaker model representing whether or not the candidate speech is likely from this speaker. This type of score is called a global score, and the Trait that the plugin implements to gain this functionality is called GlobalScorer . \"Implementing\" this Trait means that the plugin contains routine definitions that allow it to receive and appropriately respond to the the OLIVE API Messages associated with that Trait. In order to have speaker models to score against, a SID plugin also needs the ability to enroll speaker models as classes. The ability to modify enrolled classes comes from the ClassModifier Trait and its associated API messages. For a complete list of the available Traits and their associated request messages, including the appropriate reply message to each one, refer to the Plugin Traits info page and/or the API Message Reference page. For a high level overview of what this means in terms of available plugins, continue below. The following table shows the likely traits for the scoring functionality of selected OLIVE plugin task types: Plugin Type Scoring Trait Speech Activity Detection (SAD) FrameScorer and/or RegionScorer Language Identification (LID) GlobalScorer Language and Dialect Detection (LDD) RegionScorer Speaker Identification (SID) GlobalScorer Speaker Diarization and Detection (SDD) RegionScorer Keyword Spotting (KWS) RegionScorer Query By Example KWS (QBE) RegionScorer Speech Enhancement (ENH) AudioConverter Voice Type Discrimination (VTD) FrameScorer and/or RegionScorer For plugins that allow or require enrollment functionality, the associated Trait is ClassModifier . The following plugin types may currently have this Trait for the enrollment task: Plugin Type Enrollment Trait Language Identification (LID) * ClassModifier Speaker Identification (SID) ClassModifier Speaker Diarization and Detection (SDD) ClassModifier Query By Example KWS (QBE) ClassModifier *Note that not all LID plugins allow or support language/class enrollment. When in doubt, refer to individual plugin documentation, or check the plugin's implemented Traits. Please remember, these tables may not be true for all plugins and some Plugins may support additional Traits. This mapping is only intended to help introduce the OLIVE Enterprise API and its underlying Plugin Framework to new developers. Some SAD plugins also allow the end user to perform domain adaptation to improve plugin performance in certain audio conditions. The Trait listed below is associated with this task. Plugin Type Adaptation Trait Speech Activity Detection (SAD) SupervisedAdapter","title":"OLIVE Plugin Tasks and Traits"},{"location":"apiInfo.html#olive-message-requests-results-by-plugin-traits","text":"Now that you know a bit about the available Plugin Tasks and the Traits they're likely to implement, we will discuss the Messages that actually allow for requests to be made to the plugins, and for information to be passed back from the plugins to the client. A client connected to the OLIVE server can submit message to the server to request information from plugins. The table below shows what requests are generally available for selected plugin types. Note that it is possible to create plugins that may stray from this list and may implement a different Trait than what is shown below.","title":"OLIVE Message Requests / Results By Plugin Traits"},{"location":"apiInfo.html#scoring-traits","text":"Plugin Trait Task Request Message Result Message Global Scorer LID , SID GlobalScorerRequest GlobalScorerStereoRequest GlobalScorerResult GlobalScorerStereoResult RegionScorer LDD , SDD , SAD *, KWS , QBE RegionScorerRequest RegionScorerStereoRequest RegionScorerResult RegionScorerStereoResult FrameScorer SAD * FrameScorerRequest FrameScorerStereoRequest FrameScorerResult FrameScorerStereoResult *Note that not all SAD plugins support FrameScorer and/or RegionScorer. Please refer to specific plugin documentation or consult with SRI if unsure. As you can see from this table, the same few API messages are reused for most scoring requests, meaning the actual code implementation for these tasks can be kept simple.","title":"Scoring Traits"},{"location":"apiInfo.html#other-traits","text":"Plugin Trait Task Functionality Request Message Result Message AudioConverter ENH Audio Modification, Speech Enhancement AudioModificationRequest AudioModificationResult GlobalComparer FOR Forensic Audio Comparison GlobalComparerRequest GlobalComparerResult LearningTrait / SupervisedAdapter SAD Audio Condition Domain Adaptation SupervisedAdaptationRequest PreprocessAudioAdaptRequest SupervisedAdaptationResult PreprocessAudioAdaptResult","title":"Other Traits"},{"location":"apiInfo.html#other-useful-olive-message-types","text":"Besides the messages related to plugin tasking and interaction mentioned in the two sections above, there are several additional messages which are useful to know for server management and other non-plugin-specific tasks. PluginDirectoryRequest and PluginDirectoryResult The client sends a PluginDirectoryRequest to the server to request a list of available plugins, and the server sends back to the client a PluginDirectoryResult . The PluginDirectoryResult response includes supported plugins, their traits, and optional parameters they support. A comprehensive list of OLIVE API Messages is available in OLIVE API Message Reference .","title":"Other Useful OLIVE Message Types"},{"location":"apiInfo.html#information-persistence","text":"As of OLIVE 4.0 the backend OLIVE server and API no longer support persistence. It is the responsibility of the client to store, manage, and reference results from the OLIVE server. The OLIVE server does persist enrolled class models and some collected adaptation information.","title":"Information Persistence"},{"location":"apiInfo.html#dependencies","text":"The OLIVE Enterprise API utilizes the following dependencies: Google Protocol Buffers 3.4: Used to define the messages that comprise the OLIVE API. Most messages are in the form of request/reply. ZeroMQ 3.2.3: Provides inter-process communication over several possible mechanisms including TCP. Protobuf-net : Optional - needed if you wish to integrate from a .NET/Mono application. You will need versions of these software dependencies appropriate for your system architecture/operating system in order to communicate with the OLIVE server.","title":"Dependencies"},{"location":"apiInfo.html#supported-languages","text":"Given the dependencies described in the previous section it is possible to utilize the OLIVE API from the following programming languages/runtimes: Java (or other JVM languages that provide Java interoperability) C# (or other .NET language, via the protobuf.net library. This is an extra dependency) Python C++ Note that because the Java-based OLIVE UI utilizes the API, SRI has already developed a Java client library to facilitate use of the API from Java. For more information see the \u201cJava Client Library\u201d section of this document.","title":"Supported Languages"},{"location":"apiMessage.html","text":"OLIVE API Message Protocol Documentation olive.proto Protocol Buffer Definitions The messages defined on this page are what define the OLIVE Enterprise API. These messages are how a client application will interact with and provide tasks to an OLIVE Server. The format used by the OLIVE API is based on the Google Protocol Buffer . For more information on how to integrate the ability to send and receive these messages into a client application using a provided Client API implementation from SRI, refer to the Integrating the (Java) Client API Guide. For more information on creating your own reference implementation with the functionality of these messages, head over to the Creating an API Reference Implementation page that offers guidelines and information. Server Management Messages GetActiveRequest Message to request the list of ScenicMessages that are still active GetActiveResult Response to GetActiveRequest containing the ScenicMessages that are still active Field Type Label Description message_id string repeated List containing the IDs of each message still being processed on the server total_num string required Total number of messages still being processed GetStatusRequest Request a simple server status message, similar to a heartbeat except the request reply is sent on the status port and is requested by the client GetStatusResult The result of a GetStatusRequest Field Type Label Description num_pending uint32 required The number of pending jobs num_busy uint32 required the number of active jobs num_finished uint32 required The number of finished jobs version string optional The OLIVE server version Heartbeat A heartbeat Message, acknowledging that the server is running, this message is continuously broadcast by the server on it's status port (this is the only message sent on the status port) Field Type Label Description stats ServerStats optional The current status of the server, optional since status is not available when the server first starts, but a heartbeat is still sent logdir string required The location where the server writes it log files ServerStats Current status of the OLIVE server, sent as part of a Heatbeat message. Field Type Label Description cpu_percent float required The current percentage of CPU used cpu_average float required The average CPU percentage used since the server was started mem_percent float required The percentage of memory used max_mem_percent float required The most memory used since the server was started swap_percent float required The current swap used max_swap_percent float required The max (most) swap space used since the server was started pool_busy uint32 required The number of jobs currently running on the server pool_pending uint32 required The number of jobs queued on the server pool_finished uint32 required The number of jobs completed pool_reinit bool required The number of jobs that need to be re-ran max_num_jobs uint32 optional The max number of concurrent jobs server_version string optional The current version of the server LoadPluginDomainRequest Request a plugin be pre-loaded to optimize later score request(s) Field Type Label Description plugin string required The plugin domain string required The domain LoadPluginDomainResult Acknowledgment that a plugin is being loaded Field Type Label Description successful bool required True if the request is being loaded (but receipt of this message does not guarantee the plugin has finished loaded) RemovePluginDomainRequest Unload (remove from memory) a previously loaded plugin. Use to free resources on the server or force reloading of a plugin/domain Field Type Label Description plugin string required The plugin to remove domain string optional The domain to remove, if omitted all domains removed for this plugin. RemovePluginDomainResult Acknowledgment that a plugin/domain has been removed (unloaded) Field Type Label Description successful bool required True if the plugin was been removed PluginDirectoryRequest Use a PluginDirectoryRequest message to receive the list of plugins available on the server. A Plugin performs tasks such as SAD, LID, SID, or KWS. There may be multiple plugins registered for a given task. A plugin typically has one or more Domains. Plugins contain the code of the recognizer, while Domains correspond to a particular training or adaptation sessions. Domains therefore represent the data/conditions. A plugin and domain together are necessary to perform scoring. PluginDirectoryResult The collection of plugins available on the server, response to PluginDirectoryRequest Field Type Label Description plugins Plugin repeated The available plugins CancelJobsRequest Request to cancel jobs on the server. Field Type Label Description server_wide bool required True will cancel all the client's jobs CancelJobsResult Response to CancelJobsRequest . Field Type Label Description info string optional Any information in response to the canceled job request Global Scorer Messages GlobalScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a GlobalScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a GlobalScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide. GlobalScore The global score for a class Field Type Label Description class_id string required The class score float required The score associated with the class confidence float optional An optional confidence value when part of a calibration report comment string optional An optional suggested action when part of a calibration report GlobalScorerRequest Request global scoring using the specified plugin. The plugin must implement the GlobalScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of global scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio optional The audio to analyze/score. Either audio or vector must be set. vector AudioVector optional The preprocessed audio vector to analyze/score. Either audio or vector must be set. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored GlobalScorerResult The result from a GlobalScorerRequest, having zero or more GlobalScore elements Field Type Label Description score GlobalScore repeated The class scores Region Scorer Messages RegionScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a RegionScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a RegionScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide. RegionScore The basic unit of a region score. There may be multiple RegionScore values in a RegionScorerResult Field Type Label Description start_t int32 required Begin-time of the region (in seconds) end_t int32 required End-time of the region (in seconds) class_id string required Class ID associated with region score float optional Optional score associated with the class_id label RegionScorerRequest Request region scoring for the specified plugin/domain. The plugin must implement the RegionScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of region scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored RegionScorerResult The set of region score results, response to RegionScorerRequest Field Type Label Description region RegionScore repeated The scored regions Frame Scorer Messages FrameScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a FrameScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a FrameScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide. FrameScorerRequest Request frame scoring using the specified plugin and audio. The plugin must implement the FrameScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of frame scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored FrameScorerResult The results from a FrameScorerRequest Field Type Label Description result FrameScores repeated List of frame scores by class_id FrameScores The basic unit of a frame score, returned in a FrameScorerRequest Field Type Label Description class_id string required The class ID to which the frame scores pertain frame_rate int32 required The number of frames per second frame_offset double required The offset to the center of the frame 'window' score double repeated The frame-level scores for the class_id Bounding Box Messages BoundingBoxScorerRequest Field Type Label Description plugin string required The plugin to score domain string required The domain data BinaryMedia optional The image or video to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored BoundingBoxScorerResult Field Type Label Description region BoundingBoxScore BoundingBoxScore Field Type Label Description class_id string required The class ID associated with this region where the object is detected (default is 'face' for non-enrollable box scores) bbox BoundingBox required Defines the area where the object is detected.. is this a single or repeated field??? score float required Score associated with the identified object time_region AnnotationRegion optional Video will have either a timestamp or start/end time (seconds) of when the object was identified. For FDI and FRI (image) there will be no regions. FDV has a single timestamp, and FRV has a start/end time region. Text Transformation Messages TextTransformer is an OLIVE Plugin Trait for scoring using text (instead of audio). For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page. TextTransformationRequest Request the transformation of a text/string using MT Field Type Label Description plugin string required The plugin to invoke domain string required The domain text string optional The string text to analyze/score, Optional as of OLIVE 5.0 since data input(s) can be specified as part of a workflow option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored TextTransformationResult The response to a TextTransformationRequest Field Type Label Description transformation TextTransformation repeated TextTransformation The text value returned in a TextTransformationResult Field Type Label Description class_id string required A classifier for this result, usually just 'text' transformed_text string required The text result start_t float optional Begin-time of the region (in seconds) transformed - Likely only available via a workflow end_t float optional End-time of the region (in seconds) - Likely only available via a workflow source_class_id string optional An optional classifier for the original result, such as 'Spanish' if converting from Spanish to English Audio Alignment Messages AudioAlignmentScorer is an OLIVE Plugin Trait for alignment of two or more audio inputs. For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page. AudioAlignmentScoreRequest Request the alignment of two or more audio inputs Field Type Label Description plugin string required The plugin to invoke domain string required The domain audios Audio repeated The audio to analyze/score, Optional as of OLIVE 5.0 since Audio can be specified option OptionValue repeated as part of a workflow. If specified there should be two or more audio inputs Any options specified | | class_id | string | repeated | Optionally specify the classes to be scored | AudioAlignmentScoreResult The result of a AudioAlignmentScoreRequest Field Type Label Description scores AudioAlignmentScore repeated AudioAlignmentScore A score in an AudioAlignmentScoreResult Field Type Label Description reference_audio_label string required The source or reference audio name (file 1) other_audio_label string required The name of the audio input in comparison (file 2) shift_offset float required shift offset between the audion in the reference and confidence float required The confidence of this score Global Comparer Messages GlobalComparerReport The visual representation of a global comparison Field Type Label Description type ReportType required The type of report (normally a PDF) report_data bytes required The serialized report GlobalComparerRequest Request the comparison of two audio submission. The plugin must implement the GlobalComparer trait to handle this request Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio_one Audio required One of two audio submissions to analyze/score. audio_two Audio required One of two audio submissions to analyze/score. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored // todo remove! GlobalComparerResult The result of a GlobalComparerRequest Field Type Label Description results Metadata repeated The metadata/scores returned from a global compare analysis report GlobalComparerReport repeated A comparison report generated by the plugin ReportType Possible report formats Name Number Description PDF 1 PNG 2 GIF 3 JPEG 4 TIFF 5 Class Modifier Messages ClassModificationRequest Request a modification of a class for the specified plugin. The plugin must implement the ClassModifier Trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class being enrolled/modified addition Audio repeated List of Audio, action pairs to apply to the class removal Audio repeated List of Audio, action pairs to apply to the class addition_vector AudioVector repeated List of preprocessed audio vector to apply to the class finalize bool optional Whether or not to finalize the class. You can send multiple ClassModificationRequests and only finalize on the last request for efficiency. Default: true option OptionValue repeated Any modification options addition_media BinaryMedia repeated List of generic binary data for class enrollment bounding_box BoundingBox repeated List of bounding boxes for imagery enrollment ClassModificationResult Response to ClassModificationRequest . Field Type Label Description addition_result AudioResult repeated Provides feedback about the success/failure of individual audio additions removal_result AudioResult repeated Provides feedback about the success/failure of individual audio removals vector_addition_result AudioResult repeated Provides feedback about the success/failure of individual audio vector additions ClassRemovalRequest Request removal of a class in the specified plugin/domain Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The id of the class to be removed ClassRemovalResult Response to ClassRemovalRequest . Field Type Label Description class_id string optional The ID of the class from the request successful bool optional Whether the removal was successful or not message string optional Description of what occured (mostly populated when unsuccessful) AudioResult The feedback/description of class modification for a result in a ClassModificationResult message Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred on this audio Audio Converter Messages AudioModification The contents of an AudioModificationResult. Field Type Label Description audio AudioBuffer required The transformed audio message string required Description of how this audio was transformed, or an error description. Not sure if needed? scores Metadata repeated Zero or more scores (metadata) about the modifed audio. Metadata is a list of name/value pairs. AudioModificationRequest Request enhancement (modification) of the submitted audio. The plugin must support the AudioConverter trait to support this request Field Type Label Description plugin string required The plugin domain string required The domain requested_channels uint32 required Convert audio to have this number of channels requested_rate uint32 required Convert audio to this sample rate modifications Audio repeated List of Audio, action pairs to apply to the class - may have to limit to one audio submission per request, not sure how to handle multiple results option OptionValue repeated Any options specified AudioModificationResult The result of an AudioModificationRequest Field Type Label Description successful bool required Whether or not the individual audio modification succeeded modification_result AudioModification repeated Provides feedback about the success/failure of individual audio additions. Audio Vectorizer Messages AudioVector Represents audio preprocessed by a plugin/domain. Field Type Label Description plugin string required The origin plugin domain string required The origin domain data bytes required The audio vector data, varies by plugin params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing PluginAudioVectorRequest Request one or more audio submissions be vectorized (preprocesssed) by the specified plugin. The resulting vectorized audio can only be processed by the same plugin. A plugin must support the AudioVectorizer Trait to support this request. Field Type Label Description plugin string required The plugin domain string required The domain addition Audio repeated List of Audio to process PluginAudioVectorResult The result of a PluginAudioVectorRequest, containing a set of VectorResults Field Type Label Description vector_result VectorResult repeated The results of processing the submitted audio. One result per audio addition. VectorResult The status of the vector request, and if successful includes an AudioVector Field Type Label Description successful bool required Whether or not the audio was successfully processed message string optional Description of what occurred to cause an error audio_vector AudioVector optional If successful, the vectorized audio ClassExportRequest Exports an existing class enrollment (i.e. speaker enrollment) from the server for the specified class_id. The plugin must support the ClassExporter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class model to export ClassExportResult The result of an enrollment export Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred to cause an error enrollment EnrollmentModel optional If successful, then this is the exported model for the specified class. ClassImportRequest Used to import an enrollment model (exported via a ClassExportRequest). Only plugins that support the ClassExporter trait can handle this request. Only import an enrollment into the same plugin AND domain as previously exported. Field Type Label Description plugin string required The plugin domain string required The domain class_id string optional Import the model using this class name, instead of the original name enrollment EnrollmentModel required the enrollment to import ClassImportResult The status of a ClassImportRequest. Field Type Label Description successful bool required Whether or not the import succeeded message string optional Description of what occurred to cause an error EnrollmentModel An enrollment model for a specific plugin and domain. This is used to save a current enrollment or restore a class enrollment via a ClassImportRequest. This model is not used as an AudioVector in a scoring requests. Field Type Label Description plugin string required The origin plugin domain string required The origin domain class_id string required the class_id of the enrollment data bytes required The enrollment model data params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing Updater Messages ApplyUpdateRequest Used to request an update of a Plugin that supports the Update trait. Use GetUpdateStatusRequest to check if a plugin is ready for an update, otherwise this request is ignored by the server/plugin Field Type Label Description plugin string required The plugin to apply the update domain string required The domain params Metadata repeated Name/value options, plugin dependent ApplyUpdateResult This message is immediately returned after an ApplyUpdateRequest, as the updating process can take an extended time range to complete. Field Type Label Description successful bool required True if the plugin is being updated GetUpdateStatusRequest Used to request the status for a Plugin that supports the Update trait Field Type Label Description plugin string required The plugin domain string required The domain GetUpdateStatusResult The result of a GetUpdateStatusRequest message. Field Type Label Description update_ready bool required True if the plugin has determiend it is ready for an update last_update DateTime optional The date of the last update, if any params Metadata repeated Zero or Metadata values describing the update status of the plugin. Metadata is a list of name/value pairs. DateTime Date and time info Field Type Label Description year uint32 required Year month uint32 required Month day uint32 required Day hour uint32 required Hour min uint32 required Minute sec uint32 required Seconds Learning Trait Messages These messages are used by plugins that support adaptation and/or training. PreprocessAudioAdaptRequest Request preprocessing of this audio submission, which may be part of an adaptation set. Adaptation can be unsupervised (neither class_id, start_t, and end_t set) or supervised by setting class_id or class_id, start_t, and end_t. Adaptation should be finalized by calling either SupervisedAdaptationRequest or UnsupervisedAdaptationRequest. Plugins must support either the SupervisedAdapter or UnsupervisedAdapter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain audio Audio required The submitted audio adapt_space string required A unique name for the client where pre-processed data is stored class_id string optional The id of the class annotation being preprocessed (supervised training) start_t uint32 optional Begin-time of the region (in seconds) end_t uint32 optional End-time of the region (in seconds) PreprocessAudioAdaptResult The result of a PreprocessAudioAdaptRequest Field Type Label Description audio_id string required The ID of the preprocessed audio duration double required The duration of the audio Supervised Adapter Messages SupervisedAdaptationRequest Finalize adaptation of the specified plugin/domain using audio preprocessed using calls to PreprocessAudioAdaptRequest. The plugin must implement the SupervisedAdapter trait to handle this request. When you adapt or train, you are creating a new domain for the target plugin, that is based on the domain passed in to the 'domain' field of this call. This new domain is specific to a plugin, so it is created within the plugin, and will be named with the string passed to SupervisedAdaptationRequest as 'new_domain'. To actually use this new domain, future scoring or enrollment requests must specify this new domain name, instead of the original, using the value specified during adaptation as the 'new_domain' field. To have access to this new domain, either restart the server, or send a RemovePluginDomainRequest message to the server, which will force a reload of that plugin. Upon successful completion of SupervisedAdaptationRequest, a SupervisedAdaptationResult message should be received, containing the path to the newly created domain on the server's file system. The file sizes of the actual models will not change as a result of adaptation. Rather, the values stored inside these files will. Field Type Label Description plugin string required The plugin to invoke domain string required The domain to adapt new_domain string required the new domain name class_annotations ClassAnnotation repeated The annotations to use for adaptation, audio annotations are created via PreprocessAudioAdaptRequest calls adapt_space string required A unique name for the client where pre-processed data is stored SupervisedAdaptationResult Acknowledgment message that adaptation successfully completed. Informs the client of the full path of the new domain created by the SupervisedAdaptationRequest . Field Type Label Description new_domain string required Confirmation of the new domain name ClassAnnotation Set of annotations for a class Field Type Label Description class_id string required The class ID (such as speaker name or language name) annotations AudioAnnotation repeated the set of all audio annotations for this class. AudioAnnotation A set of audio annotations for a specific audio submission Field Type Label Description audio_id string required The audio ID returned in a PreprocessAudioAdaptResult or PreprocessAudioTrainResult message regions AnnotationRegion repeated Set of annotations Workflow Messages Not yet defined Basic Types Trait A Trait implemented by a plugin Field Type Label Description type TraitType optional The trait type options OptionDefinition repeated Any options specific to this plugin's implementation of the trait Plugin The description of a plugin Field Type Label Description id string optional The id of the plugin task string optional e.g. LID, SID, SAD, KWS, AED, etc. label string optional Display label for plugin desc string optional A brief description of how the plugin works/technologies it employs. vendor string optional A brief description of how the plugin works/technologies it employs. domain Domain repeated The domains owned by this plugin trait Trait repeated The traits (capabilities) of this plugin Domain A description of a domain. Field Type Label Description id string optional The ID of the domain label string optional Display label for the domain desc string optional A brief description of the domain conditions class_id string repeated The list of classes known to this domain Envelope Every message passed between the server and client is an instance of Envelope. An Envelope can contain multiple ScenicMessage instances, so it's important to iterate through them all when you receive an Envelope. Field Type Label Description message ScenicMessage repeated The messages to be sent sender_id string required string description of the message sender ScenicMessage A ScenicMessage represents a single logical message between a client and server. It is placed within an Envelope. It contains nested messages in serialized form. The message_type field is used to determine the type of the nested data. Not all ScenicMessage instance will have message_data, and some may have multiple, but they will all be of the same type. It depends on the value of message_type. Field Type Label Description message_id string required id issued by client (and unique to that client) used to track a request. Any reply for that request will have the same id. message_type MessageType required type of message message_data bytes repeated nested message data that can be deserialized according to message_type. Some messages do not have nested data, some have multiple records error string optional error message; if present an error has occurred on the server info string optional informational message, typically used to explain why message_data is empty but no error is reported MessageType The MessageType enum provides a value for each top-level SCENIC message. It is used within a ScenicMessage to indicate the type of the serialized message contained therein. Name Number Description PLUGIN_DIRECTORY_REQUEST 1 PLUGIN_DIRECTORY_RESULT 2 GLOBAL_SCORER_REQUEST 3 GLOBAL_SCORER_RESULT 4 REGION_SCORER_REQUEST 5 REGION_SCORER_RESULT 6 FRAME_SCORER_REQUEST 7 FRAME_SCORER_RESULT 8 CLASS_MODIFICATION_REQUEST 9 CLASS_MODIFICATION_RESULT 10 CLASS_REMOVAL_REQUEST 11 CLASS_REMOVAL_RESULT 12 GET_ACTIVE_REQUEST 13 GET_ACTIVE_RESULT 14 LOAD_PLUGIN_REQUEST 15 LOAD_PLUGIN_RESULT 16 GET_STATUS_REQUEST 17 GET_STATUS_RESULT 18 HEARTBEAT 19 PREPROCESS_AUDIO_TRAIN_REQUEST 20 PREPROCESS_AUDIO_TRAIN_RESULT 21 PREPROCESS_AUDIO_ADAPT_REQUEST 22 PREPROCESS_AUDIO_ADAPT_RESULT 23 SUPERVISED_TRAINING_REQUEST 24 SUPERVISED_TRAINING_RESULT 25 SUPERVISED_ADAPTATION_REQUEST 26 SUPERVISED_ADAPTATION_RESULT 27 UNSUPERVISED_ADAPTATION_REQUEST 28 UNSUPERVISED_ADAPTATION_RESULT 29 CLASS_ANNOTATION 30 AUDIO_ANNOTATION 31 ANNOTATION_REGION 32 REMOVE_PLUGIN_REQUEST 33 REMOVE_PLUGIN_RESULT 34 AUDIO_MODIFICATION_REQUEST 35 AUDIO_MODIFICATION_RESULT 36 PLUGIN_AUDIO_VECTOR_REQUEST 37 PLUGIN_AUDIO_VECTOR_RESULT 38 CLASS_EXPORT_REQUEST 39 CLASS_EXPORT_RESULT 40 CLASS_IMPORT_REQUEST 41 CLASS_IMPORT_RESULT 42 APPLY_UPDATE_REQUEST 43 APPLY_UPDATE_RESULT 44 GET_UPDATE_STATUS_REQUEST 45 GET_UPDATE_STATUS_RESULT 46 GLOBAL_COMPARER_REQUEST 47 GLOBAL_COMPARER_RESULT 48 AUDIO_ALIGN_REQUEST 68 AUDIO_ALIGN_RESULT 69 TEXT_TRANSFORM_REQUEST 70 TEXT_TRANSFORM_RESULT 71 PREPROCESSED_AUDIO_RESULT 72 DYNAMIC_PLUGIN_REQUEST 73 PLUGIN_2_PLUGIN_REQUEST 74 PLUGIN_2_PLUGIN_RESULT 75 WORKFlOW_TEXT_RESULT 76 SCORE_OUTPUT_TRANSFORMER_REQUEST 77 SCORE_OUTPUT_TRANSFORMER_RESULT 78 DATA_OUTPUT_TRANSFORMER_REQUEST 79 DATA_OUTPUT_TRANSFORMER_RESULT 80 BOUNDING_BOX_REQUEST 81 BOUNDING_BOX_RESULT 82 BINARY_MEDIA_RESULT 83 START_STREAMING_REQUEST 84 START_STREAMING_RESULT 85 AUDIO_STREAM 86 REGION_SCORER_STREAMING_REQUEST 87 REGION_SCORER_STREAMING_RESULT 88 GLOBAL_SCORER_STREAMING_REQUEST 89 GLOBAL_SCORER_STREAMING_RESULT 90 TEXT_TRANSFORMER_STREAMING_REQUEST 91 TEXT_TRANSFORMER_STREAMING_RESULT 92 STOP_STREAMING_REQUEST 93 STOP_STREAMING_RESULT 94 FLUSH_STREAMING_REQUEST 95 FLUSH_STREAMING_RESULT 96 SYNC_STREAMING_REQUEST 97 DRAIN_STREAMING_REQUEST 98 DRAIN_STREAMING_RESULT 99 CANCEL_JOBS_REQUEST 100 CANCEL_JOBS_RESULT 101 INVALID_MESSAGE 102 OptionType Classifies how a OptionDefinition (TraitOption) should be represented in a UI widget Name Number Description BOOLEAN 1 CHOICE 2 TraitType The list of possible traits that a plugin can implement Name Number Description GLOBAL_SCORER 1 REGION_SCORER 2 FRAME_SCORER 3 CLASS_ENROLLER 4 CLASS_MODIFIER 5 SUPERVISED_TRAINER 6 SUPERVISED_ADAPTER 7 UNSUPERVISED_ADAPTER 8 AUDIO_CONVERTER 9 AUDIO_VECTORIZER 10 CLASS_EXPORTER 11 UPDATER 12 LEARNING_TRAIT 13 GLOBAL_COMPARER 14 TEXT_TRANSFORMER 15 AUDIO_ALIGNMENT_SCORER 16 TaskType Name Number Description SAD 1 Speech Activity Detection SID 2 Speaker ID SDD 3 Speaker ID, but output in regions LID 4 Language ID LDD 5 Language ID, but output in regions KWS 6 Keyword Spotting TPD 7 Topic Detection VTD 8 Voice Type Discrimination GID 9 Gender ID GDD 10 Gender ID, but output in regions ASR 11 Automatic Speech Recognition ENH 12 Audio Enhancement CMP 13 Audio Comparison SDD 14 Speaker Detection DIA 15 Diarization QBE 16 Query by Example SHL 17 Speaker Highlighing FID 18 Face ID TMT 19 Text Machine Translation ALN 20 Audio Alignment OptionDefinition A plugin TraitOption, describing how a plugin trait is used Field Type Label Description name string required The name/ID of the option label string required Display label for the option desc string optional A description of the option type OptionType required The type of the option (boolean, choice/drop-down, etc) choice string repeated Optional list of choices used by CHOICE type options default string optional The default option in the list of Options OptionValue A name/value property pair Field Type Label Description name string required The name/ID of the option value string required The option value as a string InputDataType Workflow(?) Input Data Types Name Number Description AUDIO 1 VIDEO 2 TEXT 3 IMAGE 4 InputType Name Number Description FRAME 1 REGION 2 JobClass Field Type Label Description job_name string required The parent job name in a Workflow JobDefinition task TaskClass repeated TaskClass Field Type Label Description task_name string required The ID from the associated WorkflowTask (consumer_result_label) class_id string repeated Zero or more class IDs available to this task. Some tasks do not support classes class_label string optional An optional label/name to describe the classes used by this task such a 'speaker' or 'language' classes_label string optional The speaker label when refering to plural classes, such as speakers, or languages Shared Types Audio Represents an audio. Can either refer to a local file or embed an audio buffer directly. The path and audioSamples fields should be treated as mutually exclusive, with one and only one of these fields implemented For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description path string optional Path to the audio file represented by this record (if not specified then audio is input as a buffer) audioSamples AudioBuffer optional Audio included as a buffer (if not specified, then path must be set) selected_channel uint32 optional Optional - if using multi-channel audio and 'mode' is SELECTED, then this channel is provided to the plugin(s). regions AnnotationRegion repeated Optional annotated regions for this audio label string optional Optional - label used to identify this audio input AnnotationRegion A single pair of timestamps (start and end) that make up an annotated region. Timestamps are in seconds. Field Type Label Description start_t uint32 required Begin-time of the region (in seconds) end_t uint32 required End-time of the region (in seconds) AudioBuffer Audio is contained in a buffer (and the path filed is NOT set in Audio) - by default the audio in the buffer should be PCM encoded, unless the buffer contains a serialized file (unencoded) in which case the serialized_file must be set to true. If the data has been decoded and is not PCM encoded data, then the encoding field must be specified For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description channels uint32 optional The number of channels contained in data, ignored for serialized buffers samples uint32 optional The number of samples (in each channel), ignored for serialized buffers rate uint32 optional The sample rate, ignored for serialized buffers bit_depth AudioBitDepth optional The number of bits in each sample, ignored for serialized buffers data bytes required Should be channels * samples long, striped by channels serialized_file bool optional Optional - true if data contains a serialized buffer encoding AudioEncodingType optional Optional - Not Yet supported - the audio encoding type. Assumed to be PCM if not specified. Ignored for serialized buffer AudioEncodingType Audio encoding types Name Number Description PCMU8 1 PCMS8 2 PCM16 3 PCM24 4 PCM32 5 FLOAT32 6 FLOAT64 7 ULAW 8 ALAW 9 IMA_ADPCM 10 MS_ADPCM 11 GSM610 12 G723_24 13 G721_32 14 DWW12 15 DWW16 16 DWW24 17 VORBIS 18 VOX_ADPCM 19 DPCM16 20 DPCM8 21 AudioBitDepth Number of bits in each audio sample Name Number Description BIT_DEPTH_8 1 BIT_DEPTH_16 2 BIT_DEPTH_24 3 BIT_DEPTH_32 4 BinaryAnnotation Annotation for any binary media. Field Type Label Description task_label string optional When used in a workflow, restrict these annotations to this task (consumer_result_label). If not set, these annotaions are used for all task in a workflow regions AnnotationRegion repeated Audio annotations bbox BoundingBox repeated Optional regions for visual media region_label string optional An optional label this set of annotations, if not specified 'region' is used BinaryBuffer Support data being sent as a binary buffer. Field Type Label Description data bytes required frames_per_second uint32 optional The frames per second (if audio) BinaryMedia A generic binary media type. Field Type Label Description path string optional Path to the data file represented by this record (if not specified then data is input as a buffer) buffer BinaryBuffer optional Data included as a serialized buffer (if not specified, then path must be set) regions BinaryAnnotation repeated Optional annotated regions for the data - repeated in case we need multiple region (like 'region', 'speaker') selected_channel uint32 optional If using multi-channel data then this channel should be used for processing. label string optional Label used to identify this data input BoundingBox A bounding box. Field Type Label Description x1 int32 required Top left x y1 int32 required Top left y x2 int32 required Bottom right x y2 int32 required Bottom right y Metadata The parent container for Metadata so that typed name/value properties can be transported in a generalized way Field Type Label Description type MetadataType required Indicates the type of this metadata, so it can be deserialized to the appropriate type name string required The name (key) for this metadata value bytes required The value is one of MetadataType, must be deserialized by the client into the type specified by type MetadataType Data types supported in an AudioModificationResult's metadata: Name Number Description STRING_META 1 INTEGER_META 2 DOUBLE_META 3 BOOLEAN_META 4 LIST_META 5 BooleanMetadata Value as boolean Field Type Label Description value bool required DoubleMetadata Value as a double Field Type Label Description value double required IntegerMetadata Value as an integer Field Type Label Description value int32 required ListMetadata Value as list of Metadata values Field Type Label Description type MetadataType repeated The type for the corresponding element value bytes repeated The value is one or more MetadataType elements, each element must be deserialized by the client into the type specified by type. For example, for the type, STRING_META, deserialize data as StringMetadata StringMetadata Value as a string Field Type Label Description value string required Scalar Value Types .proto Type Notes C++ Type Java Type Python Type double double double float float float float float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long uint32 Uses variable-length encoding. uint32 int int/long uint64 Uses variable-length encoding. uint64 long int/long sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long sfixed32 Always four bytes. int32 int int sfixed64 Always eight bytes. int64 long int/long bool bool boolean boolean string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode bytes May contain any arbitrary sequence of bytes. string ByteString str","title":"Enterprise API Message Reference"},{"location":"apiMessage.html#olive-api-message-protocol-documentation","text":"","title":"OLIVE API Message Protocol Documentation"},{"location":"apiMessage.html#oliveproto-protocol-buffer-definitions","text":"The messages defined on this page are what define the OLIVE Enterprise API. These messages are how a client application will interact with and provide tasks to an OLIVE Server. The format used by the OLIVE API is based on the Google Protocol Buffer . For more information on how to integrate the ability to send and receive these messages into a client application using a provided Client API implementation from SRI, refer to the Integrating the (Java) Client API Guide. For more information on creating your own reference implementation with the functionality of these messages, head over to the Creating an API Reference Implementation page that offers guidelines and information.","title":"olive.proto Protocol Buffer Definitions"},{"location":"apiMessage.html#server-management-messages","text":"","title":"Server Management Messages"},{"location":"apiMessage.html#getactiverequest","text":"Message to request the list of ScenicMessages that are still active","title":"GetActiveRequest"},{"location":"apiMessage.html#getactiveresult","text":"Response to GetActiveRequest containing the ScenicMessages that are still active Field Type Label Description message_id string repeated List containing the IDs of each message still being processed on the server total_num string required Total number of messages still being processed","title":"GetActiveResult"},{"location":"apiMessage.html#getstatusrequest","text":"Request a simple server status message, similar to a heartbeat except the request reply is sent on the status port and is requested by the client","title":"GetStatusRequest"},{"location":"apiMessage.html#getstatusresult","text":"The result of a GetStatusRequest Field Type Label Description num_pending uint32 required The number of pending jobs num_busy uint32 required the number of active jobs num_finished uint32 required The number of finished jobs version string optional The OLIVE server version","title":"GetStatusResult"},{"location":"apiMessage.html#heartbeat","text":"A heartbeat Message, acknowledging that the server is running, this message is continuously broadcast by the server on it's status port (this is the only message sent on the status port) Field Type Label Description stats ServerStats optional The current status of the server, optional since status is not available when the server first starts, but a heartbeat is still sent logdir string required The location where the server writes it log files","title":"Heartbeat"},{"location":"apiMessage.html#serverstats","text":"Current status of the OLIVE server, sent as part of a Heatbeat message. Field Type Label Description cpu_percent float required The current percentage of CPU used cpu_average float required The average CPU percentage used since the server was started mem_percent float required The percentage of memory used max_mem_percent float required The most memory used since the server was started swap_percent float required The current swap used max_swap_percent float required The max (most) swap space used since the server was started pool_busy uint32 required The number of jobs currently running on the server pool_pending uint32 required The number of jobs queued on the server pool_finished uint32 required The number of jobs completed pool_reinit bool required The number of jobs that need to be re-ran max_num_jobs uint32 optional The max number of concurrent jobs server_version string optional The current version of the server","title":"ServerStats"},{"location":"apiMessage.html#loadplugindomainrequest","text":"Request a plugin be pre-loaded to optimize later score request(s) Field Type Label Description plugin string required The plugin domain string required The domain","title":"LoadPluginDomainRequest"},{"location":"apiMessage.html#loadplugindomainresult","text":"Acknowledgment that a plugin is being loaded Field Type Label Description successful bool required True if the request is being loaded (but receipt of this message does not guarantee the plugin has finished loaded)","title":"LoadPluginDomainResult"},{"location":"apiMessage.html#removeplugindomainrequest","text":"Unload (remove from memory) a previously loaded plugin. Use to free resources on the server or force reloading of a plugin/domain Field Type Label Description plugin string required The plugin to remove domain string optional The domain to remove, if omitted all domains removed for this plugin.","title":"RemovePluginDomainRequest"},{"location":"apiMessage.html#removeplugindomainresult","text":"Acknowledgment that a plugin/domain has been removed (unloaded) Field Type Label Description successful bool required True if the plugin was been removed","title":"RemovePluginDomainResult"},{"location":"apiMessage.html#plugindirectoryrequest","text":"Use a PluginDirectoryRequest message to receive the list of plugins available on the server. A Plugin performs tasks such as SAD, LID, SID, or KWS. There may be multiple plugins registered for a given task. A plugin typically has one or more Domains. Plugins contain the code of the recognizer, while Domains correspond to a particular training or adaptation sessions. Domains therefore represent the data/conditions. A plugin and domain together are necessary to perform scoring.","title":"PluginDirectoryRequest"},{"location":"apiMessage.html#plugindirectoryresult","text":"The collection of plugins available on the server, response to PluginDirectoryRequest Field Type Label Description plugins Plugin repeated The available plugins","title":"PluginDirectoryResult"},{"location":"apiMessage.html#canceljobsrequest","text":"Request to cancel jobs on the server. Field Type Label Description server_wide bool required True will cancel all the client's jobs","title":"CancelJobsRequest"},{"location":"apiMessage.html#canceljobsresult","text":"Response to CancelJobsRequest . Field Type Label Description info string optional Any information in response to the canceled job request","title":"CancelJobsResult"},{"location":"apiMessage.html#global-scorer-messages","text":"GlobalScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a GlobalScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a GlobalScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide.","title":"Global Scorer Messages"},{"location":"apiMessage.html#globalscore","text":"The global score for a class Field Type Label Description class_id string required The class score float required The score associated with the class confidence float optional An optional confidence value when part of a calibration report comment string optional An optional suggested action when part of a calibration report","title":"GlobalScore"},{"location":"apiMessage.html#globalscorerrequest","text":"Request global scoring using the specified plugin. The plugin must implement the GlobalScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of global scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio optional The audio to analyze/score. Either audio or vector must be set. vector AudioVector optional The preprocessed audio vector to analyze/score. Either audio or vector must be set. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"GlobalScorerRequest"},{"location":"apiMessage.html#globalscorerresult","text":"The result from a GlobalScorerRequest, having zero or more GlobalScore elements Field Type Label Description score GlobalScore repeated The class scores","title":"GlobalScorerResult"},{"location":"apiMessage.html#region-scorer-messages","text":"RegionScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a RegionScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a RegionScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide.","title":"Region Scorer Messages"},{"location":"apiMessage.html#regionscore","text":"The basic unit of a region score. There may be multiple RegionScore values in a RegionScorerResult Field Type Label Description start_t int32 required Begin-time of the region (in seconds) end_t int32 required End-time of the region (in seconds) class_id string required Class ID associated with region score float optional Optional score associated with the class_id label","title":"RegionScore"},{"location":"apiMessage.html#regionscorerrequest","text":"Request region scoring for the specified plugin/domain. The plugin must implement the RegionScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of region scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"RegionScorerRequest"},{"location":"apiMessage.html#regionscorerresult","text":"The set of region score results, response to RegionScorerRequest Field Type Label Description region RegionScore repeated The scored regions","title":"RegionScorerResult"},{"location":"apiMessage.html#frame-scorer-messages","text":"FrameScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a FrameScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a FrameScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide.","title":"Frame Scorer Messages"},{"location":"apiMessage.html#framescorerrequest","text":"Request frame scoring using the specified plugin and audio. The plugin must implement the FrameScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of frame scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"FrameScorerRequest"},{"location":"apiMessage.html#framescorerresult","text":"The results from a FrameScorerRequest Field Type Label Description result FrameScores repeated List of frame scores by class_id","title":"FrameScorerResult"},{"location":"apiMessage.html#framescores","text":"The basic unit of a frame score, returned in a FrameScorerRequest Field Type Label Description class_id string required The class ID to which the frame scores pertain frame_rate int32 required The number of frames per second frame_offset double required The offset to the center of the frame 'window' score double repeated The frame-level scores for the class_id","title":"FrameScores"},{"location":"apiMessage.html#bounding-box-messages","text":"","title":"Bounding Box Messages"},{"location":"apiMessage.html#boundingboxscorerrequest","text":"Field Type Label Description plugin string required The plugin to score domain string required The domain data BinaryMedia optional The image or video to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"BoundingBoxScorerRequest"},{"location":"apiMessage.html#boundingboxscorerresult","text":"Field Type Label Description region BoundingBoxScore","title":"BoundingBoxScorerResult"},{"location":"apiMessage.html#boundingboxscore","text":"Field Type Label Description class_id string required The class ID associated with this region where the object is detected (default is 'face' for non-enrollable box scores) bbox BoundingBox required Defines the area where the object is detected.. is this a single or repeated field??? score float required Score associated with the identified object time_region AnnotationRegion optional Video will have either a timestamp or start/end time (seconds) of when the object was identified. For FDI and FRI (image) there will be no regions. FDV has a single timestamp, and FRV has a start/end time region.","title":"BoundingBoxScore"},{"location":"apiMessage.html#text-transformation-messages","text":"TextTransformer is an OLIVE Plugin Trait for scoring using text (instead of audio). For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page.","title":"Text Transformation Messages"},{"location":"apiMessage.html#texttransformationrequest","text":"Request the transformation of a text/string using MT Field Type Label Description plugin string required The plugin to invoke domain string required The domain text string optional The string text to analyze/score, Optional as of OLIVE 5.0 since data input(s) can be specified as part of a workflow option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"TextTransformationRequest"},{"location":"apiMessage.html#texttransformationresult","text":"The response to a TextTransformationRequest Field Type Label Description transformation TextTransformation repeated","title":"TextTransformationResult"},{"location":"apiMessage.html#texttransformation","text":"The text value returned in a TextTransformationResult Field Type Label Description class_id string required A classifier for this result, usually just 'text' transformed_text string required The text result start_t float optional Begin-time of the region (in seconds) transformed - Likely only available via a workflow end_t float optional End-time of the region (in seconds) - Likely only available via a workflow source_class_id string optional An optional classifier for the original result, such as 'Spanish' if converting from Spanish to English","title":"TextTransformation"},{"location":"apiMessage.html#audio-alignment-messages","text":"AudioAlignmentScorer is an OLIVE Plugin Trait for alignment of two or more audio inputs. For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page.","title":"Audio Alignment  Messages"},{"location":"apiMessage.html#audioalignmentscorerequest","text":"Request the alignment of two or more audio inputs Field Type Label Description plugin string required The plugin to invoke domain string required The domain audios Audio repeated The audio to analyze/score, Optional as of OLIVE 5.0 since Audio can be specified option OptionValue repeated as part of a workflow. If specified there should be two or more audio inputs Any options specified | | class_id | string | repeated | Optionally specify the classes to be scored |","title":"AudioAlignmentScoreRequest"},{"location":"apiMessage.html#audioalignmentscoreresult","text":"The result of a AudioAlignmentScoreRequest Field Type Label Description scores AudioAlignmentScore repeated","title":"AudioAlignmentScoreResult"},{"location":"apiMessage.html#audioalignmentscore","text":"A score in an AudioAlignmentScoreResult Field Type Label Description reference_audio_label string required The source or reference audio name (file 1) other_audio_label string required The name of the audio input in comparison (file 2) shift_offset float required shift offset between the audion in the reference and confidence float required The confidence of this score","title":"AudioAlignmentScore"},{"location":"apiMessage.html#global-comparer-messages","text":"","title":"Global Comparer Messages"},{"location":"apiMessage.html#globalcomparerreport","text":"The visual representation of a global comparison Field Type Label Description type ReportType required The type of report (normally a PDF) report_data bytes required The serialized report","title":"GlobalComparerReport"},{"location":"apiMessage.html#globalcomparerrequest","text":"Request the comparison of two audio submission. The plugin must implement the GlobalComparer trait to handle this request Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio_one Audio required One of two audio submissions to analyze/score. audio_two Audio required One of two audio submissions to analyze/score. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored // todo remove!","title":"GlobalComparerRequest"},{"location":"apiMessage.html#globalcomparerresult","text":"The result of a GlobalComparerRequest Field Type Label Description results Metadata repeated The metadata/scores returned from a global compare analysis report GlobalComparerReport repeated A comparison report generated by the plugin","title":"GlobalComparerResult"},{"location":"apiMessage.html#reporttype","text":"Possible report formats Name Number Description PDF 1 PNG 2 GIF 3 JPEG 4 TIFF 5","title":"ReportType"},{"location":"apiMessage.html#class-modifier-messages","text":"","title":"Class Modifier Messages"},{"location":"apiMessage.html#classmodificationrequest","text":"Request a modification of a class for the specified plugin. The plugin must implement the ClassModifier Trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class being enrolled/modified addition Audio repeated List of Audio, action pairs to apply to the class removal Audio repeated List of Audio, action pairs to apply to the class addition_vector AudioVector repeated List of preprocessed audio vector to apply to the class finalize bool optional Whether or not to finalize the class. You can send multiple ClassModificationRequests and only finalize on the last request for efficiency. Default: true option OptionValue repeated Any modification options addition_media BinaryMedia repeated List of generic binary data for class enrollment bounding_box BoundingBox repeated List of bounding boxes for imagery enrollment","title":"ClassModificationRequest"},{"location":"apiMessage.html#classmodificationresult","text":"Response to ClassModificationRequest . Field Type Label Description addition_result AudioResult repeated Provides feedback about the success/failure of individual audio additions removal_result AudioResult repeated Provides feedback about the success/failure of individual audio removals vector_addition_result AudioResult repeated Provides feedback about the success/failure of individual audio vector additions","title":"ClassModificationResult"},{"location":"apiMessage.html#classremovalrequest","text":"Request removal of a class in the specified plugin/domain Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The id of the class to be removed","title":"ClassRemovalRequest"},{"location":"apiMessage.html#classremovalresult","text":"Response to ClassRemovalRequest . Field Type Label Description class_id string optional The ID of the class from the request successful bool optional Whether the removal was successful or not message string optional Description of what occured (mostly populated when unsuccessful)","title":"ClassRemovalResult"},{"location":"apiMessage.html#audioresult","text":"The feedback/description of class modification for a result in a ClassModificationResult message Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred on this audio","title":"AudioResult"},{"location":"apiMessage.html#audio-converter-messages","text":"","title":"Audio Converter Messages"},{"location":"apiMessage.html#audiomodification","text":"The contents of an AudioModificationResult. Field Type Label Description audio AudioBuffer required The transformed audio message string required Description of how this audio was transformed, or an error description. Not sure if needed? scores Metadata repeated Zero or more scores (metadata) about the modifed audio. Metadata is a list of name/value pairs.","title":"AudioModification"},{"location":"apiMessage.html#audiomodificationrequest","text":"Request enhancement (modification) of the submitted audio. The plugin must support the AudioConverter trait to support this request Field Type Label Description plugin string required The plugin domain string required The domain requested_channels uint32 required Convert audio to have this number of channels requested_rate uint32 required Convert audio to this sample rate modifications Audio repeated List of Audio, action pairs to apply to the class - may have to limit to one audio submission per request, not sure how to handle multiple results option OptionValue repeated Any options specified","title":"AudioModificationRequest"},{"location":"apiMessage.html#audiomodificationresult","text":"The result of an AudioModificationRequest Field Type Label Description successful bool required Whether or not the individual audio modification succeeded modification_result AudioModification repeated Provides feedback about the success/failure of individual audio additions.","title":"AudioModificationResult"},{"location":"apiMessage.html#audio-vectorizer-messages","text":"","title":"Audio Vectorizer Messages"},{"location":"apiMessage.html#audiovector","text":"Represents audio preprocessed by a plugin/domain. Field Type Label Description plugin string required The origin plugin domain string required The origin domain data bytes required The audio vector data, varies by plugin params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing","title":"AudioVector"},{"location":"apiMessage.html#pluginaudiovectorrequest","text":"Request one or more audio submissions be vectorized (preprocesssed) by the specified plugin. The resulting vectorized audio can only be processed by the same plugin. A plugin must support the AudioVectorizer Trait to support this request. Field Type Label Description plugin string required The plugin domain string required The domain addition Audio repeated List of Audio to process","title":"PluginAudioVectorRequest"},{"location":"apiMessage.html#pluginaudiovectorresult","text":"The result of a PluginAudioVectorRequest, containing a set of VectorResults Field Type Label Description vector_result VectorResult repeated The results of processing the submitted audio. One result per audio addition.","title":"PluginAudioVectorResult"},{"location":"apiMessage.html#vectorresult","text":"The status of the vector request, and if successful includes an AudioVector Field Type Label Description successful bool required Whether or not the audio was successfully processed message string optional Description of what occurred to cause an error audio_vector AudioVector optional If successful, the vectorized audio","title":"VectorResult"},{"location":"apiMessage.html#classexportrequest","text":"Exports an existing class enrollment (i.e. speaker enrollment) from the server for the specified class_id. The plugin must support the ClassExporter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class model to export","title":"ClassExportRequest"},{"location":"apiMessage.html#classexportresult","text":"The result of an enrollment export Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred to cause an error enrollment EnrollmentModel optional If successful, then this is the exported model for the specified class.","title":"ClassExportResult"},{"location":"apiMessage.html#classimportrequest","text":"Used to import an enrollment model (exported via a ClassExportRequest). Only plugins that support the ClassExporter trait can handle this request. Only import an enrollment into the same plugin AND domain as previously exported. Field Type Label Description plugin string required The plugin domain string required The domain class_id string optional Import the model using this class name, instead of the original name enrollment EnrollmentModel required the enrollment to import","title":"ClassImportRequest"},{"location":"apiMessage.html#classimportresult","text":"The status of a ClassImportRequest. Field Type Label Description successful bool required Whether or not the import succeeded message string optional Description of what occurred to cause an error","title":"ClassImportResult"},{"location":"apiMessage.html#enrollmentmodel","text":"An enrollment model for a specific plugin and domain. This is used to save a current enrollment or restore a class enrollment via a ClassImportRequest. This model is not used as an AudioVector in a scoring requests. Field Type Label Description plugin string required The origin plugin domain string required The origin domain class_id string required the class_id of the enrollment data bytes required The enrollment model data params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing","title":"EnrollmentModel"},{"location":"apiMessage.html#updater-messages","text":"","title":"Updater Messages"},{"location":"apiMessage.html#applyupdaterequest","text":"Used to request an update of a Plugin that supports the Update trait. Use GetUpdateStatusRequest to check if a plugin is ready for an update, otherwise this request is ignored by the server/plugin Field Type Label Description plugin string required The plugin to apply the update domain string required The domain params Metadata repeated Name/value options, plugin dependent","title":"ApplyUpdateRequest"},{"location":"apiMessage.html#applyupdateresult","text":"This message is immediately returned after an ApplyUpdateRequest, as the updating process can take an extended time range to complete. Field Type Label Description successful bool required True if the plugin is being updated","title":"ApplyUpdateResult"},{"location":"apiMessage.html#getupdatestatusrequest","text":"Used to request the status for a Plugin that supports the Update trait Field Type Label Description plugin string required The plugin domain string required The domain","title":"GetUpdateStatusRequest"},{"location":"apiMessage.html#getupdatestatusresult","text":"The result of a GetUpdateStatusRequest message. Field Type Label Description update_ready bool required True if the plugin has determiend it is ready for an update last_update DateTime optional The date of the last update, if any params Metadata repeated Zero or Metadata values describing the update status of the plugin. Metadata is a list of name/value pairs.","title":"GetUpdateStatusResult"},{"location":"apiMessage.html#datetime","text":"Date and time info Field Type Label Description year uint32 required Year month uint32 required Month day uint32 required Day hour uint32 required Hour min uint32 required Minute sec uint32 required Seconds","title":"DateTime"},{"location":"apiMessage.html#learning-trait-messages","text":"These messages are used by plugins that support adaptation and/or training.","title":"Learning Trait Messages"},{"location":"apiMessage.html#preprocessaudioadaptrequest","text":"Request preprocessing of this audio submission, which may be part of an adaptation set. Adaptation can be unsupervised (neither class_id, start_t, and end_t set) or supervised by setting class_id or class_id, start_t, and end_t. Adaptation should be finalized by calling either SupervisedAdaptationRequest or UnsupervisedAdaptationRequest. Plugins must support either the SupervisedAdapter or UnsupervisedAdapter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain audio Audio required The submitted audio adapt_space string required A unique name for the client where pre-processed data is stored class_id string optional The id of the class annotation being preprocessed (supervised training) start_t uint32 optional Begin-time of the region (in seconds) end_t uint32 optional End-time of the region (in seconds)","title":"PreprocessAudioAdaptRequest"},{"location":"apiMessage.html#preprocessaudioadaptresult","text":"The result of a PreprocessAudioAdaptRequest Field Type Label Description audio_id string required The ID of the preprocessed audio duration double required The duration of the audio","title":"PreprocessAudioAdaptResult"},{"location":"apiMessage.html#supervised-adapter-messages","text":"","title":"Supervised Adapter Messages"},{"location":"apiMessage.html#supervisedadaptationrequest","text":"Finalize adaptation of the specified plugin/domain using audio preprocessed using calls to PreprocessAudioAdaptRequest. The plugin must implement the SupervisedAdapter trait to handle this request. When you adapt or train, you are creating a new domain for the target plugin, that is based on the domain passed in to the 'domain' field of this call. This new domain is specific to a plugin, so it is created within the plugin, and will be named with the string passed to SupervisedAdaptationRequest as 'new_domain'. To actually use this new domain, future scoring or enrollment requests must specify this new domain name, instead of the original, using the value specified during adaptation as the 'new_domain' field. To have access to this new domain, either restart the server, or send a RemovePluginDomainRequest message to the server, which will force a reload of that plugin. Upon successful completion of SupervisedAdaptationRequest, a SupervisedAdaptationResult message should be received, containing the path to the newly created domain on the server's file system. The file sizes of the actual models will not change as a result of adaptation. Rather, the values stored inside these files will. Field Type Label Description plugin string required The plugin to invoke domain string required The domain to adapt new_domain string required the new domain name class_annotations ClassAnnotation repeated The annotations to use for adaptation, audio annotations are created via PreprocessAudioAdaptRequest calls adapt_space string required A unique name for the client where pre-processed data is stored","title":"SupervisedAdaptationRequest"},{"location":"apiMessage.html#supervisedadaptationresult","text":"Acknowledgment message that adaptation successfully completed. Informs the client of the full path of the new domain created by the SupervisedAdaptationRequest . Field Type Label Description new_domain string required Confirmation of the new domain name","title":"SupervisedAdaptationResult"},{"location":"apiMessage.html#classannotation","text":"Set of annotations for a class Field Type Label Description class_id string required The class ID (such as speaker name or language name) annotations AudioAnnotation repeated the set of all audio annotations for this class.","title":"ClassAnnotation"},{"location":"apiMessage.html#audioannotation","text":"A set of audio annotations for a specific audio submission Field Type Label Description audio_id string required The audio ID returned in a PreprocessAudioAdaptResult or PreprocessAudioTrainResult message regions AnnotationRegion repeated Set of annotations","title":"AudioAnnotation"},{"location":"apiMessage.html#workflow-messages","text":"Not yet defined","title":"Workflow Messages"},{"location":"apiMessage.html#basic-types","text":"","title":"Basic Types"},{"location":"apiMessage.html#trait","text":"A Trait implemented by a plugin Field Type Label Description type TraitType optional The trait type options OptionDefinition repeated Any options specific to this plugin's implementation of the trait","title":"Trait"},{"location":"apiMessage.html#plugin","text":"The description of a plugin Field Type Label Description id string optional The id of the plugin task string optional e.g. LID, SID, SAD, KWS, AED, etc. label string optional Display label for plugin desc string optional A brief description of how the plugin works/technologies it employs. vendor string optional A brief description of how the plugin works/technologies it employs. domain Domain repeated The domains owned by this plugin trait Trait repeated The traits (capabilities) of this plugin","title":"Plugin"},{"location":"apiMessage.html#domain","text":"A description of a domain. Field Type Label Description id string optional The ID of the domain label string optional Display label for the domain desc string optional A brief description of the domain conditions class_id string repeated The list of classes known to this domain","title":"Domain"},{"location":"apiMessage.html#envelope","text":"Every message passed between the server and client is an instance of Envelope. An Envelope can contain multiple ScenicMessage instances, so it's important to iterate through them all when you receive an Envelope. Field Type Label Description message ScenicMessage repeated The messages to be sent sender_id string required string description of the message sender","title":"Envelope"},{"location":"apiMessage.html#scenicmessage","text":"A ScenicMessage represents a single logical message between a client and server. It is placed within an Envelope. It contains nested messages in serialized form. The message_type field is used to determine the type of the nested data. Not all ScenicMessage instance will have message_data, and some may have multiple, but they will all be of the same type. It depends on the value of message_type. Field Type Label Description message_id string required id issued by client (and unique to that client) used to track a request. Any reply for that request will have the same id. message_type MessageType required type of message message_data bytes repeated nested message data that can be deserialized according to message_type. Some messages do not have nested data, some have multiple records error string optional error message; if present an error has occurred on the server info string optional informational message, typically used to explain why message_data is empty but no error is reported","title":"ScenicMessage"},{"location":"apiMessage.html#messagetype","text":"The MessageType enum provides a value for each top-level SCENIC message. It is used within a ScenicMessage to indicate the type of the serialized message contained therein. Name Number Description PLUGIN_DIRECTORY_REQUEST 1 PLUGIN_DIRECTORY_RESULT 2 GLOBAL_SCORER_REQUEST 3 GLOBAL_SCORER_RESULT 4 REGION_SCORER_REQUEST 5 REGION_SCORER_RESULT 6 FRAME_SCORER_REQUEST 7 FRAME_SCORER_RESULT 8 CLASS_MODIFICATION_REQUEST 9 CLASS_MODIFICATION_RESULT 10 CLASS_REMOVAL_REQUEST 11 CLASS_REMOVAL_RESULT 12 GET_ACTIVE_REQUEST 13 GET_ACTIVE_RESULT 14 LOAD_PLUGIN_REQUEST 15 LOAD_PLUGIN_RESULT 16 GET_STATUS_REQUEST 17 GET_STATUS_RESULT 18 HEARTBEAT 19 PREPROCESS_AUDIO_TRAIN_REQUEST 20 PREPROCESS_AUDIO_TRAIN_RESULT 21 PREPROCESS_AUDIO_ADAPT_REQUEST 22 PREPROCESS_AUDIO_ADAPT_RESULT 23 SUPERVISED_TRAINING_REQUEST 24 SUPERVISED_TRAINING_RESULT 25 SUPERVISED_ADAPTATION_REQUEST 26 SUPERVISED_ADAPTATION_RESULT 27 UNSUPERVISED_ADAPTATION_REQUEST 28 UNSUPERVISED_ADAPTATION_RESULT 29 CLASS_ANNOTATION 30 AUDIO_ANNOTATION 31 ANNOTATION_REGION 32 REMOVE_PLUGIN_REQUEST 33 REMOVE_PLUGIN_RESULT 34 AUDIO_MODIFICATION_REQUEST 35 AUDIO_MODIFICATION_RESULT 36 PLUGIN_AUDIO_VECTOR_REQUEST 37 PLUGIN_AUDIO_VECTOR_RESULT 38 CLASS_EXPORT_REQUEST 39 CLASS_EXPORT_RESULT 40 CLASS_IMPORT_REQUEST 41 CLASS_IMPORT_RESULT 42 APPLY_UPDATE_REQUEST 43 APPLY_UPDATE_RESULT 44 GET_UPDATE_STATUS_REQUEST 45 GET_UPDATE_STATUS_RESULT 46 GLOBAL_COMPARER_REQUEST 47 GLOBAL_COMPARER_RESULT 48 AUDIO_ALIGN_REQUEST 68 AUDIO_ALIGN_RESULT 69 TEXT_TRANSFORM_REQUEST 70 TEXT_TRANSFORM_RESULT 71 PREPROCESSED_AUDIO_RESULT 72 DYNAMIC_PLUGIN_REQUEST 73 PLUGIN_2_PLUGIN_REQUEST 74 PLUGIN_2_PLUGIN_RESULT 75 WORKFlOW_TEXT_RESULT 76 SCORE_OUTPUT_TRANSFORMER_REQUEST 77 SCORE_OUTPUT_TRANSFORMER_RESULT 78 DATA_OUTPUT_TRANSFORMER_REQUEST 79 DATA_OUTPUT_TRANSFORMER_RESULT 80 BOUNDING_BOX_REQUEST 81 BOUNDING_BOX_RESULT 82 BINARY_MEDIA_RESULT 83 START_STREAMING_REQUEST 84 START_STREAMING_RESULT 85 AUDIO_STREAM 86 REGION_SCORER_STREAMING_REQUEST 87 REGION_SCORER_STREAMING_RESULT 88 GLOBAL_SCORER_STREAMING_REQUEST 89 GLOBAL_SCORER_STREAMING_RESULT 90 TEXT_TRANSFORMER_STREAMING_REQUEST 91 TEXT_TRANSFORMER_STREAMING_RESULT 92 STOP_STREAMING_REQUEST 93 STOP_STREAMING_RESULT 94 FLUSH_STREAMING_REQUEST 95 FLUSH_STREAMING_RESULT 96 SYNC_STREAMING_REQUEST 97 DRAIN_STREAMING_REQUEST 98 DRAIN_STREAMING_RESULT 99 CANCEL_JOBS_REQUEST 100 CANCEL_JOBS_RESULT 101 INVALID_MESSAGE 102","title":"MessageType"},{"location":"apiMessage.html#optiontype","text":"Classifies how a OptionDefinition (TraitOption) should be represented in a UI widget Name Number Description BOOLEAN 1 CHOICE 2","title":"OptionType"},{"location":"apiMessage.html#traittype","text":"The list of possible traits that a plugin can implement Name Number Description GLOBAL_SCORER 1 REGION_SCORER 2 FRAME_SCORER 3 CLASS_ENROLLER 4 CLASS_MODIFIER 5 SUPERVISED_TRAINER 6 SUPERVISED_ADAPTER 7 UNSUPERVISED_ADAPTER 8 AUDIO_CONVERTER 9 AUDIO_VECTORIZER 10 CLASS_EXPORTER 11 UPDATER 12 LEARNING_TRAIT 13 GLOBAL_COMPARER 14 TEXT_TRANSFORMER 15 AUDIO_ALIGNMENT_SCORER 16","title":"TraitType"},{"location":"apiMessage.html#tasktype","text":"Name Number Description SAD 1 Speech Activity Detection SID 2 Speaker ID SDD 3 Speaker ID, but output in regions LID 4 Language ID LDD 5 Language ID, but output in regions KWS 6 Keyword Spotting TPD 7 Topic Detection VTD 8 Voice Type Discrimination GID 9 Gender ID GDD 10 Gender ID, but output in regions ASR 11 Automatic Speech Recognition ENH 12 Audio Enhancement CMP 13 Audio Comparison SDD 14 Speaker Detection DIA 15 Diarization QBE 16 Query by Example SHL 17 Speaker Highlighing FID 18 Face ID TMT 19 Text Machine Translation ALN 20 Audio Alignment","title":"TaskType"},{"location":"apiMessage.html#optiondefinition","text":"A plugin TraitOption, describing how a plugin trait is used Field Type Label Description name string required The name/ID of the option label string required Display label for the option desc string optional A description of the option type OptionType required The type of the option (boolean, choice/drop-down, etc) choice string repeated Optional list of choices used by CHOICE type options default string optional The default option in the list of Options","title":"OptionDefinition"},{"location":"apiMessage.html#optionvalue","text":"A name/value property pair Field Type Label Description name string required The name/ID of the option value string required The option value as a string","title":"OptionValue"},{"location":"apiMessage.html#inputdatatype","text":"Workflow(?) Input Data Types Name Number Description AUDIO 1 VIDEO 2 TEXT 3 IMAGE 4","title":"InputDataType"},{"location":"apiMessage.html#inputtype","text":"Name Number Description FRAME 1 REGION 2","title":"InputType"},{"location":"apiMessage.html#jobclass","text":"Field Type Label Description job_name string required The parent job name in a Workflow JobDefinition task TaskClass repeated","title":"JobClass"},{"location":"apiMessage.html#taskclass","text":"Field Type Label Description task_name string required The ID from the associated WorkflowTask (consumer_result_label) class_id string repeated Zero or more class IDs available to this task. Some tasks do not support classes class_label string optional An optional label/name to describe the classes used by this task such a 'speaker' or 'language' classes_label string optional The speaker label when refering to plural classes, such as speakers, or languages","title":"TaskClass"},{"location":"apiMessage.html#shared-types","text":"","title":"Shared Types"},{"location":"apiMessage.html#audio","text":"Represents an audio. Can either refer to a local file or embed an audio buffer directly. The path and audioSamples fields should be treated as mutually exclusive, with one and only one of these fields implemented For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description path string optional Path to the audio file represented by this record (if not specified then audio is input as a buffer) audioSamples AudioBuffer optional Audio included as a buffer (if not specified, then path must be set) selected_channel uint32 optional Optional - if using multi-channel audio and 'mode' is SELECTED, then this channel is provided to the plugin(s). regions AnnotationRegion repeated Optional annotated regions for this audio label string optional Optional - label used to identify this audio input","title":"Audio"},{"location":"apiMessage.html#annotationregion","text":"A single pair of timestamps (start and end) that make up an annotated region. Timestamps are in seconds. Field Type Label Description start_t uint32 required Begin-time of the region (in seconds) end_t uint32 required End-time of the region (in seconds)","title":"AnnotationRegion"},{"location":"apiMessage.html#audiobuffer","text":"Audio is contained in a buffer (and the path filed is NOT set in Audio) - by default the audio in the buffer should be PCM encoded, unless the buffer contains a serialized file (unencoded) in which case the serialized_file must be set to true. If the data has been decoded and is not PCM encoded data, then the encoding field must be specified For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description channels uint32 optional The number of channels contained in data, ignored for serialized buffers samples uint32 optional The number of samples (in each channel), ignored for serialized buffers rate uint32 optional The sample rate, ignored for serialized buffers bit_depth AudioBitDepth optional The number of bits in each sample, ignored for serialized buffers data bytes required Should be channels * samples long, striped by channels serialized_file bool optional Optional - true if data contains a serialized buffer encoding AudioEncodingType optional Optional - Not Yet supported - the audio encoding type. Assumed to be PCM if not specified. Ignored for serialized buffer","title":"AudioBuffer"},{"location":"apiMessage.html#audioencodingtype","text":"Audio encoding types Name Number Description PCMU8 1 PCMS8 2 PCM16 3 PCM24 4 PCM32 5 FLOAT32 6 FLOAT64 7 ULAW 8 ALAW 9 IMA_ADPCM 10 MS_ADPCM 11 GSM610 12 G723_24 13 G721_32 14 DWW12 15 DWW16 16 DWW24 17 VORBIS 18 VOX_ADPCM 19 DPCM16 20 DPCM8 21","title":"AudioEncodingType"},{"location":"apiMessage.html#audiobitdepth","text":"Number of bits in each audio sample Name Number Description BIT_DEPTH_8 1 BIT_DEPTH_16 2 BIT_DEPTH_24 3 BIT_DEPTH_32 4","title":"AudioBitDepth"},{"location":"apiMessage.html#binaryannotation","text":"Annotation for any binary media. Field Type Label Description task_label string optional When used in a workflow, restrict these annotations to this task (consumer_result_label). If not set, these annotaions are used for all task in a workflow regions AnnotationRegion repeated Audio annotations bbox BoundingBox repeated Optional regions for visual media region_label string optional An optional label this set of annotations, if not specified 'region' is used","title":"BinaryAnnotation"},{"location":"apiMessage.html#binarybuffer","text":"Support data being sent as a binary buffer. Field Type Label Description data bytes required frames_per_second uint32 optional The frames per second (if audio)","title":"BinaryBuffer"},{"location":"apiMessage.html#binarymedia","text":"A generic binary media type. Field Type Label Description path string optional Path to the data file represented by this record (if not specified then data is input as a buffer) buffer BinaryBuffer optional Data included as a serialized buffer (if not specified, then path must be set) regions BinaryAnnotation repeated Optional annotated regions for the data - repeated in case we need multiple region (like 'region', 'speaker') selected_channel uint32 optional If using multi-channel data then this channel should be used for processing. label string optional Label used to identify this data input","title":"BinaryMedia"},{"location":"apiMessage.html#boundingbox","text":"A bounding box. Field Type Label Description x1 int32 required Top left x y1 int32 required Top left y x2 int32 required Bottom right x y2 int32 required Bottom right y","title":"BoundingBox"},{"location":"apiMessage.html#metadata","text":"The parent container for Metadata so that typed name/value properties can be transported in a generalized way Field Type Label Description type MetadataType required Indicates the type of this metadata, so it can be deserialized to the appropriate type name string required The name (key) for this metadata value bytes required The value is one of MetadataType, must be deserialized by the client into the type specified by type","title":"Metadata"},{"location":"apiMessage.html#metadatatype","text":"Data types supported in an AudioModificationResult's metadata: Name Number Description STRING_META 1 INTEGER_META 2 DOUBLE_META 3 BOOLEAN_META 4 LIST_META 5","title":"MetadataType"},{"location":"apiMessage.html#booleanmetadata","text":"Value as boolean Field Type Label Description value bool required","title":"BooleanMetadata"},{"location":"apiMessage.html#doublemetadata","text":"Value as a double Field Type Label Description value double required","title":"DoubleMetadata"},{"location":"apiMessage.html#integermetadata","text":"Value as an integer Field Type Label Description value int32 required","title":"IntegerMetadata"},{"location":"apiMessage.html#listmetadata","text":"Value as list of Metadata values Field Type Label Description type MetadataType repeated The type for the corresponding element value bytes repeated The value is one or more MetadataType elements, each element must be deserialized by the client into the type specified by type. For example, for the type, STRING_META, deserialize data as StringMetadata","title":"ListMetadata"},{"location":"apiMessage.html#stringmetadata","text":"Value as a string Field Type Label Description value string required","title":"StringMetadata"},{"location":"apiMessage.html#scalar-value-types","text":".proto Type Notes C++ Type Java Type Python Type double double double float float float float float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long uint32 Uses variable-length encoding. uint32 int int/long uint64 Uses variable-length encoding. uint64 long int/long sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long sfixed32 Always four bytes. int32 int int sfixed64 Always eight bytes. int64 long int/long bool bool boolean boolean string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode bytes May contain any arbitrary sequence of bytes. string ByteString str","title":"Scalar Value Types"},{"location":"audioFormats.html","text":"OLIVE Supported Audio Formats Overview There are five main methods of interacting with the OLIVE system that carry different implications with respect to the support audio formats. They are as follows: NIGHTINGALE (Forensic) GUI \u2013 Submitting audio files using the Nightingale GUI, also known as the Forensic GUI. OLIVE (Batch) GUI \u2013 Submitting audio files using the OLIVE Batch GUI, also known as the Batch GUI or SCENIC Batch GUI, for submitting large file-based audio processing jobs. COMMAND LINE INTERFACE (CLI) TOOLS \u2013 submitting audio through the localenroll, localanalyze, localtrain command line tools. OLIVE API (BUFFERED) \u2013 sending pre-loaded memory buffers of audio samples to the server through the OLIVE API. OLIVE API (SERIALIZED) \u2013 sending a serialized object to the server that consists of an entire audio file with its header intact. These interaction methods can be combined into three groups that share limitations: Local file-based processing by server. Dictates compatibility for 2. OLIVE Batch GUI , 3. Command Line Interface Tools , and 5. OLIVE API (Serialized) . Audio files being opened and processed by Java. Dictates compatibility for 1. Nightingale GUI . Audio samples buffered into memory. Dictates compatibility for 4. OLIVE API (Buffered) . The limitations of each group are defined below. Local file-based processing by server The audio file compatibility for this group of OLIVE interactions is dictated by the libsndfile for reading and writing audio files. All files submitted to the OLIVE Batch GUI, through the localenroll, localanalyze, and localtrain command line tools, or as Serialized files through the OLIVE API can be of any audio file format and type supported by the libsndfile package. Stereo files are supported, but are merged into a single channel before scoring when submitting to the OLIVE Batch GUI and Command Line Interface Tools. When submitting files as serialized objects through the API, there is flexibility regarding how the channels are processed \u2013 please refer to the API Documentation for more details. Supported audio formats include: Microsoft WAV SGI/Apple AIFF/AIFC Sun AU/Snd Raw (headerless) Paris Audio File (PAF) Commodore IFF/SVX Sphere/NIST WAV IRCAM SF Creative VOC SoundForge W64 GNU Octave MAT4.4 Portable Voice Format Fasttracker 2 XI HMM Tool Kit HTK Apple CAF Sound Designer II SD2 Free Lossless Audio Codec (FLAC) Supported encodings vary by the format used (see the link below for a comprehensive compatibility table), but samples of several supported encodings are as follows: Unsigned and signed 8, 16, 24 and 32 bit PCM IEEE 32 and 64 floating point U-LAW A-LAW IMA ADPCM MS ADPCM GSM 6.10 G721/723 ADPCM 12/16/24 bit DWVW OK Dialogic ADPCM 8/16 DPCM More information on libsndfile supported audio formats can be found here: http://www.mega-nerd.com/libsndfile/#Features Audio files being opened and processed by Java Any files being opened in the Nightingale GUI for close analysis work must be able to be opened and read by the underlying Java code libraries: Java Media Framework JavaX.Sound FLAC Only mono files are currently supported. Supported sample rates include: 8 kHz multiples to 48 kHz 11.025 kHz multiples to 44.1 kHz Supported container formats: FLAC RIFF (.wav) AIFF AIFC AU Supported encoding formats: Compressed: FLAC PCM: 16 bit signed int, big or little endian 8 bit signed or unsigned int 32 bit float, little endian only (i.e. RIFF, not RIFX) 8 bit mulaw or alaw ADPCM: Microsoft or IMA ADPCM Audio samples buffered into memory Raw buffered audio samples being sent to the OLIVE server for enrollment or scoring are read and processed under the assumption that they are raw 16-bit Linear PCM sampes at an 8 kHz sampling rate. Serialized buffered audio files sent to the server are not processed by the client or assumed to be anything specific; rather they are interpreted by the server as a complete (header-intact!) audio file.","title":"Supported Audio Formats"},{"location":"audioFormats.html#olive-supported-audio-formats","text":"","title":"OLIVE Supported Audio Formats"},{"location":"audioFormats.html#overview","text":"There are five main methods of interacting with the OLIVE system that carry different implications with respect to the support audio formats. They are as follows: NIGHTINGALE (Forensic) GUI \u2013 Submitting audio files using the Nightingale GUI, also known as the Forensic GUI. OLIVE (Batch) GUI \u2013 Submitting audio files using the OLIVE Batch GUI, also known as the Batch GUI or SCENIC Batch GUI, for submitting large file-based audio processing jobs. COMMAND LINE INTERFACE (CLI) TOOLS \u2013 submitting audio through the localenroll, localanalyze, localtrain command line tools. OLIVE API (BUFFERED) \u2013 sending pre-loaded memory buffers of audio samples to the server through the OLIVE API. OLIVE API (SERIALIZED) \u2013 sending a serialized object to the server that consists of an entire audio file with its header intact. These interaction methods can be combined into three groups that share limitations: Local file-based processing by server. Dictates compatibility for 2. OLIVE Batch GUI , 3. Command Line Interface Tools , and 5. OLIVE API (Serialized) . Audio files being opened and processed by Java. Dictates compatibility for 1. Nightingale GUI . Audio samples buffered into memory. Dictates compatibility for 4. OLIVE API (Buffered) . The limitations of each group are defined below.","title":"Overview"},{"location":"audioFormats.html#local-file-based-processing-by-server","text":"The audio file compatibility for this group of OLIVE interactions is dictated by the libsndfile for reading and writing audio files. All files submitted to the OLIVE Batch GUI, through the localenroll, localanalyze, and localtrain command line tools, or as Serialized files through the OLIVE API can be of any audio file format and type supported by the libsndfile package. Stereo files are supported, but are merged into a single channel before scoring when submitting to the OLIVE Batch GUI and Command Line Interface Tools. When submitting files as serialized objects through the API, there is flexibility regarding how the channels are processed \u2013 please refer to the API Documentation for more details. Supported audio formats include: Microsoft WAV SGI/Apple AIFF/AIFC Sun AU/Snd Raw (headerless) Paris Audio File (PAF) Commodore IFF/SVX Sphere/NIST WAV IRCAM SF Creative VOC SoundForge W64 GNU Octave MAT4.4 Portable Voice Format Fasttracker 2 XI HMM Tool Kit HTK Apple CAF Sound Designer II SD2 Free Lossless Audio Codec (FLAC) Supported encodings vary by the format used (see the link below for a comprehensive compatibility table), but samples of several supported encodings are as follows: Unsigned and signed 8, 16, 24 and 32 bit PCM IEEE 32 and 64 floating point U-LAW A-LAW IMA ADPCM MS ADPCM GSM 6.10 G721/723 ADPCM 12/16/24 bit DWVW OK Dialogic ADPCM 8/16 DPCM More information on libsndfile supported audio formats can be found here: http://www.mega-nerd.com/libsndfile/#Features","title":"Local file-based processing by server"},{"location":"audioFormats.html#audio-files-being-opened-and-processed-by-java","text":"Any files being opened in the Nightingale GUI for close analysis work must be able to be opened and read by the underlying Java code libraries: Java Media Framework JavaX.Sound FLAC Only mono files are currently supported. Supported sample rates include: 8 kHz multiples to 48 kHz 11.025 kHz multiples to 44.1 kHz Supported container formats: FLAC RIFF (.wav) AIFF AIFC AU Supported encoding formats: Compressed: FLAC PCM: 16 bit signed int, big or little endian 8 bit signed or unsigned int 32 bit float, little endian only (i.e. RIFF, not RIFX) 8 bit mulaw or alaw ADPCM: Microsoft or IMA ADPCM","title":"Audio files being opened and processed by Java"},{"location":"audioFormats.html#audio-samples-buffered-into-memory","text":"Raw buffered audio samples being sent to the OLIVE server for enrollment or scoring are read and processed under the assumption that they are raw 16-bit Linear PCM sampes at an 8 kHz sampling rate. Serialized buffered audio files sent to the server are not processed by the client or assumed to be anything specific; rather they are interpreted by the server as a complete (header-intact!) audio file.","title":"Audio samples buffered into memory"},{"location":"cli.html","text":"OLIVE Command Line Interface Guide (Legacy) Disclaimer Note that the tools described below are legacy tools that are mostly used for internal testing and development. With docker-based deliveries, these utilities are difficult to access and have many performance tradeoffs versus using the OLIVE server through a client - they should not be used for integration, only for very basic experimentation. The functionality offered by these should instead be accessed through the provided Java example client (OliveAnalyze, OliveEnroll, etc.) or Python example client (olivepyanalyze, olivepyenroll, etc.). Documentation for these utilities is under construction and will be provided soon - but each utility has a help statement that provides instructions for running each. Introduction This document describes running the OLIVE (formerly SCENIC) system from a command line. Our command line applications are geared toward a variety of specialized users such as researchers, system evaluators (i.e. Leidos for the DARPA RATS program), and testers. Casual users should consider using our graphical application. However, our command line applications can function as general-purpose tools, but may require specially formatted files such the RATS XML files for audio analysis and LDC-format TSV files for training annotations. 1: Overview OLIVE command line interface (CLI) tools include: localenroll \u2013 Used to enroll \u2018targets\u2019 into the system, such as a target speaker for speaker identification (SID), a topic of interest for topic identification (TID), or a keyword or phrase of interest for query-by-example keyword spotting (QBE). localanalyze \u2013 Used to query the OLIVE server to score audio to find speech with a speech activity detection (SAD) plugin, report scores for potential speakers or languages of interest for SID or language identification (LID) plugins, report likelihood and location(s) of conversation topics or keywords of interest (TID, QBE, KWS). localtrain \u2013 Used to train or adapt plugins that support the LearningTrait (SupervisedAdapter, SupervisedTrainer, or UnsupervisedAdapter) with examples of new audio conditions to improve performance in such conditions. Also used to add new language recognition capabilities to a LID plugin, and to retrain the background models of a SID plugin to prepare it for new audio conditions. Training and adaptation are not available in all plugins, please refer to individual plugin documentation or plugin capabilities matrix to verify availability of training or adaptation. 2: Command Line Testing and Analysis A. Enrollment with localenroll The localenroll command is used to enroll audio for SID and TID. It can be invoked from a BASH or C-shell terminal. It takes a simply formatted text file as input and does not produce an output file. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TID, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in the Plugin Appendix. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7 The basic syntax for calling localenroll (more details and options below) is: $ ./localenroll <path_to_plugin_domain> <path_to_enrollment_file> Where an example of that may be: $ ./localenroll plugins/sid-embed-v1/domains/multi-v1/ /data/sid/smoke_enroll.lst The numerous options available in localenroll can be seen by executing localenroll --help, the output of which is shown below: usage : localenroll [- h ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path enrollment_file positional arguments : plugin_domain_path path to plugin domain used for analysis enrollment_file List of enrollments of the form < audio_path > < class_id > OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have a configuration section named enrollment . Only values from the enrollment section are read -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting B. Scoring and processing with localanalyze I. Invoking localanalyze The localanalyze utility is used to perform OLIVE scoring and analysis with most plugins (SAD, SID, SDD, LID, KWS, QBE, GID, TID), or processing with an ENH plugin, all on list-based input files. It can be invoked from a BASH or C-shell terminal. A path to a valid OLIVE plugin and domain as well as an audio paths input file are required for all tasks. For some plugins, like LID and SID, an optional IDs input file can be specified via the --class_ids argument to limit which languages or speakers are scored. This IDs input file is also how a keyword spotting plugin is informed what the keywords of interest are for a given analysis. The exact details for invoking localanalyze will depend upon the plugin technology being used, and may vary slightly depending upon the options available to each individual plugin, but the general format for running this utility is: $ localanalyze <path_to_plugin_domain> <list_of_files_to_analyze_or_process> With an example (SID): $ localanalyze plugins/sid-embed-v1/domains/multi-v1/ /data/sid/test_data.lst The format of the audio input file is simply a list of one or more newline-separated lines containing a path to an audio file: <audio_path> Example audio input file: /data/sid/test/unknownSpkr1.wav /data/sid/test/unknownSpkr27.wav As mentioned above, if you would only like to score a subset of the enrolled speakers or languages, you can optionally pass a list of these identifiers as a newline-separated list text file, with the --class_ids command line argument. This same argument is how you select keywords to search for when running localanalyze with a keyword spotting plugin (see KWS section in the Plugin Appendix for more information). IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Speaker Identification IDs example: Chris Billy Spkr3 A Keyword Spotting IDs example: turn left torpedo watermelon Example (KWS) of a localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/test-audio.lst Note that re-running localanalyze will overwrite the contents of the output.txt file or OUTPUT directory, depending on what type of plugin is being run. The OLIVE usage/help statement for localanalyze : usage : localanalyze [- h ] [-- output OUTPUT_PATH ] [-- thresholds THRESHOLDS ] [-- class_ids ID_LIST_PATH ] [-- options OPTIONS_PATH ] [-- regions REGION_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path audio_paths_file positional arguments : plugin_domain_path path to plugin domain used for analysis audio_paths_file List of audio files to analyze OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- output OUTPUT_PATH , - o OUTPUT_PATH path to output file or directory -- thresholds THRESHOLDS Optional comma - separated threshold values to apply to frame - level scores , e . g . 0.0 , 1.5 . Use syntax '-- thresholds=' for negative values , e . g -- thresholds =- 2.0 ,- 1.0 -- class_ids ID_LIST_PATH , - i ID_LIST_PATH Optional file that specifies class ids to be scored . E . g . limit the speakers that scored . -- options OPTIONS_PATH Optional file containing plugin specific name / value pairs . The option file may have more or more section headings s for each plugin type . Common section names are 'frame scoring' , , 'global scoring' or 'region scoring' -- regions REGION_PATH , - r REGION_PATH Optional flag indicating that the audio paths file should be supplemented with regions from a PEM formated file , it is up to the plugin to utilize these regions to supplement its scoring . This flag is ignored if the audio input list ( audio_paths_file ) is a PEM formatted file . -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting II. Output Plugin Scoring Types In general, the output format and location of a call to localanalyze will depend on the type of \u2018scorer\u2019 the plugin being used is. There are currently four types of plugins in OLIVE: Global scorer Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Currently SID, LID, and GID are the only global scoring plugins. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Example \u2013 sid-embed-v1, lid-embed-v1 Region scorer Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. TID, SDD, QBE, and KWS are all region scorers. Example \u2013 sdd-embed-v1, qbe-tdnn-v4, kws-batch-v9 Frame scorer A frame scoring plugin provides a score for every \u2018frame\u2019 of audio within every test file passed to localanalyze . This allows OLIVE to find distinct regions of speech with high precision in recordings with noise and/or silence. SAD is a frame scoring plugin. It is also possible to apply a threshold to a frame scoring plugin at run-time to report regions of detection instead of frame scores. For a plugin like SAD, this allows OLIVE to provide output in the form of speech regions. A frame is a short segment of audio that typically consists of 10 milliseconds of audio (100 frames per second). Example \u2013 sad-dnn-v4 Audio to audio This plugin takes an audio file as input, and also returns an audio file as output. Currently the only plugins that fall into this category are speech/audio enhancement plugins, where the goal is removing noise and distortion from an audio file to improve the human listening experience and intelligibility. Example \u2013 enh-mmse-v1 Global Scorer Output In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> The name and location of the output file can be overridden by passing it as the argument to the -o or --output argument when calling localanalyze . To see specific examples for each plugin type, please refer to the appropriate section of the Plugin Appendix. Region Scorer Output Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: < audio_path > < region_start_timestamp > < region_end_timestamp > < class_id > < score > Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. Specific examples can be found in the Plugin Appendix at the end of this document. Frame Scorer Output In the case of frame scorers like SAD, an output file is generated for each audio input file, where each audio output file contains a score for each frame in the audio input. There is one frame score per line. Alternatively, an option exists to produce segmentation scores from SAD results by using the --threshold argument. When using the --threshold argument, the output file adheres to standard 5-column PEM format. Without supplying a threshold to localanalyze , the frame scorer output looks like this: <frame_1_score> <frame_2_score> \u2026 <frame_N_score> When a threshold is provided, the output file will resemble the following: < filename > , < channel > , < label ( \u201c speech \u201d ) > , < speech region start time ( seconds ) > , < end time ( seconds ) > Audio to Audio Output An audio-to-audio plugin takes an audio file as input and returns a corresponding audio file as output. Currently, this plugin type is used to supply enhancement capabilities to OLIVE, to allow OLIVE to improve the quality, intelligibility, or just general human listening experience for an audio file. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav 3: Command Line Field Adaptation A. Command Line Field Adaptation Overview In general, training and adaptation are very resource and time intensive operations. Very large amounts of RAM are used at certain steps in training. When attempting to train or adapt, the machine should be dedicated to that operation. If the plugin path contains a domain then adaptation is implied, otherwise training is implied. The high-level difference between training and adaptation is that adaptation will use the new data supplied during adaptation in addition to the data already used to train the model used by the plugin/domain. Training, on the other hand, ignores the data originally used for training a model and retrains from scratch using only the new data provided. When performing training, none of the data in the base plugin will be used, but the feature configs will. Check the plugin\u2019s traits to determine if full training and/or adaptation are supported. B. Invoking localtrain Not to be confused with enrollment, the localtrain command line application is used to perform field adaptations for SAD, LID & SID. localtrain takes a plugin or plugin_domain path, and one or more data input files formatted for: Unsupervised data - a newline separated list of audio file paths Supervised data with file level annotations - a newline separated list of audio files paths with a class Id (i.e. \u201caudio_file1.flac fas\\n\u201d) Supervised data with region level annotations - a newline separated list of audio file paths, start time (seconds), end time (seconds), and class ID (i.e. \u201caudio_file1.flac 1.25 3.5 fas\\n\u201d) If multiple data files are specified then they must all use the same annotation format. The localtrain utility outputs a new domain in the plugin path. The details of the localtrain executable are below: usage : localtrain [- h ] -- domain - id DOMAIN_ID [-- overwrite ] [-- preprocess ] [-- finalize ] [-- unique ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_or_domain_path data [ data ...] Train or adapt OLIVE audio recognition systems positional arguments : plugin_or_domain_path Path to the plugin or domain . A plugin path implies full training . A domain path implies adaptation of the specified domain . data paths to data files for training / adapation . The files can have one of three forms . 1 : < audio_path >\\ n 2 : < audio_path > < class_id >\\ n 3 : < audio_path > < class_id > < start > < end > \\ n . The first form has no annotations and implies unsupervised . The second form provides for file - level annotations while the third form supports region - level annotations . Start and end times should be in seconds . If multiple files are specified , they must have the same form . optional arguments : - h , -- help show this help message and exit -- domain - id DOMAIN_ID The id of the new domain you 're creating through training or adaptation. Should be a string that is somewhat descriptive of the conditions --overwrite Forcefully overwite an existing domain --preprocess Pre-process audio only, do not finalize training/adaptation --finalize Pre-process audio only, do not finalize training/adaptation --unique gurantees log files are written to unique directoires/files. Helpful when running in SGE mode --options OPTIONS_PATH Optional file containing plugin specific name/value pairs. The option file must have one or more sections for each plugin type. Common section names are ' supervised trainer ', ,' supervised adapter ', ' unsupervised trainer ' or ' unsupervised adapter ' --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- version show program ' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting In order to use an adapted system plugin, simply pass the full path of the domain generated by localtrain to localenroll or localanalyze as the plugin_domain_path argument. For training, do not include the domain in the plugin path. When running on a SGE, you may split the audio processing from the finalization step by using the --preprocess flag to first pre-process audio files, then use invoke localtrain with the --finalize argument to finalize training. Guidelines for the minimum amount of audio data required to successfully execute localtrain are listed in the table below. Task Operation Speech Duration SAD Adapt to new channel 1h LID Adapt to new channel 20m LID Train a new language 3h SID Adapt to new channel 1h i. Examples SAD Adaptation Example: $ localtrain ./plugins/sad-dnn-v1/domains/ptt-v1/ adaptation-data.lst Where each line of adaptation-data.lst has the following format: /path/to/audio.wav label C. LID Training/Adaptation When training new channel conditions, it is recommended to train all supported languages in the LID model to produce the best results. The out of set language is labeled as \u2018xxx\u2019. Use this language ID when training to add languages that you do not want to target in the LID task but are known to be in the test dataset. 4: Log Files a. OLIVE Command Line Logging When executing localtrain , localenroll , and localanalyze , here are three named log files that may be of interest should something go awry. The top-level log file: This log file corresponds to the -l option to the localtrain , localenroll , and localanalyze utilities. By default, it is named the same as the utility being used with \u201c.log\u201d appended (i.e. localanalyze.log when running localanalyze ) and will be written to the directory from which you executed the utility. The pool executor log file: This file will be written to [work_directory]/logs/pool_executor.log, where work_directory corresponds to the -w option to localtrain / localenroll / localanalyze and defaults to your current directory/WORK. The pool executor log file is the best log file to look at if unexpected errors occur. It corresponds to our internal job scheduler also known as the pool executor. The pool monitor log file: This file will be written to [work_directory]/logs/pool_monitor.log, where work_directory corresponds to the -w option to the localtrain , localenroll , localanalyze utilities and defaults to your current directory/WORK/. This log contains stats about memory and CPU utilization. All three of these log files will exhibit log rotation behavior. In the event of errors, [work_directory]/logs may also contain log files named [order_id].failed , where order_id generally corresponds to the file names of the audio files being used for adaptation/training, enrollment, or analysis. The id can be used to tie errors in the pool executor log file to the \u201c.failed\u201d log files. If you run the OLIVE CLI utilities in debug mode (-d), all log files will be maintained, even if they were successful. b. Rotating Log Files OLIVE employs rotating log files in many places. In this context, rotating refers to a log file that is rewritten each time the application is run. The old log file, if any, is renamed with an integer suffix denoting how many invocations in the past it corresponds to. For instance, if you run localanalyze and don\u2019t specify a -l option, you\u2019ll get the default localanalyze.log file. If localanalyze.log already exists, it is moved to localanalyze.log.1 . The system will keep the 10 most recent log files. A file named localanalyze.log.8 means that the file corresponds to eight invocations ago. 5: Plugin Appendix Plugin Types and Acronyms Currently, OLIVE supports the plugin technologies listed in the following list. For operating instructions that apply to only a specific technology, refer to that section within this appendix. SAD \u2013 Speech activity detection. SID \u2013 Speaker identification. LID \u2013 Language identification. KWS \u2013 Keyword spotting. QBE \u2013 Query by example based keyword spotting. TID \u2013 Topic identification. SDD \u2013 Speaker diarization and detection. GID \u2013 Gender identification. ENH \u2013 Speech and audio enhancement. Speech Activity Detection (SAD) SAD plugins are frame scorers that take an audio list file and annotate the presence and location of speech in each audio file in that list. In standard operation, SAD plugins produce a single output file for each input file, by default in a directory called OUTPUT in the location localanalyze was called from. Output files carry the name of the original input file, but with a new extension \u201c.scores\u201d \u2013 for example, audioFile1.wav will become audioFile1.wav.scores, saved inside OUTPUT/. The format of these results files is a newline separated list of numerical values representing the likelihood that each 10ms frame of the audio file contains speech. Typically, a score above 0 represents speech detection, and a score below 0 represents no speech. SAD analysis example: $ localanalyze /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst Output files: OUTPUT/audioFile1.wav.scores OUTPUT/audioFile2.wav.scores Example audioFile1.wav.scores contents: -0.22624 -0.10081 0.00925 0.12365 Alternatively, SAD plugins can be run with the --thresholds flag to have localanalyze automatically convert the frame scores to regions of speech, by applying the provided threshold. SAD analysis example using thresholds: $ localanalyze --thresholds = 0 .0 /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst This will provide a single output file in OUTPUT directory corresponding to the provided threshold: 0.0.pem. If more than one threshold is provided, there will be a PEM file placed into OUTPUT corresponding to each provided threshold. Example PEM output: /data/sad/test/audioFile1.wav 1 speech 63.110 66.060 /data/sad/test/audioFile1.wav 1 speech 66.510 69.230 /data/sad/test/audioFile1.wav 1 speech 93.480 96.090 /data/sad/test/audioFile1.wav 1 speech 96.570 100.760 Note that if negative thresholds are to be used, it is very important to specify the thresholds using an \u2018=\u2019 character. For example, this threshold specification is valid: --thresholds=-2.0,4.0 And this is not valid: --thresholds -2.0,4.0 If only thresholds of 0 or above are going to be used, it is acceptable to omit the equals sign. Speaker Identification (SID) SID plugins are global scorers that take an audio list file and return a score for each enrolled speaker model scored against the audio in each input audio file. Generally, a score above 0 for an enrolled speaker model represents that speaker being detected in the respective audio file. In order to perform analysis on a file with a SID plugin you must first enroll one or more target speakers. The enrollment list file for a SID plugin follows this format for each line: <audio_file_path> <speaker_id> An example enroll.lst: /data/spkr_example_audio_5760.wav UIM1 /data/spkr_example_audio_5761.wav UIM1 /data/spkr_example_audio_5762.wav John /data/spkr_example_audio_5763.wav John Enrolling these speakers with localenroll : $ localenroll /path/to/plugins/sid-embed-v2/domains/multi-v1/ enroll.lst Example localanalyze call: $ localanalyze ./plugins/sid-embed-v2/domains/multi-v1/ ./data/sid/test/testAudio.lst By default, the output of this call is written to output.txt in the directory the command was run. The format of output.txt contains one line for each enrolled speaker model, for each input audio file, and the corresponding score: <audio_file_path> <speaker_id> <score> Example output.txt: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Trial-based Calibration Speaker Identification (SID TBC) Trial-based Calibration SID plugins are identified by \u2018tbc\u2019 in the plugin name. They are used very similarly to a basic SID plugin, using localenroll and localanalyze just like the examples shown in the previous example. The benefit of TBC plugins is that they allow OLIVE to perform calibration at test-time, based on the actual data conditions being encountered, rather than being forced to use a single, global calibration model that has been trained a priori. The basics of TBC enrollment and testing follow the previous SID examples, but the additional options and outputs available to TBC are detailed below. The standard approach to calibration uses a \u201cone size fits all\u201d calibration model based on the developer\u2019s best understanding of potential operating conditions. This is problematic when the user either doesn\u2019t know ahead of time what likely conditions are, or when operating conditions may vary widely. Trial-based calibration was developed as a means of providing calibration that is responsive to the particular conditions of a trial, and adapts its calibration model based on the conditions encountered. There are two ways we have developed to do this. The first draws from a pool of available data (either provided by the developer or augmented with user-provided data) and uses measures of the conditions found within this data and the trial conditions to build an ideal calibration set on the fly. This is useful in that this approach can also determine when a trial CANNOT be calibrated, and to measure the success of calibration when it is possible. The clear downside of this approach is that it is quite slow. A second approach to TBC is to use a model that has used a DNN to learn to predict both calibration parameters and confidence from large sets of trials and available calibration data. This approach is very fast (about 5000 times faster than the first approach) but has the downside that expanding the calibration set by the user\u2019s data isn\u2019t possible. This plug-in provides both approaches in the TBC plug-in, as two separate domains. In addition to the output score file detailed in the SID section, TBC plugins have additional possible outputs. Speech Detection Output Segmentation files are used to label time regions in the speech signal. We use this format for voice activity detection (VAD). If an output_ivs_dump_path is provided as an option to localenroll or localanalyze , the system will produce this file in a folder corresponding to the wav_id for all registered waveforms. The format is the following: md5sum start end ( in seconds ) Example: b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.060 0.060 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.090 0.090 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.110 0.170 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.200 0.200 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.560 3.550 Persistent I-Vectors For both localenroll and localanalyze , if the output_ivs_dump_path is defined via an options file with the --options flag, an i-vector from each audio file is saved in this directory for re-use. This reduces unnecessary computation when there exists overlap between lists of wave files to be processed. For instance, if the enroll and test wave file lists are identical (i.e., the case of an exhaustive comparison of a set of files), i-vector persistence will reduce overall computation by almost a factor of 2 since i-vector extraction consumes more of the computation required for an evaluation. I-vectors will be saved in a sub-directory of output_ivs_dump_path based on the base name of the wave file. In addition to this optional feature, the enrollment vectors are loaded in memory prior to verification and if the md5sum of a test audio file matches one used in the enrollment process, the corresponding vector will be used instead of re-processing the audio. This is because vector extraction is identical between enrollment and verification. Trial-based Calibration Trial-based calibration (TBC) does not change the way calibration works but changes the way calibration is used. It relaxes the constraint on the system developers to train a calibration model that is ideally matched to the end use conditions. Rather than train a calibration model a priori, the system postpones this training until the conditions of the particular verification trial are known to the system; a trial consists of comparing test audio to an enrolled speaker model. The goal of trial-based calibration is to use information about the trial to generate an ideal calibration set for the trial conditions using the reservoir of possible calibration audio files available. Using this set, a calibration model tailored to the conditions of the trial can be trained and used to effectively calibrate the verification score. The TBC operation and output differs from traditional SID plugins; it may choose to reject a trial and NOT output a score if insufficient data is available for calibrating for those conditions. For instance, the output may look similar to the following: waves/T6_ACK2.sph T6 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00) waves/T6_ACK3.sph T1 0.0 -inf Unable to calibrate with only 12 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T2 0.0 -inf Unable to calibrate with only 3 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T3 0.0 -inf Unable to calibrate with only 0 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T4 0.0 -inf Unable to calibrate with only 2 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T6 0.0 -inf Unable to calibrate with only 9 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T4_ACK3.sph T5 0.96153318882 4.04821968079 Used 95 target trials in calibration waves/T4_RC.sph T4 3.46785068512 4.07499170303 Used 95 target trials in calibration waves/T5_Tip2.sph T5 8.90770149231 4.07352733612 Used 98 target trials in calibration waves/T5_RC.sph T5 10.2386112213 4.03855705261 Used 47 target trials in calibration waves/T4_Tip2.sph T4 10.8663234711 4.07404613495 Used 218 target trials in calibration waves/T4_Tip1.sph T4 11.793006897 3.98730397224 Used 164 target trials in calibration waves/T4_ACK2.sph T4 11.8091144562 3.90610170364 Used 119 target trials in calibration waves/T4_ACK1.sph T4 12.2115001678 4.16342687607 Used 208 target trials in calibration waves/T5_ACK1.sph T5 13.8099250793 3.99625587463 Used 99 target trials in calibration waves/T5_Tip1.sph T5 14.9411458969 3.96994686127 Used 83 target trials in calibration waves/T4_ACK3.sph T4 16.003446579 4.05554199219 Used 146 target trials in calibration The output follows the structure: <testwave> <modelid> <score> <confidence> <info> In the instance of insufficient calibration segments being located, a score of 0.0 and a calibration confidence of -inf is given. In contrast, when sufficient data is found for calibration, the number of segments used in calibration is reported. There exist two options for applying calibration with the current plugin: DNN-assisted TBC, normal TBC, or global calibration. Each of these options use duration information to reduce the impact of duration variation impacting calibration performance. Note that changing calibration domains does NOT require re-enrollment of models as these are done in a domain-independent way for any TBC-enabled plugin. DNN-assisted Trial-based Calibration DNN-assisted TBC is invoked by passing the tbcdnn-v1 domain to localanalyze . This is a very fast and newly pioneered effort by SRI to reduce the computation needed to apply dynamic calibration methods to speaker recognition and operates with very low overhead compared to global calibration, and yet significantly benefits calibration performance in varying conditions or conditions that differ from the development conditions. localanalyze ... <plugin>/domains/tbcdnn-v1 test.lst Normal Trial-based Calibration TBC is applied by default with the 'sid-embedDnnTbc-v1' plugin. The data within the domain (such as 'tbc-v1') is used as candidate calibration data. localanalyze ... <plugin>/domains/tbc-v1 test.lst TBC is applied to verification scores on a trial-by-trial basis. As such, verification using TBC will operate at a speed much slower than global or DNN-assisted TBC depending on the size and make-up of the TBC data. This should be considered when using TBC in a cluster environment where it is the number of trials (model vs test comparisons) that determine the running time instead of the number of test files. Global Calibration Each domain can be used to invoke global calibration. This is particularly useful for user-defined data as it provides a rapid means of improving calibration performance without a dramatic increase in computation time. In this case, verification will operate at a much faster pace since TBC is essentially disabled and the global calibration model parameters are applied to all scores. In order to invoke global calibration, and optional parameter must be passed to localanalyze via an options file: echo \"[global scoring] global_calibration = True \" > options.lst localanalyze -- options options . lst ... < plugin >/ domains / tbc - v1 test . lst Optional Parameters The TBC-based plugins offer several tunable parameters via the options parameter to localenroll or localanalyze . These can be passed to the enrollment phase or verification phase by preceding the options in an ascii text file as such: $ cat options . lst [ enrollment ] ... enrollment options per line ... [ global scoring ] ... verification options per line ... The optional parameters and their purpose are provided below. tbc_confidence_threshold = 3 . 0 , # Similarity threshold for processing a trial with TBC score_threshold = 0 . 0 , # Score offset subtracted from output LLRs to assist in making 0 threshold output tgt_max = 300 , # The maxmimum number of target trials used for TBC of a trial imp_max = 3000 , # The maxmimum number of impostor trials used for TBC of a trial tgt_imp_min = 20 , # The mimum number of relevant target and impostor calibration trials needed to use TBC ( rejected otherwise ) global_calibration = False , # Apply global calibration instead of TBC ivs_dump_path = None , # Output path for dumping vectors and meta information sad_threshold = 0 . 5 , # Threshold for speech activity detection ( higher results in less speech ) sad_filter = 1 , # Smoothing of LLRs from SAD DNN prior to thresholding sad_interpolate = 1 , # If > 1 , a speed up of SAD by interpolating values between frames ( 4 works well ) Utilizing these parameters in an options file may look like this: echo \"[enrollment] sad_threshold = 1.0 ivs_dump_path = . / embeddings [ global scoring ] sad_threshold = 1.0 ivs_dump_path = . / embeddings tgt_max = 100 tgt_imp_min = 50 \" > options.lst localenroll -- options options . lst ... < plugin >/ domains /< domain > enroll . lst localanalyze -- options options . lst ... < plugin >/ domains /< domain > test . lst Verification Trial Output The format for the verification trial is the following. Note that for global calibration, the optional parameters (calibration_confidence and calibration_remarks) are not output. Output format: wav_id speaker_id score [ calibration_confidence calibration_remarks ] Here is an example of score executed with Global Calibration: waves/T1_ACK1.sph T6 5.19274568558 waves/T1_ACK1.sph T4 1.204241395 waves/T1_ACK1.sph T5 1.69025540352 Here is an example of scores executed with DNN-assisted TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used DNN-assisted TBC with confidence 5.751 waves/T1_ACK1.sph T4 1.204241395 5.12 Used DNN-assisted TBC with confidence 3.122 waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with confidence above threshold (3.00) Here is an example of scores executed with normal TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used 67 target trials in calibration waves/T1_ACK1.sph T4 1.204241395 5.12 Used 73 target trials in calibration waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00) Speaker Diarization and Detection (SDD) The overall goal of SDD plugins is to detect regions of speech within an audio recording that are associated with different speakers, and then identify those speakers if possible. SDD plugins have three different modes of operation, as outlined below. Changing the execution mode for SDD is done by passing an options file to localanalyze as an argument to the --options flag. The main behavior and premise of the technology and plugin remain the same, but each mode changes the format and information contained in the output file. Running the SDD plugin is very similar to running SID plugins, with the same syntax for enrollment and testing. Currently, training or adaptation through localtrain is not supported, but enrolling new speakers and testing against enrolled speakers is as simple as: $ localenroll /path/to/plugins/sdd-embed-v1/domains/multi-v1/ enrollmentAudio.lst $ localanalyze /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst The enrollment and test audio file lists in this example follow the same format as the lists used by SID plugins, described above. By default, if run as above with no options, the plugin will run in Speaker Detection mode, and provide the output described above. In order to run in SID or SID Exhaustive mode, you will need to provide an options file to specify that behavior: $ localanalyze --options options.lst /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst Where options.lst is a text file with contents similar to: [region scoring] mode: SID_EXHAUSTIVE sad_threshold: 0.0 diarization_max_num_speakers: 2 The [region scoring] header is alerting the plugin that the options are being passed for scoring, and all of the parameters shown above (sad_threshold, diarization_max_num_speakers, mode) are optional parameters. The mode option controls the output behavior as described above, and the possible options are SID, SID_EXHAUSTIVE, and SPEAKER_DETECTION, and described directly below. The sad_threshold defaults to 2.0, and is used to fine tune the threshold for the internal speech activity detection plugin if necessary. The parameter diarization_max_num_speakers defaults to 4, and is the largest number of speakers the plugin will attempt to consider when clustering segments within the file. SDD Execution Modes To facilitate the understanding of each mode\u2019s output, consider that speech from an audio file is made up of clusters of speakers, and each cluster will have one or more contiguous segment of speech. SPEAKER_DETECTION The goal of Speaker Detection is to show the most probable speaker model for each segment of the input audio file. As output, this mode gives one line per segment within the file, along with the top scoring enrolled model for the cluster that segment belongs to, and that cluster's score for the given model. Note that many scores will be repeated in the output file, since each segment in the cluster shares the same score for a given speaker model. This mode is performed by default if no options file with a mode override is given. SID This mode is meant for triaging large amounts of audio files when the main goal is just finding which of these files may contain speech from one of the enrolled speakers. The output is the maximum score for each enrolled speaker within the audio file after scoring against each cluster in the file, as well as the timestamps for the beginning and end of the longest segment within the cluster that scored the highest for that model. This gives a specific segment to spot check and evaluate the plugin's decision if needed. SID_EXHAUSTIVE When using SID Exhaustive, each diarized cluster is scored against each enrolled model. The output is a complete listing for every speech segment of the input audio file, the score from testing every enrolled model against the cluster that the segment belongs to. Many scores will be repeated in the output file, since each segment in the cluster shares the same score. In this example, Chris and Jimmy are the only enrolled models, and 5 total segments were identified within the file. Language Identification (LID) LID plugins are global scorers that act very similar to SID with respect to scoring, except that each score corresponds to a language model rather than a speaker model. In most cases, LID plugins will be delivered from SRI with a set of languages already enrolled. Languages can be added to some plugins by the user if enough appropriate data is available, through the localtrain CLI call. Details on this will be added to this document in a later revision. Example localanalyze call: $ localanalyze /path/to/plugins/lid-embed-v2/domains/multi-v1/ testAudio.lst Output format: <audio_file_path> <language_id> <score> Output example: /data/lid/audio/file1.wav fre -0.5348 /data/lid/audio/file1.wav eng 3.2122 /data/lid/audio/file1.wav spa -5.5340 /data/lid/audio/file1.wav rus 0.5333 /data/lid/audio/file1.wav ara -4.9444 /data/lid/audio/file2.wav fre -2.6564 Keyword Spotting (KWS) KWS is an automatic speech recognition (ASR) based approach to detecting spoken keywords in audio. Rather than enrolling target keywords from audio, as you would with query-by-example, telling the plugin what keywords to search for is done by passing an IDs file to localanalyze . The format of the IDs file is: IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Keyword Spotting IDs example, search_list.lst: remote torpedo voice recognition Example KWS localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst /path/to/plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/testAudio.lst The output format for KWS plugins is identical to that of QBE. It is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/kws/testFile1.wav 7.170 7.570 remote 1.0 /data/kws/testFile1.wav 10.390 10.930 remote 0.693357 /data/kws/testFile1.wav 1.639 2.549 voice recognition 1.0 Automatic Speech Recognition (ASR) Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. ASR plugins do not require any enrollment of words (like QBE) or specification of words of interest (like traditional KWS), but instead rely on the vocabulary model built into the domain to define the list of available words. All that is necessary for scoring an audio file for ASR is a list of input files to be scored, which follows the format below. Generic input audio list format: <audioFile_1> <audioFile_2 (opt)> ... <audioFile_N (opt)> A specific example of this, called testAudio.lst, might look like: /data/asr/testFile1.wav /data/asr/testFile2.wav /data/asr/testFile3.wav Note that if the files are not contained within the directory that localanalyze is being run from, or if a relative path from that location is not provided, the full file path to each file is necessary. An example ASR localanalyze call: $ localanalyze /home/user/oliveAppData/plugins/asr-dynapy-v1/domains/eng-tdnnChain-tel-v1/ /data/asr/test/testAudio.lst The output format for KWS plugins is identical to that of QBE and other region-scoring OLIVE plugins. ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. Output is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <word> <score> An example in English: /data/asr/testEnglish1.wav 0.000 0.190 and 43.00000000 /data/asr/testEnglish1.wav 0.210 0.340 we're 44.00000000 /data/asr/testEnglish1.wav 0.330 0.460 going 97.00000000 /data/asr/testEnglish1.wav 0.450 0.520 to 97.00000000 /data/asr/testEnglish1.wav 0.510 0.940 fly 66.00000000 /data/asr/testEnglish1.wav 1.080 1.300 was 31.00000000 /data/asr/testEnglish1.wav 1.290 1.390 that 24.00000000 /data/asr/testEnglish1.wav 1.290 1.390 it 22.00000000 /data/asr/testEnglish1.wav 1.380 1.510 we're 27.00000000 /data/asr/testEnglish1.wav 1.500 1.660 going 97.00000000 /data/asr/testEnglish1.wav 1.650 1.720 to 98.00000000 /data/asr/testEnglish1.wav 1.710 1.930 fly 94.00000000 /data/asr/testEnglish1.wav 1.920 2.110 over 79.00000000 /data/asr/testEnglish1.wav 2.100 2.380 saint 93.00000000 /data/asr/testEnglish1.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: /data/asr/testMandarin1.wav 0.280 0.610 \u6218\u6597 99.00000000 /data/asr/testMandarin1.wav 0.600 0.880 \u7206\u53d1 98.00000000 /data/asr/testMandarin1.wav 0.870 0.970 \u7684 99.00000000 /data/asr/testMandarin1.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 /data/asr/testMandarin1.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 /data/asr/testMandarin1.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 /data/asr/testMandarin1.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 /data/asr/testMandarin1.wav 3.130 3.340 \u7684 100.00000000 /data/asr/testMandarin1.wav 3.330 3.720 \u6b66\u88c5 55.00000000 /data/asr/testMandarin1.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: /data/asr/testFarsi1.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 /data/asr/testFarsi1.wav 0.470 0.740 51.00000000 \u0627\u06cc /data/asr/testFarsi1.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 /data/asr/testFarsi1.wav 0.310 0.460 99.00000000 \u0645\u06cc /data/asr/testFarsi1.wav 0.450 0.680 99.00000000 \u06af\u0645 /data/asr/testFarsi1.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f /data/asr/testFarsi1.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Query by Example Keyword Spotting (QBE) Query by example is a specific type of keyword spotting plugin that searches for keywords matching a spoken word or phrase example, rather than from a text example like traditional KWS. This means that it is necessary to enroll keywords into the system from audio examples with localenroll before using QBE to search audio for these keywords or phrases. Enrollment follows the same format as enrolling speakers into a SID plugin, with the enrollment audio list following this format: <audio_file_path> <keyword_id> Example: /data/qbe/enroll/watermelon_example1.wav Watermelon /data/qbe/enroll/watermelon_example2.wav Watermelon /data/qbe/enroll/airplane_example.wav Airplane /data/qbe/enroll/keyword_example.wav Keyword Note that currently each enrollment audio file must contain ONLY the keyword that is desired to be enrolled. Also note that the text label in the second column of the enrollment file is only for user readability and is not used by the system when determining what to search the audio file for. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006 Topic Identification (TID) Topic Identification plugins attempt to detect and categorize the topic being discussed within an audio recording from a known and pre-enrolled set of available topics, and report this topic (if any are detected) to the user. Each domain of a TID plugin is language-dependent, and should display the target language as the first string of the domain\u2019s name. Some TID plugins may be delivered with pre-enrolled topics \u2013 please consult the documentation that accompanied your delivery if you are unsure if this is the case. If no topics are enrolled, however, or if you wish to enroll new topics, the format is very similar to enrolling new speakers into a SID plugin, and follows the same CLI call structure, with one caveat. $ localenroll --local $domain $enroll_list Due to limitations with the current TID technology, enrollment must be performed with the --local flag set. This limits OLIVE to serialized processing, which will process the enrollment slightly slower, but avoid resource-competition issues that may cause the system to become unstable and crash. Enrollment audio lists for TID follow the same format as SID, but substitute a topic name for a speaker label. Note: in the current version, we require the user to provide audio examples that are primarily about the topic of interest as enrollment examples. If there are significant portions of an audio file that are off-topic, we suggest the file be cut and fed as separate examples. Enroll list format: <audio_file_path> <topic_id> Example: /tid/enroll/topic_example_audio_5760.wav Travel /tid/enroll/topic_example_audio_5761.wav Travel /tid/enroll/topic_example_audio_5762.wav Travel /tid/enroll/topic_example_audio_5763.wav Travel To run TID, once target topics have been enrolled, the call to localanalyze is very similar to other plugin types, with the current plugin again requiring the --local flag. $ localanalyze --local /path/to/plugins/tid-svm-v2/domains/r-tel-v1/ testAudio.lst As with the SID and LID plugins, by default the plugin\u2019s output will be written to the file \u201coutput.txt\u201d in the directory localanalyze was called from. This can be overridden by passing localanalyze the -o flag, as well as an alternate file to save the results to. The TID results structure is very similar to KWS, with the following format for each line: <audio_file_path> <start_time_s> <end_time_s> <topic_id> <confidence_score> Example: /data/tid-example_wavs/20110511_GET-TOGETHER.wav 18.790 55.300 transportation 0.0222 /data/tid-example_wavs/20110511_TRANSPORTATION.wav 4.010 19.140 transportation 0.4532 The start and end timestamps above are provided in seconds. The will be displayed as it was supplied in the second column of the enrollment list. The will be between 0 and 1, and marks the confidence of the system in the decision of this topic. Please note that output.txt will be overwritten by each successive experiment. Please back it up or use the -o option to localanalyze if you would like to save the results. Also note that the start and end times for each topic refer to the chunk in each audio that has the highest probability of being about that topic. Currently, we report only ONE such segment per file in order to help the user locate the most useful part. The score associated with that segment is global, in that it represents the likelihood that this topic is present anywhere in the document. Important Background Example Information In order to train a Topic detector, we currently use an SVM classifier. This model uses \"positive\" examples of the topic as provided by the user during the enrollment phase, as well as \"negative\" examples to model what is not the topic. Those negative examples can be crucial to the performance of the final system. Currently, those \"negative\" examples come pre-processed as a python numpy archive and cannot be modified by the user explicitly. We do provide two different numpy archives that can be tried by a user: BG_RUS001-train-acc+neg-no-travel.npz (default) BG_RUS001-train-test-random-all-chunk-plugin.npz Archive 2) includes only data from the RUS001 conversational corpus, which didn't have very topic-specific prompts. Archive 1) includes a mix of RUS001 data as well as a subset of the RU_CTS conversational corpus which was topic annotated. We excluded examples pertaining to 'TRAVEL' in this archive, but this archive contains conversations about the following (loosely defined) topics: ACTIVITIES BIRTHDAY_WISHES CHILDREN ECONOMY EDUCATION ENTERTAINMENT FOOD_DRINK FRIENDS_RELATIVES GET-TOGETHER HEALTH HOME HOME_MAINTENANCE IMMIGRATION LANGUAGE_COMMUNICATION LEISURE LIFE_PHILOSOPHY_RELATIONSHIPS LOCATION_DESCRIPTION MARRIAGE MOOD_PHYSICAL MOVING_HOMES MUSIC PERFORMANCE_REHEARSAL PETS POLITICS PROJECT READING_WRITING RELIGION_HOLIDAY SPEECH_COLLECTION_PROJECT TECHNOLOGY TRANSPORTATION TV_MOVIES WEATHER_CLIMATE WORK If the topic you are training for is very similar to a topic listed above, it might be worth it trying archive (2) as well. In the future, we will provide the opportunity for the user to feed his own negative examples. Gender Identification (GID) Gender ID plugins allow for triage of audio files to identify only those containing speakers of a certain gender. For scoring files, gender identification plugins operate in the same manner as SID and LID plugins. GID plugins are delivered with two pre-enrolled classes; \u2018m\u2019 and \u2018f\u2019, for male and female, respectively, so user-side enrollment is not necessary. To score files with a GID plugin using localanalyze , use the following syntax: $ localanalyze /path/to/plugins/gid-gb-v1/domains/clean-v1/ testAudio.lst Where the output follows this format: <audio file 1> m <male likelihood score> <audio file 1> f <female likelihood score> \u2026 <audio file N> m <male likelihood score> <audio file N> f <female likelihood score> Example: /data/gender/m-testFile1.wav m 0.999999777927 /data/gender/m-testFile1.wav f -0.22073142865 Enhancement (ENH) Enhancement or AudioConverter plugins are audio-to-audio plugins that take an audio file as input and provide a second audio file as output. Currently they are used to enhance the input audio file, for listening comfort and/or intelligibility. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav Running speech enhancement is as simple as: $ localanalyze /path/to/plugins/enh-mmse-v1/domains/multi-v1/ inputAudio.lst In addition, you can pass an optional PEM file that specifies optional regions to the plugin \u2013 the current enhancement plugin uses this file to pass the \u2018noise regions\u2019 to OLIVE to allow the plugin to build a noise profile for more accurate characterization and removal of the noise present in that audio file. 6: Testing In this section you will find a description of benchmarking that is performed with each release of OLIVE to measure the performance of each plugin with respect to speed and memory usage. The hardware and software details used for these results are provided in the next section, with each of the current plugins\u2019 memory and speed results following. a. Benchmarking Setup (Hardware/Software) Each data point below was obtained by running the OLIVE 4.9.1 software with a 4.8.0 runtime that has been patched to 4.9.1 (contains libraries needed by the new Topic Identification and Enhancement plugins). Tests were performed on the CentOS 7 operating system, on a Gigabyte BRIX GB-BXI7-5500. This machine has 16GB RAM available, and an Intel i7-5500U processor, which is a dual core (quad-thread) processor that runs at 2.4 GHz base (3.0 GHz turbo). b. Plugin Memory Usage Results These results were generated by running several files through each plugin via the OLIVE CLI using localanalyze , one at a time, while measuring the memory used by the localanalyze utility. The files vary in length from 10 seconds through 4 hours, and this allows us to see how each plugin handles scaling the audio file length up, and also compare overall resource utilization between individual plugins. The values reported for each plugin and audio file are the peak memory usage, in MB \u2013 lower values are better. Note that this report is for processing a single file at a time and is representative of memory utilization that can be expected for serialized processing, or processing on a machine with a single processing core. Parallel processing can cause memory usage to rise. TODO: Need to put the charts/resuts in here! SAD SAD Memory Usage (MB) Plugin sad-dnn-v4 sad-dnn-v4 sad-dnn-v4 Domain digPtt-v1 ptt-v1 tel-v1 10s 142 142 142 1 min 158 158 158 10 min 221 221 221 30 min 408 408 408 2 hr 1,262 1,262 1,262 4 hr 2,377 2,377 2,377 c. Plugin Speed Analysis Results The following charts show the speed performance of the current release of each OLIVE plugin. Values are reported as the speed of the plugin in \u2018times faster than real time\u2019 and represent how fast the plugin is able to process the input audio data, with respect to the length of that data \u2013 higher is better. Each plugin is fed 10 hours of total data consisting of roughly 4-minute audio cuts to achieve this measurement. For this test, OLIVE has been limited to using a single core for processing, in order to keep measurements and results consistent. Note that enabling parallel processing if multiple CPU cores are available will improve performance. Plugin Speed Statistics Reported in Times Faster than Real Time Plugin Domain Speed (x RT) sad-dnn-v4 digPtt-v1 104.8 sad-dnn-v4 ptt-v1 111.5 sad-dnn-v4 tel-v1 117.0 sid-embed-v2 multi-v1 51.6 sid-embedDnnTbc-v1 tbc-v1 43.9 sid-embedDnnTbc-v1 tbcdnn-v1 64.5 sid-embedTbc-v1 tbc-v1 40.2 lid-embed-v2 multi-v1 39.7 kws-batch-v9 eng-tel-v1 1.11 kws-batch-v9 eng-tel-v2 1.09 kws-batch-v9 f-tel-v1 1.23 kws-batch-v9 r-tel-v1 1.61 kws-batch-v9 r-tel-v2 1.61 qbe-tdnn-v4 digPtt-v1 15.2 qbe-tdnn-v4 multi-v1 16.1","title":"Command Line Tools (Deprecated)"},{"location":"cli.html#olive-command-line-interface-guide-legacy","text":"","title":"OLIVE Command Line Interface Guide (Legacy)"},{"location":"cli.html#disclaimer","text":"Note that the tools described below are legacy tools that are mostly used for internal testing and development. With docker-based deliveries, these utilities are difficult to access and have many performance tradeoffs versus using the OLIVE server through a client - they should not be used for integration, only for very basic experimentation. The functionality offered by these should instead be accessed through the provided Java example client (OliveAnalyze, OliveEnroll, etc.) or Python example client (olivepyanalyze, olivepyenroll, etc.). Documentation for these utilities is under construction and will be provided soon - but each utility has a help statement that provides instructions for running each.","title":"Disclaimer"},{"location":"cli.html#introduction","text":"This document describes running the OLIVE (formerly SCENIC) system from a command line. Our command line applications are geared toward a variety of specialized users such as researchers, system evaluators (i.e. Leidos for the DARPA RATS program), and testers. Casual users should consider using our graphical application. However, our command line applications can function as general-purpose tools, but may require specially formatted files such the RATS XML files for audio analysis and LDC-format TSV files for training annotations.","title":"Introduction"},{"location":"cli.html#1-overview","text":"OLIVE command line interface (CLI) tools include: localenroll \u2013 Used to enroll \u2018targets\u2019 into the system, such as a target speaker for speaker identification (SID), a topic of interest for topic identification (TID), or a keyword or phrase of interest for query-by-example keyword spotting (QBE). localanalyze \u2013 Used to query the OLIVE server to score audio to find speech with a speech activity detection (SAD) plugin, report scores for potential speakers or languages of interest for SID or language identification (LID) plugins, report likelihood and location(s) of conversation topics or keywords of interest (TID, QBE, KWS). localtrain \u2013 Used to train or adapt plugins that support the LearningTrait (SupervisedAdapter, SupervisedTrainer, or UnsupervisedAdapter) with examples of new audio conditions to improve performance in such conditions. Also used to add new language recognition capabilities to a LID plugin, and to retrain the background models of a SID plugin to prepare it for new audio conditions. Training and adaptation are not available in all plugins, please refer to individual plugin documentation or plugin capabilities matrix to verify availability of training or adaptation.","title":"1: Overview"},{"location":"cli.html#2-command-line-testing-and-analysis","text":"","title":"2: Command Line Testing and Analysis"},{"location":"cli.html#a-enrollment-with-localenroll","text":"The localenroll command is used to enroll audio for SID and TID. It can be invoked from a BASH or C-shell terminal. It takes a simply formatted text file as input and does not produce an output file. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TID, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in the Plugin Appendix. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7 The basic syntax for calling localenroll (more details and options below) is: $ ./localenroll <path_to_plugin_domain> <path_to_enrollment_file> Where an example of that may be: $ ./localenroll plugins/sid-embed-v1/domains/multi-v1/ /data/sid/smoke_enroll.lst The numerous options available in localenroll can be seen by executing localenroll --help, the output of which is shown below: usage : localenroll [- h ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path enrollment_file positional arguments : plugin_domain_path path to plugin domain used for analysis enrollment_file List of enrollments of the form < audio_path > < class_id > OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have a configuration section named enrollment . Only values from the enrollment section are read -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting","title":"A.  Enrollment with localenroll"},{"location":"cli.html#b-scoring-and-processing-with-localanalyze","text":"","title":"B.  Scoring and processing with localanalyze"},{"location":"cli.html#i-invoking-localanalyze","text":"The localanalyze utility is used to perform OLIVE scoring and analysis with most plugins (SAD, SID, SDD, LID, KWS, QBE, GID, TID), or processing with an ENH plugin, all on list-based input files. It can be invoked from a BASH or C-shell terminal. A path to a valid OLIVE plugin and domain as well as an audio paths input file are required for all tasks. For some plugins, like LID and SID, an optional IDs input file can be specified via the --class_ids argument to limit which languages or speakers are scored. This IDs input file is also how a keyword spotting plugin is informed what the keywords of interest are for a given analysis. The exact details for invoking localanalyze will depend upon the plugin technology being used, and may vary slightly depending upon the options available to each individual plugin, but the general format for running this utility is: $ localanalyze <path_to_plugin_domain> <list_of_files_to_analyze_or_process> With an example (SID): $ localanalyze plugins/sid-embed-v1/domains/multi-v1/ /data/sid/test_data.lst The format of the audio input file is simply a list of one or more newline-separated lines containing a path to an audio file: <audio_path> Example audio input file: /data/sid/test/unknownSpkr1.wav /data/sid/test/unknownSpkr27.wav As mentioned above, if you would only like to score a subset of the enrolled speakers or languages, you can optionally pass a list of these identifiers as a newline-separated list text file, with the --class_ids command line argument. This same argument is how you select keywords to search for when running localanalyze with a keyword spotting plugin (see KWS section in the Plugin Appendix for more information). IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Speaker Identification IDs example: Chris Billy Spkr3 A Keyword Spotting IDs example: turn left torpedo watermelon Example (KWS) of a localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/test-audio.lst Note that re-running localanalyze will overwrite the contents of the output.txt file or OUTPUT directory, depending on what type of plugin is being run. The OLIVE usage/help statement for localanalyze : usage : localanalyze [- h ] [-- output OUTPUT_PATH ] [-- thresholds THRESHOLDS ] [-- class_ids ID_LIST_PATH ] [-- options OPTIONS_PATH ] [-- regions REGION_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path audio_paths_file positional arguments : plugin_domain_path path to plugin domain used for analysis audio_paths_file List of audio files to analyze OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- output OUTPUT_PATH , - o OUTPUT_PATH path to output file or directory -- thresholds THRESHOLDS Optional comma - separated threshold values to apply to frame - level scores , e . g . 0.0 , 1.5 . Use syntax '-- thresholds=' for negative values , e . g -- thresholds =- 2.0 ,- 1.0 -- class_ids ID_LIST_PATH , - i ID_LIST_PATH Optional file that specifies class ids to be scored . E . g . limit the speakers that scored . -- options OPTIONS_PATH Optional file containing plugin specific name / value pairs . The option file may have more or more section headings s for each plugin type . Common section names are 'frame scoring' , , 'global scoring' or 'region scoring' -- regions REGION_PATH , - r REGION_PATH Optional flag indicating that the audio paths file should be supplemented with regions from a PEM formated file , it is up to the plugin to utilize these regions to supplement its scoring . This flag is ignored if the audio input list ( audio_paths_file ) is a PEM formatted file . -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting","title":"I. Invoking localanalyze"},{"location":"cli.html#ii-output","text":"","title":"II.    Output"},{"location":"cli.html#plugin-scoring-types","text":"In general, the output format and location of a call to localanalyze will depend on the type of \u2018scorer\u2019 the plugin being used is. There are currently four types of plugins in OLIVE: Global scorer Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Currently SID, LID, and GID are the only global scoring plugins. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Example \u2013 sid-embed-v1, lid-embed-v1 Region scorer Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. TID, SDD, QBE, and KWS are all region scorers. Example \u2013 sdd-embed-v1, qbe-tdnn-v4, kws-batch-v9 Frame scorer A frame scoring plugin provides a score for every \u2018frame\u2019 of audio within every test file passed to localanalyze . This allows OLIVE to find distinct regions of speech with high precision in recordings with noise and/or silence. SAD is a frame scoring plugin. It is also possible to apply a threshold to a frame scoring plugin at run-time to report regions of detection instead of frame scores. For a plugin like SAD, this allows OLIVE to provide output in the form of speech regions. A frame is a short segment of audio that typically consists of 10 milliseconds of audio (100 frames per second). Example \u2013 sad-dnn-v4 Audio to audio This plugin takes an audio file as input, and also returns an audio file as output. Currently the only plugins that fall into this category are speech/audio enhancement plugins, where the goal is removing noise and distortion from an audio file to improve the human listening experience and intelligibility. Example \u2013 enh-mmse-v1","title":"Plugin Scoring Types"},{"location":"cli.html#global-scorer-output","text":"In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> The name and location of the output file can be overridden by passing it as the argument to the -o or --output argument when calling localanalyze . To see specific examples for each plugin type, please refer to the appropriate section of the Plugin Appendix.","title":"Global Scorer Output"},{"location":"cli.html#region-scorer-output","text":"Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: < audio_path > < region_start_timestamp > < region_end_timestamp > < class_id > < score > Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. Specific examples can be found in the Plugin Appendix at the end of this document.","title":"Region Scorer Output"},{"location":"cli.html#frame-scorer-output","text":"In the case of frame scorers like SAD, an output file is generated for each audio input file, where each audio output file contains a score for each frame in the audio input. There is one frame score per line. Alternatively, an option exists to produce segmentation scores from SAD results by using the --threshold argument. When using the --threshold argument, the output file adheres to standard 5-column PEM format. Without supplying a threshold to localanalyze , the frame scorer output looks like this: <frame_1_score> <frame_2_score> \u2026 <frame_N_score> When a threshold is provided, the output file will resemble the following: < filename > , < channel > , < label ( \u201c speech \u201d ) > , < speech region start time ( seconds ) > , < end time ( seconds ) >","title":"Frame Scorer Output"},{"location":"cli.html#audio-to-audio-output","text":"An audio-to-audio plugin takes an audio file as input and returns a corresponding audio file as output. Currently, this plugin type is used to supply enhancement capabilities to OLIVE, to allow OLIVE to improve the quality, intelligibility, or just general human listening experience for an audio file. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav","title":"Audio to Audio Output"},{"location":"cli.html#3-command-line-field-adaptation","text":"","title":"3: Command Line Field Adaptation"},{"location":"cli.html#a-command-line-field-adaptation-overview","text":"In general, training and adaptation are very resource and time intensive operations. Very large amounts of RAM are used at certain steps in training. When attempting to train or adapt, the machine should be dedicated to that operation. If the plugin path contains a domain then adaptation is implied, otherwise training is implied. The high-level difference between training and adaptation is that adaptation will use the new data supplied during adaptation in addition to the data already used to train the model used by the plugin/domain. Training, on the other hand, ignores the data originally used for training a model and retrains from scratch using only the new data provided. When performing training, none of the data in the base plugin will be used, but the feature configs will. Check the plugin\u2019s traits to determine if full training and/or adaptation are supported.","title":"A.  Command Line Field Adaptation Overview"},{"location":"cli.html#b-invoking-localtrain","text":"Not to be confused with enrollment, the localtrain command line application is used to perform field adaptations for SAD, LID & SID. localtrain takes a plugin or plugin_domain path, and one or more data input files formatted for: Unsupervised data - a newline separated list of audio file paths Supervised data with file level annotations - a newline separated list of audio files paths with a class Id (i.e. \u201caudio_file1.flac fas\\n\u201d) Supervised data with region level annotations - a newline separated list of audio file paths, start time (seconds), end time (seconds), and class ID (i.e. \u201caudio_file1.flac 1.25 3.5 fas\\n\u201d) If multiple data files are specified then they must all use the same annotation format. The localtrain utility outputs a new domain in the plugin path. The details of the localtrain executable are below: usage : localtrain [- h ] -- domain - id DOMAIN_ID [-- overwrite ] [-- preprocess ] [-- finalize ] [-- unique ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_or_domain_path data [ data ...] Train or adapt OLIVE audio recognition systems positional arguments : plugin_or_domain_path Path to the plugin or domain . A plugin path implies full training . A domain path implies adaptation of the specified domain . data paths to data files for training / adapation . The files can have one of three forms . 1 : < audio_path >\\ n 2 : < audio_path > < class_id >\\ n 3 : < audio_path > < class_id > < start > < end > \\ n . The first form has no annotations and implies unsupervised . The second form provides for file - level annotations while the third form supports region - level annotations . Start and end times should be in seconds . If multiple files are specified , they must have the same form . optional arguments : - h , -- help show this help message and exit -- domain - id DOMAIN_ID The id of the new domain you 're creating through training or adaptation. Should be a string that is somewhat descriptive of the conditions --overwrite Forcefully overwite an existing domain --preprocess Pre-process audio only, do not finalize training/adaptation --finalize Pre-process audio only, do not finalize training/adaptation --unique gurantees log files are written to unique directoires/files. Helpful when running in SGE mode --options OPTIONS_PATH Optional file containing plugin specific name/value pairs. The option file must have one or more sections for each plugin type. Common section names are ' supervised trainer ', ,' supervised adapter ', ' unsupervised trainer ' or ' unsupervised adapter ' --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- version show program ' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting In order to use an adapted system plugin, simply pass the full path of the domain generated by localtrain to localenroll or localanalyze as the plugin_domain_path argument. For training, do not include the domain in the plugin path. When running on a SGE, you may split the audio processing from the finalization step by using the --preprocess flag to first pre-process audio files, then use invoke localtrain with the --finalize argument to finalize training. Guidelines for the minimum amount of audio data required to successfully execute localtrain are listed in the table below. Task Operation Speech Duration SAD Adapt to new channel 1h LID Adapt to new channel 20m LID Train a new language 3h SID Adapt to new channel 1h","title":"B.  Invoking localtrain"},{"location":"cli.html#i-examples","text":"","title":"i. Examples"},{"location":"cli.html#sad-adaptation-example","text":"$ localtrain ./plugins/sad-dnn-v1/domains/ptt-v1/ adaptation-data.lst Where each line of adaptation-data.lst has the following format: /path/to/audio.wav label","title":"SAD Adaptation Example:"},{"location":"cli.html#c-lid-trainingadaptation","text":"When training new channel conditions, it is recommended to train all supported languages in the LID model to produce the best results. The out of set language is labeled as \u2018xxx\u2019. Use this language ID when training to add languages that you do not want to target in the LID task but are known to be in the test dataset.","title":"C.  LID Training/Adaptation"},{"location":"cli.html#4-log-files","text":"","title":"4: Log Files"},{"location":"cli.html#a-olive-command-line-logging","text":"When executing localtrain , localenroll , and localanalyze , here are three named log files that may be of interest should something go awry. The top-level log file: This log file corresponds to the -l option to the localtrain , localenroll , and localanalyze utilities. By default, it is named the same as the utility being used with \u201c.log\u201d appended (i.e. localanalyze.log when running localanalyze ) and will be written to the directory from which you executed the utility. The pool executor log file: This file will be written to [work_directory]/logs/pool_executor.log, where work_directory corresponds to the -w option to localtrain / localenroll / localanalyze and defaults to your current directory/WORK. The pool executor log file is the best log file to look at if unexpected errors occur. It corresponds to our internal job scheduler also known as the pool executor. The pool monitor log file: This file will be written to [work_directory]/logs/pool_monitor.log, where work_directory corresponds to the -w option to the localtrain , localenroll , localanalyze utilities and defaults to your current directory/WORK/. This log contains stats about memory and CPU utilization. All three of these log files will exhibit log rotation behavior. In the event of errors, [work_directory]/logs may also contain log files named [order_id].failed , where order_id generally corresponds to the file names of the audio files being used for adaptation/training, enrollment, or analysis. The id can be used to tie errors in the pool executor log file to the \u201c.failed\u201d log files. If you run the OLIVE CLI utilities in debug mode (-d), all log files will be maintained, even if they were successful.","title":"a.  OLIVE Command Line Logging"},{"location":"cli.html#b-rotating-log-files","text":"OLIVE employs rotating log files in many places. In this context, rotating refers to a log file that is rewritten each time the application is run. The old log file, if any, is renamed with an integer suffix denoting how many invocations in the past it corresponds to. For instance, if you run localanalyze and don\u2019t specify a -l option, you\u2019ll get the default localanalyze.log file. If localanalyze.log already exists, it is moved to localanalyze.log.1 . The system will keep the 10 most recent log files. A file named localanalyze.log.8 means that the file corresponds to eight invocations ago.","title":"b.  Rotating Log Files"},{"location":"cli.html#5-plugin-appendix","text":"","title":"5: Plugin Appendix"},{"location":"cli.html#plugin-types-and-acronyms","text":"Currently, OLIVE supports the plugin technologies listed in the following list. For operating instructions that apply to only a specific technology, refer to that section within this appendix. SAD \u2013 Speech activity detection. SID \u2013 Speaker identification. LID \u2013 Language identification. KWS \u2013 Keyword spotting. QBE \u2013 Query by example based keyword spotting. TID \u2013 Topic identification. SDD \u2013 Speaker diarization and detection. GID \u2013 Gender identification. ENH \u2013 Speech and audio enhancement.","title":"Plugin Types and Acronyms"},{"location":"cli.html#speech-activity-detection-sad","text":"SAD plugins are frame scorers that take an audio list file and annotate the presence and location of speech in each audio file in that list. In standard operation, SAD plugins produce a single output file for each input file, by default in a directory called OUTPUT in the location localanalyze was called from. Output files carry the name of the original input file, but with a new extension \u201c.scores\u201d \u2013 for example, audioFile1.wav will become audioFile1.wav.scores, saved inside OUTPUT/. The format of these results files is a newline separated list of numerical values representing the likelihood that each 10ms frame of the audio file contains speech. Typically, a score above 0 represents speech detection, and a score below 0 represents no speech. SAD analysis example: $ localanalyze /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst Output files: OUTPUT/audioFile1.wav.scores OUTPUT/audioFile2.wav.scores Example audioFile1.wav.scores contents: -0.22624 -0.10081 0.00925 0.12365 Alternatively, SAD plugins can be run with the --thresholds flag to have localanalyze automatically convert the frame scores to regions of speech, by applying the provided threshold. SAD analysis example using thresholds: $ localanalyze --thresholds = 0 .0 /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst This will provide a single output file in OUTPUT directory corresponding to the provided threshold: 0.0.pem. If more than one threshold is provided, there will be a PEM file placed into OUTPUT corresponding to each provided threshold. Example PEM output: /data/sad/test/audioFile1.wav 1 speech 63.110 66.060 /data/sad/test/audioFile1.wav 1 speech 66.510 69.230 /data/sad/test/audioFile1.wav 1 speech 93.480 96.090 /data/sad/test/audioFile1.wav 1 speech 96.570 100.760 Note that if negative thresholds are to be used, it is very important to specify the thresholds using an \u2018=\u2019 character. For example, this threshold specification is valid: --thresholds=-2.0,4.0 And this is not valid: --thresholds -2.0,4.0 If only thresholds of 0 or above are going to be used, it is acceptable to omit the equals sign.","title":"Speech Activity Detection (SAD)"},{"location":"cli.html#speaker-identification-sid","text":"SID plugins are global scorers that take an audio list file and return a score for each enrolled speaker model scored against the audio in each input audio file. Generally, a score above 0 for an enrolled speaker model represents that speaker being detected in the respective audio file. In order to perform analysis on a file with a SID plugin you must first enroll one or more target speakers. The enrollment list file for a SID plugin follows this format for each line: <audio_file_path> <speaker_id> An example enroll.lst: /data/spkr_example_audio_5760.wav UIM1 /data/spkr_example_audio_5761.wav UIM1 /data/spkr_example_audio_5762.wav John /data/spkr_example_audio_5763.wav John Enrolling these speakers with localenroll : $ localenroll /path/to/plugins/sid-embed-v2/domains/multi-v1/ enroll.lst Example localanalyze call: $ localanalyze ./plugins/sid-embed-v2/domains/multi-v1/ ./data/sid/test/testAudio.lst By default, the output of this call is written to output.txt in the directory the command was run. The format of output.txt contains one line for each enrolled speaker model, for each input audio file, and the corresponding score: <audio_file_path> <speaker_id> <score> Example output.txt: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Speaker Identification (SID)"},{"location":"cli.html#trial-based-calibration-speaker-identification-sid-tbc","text":"Trial-based Calibration SID plugins are identified by \u2018tbc\u2019 in the plugin name. They are used very similarly to a basic SID plugin, using localenroll and localanalyze just like the examples shown in the previous example. The benefit of TBC plugins is that they allow OLIVE to perform calibration at test-time, based on the actual data conditions being encountered, rather than being forced to use a single, global calibration model that has been trained a priori. The basics of TBC enrollment and testing follow the previous SID examples, but the additional options and outputs available to TBC are detailed below. The standard approach to calibration uses a \u201cone size fits all\u201d calibration model based on the developer\u2019s best understanding of potential operating conditions. This is problematic when the user either doesn\u2019t know ahead of time what likely conditions are, or when operating conditions may vary widely. Trial-based calibration was developed as a means of providing calibration that is responsive to the particular conditions of a trial, and adapts its calibration model based on the conditions encountered. There are two ways we have developed to do this. The first draws from a pool of available data (either provided by the developer or augmented with user-provided data) and uses measures of the conditions found within this data and the trial conditions to build an ideal calibration set on the fly. This is useful in that this approach can also determine when a trial CANNOT be calibrated, and to measure the success of calibration when it is possible. The clear downside of this approach is that it is quite slow. A second approach to TBC is to use a model that has used a DNN to learn to predict both calibration parameters and confidence from large sets of trials and available calibration data. This approach is very fast (about 5000 times faster than the first approach) but has the downside that expanding the calibration set by the user\u2019s data isn\u2019t possible. This plug-in provides both approaches in the TBC plug-in, as two separate domains. In addition to the output score file detailed in the SID section, TBC plugins have additional possible outputs.","title":"Trial-based Calibration Speaker Identification (SID TBC)"},{"location":"cli.html#speech-detection-output","text":"Segmentation files are used to label time regions in the speech signal. We use this format for voice activity detection (VAD). If an output_ivs_dump_path is provided as an option to localenroll or localanalyze , the system will produce this file in a folder corresponding to the wav_id for all registered waveforms. The format is the following: md5sum start end ( in seconds ) Example: b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.060 0.060 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.090 0.090 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.110 0.170 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.200 0.200 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.560 3.550","title":"Speech Detection Output"},{"location":"cli.html#persistent-i-vectors","text":"For both localenroll and localanalyze , if the output_ivs_dump_path is defined via an options file with the --options flag, an i-vector from each audio file is saved in this directory for re-use. This reduces unnecessary computation when there exists overlap between lists of wave files to be processed. For instance, if the enroll and test wave file lists are identical (i.e., the case of an exhaustive comparison of a set of files), i-vector persistence will reduce overall computation by almost a factor of 2 since i-vector extraction consumes more of the computation required for an evaluation. I-vectors will be saved in a sub-directory of output_ivs_dump_path based on the base name of the wave file. In addition to this optional feature, the enrollment vectors are loaded in memory prior to verification and if the md5sum of a test audio file matches one used in the enrollment process, the corresponding vector will be used instead of re-processing the audio. This is because vector extraction is identical between enrollment and verification.","title":"Persistent I-Vectors"},{"location":"cli.html#trial-based-calibration","text":"Trial-based calibration (TBC) does not change the way calibration works but changes the way calibration is used. It relaxes the constraint on the system developers to train a calibration model that is ideally matched to the end use conditions. Rather than train a calibration model a priori, the system postpones this training until the conditions of the particular verification trial are known to the system; a trial consists of comparing test audio to an enrolled speaker model. The goal of trial-based calibration is to use information about the trial to generate an ideal calibration set for the trial conditions using the reservoir of possible calibration audio files available. Using this set, a calibration model tailored to the conditions of the trial can be trained and used to effectively calibrate the verification score. The TBC operation and output differs from traditional SID plugins; it may choose to reject a trial and NOT output a score if insufficient data is available for calibrating for those conditions. For instance, the output may look similar to the following: waves/T6_ACK2.sph T6 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00) waves/T6_ACK3.sph T1 0.0 -inf Unable to calibrate with only 12 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T2 0.0 -inf Unable to calibrate with only 3 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T3 0.0 -inf Unable to calibrate with only 0 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T4 0.0 -inf Unable to calibrate with only 2 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T6 0.0 -inf Unable to calibrate with only 9 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T4_ACK3.sph T5 0.96153318882 4.04821968079 Used 95 target trials in calibration waves/T4_RC.sph T4 3.46785068512 4.07499170303 Used 95 target trials in calibration waves/T5_Tip2.sph T5 8.90770149231 4.07352733612 Used 98 target trials in calibration waves/T5_RC.sph T5 10.2386112213 4.03855705261 Used 47 target trials in calibration waves/T4_Tip2.sph T4 10.8663234711 4.07404613495 Used 218 target trials in calibration waves/T4_Tip1.sph T4 11.793006897 3.98730397224 Used 164 target trials in calibration waves/T4_ACK2.sph T4 11.8091144562 3.90610170364 Used 119 target trials in calibration waves/T4_ACK1.sph T4 12.2115001678 4.16342687607 Used 208 target trials in calibration waves/T5_ACK1.sph T5 13.8099250793 3.99625587463 Used 99 target trials in calibration waves/T5_Tip1.sph T5 14.9411458969 3.96994686127 Used 83 target trials in calibration waves/T4_ACK3.sph T4 16.003446579 4.05554199219 Used 146 target trials in calibration The output follows the structure: <testwave> <modelid> <score> <confidence> <info> In the instance of insufficient calibration segments being located, a score of 0.0 and a calibration confidence of -inf is given. In contrast, when sufficient data is found for calibration, the number of segments used in calibration is reported. There exist two options for applying calibration with the current plugin: DNN-assisted TBC, normal TBC, or global calibration. Each of these options use duration information to reduce the impact of duration variation impacting calibration performance. Note that changing calibration domains does NOT require re-enrollment of models as these are done in a domain-independent way for any TBC-enabled plugin.","title":"Trial-based Calibration"},{"location":"cli.html#dnn-assisted-trial-based-calibration","text":"DNN-assisted TBC is invoked by passing the tbcdnn-v1 domain to localanalyze . This is a very fast and newly pioneered effort by SRI to reduce the computation needed to apply dynamic calibration methods to speaker recognition and operates with very low overhead compared to global calibration, and yet significantly benefits calibration performance in varying conditions or conditions that differ from the development conditions. localanalyze ... <plugin>/domains/tbcdnn-v1 test.lst","title":"DNN-assisted Trial-based Calibration"},{"location":"cli.html#normal-trial-based-calibration","text":"TBC is applied by default with the 'sid-embedDnnTbc-v1' plugin. The data within the domain (such as 'tbc-v1') is used as candidate calibration data. localanalyze ... <plugin>/domains/tbc-v1 test.lst TBC is applied to verification scores on a trial-by-trial basis. As such, verification using TBC will operate at a speed much slower than global or DNN-assisted TBC depending on the size and make-up of the TBC data. This should be considered when using TBC in a cluster environment where it is the number of trials (model vs test comparisons) that determine the running time instead of the number of test files.","title":"Normal Trial-based Calibration"},{"location":"cli.html#global-calibration","text":"Each domain can be used to invoke global calibration. This is particularly useful for user-defined data as it provides a rapid means of improving calibration performance without a dramatic increase in computation time. In this case, verification will operate at a much faster pace since TBC is essentially disabled and the global calibration model parameters are applied to all scores. In order to invoke global calibration, and optional parameter must be passed to localanalyze via an options file: echo \"[global scoring] global_calibration = True \" > options.lst localanalyze -- options options . lst ... < plugin >/ domains / tbc - v1 test . lst","title":"Global Calibration"},{"location":"cli.html#optional-parameters","text":"The TBC-based plugins offer several tunable parameters via the options parameter to localenroll or localanalyze . These can be passed to the enrollment phase or verification phase by preceding the options in an ascii text file as such: $ cat options . lst [ enrollment ] ... enrollment options per line ... [ global scoring ] ... verification options per line ... The optional parameters and their purpose are provided below. tbc_confidence_threshold = 3 . 0 , # Similarity threshold for processing a trial with TBC score_threshold = 0 . 0 , # Score offset subtracted from output LLRs to assist in making 0 threshold output tgt_max = 300 , # The maxmimum number of target trials used for TBC of a trial imp_max = 3000 , # The maxmimum number of impostor trials used for TBC of a trial tgt_imp_min = 20 , # The mimum number of relevant target and impostor calibration trials needed to use TBC ( rejected otherwise ) global_calibration = False , # Apply global calibration instead of TBC ivs_dump_path = None , # Output path for dumping vectors and meta information sad_threshold = 0 . 5 , # Threshold for speech activity detection ( higher results in less speech ) sad_filter = 1 , # Smoothing of LLRs from SAD DNN prior to thresholding sad_interpolate = 1 , # If > 1 , a speed up of SAD by interpolating values between frames ( 4 works well ) Utilizing these parameters in an options file may look like this: echo \"[enrollment] sad_threshold = 1.0 ivs_dump_path = . / embeddings [ global scoring ] sad_threshold = 1.0 ivs_dump_path = . / embeddings tgt_max = 100 tgt_imp_min = 50 \" > options.lst localenroll -- options options . lst ... < plugin >/ domains /< domain > enroll . lst localanalyze -- options options . lst ... < plugin >/ domains /< domain > test . lst","title":"Optional Parameters"},{"location":"cli.html#verification-trial-output","text":"The format for the verification trial is the following. Note that for global calibration, the optional parameters (calibration_confidence and calibration_remarks) are not output. Output format: wav_id speaker_id score [ calibration_confidence calibration_remarks ] Here is an example of score executed with Global Calibration: waves/T1_ACK1.sph T6 5.19274568558 waves/T1_ACK1.sph T4 1.204241395 waves/T1_ACK1.sph T5 1.69025540352 Here is an example of scores executed with DNN-assisted TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used DNN-assisted TBC with confidence 5.751 waves/T1_ACK1.sph T4 1.204241395 5.12 Used DNN-assisted TBC with confidence 3.122 waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with confidence above threshold (3.00) Here is an example of scores executed with normal TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used 67 target trials in calibration waves/T1_ACK1.sph T4 1.204241395 5.12 Used 73 target trials in calibration waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00)","title":"Verification Trial Output"},{"location":"cli.html#speaker-diarization-and-detection-sdd","text":"The overall goal of SDD plugins is to detect regions of speech within an audio recording that are associated with different speakers, and then identify those speakers if possible. SDD plugins have three different modes of operation, as outlined below. Changing the execution mode for SDD is done by passing an options file to localanalyze as an argument to the --options flag. The main behavior and premise of the technology and plugin remain the same, but each mode changes the format and information contained in the output file. Running the SDD plugin is very similar to running SID plugins, with the same syntax for enrollment and testing. Currently, training or adaptation through localtrain is not supported, but enrolling new speakers and testing against enrolled speakers is as simple as: $ localenroll /path/to/plugins/sdd-embed-v1/domains/multi-v1/ enrollmentAudio.lst $ localanalyze /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst The enrollment and test audio file lists in this example follow the same format as the lists used by SID plugins, described above. By default, if run as above with no options, the plugin will run in Speaker Detection mode, and provide the output described above. In order to run in SID or SID Exhaustive mode, you will need to provide an options file to specify that behavior: $ localanalyze --options options.lst /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst Where options.lst is a text file with contents similar to: [region scoring] mode: SID_EXHAUSTIVE sad_threshold: 0.0 diarization_max_num_speakers: 2 The [region scoring] header is alerting the plugin that the options are being passed for scoring, and all of the parameters shown above (sad_threshold, diarization_max_num_speakers, mode) are optional parameters. The mode option controls the output behavior as described above, and the possible options are SID, SID_EXHAUSTIVE, and SPEAKER_DETECTION, and described directly below. The sad_threshold defaults to 2.0, and is used to fine tune the threshold for the internal speech activity detection plugin if necessary. The parameter diarization_max_num_speakers defaults to 4, and is the largest number of speakers the plugin will attempt to consider when clustering segments within the file.","title":"Speaker Diarization and Detection (SDD)"},{"location":"cli.html#sdd-execution-modes","text":"To facilitate the understanding of each mode\u2019s output, consider that speech from an audio file is made up of clusters of speakers, and each cluster will have one or more contiguous segment of speech. SPEAKER_DETECTION The goal of Speaker Detection is to show the most probable speaker model for each segment of the input audio file. As output, this mode gives one line per segment within the file, along with the top scoring enrolled model for the cluster that segment belongs to, and that cluster's score for the given model. Note that many scores will be repeated in the output file, since each segment in the cluster shares the same score for a given speaker model. This mode is performed by default if no options file with a mode override is given. SID This mode is meant for triaging large amounts of audio files when the main goal is just finding which of these files may contain speech from one of the enrolled speakers. The output is the maximum score for each enrolled speaker within the audio file after scoring against each cluster in the file, as well as the timestamps for the beginning and end of the longest segment within the cluster that scored the highest for that model. This gives a specific segment to spot check and evaluate the plugin's decision if needed. SID_EXHAUSTIVE When using SID Exhaustive, each diarized cluster is scored against each enrolled model. The output is a complete listing for every speech segment of the input audio file, the score from testing every enrolled model against the cluster that the segment belongs to. Many scores will be repeated in the output file, since each segment in the cluster shares the same score. In this example, Chris and Jimmy are the only enrolled models, and 5 total segments were identified within the file.","title":"SDD Execution Modes"},{"location":"cli.html#language-identification-lid","text":"LID plugins are global scorers that act very similar to SID with respect to scoring, except that each score corresponds to a language model rather than a speaker model. In most cases, LID plugins will be delivered from SRI with a set of languages already enrolled. Languages can be added to some plugins by the user if enough appropriate data is available, through the localtrain CLI call. Details on this will be added to this document in a later revision. Example localanalyze call: $ localanalyze /path/to/plugins/lid-embed-v2/domains/multi-v1/ testAudio.lst Output format: <audio_file_path> <language_id> <score> Output example: /data/lid/audio/file1.wav fre -0.5348 /data/lid/audio/file1.wav eng 3.2122 /data/lid/audio/file1.wav spa -5.5340 /data/lid/audio/file1.wav rus 0.5333 /data/lid/audio/file1.wav ara -4.9444 /data/lid/audio/file2.wav fre -2.6564","title":"Language Identification (LID)"},{"location":"cli.html#keyword-spotting-kws","text":"KWS is an automatic speech recognition (ASR) based approach to detecting spoken keywords in audio. Rather than enrolling target keywords from audio, as you would with query-by-example, telling the plugin what keywords to search for is done by passing an IDs file to localanalyze . The format of the IDs file is: IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Keyword Spotting IDs example, search_list.lst: remote torpedo voice recognition Example KWS localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst /path/to/plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/testAudio.lst The output format for KWS plugins is identical to that of QBE. It is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/kws/testFile1.wav 7.170 7.570 remote 1.0 /data/kws/testFile1.wav 10.390 10.930 remote 0.693357 /data/kws/testFile1.wav 1.639 2.549 voice recognition 1.0","title":"Keyword Spotting (KWS)"},{"location":"cli.html#automatic-speech-recognition-asr","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. ASR plugins do not require any enrollment of words (like QBE) or specification of words of interest (like traditional KWS), but instead rely on the vocabulary model built into the domain to define the list of available words. All that is necessary for scoring an audio file for ASR is a list of input files to be scored, which follows the format below. Generic input audio list format: <audioFile_1> <audioFile_2 (opt)> ... <audioFile_N (opt)> A specific example of this, called testAudio.lst, might look like: /data/asr/testFile1.wav /data/asr/testFile2.wav /data/asr/testFile3.wav Note that if the files are not contained within the directory that localanalyze is being run from, or if a relative path from that location is not provided, the full file path to each file is necessary. An example ASR localanalyze call: $ localanalyze /home/user/oliveAppData/plugins/asr-dynapy-v1/domains/eng-tdnnChain-tel-v1/ /data/asr/test/testAudio.lst The output format for KWS plugins is identical to that of QBE and other region-scoring OLIVE plugins. ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. Output is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <word> <score> An example in English: /data/asr/testEnglish1.wav 0.000 0.190 and 43.00000000 /data/asr/testEnglish1.wav 0.210 0.340 we're 44.00000000 /data/asr/testEnglish1.wav 0.330 0.460 going 97.00000000 /data/asr/testEnglish1.wav 0.450 0.520 to 97.00000000 /data/asr/testEnglish1.wav 0.510 0.940 fly 66.00000000 /data/asr/testEnglish1.wav 1.080 1.300 was 31.00000000 /data/asr/testEnglish1.wav 1.290 1.390 that 24.00000000 /data/asr/testEnglish1.wav 1.290 1.390 it 22.00000000 /data/asr/testEnglish1.wav 1.380 1.510 we're 27.00000000 /data/asr/testEnglish1.wav 1.500 1.660 going 97.00000000 /data/asr/testEnglish1.wav 1.650 1.720 to 98.00000000 /data/asr/testEnglish1.wav 1.710 1.930 fly 94.00000000 /data/asr/testEnglish1.wav 1.920 2.110 over 79.00000000 /data/asr/testEnglish1.wav 2.100 2.380 saint 93.00000000 /data/asr/testEnglish1.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: /data/asr/testMandarin1.wav 0.280 0.610 \u6218\u6597 99.00000000 /data/asr/testMandarin1.wav 0.600 0.880 \u7206\u53d1 98.00000000 /data/asr/testMandarin1.wav 0.870 0.970 \u7684 99.00000000 /data/asr/testMandarin1.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 /data/asr/testMandarin1.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 /data/asr/testMandarin1.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 /data/asr/testMandarin1.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 /data/asr/testMandarin1.wav 3.130 3.340 \u7684 100.00000000 /data/asr/testMandarin1.wav 3.330 3.720 \u6b66\u88c5 55.00000000 /data/asr/testMandarin1.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: /data/asr/testFarsi1.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 /data/asr/testFarsi1.wav 0.470 0.740 51.00000000 \u0627\u06cc /data/asr/testFarsi1.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 /data/asr/testFarsi1.wav 0.310 0.460 99.00000000 \u0645\u06cc /data/asr/testFarsi1.wav 0.450 0.680 99.00000000 \u06af\u0645 /data/asr/testFarsi1.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f /data/asr/testFarsi1.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Automatic Speech Recognition (ASR)"},{"location":"cli.html#query-by-example-keyword-spotting-qbe","text":"Query by example is a specific type of keyword spotting plugin that searches for keywords matching a spoken word or phrase example, rather than from a text example like traditional KWS. This means that it is necessary to enroll keywords into the system from audio examples with localenroll before using QBE to search audio for these keywords or phrases. Enrollment follows the same format as enrolling speakers into a SID plugin, with the enrollment audio list following this format: <audio_file_path> <keyword_id> Example: /data/qbe/enroll/watermelon_example1.wav Watermelon /data/qbe/enroll/watermelon_example2.wav Watermelon /data/qbe/enroll/airplane_example.wav Airplane /data/qbe/enroll/keyword_example.wav Keyword Note that currently each enrollment audio file must contain ONLY the keyword that is desired to be enrolled. Also note that the text label in the second column of the enrollment file is only for user readability and is not used by the system when determining what to search the audio file for. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006","title":"Query by Example Keyword Spotting (QBE)"},{"location":"cli.html#topic-identification-tid","text":"Topic Identification plugins attempt to detect and categorize the topic being discussed within an audio recording from a known and pre-enrolled set of available topics, and report this topic (if any are detected) to the user. Each domain of a TID plugin is language-dependent, and should display the target language as the first string of the domain\u2019s name. Some TID plugins may be delivered with pre-enrolled topics \u2013 please consult the documentation that accompanied your delivery if you are unsure if this is the case. If no topics are enrolled, however, or if you wish to enroll new topics, the format is very similar to enrolling new speakers into a SID plugin, and follows the same CLI call structure, with one caveat. $ localenroll --local $domain $enroll_list Due to limitations with the current TID technology, enrollment must be performed with the --local flag set. This limits OLIVE to serialized processing, which will process the enrollment slightly slower, but avoid resource-competition issues that may cause the system to become unstable and crash. Enrollment audio lists for TID follow the same format as SID, but substitute a topic name for a speaker label. Note: in the current version, we require the user to provide audio examples that are primarily about the topic of interest as enrollment examples. If there are significant portions of an audio file that are off-topic, we suggest the file be cut and fed as separate examples. Enroll list format: <audio_file_path> <topic_id> Example: /tid/enroll/topic_example_audio_5760.wav Travel /tid/enroll/topic_example_audio_5761.wav Travel /tid/enroll/topic_example_audio_5762.wav Travel /tid/enroll/topic_example_audio_5763.wav Travel To run TID, once target topics have been enrolled, the call to localanalyze is very similar to other plugin types, with the current plugin again requiring the --local flag. $ localanalyze --local /path/to/plugins/tid-svm-v2/domains/r-tel-v1/ testAudio.lst As with the SID and LID plugins, by default the plugin\u2019s output will be written to the file \u201coutput.txt\u201d in the directory localanalyze was called from. This can be overridden by passing localanalyze the -o flag, as well as an alternate file to save the results to. The TID results structure is very similar to KWS, with the following format for each line: <audio_file_path> <start_time_s> <end_time_s> <topic_id> <confidence_score> Example: /data/tid-example_wavs/20110511_GET-TOGETHER.wav 18.790 55.300 transportation 0.0222 /data/tid-example_wavs/20110511_TRANSPORTATION.wav 4.010 19.140 transportation 0.4532 The start and end timestamps above are provided in seconds. The will be displayed as it was supplied in the second column of the enrollment list. The will be between 0 and 1, and marks the confidence of the system in the decision of this topic. Please note that output.txt will be overwritten by each successive experiment. Please back it up or use the -o option to localanalyze if you would like to save the results. Also note that the start and end times for each topic refer to the chunk in each audio that has the highest probability of being about that topic. Currently, we report only ONE such segment per file in order to help the user locate the most useful part. The score associated with that segment is global, in that it represents the likelihood that this topic is present anywhere in the document.","title":"Topic Identification (TID)"},{"location":"cli.html#important-background-example-information","text":"In order to train a Topic detector, we currently use an SVM classifier. This model uses \"positive\" examples of the topic as provided by the user during the enrollment phase, as well as \"negative\" examples to model what is not the topic. Those negative examples can be crucial to the performance of the final system. Currently, those \"negative\" examples come pre-processed as a python numpy archive and cannot be modified by the user explicitly. We do provide two different numpy archives that can be tried by a user: BG_RUS001-train-acc+neg-no-travel.npz (default) BG_RUS001-train-test-random-all-chunk-plugin.npz Archive 2) includes only data from the RUS001 conversational corpus, which didn't have very topic-specific prompts. Archive 1) includes a mix of RUS001 data as well as a subset of the RU_CTS conversational corpus which was topic annotated. We excluded examples pertaining to 'TRAVEL' in this archive, but this archive contains conversations about the following (loosely defined) topics: ACTIVITIES BIRTHDAY_WISHES CHILDREN ECONOMY EDUCATION ENTERTAINMENT FOOD_DRINK FRIENDS_RELATIVES GET-TOGETHER HEALTH HOME HOME_MAINTENANCE IMMIGRATION LANGUAGE_COMMUNICATION LEISURE LIFE_PHILOSOPHY_RELATIONSHIPS LOCATION_DESCRIPTION MARRIAGE MOOD_PHYSICAL MOVING_HOMES MUSIC PERFORMANCE_REHEARSAL PETS POLITICS PROJECT READING_WRITING RELIGION_HOLIDAY SPEECH_COLLECTION_PROJECT TECHNOLOGY TRANSPORTATION TV_MOVIES WEATHER_CLIMATE WORK If the topic you are training for is very similar to a topic listed above, it might be worth it trying archive (2) as well. In the future, we will provide the opportunity for the user to feed his own negative examples.","title":"Important Background Example Information"},{"location":"cli.html#gender-identification-gid","text":"Gender ID plugins allow for triage of audio files to identify only those containing speakers of a certain gender. For scoring files, gender identification plugins operate in the same manner as SID and LID plugins. GID plugins are delivered with two pre-enrolled classes; \u2018m\u2019 and \u2018f\u2019, for male and female, respectively, so user-side enrollment is not necessary. To score files with a GID plugin using localanalyze , use the following syntax: $ localanalyze /path/to/plugins/gid-gb-v1/domains/clean-v1/ testAudio.lst Where the output follows this format: <audio file 1> m <male likelihood score> <audio file 1> f <female likelihood score> \u2026 <audio file N> m <male likelihood score> <audio file N> f <female likelihood score> Example: /data/gender/m-testFile1.wav m 0.999999777927 /data/gender/m-testFile1.wav f -0.22073142865","title":"Gender Identification (GID)"},{"location":"cli.html#enhancement-enh","text":"Enhancement or AudioConverter plugins are audio-to-audio plugins that take an audio file as input and provide a second audio file as output. Currently they are used to enhance the input audio file, for listening comfort and/or intelligibility. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav Running speech enhancement is as simple as: $ localanalyze /path/to/plugins/enh-mmse-v1/domains/multi-v1/ inputAudio.lst In addition, you can pass an optional PEM file that specifies optional regions to the plugin \u2013 the current enhancement plugin uses this file to pass the \u2018noise regions\u2019 to OLIVE to allow the plugin to build a noise profile for more accurate characterization and removal of the noise present in that audio file.","title":"Enhancement (ENH)"},{"location":"cli.html#6-testing","text":"In this section you will find a description of benchmarking that is performed with each release of OLIVE to measure the performance of each plugin with respect to speed and memory usage. The hardware and software details used for these results are provided in the next section, with each of the current plugins\u2019 memory and speed results following.","title":"6: Testing"},{"location":"cli.html#a-benchmarking-setup-hardwaresoftware","text":"Each data point below was obtained by running the OLIVE 4.9.1 software with a 4.8.0 runtime that has been patched to 4.9.1 (contains libraries needed by the new Topic Identification and Enhancement plugins). Tests were performed on the CentOS 7 operating system, on a Gigabyte BRIX GB-BXI7-5500. This machine has 16GB RAM available, and an Intel i7-5500U processor, which is a dual core (quad-thread) processor that runs at 2.4 GHz base (3.0 GHz turbo).","title":"a.  Benchmarking Setup (Hardware/Software)"},{"location":"cli.html#b-plugin-memory-usage-results","text":"These results were generated by running several files through each plugin via the OLIVE CLI using localanalyze , one at a time, while measuring the memory used by the localanalyze utility. The files vary in length from 10 seconds through 4 hours, and this allows us to see how each plugin handles scaling the audio file length up, and also compare overall resource utilization between individual plugins. The values reported for each plugin and audio file are the peak memory usage, in MB \u2013 lower values are better. Note that this report is for processing a single file at a time and is representative of memory utilization that can be expected for serialized processing, or processing on a machine with a single processing core. Parallel processing can cause memory usage to rise. TODO: Need to put the charts/resuts in here!","title":"b.  Plugin Memory Usage Results"},{"location":"cli.html#sad","text":"","title":"SAD"},{"location":"cli.html#sad-memory-usage-mb","text":"Plugin sad-dnn-v4 sad-dnn-v4 sad-dnn-v4 Domain digPtt-v1 ptt-v1 tel-v1 10s 142 142 142 1 min 158 158 158 10 min 221 221 221 30 min 408 408 408 2 hr 1,262 1,262 1,262 4 hr 2,377 2,377 2,377","title":"SAD Memory Usage (MB)"},{"location":"cli.html#c-plugin-speed-analysis-results","text":"The following charts show the speed performance of the current release of each OLIVE plugin. Values are reported as the speed of the plugin in \u2018times faster than real time\u2019 and represent how fast the plugin is able to process the input audio data, with respect to the length of that data \u2013 higher is better. Each plugin is fed 10 hours of total data consisting of roughly 4-minute audio cuts to achieve this measurement. For this test, OLIVE has been limited to using a single core for processing, in order to keep measurements and results consistent. Note that enabling parallel processing if multiple CPU cores are available will improve performance.","title":"c.  Plugin Speed Analysis Results"},{"location":"cli.html#plugin-speed-statistics-reported-in-times-faster-than-real-time","text":"Plugin Domain Speed (x RT) sad-dnn-v4 digPtt-v1 104.8 sad-dnn-v4 ptt-v1 111.5 sad-dnn-v4 tel-v1 117.0 sid-embed-v2 multi-v1 51.6 sid-embedDnnTbc-v1 tbc-v1 43.9 sid-embedDnnTbc-v1 tbcdnn-v1 64.5 sid-embedTbc-v1 tbc-v1 40.2 lid-embed-v2 multi-v1 39.7 kws-batch-v9 eng-tel-v1 1.11 kws-batch-v9 eng-tel-v2 1.09 kws-batch-v9 f-tel-v1 1.23 kws-batch-v9 r-tel-v1 1.61 kws-batch-v9 r-tel-v2 1.61 qbe-tdnn-v4 digPtt-v1 15.2 qbe-tdnn-v4 multi-v1 16.1","title":"Plugin Speed Statistics Reported in Times Faster than Real Time"},{"location":"clients.html","text":"OLIVE Java and Python Clients Introduction Each OLIVE delivery includes two OLIVE client utilities - one written in Java, one written in Python. Out of the box, these tools allow a user to jump right in with running OLIVE if the GUI is not desired. These can also serve as code examples for integrating with OLIVE. This page primarily covers using these clients for processing audio, rather than integrating with the OLIVE API. For more information on integration, the nitty-gritty details of the OLIVE Enterprise API, and code examples, refer to these integration-focused pages instead: OLIVE Enterprise API Primer OLIVE Python Client API Documentation Integrating a Client API with OLIVE Building an OLIVE API Reference Implementation As far as the usage and capabilities of these tools, they were meant to mirror the Legacy CLI Tools as closely as possible, and shares many input/output formats and assumptions with those tools. As this document is still under construction, referring to this older guide may help fill in some useful information that may currently be missing from this page. Note that unlike the Legacy CLI tools, that are calling plugin code directly, these client tools require a running OLIVE Server. They are client utilities that are queueing and submitting job requests to the OLIVE server, which then manages the plugins themselves and actual audio processing. If you haven't already, please refer to the appropriate guide for setting up and starting an OLIVE server depending on your installation type: OLIVE Martini Docker-based Installation OLIVE Standalone Docker-based Installation Redhat/CentOS 7 Native Linux Installation OLIVE Server Guide Client Setup, Installation, Requirements As a quick review, the contents of an OLIVE package typically look like this: olive6.0.0/ api/ java/ python/ docs/ martini/ -or- docker/ -or- runtime/ OliveGUI/ - (Optional) The OLIVE Nightingale GUI (not included in all deliveries) oliveAppData/ The clients this page describes are contained in the bolded api/ directory above. Java (OliveAnalyze) The Java tools are the most full-featured with respect to tasking individual plugins. They are asynchronous, and better able to deal with large amounts of file submissions by parallelizing the submission of large lists of files. If the primary task is enrolling and scoring audio files with individual plugins, the Java tools, what we call the OliveAnalyze suite. The tools themselves do not need to be 'installed'. For convenience, their directory can be added to your $PATH environment variable, so that they can be called from anywhere: $ export PATH =$ PATH : < path >/ olive6 . 0.0 / api / java / bin / $ OliveAnalyze - h But they can also be left alone and called directly, as long as their full or relative path is present: # From inside olive6.0.0/api/java/bin: $ ./OliveAnalyze -h # From inside olive6.0.0/: $ ./api/java/bin/OliveAnalyze -h # From elsewhere: $ <path>/olive6.0.0/api/java/bin/OliveAnalyze -h These tools depend on OpenJDK 11 or newer being installed. Refer to OpenJDK for more information on downloading and installing this for your operating system. The full list of utilities in this suite are as follows: OliveAnalyze OliveAnalyzeText OliveEnroll OliveLearn (rarely used) OliveWorkflow But the most commonly used are OliveAnalyze for scoring requests, and OliveEnroll for enrollment requests. Examples are provided for each of these below, and for more advanced users that need different tools, each utility has its own help statement that can be accessed with the -h flag: $ OliveAnalyzeText -h The arguments and formatting for each tool is very similar, so familiarity with the OliveAnalyze and OliveEnroll examples below should allow use of most of these tools. Python (olivepyanalyze) The Python client, what we call the olivepyanalyze suite, is not as fully-featured with respect to batch-processing of audio files. It performs synchronous requests to the OLIVE server, and so it will sequentially score each provided audio file, rather than submitting jobs in parallel. For this reason, the Java OliveAnalyze tools are recommended for batch processing of individual plugin tasks. The python client tools require Python 3.8 or newer - please refer to Python for downloading and installing Python. Installing these tools has been simplified by providing them in the form of a Python wheel, that can be easily installed with pip . Standard $ cd olive6.0.0/api/python $ ls olivepy-6.0.0-py3-none-any.whl olivepy-6.0.0.tar.gz $ python3 -m pip install --upgrade pip setuptools wheel $ python3 -m pip install olivepy-6.0.0-py3-none-any.whl Virtual Environment Install (optional) $ cd olive6.0.0/api/python $ ls olivepy-6.0.0-py3-none-any.whl olivepy-6.0.0.tar.gz $ python3 -m venv olivepy-virtualenvironment $ source olivepy-virtualenvironment/bin/activate ( olivepy-virtualenvironment ) $ python3 -m pip install --upgrade pip setuptools wheel ( olivepy-virtualenvironment ) $ python3 -m pip install olivepy-6.0.0-py3-none-any.whl This will fetch and install (if necessary) the olivepy dependencies, and install the olivepy tools. Those dependencies are: protobuf soundfile numpy zmq The olivepy utilities closely mirror the Java utilities, with the addition of the workflow tool, and are as follows: olivepyanalyze olivepyenroll olivepylearn (rarely used) olivepyworkflow olivepyworkflowenroll The olivepyworkflow tools are the most important, and examples are provided below for both scoring with olivepyworkflow and enrollment with olivepyworkflowenroll . We also provide examples for olivepyanalyze and olivepyenroll that mirror the Java examples. Scoring/Analysis Requests Background: Plugin Scoring Types In general, the output format will depend on the type of \u2018scorer\u2019 the plugin being used is. For a deeper dive into OLIVE scoring types, please refer to the appropriate section in the OLIVE Plugin Traits Guide , but a brief overview follows. The most common types of plugins in OLIVE are: Global Scorer Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Speaker and Language Identification are examples of global scorers. OLIVE typically calls a global scoring plugin an \"Identification\" plugin, whereas a region scoring plugin to pinpoint the same class types would instead be called a \"Detection\" plugin. For example, Speaker Identification versus Speaker Detection; the former assumes the entire audio contains a single speaker, where the latter makes no such assumption, and attempts to localize any detected speakers of interest. Global Scorer Output In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> For example, a Speaker Identification analysis run, with three enrolled speakers (Alex, Taylor, Blake) might return: /data/sid/audio/file1.wav Alex -0.5348 /data/sid/audio/file1.wav Taylor 3.2122 /data/sid/audio/file1.wav Blake -5.5340 /data/sid/audio/file2.wav Alex 0.5333 /data/sid/audio/file2.wav Taylor -4.9444 /data/sid/audio/file2.wav Blake -2.6564 Note the actual meanings of the scores and available classes will vary from plugin-to-plugin. Please refer to individual plugin documentation for more guidance on what the scores mean and what ranges are acceptable. Also note that the output format described here is literally what will be returned when calling a plugin directly with OliveAnalyze or olivepyanalyze - but when performing a global-scoring task as part of analysis with a workflow,these same informational pieces (audio_path or object, class_id, score) are still provided, but packed into a json structure. Region scorer Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. Automatic Speech Recognition (ASR), Language Detection (LDD), and Speaker Detection (SDD) are all region scorers. OLIVE typically calls a global scoring plugin an \"Identification\" plugin, whereas a region scoring plugin to pinpoint the same class types would instead be called a \"Detection\" plugin. For example, Speaker Identification versus Speaker Detection; the former assumes the entire audio contains a single speaker, where the latter makes no such assumption, and attempts to localize any detected speakers of interest. Region Scorer Output Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: < audio_path > < region_start_timestamp > < region_end_timestamp > < class_id > < score > For example, a language detection plugin might output something like this: /data/mixed-language/testFile1.wav 2.170 9.570 Arabic 0.912 /data/mixed-language/testFile1.wav 10.390 15.930 French 0.693 /data/mixed-language/testFile1.wav 17.639 22.549 English 0.832 /data/mixed-language/testFile2.wav 0.142 35.223 Pashto 0.977 Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. More specific examples can be found in the respective plugin-specific documentation pages. As with global scoring, note that the output format described here is literally what will be returned when calling a plugin directly with OliveAnalyze or olivepyanalyze - but when performing a region-scoring task as part of analysis with a workflow,these same informational pieces (audio_path or object, region start and end timestamps, class_id, score) are still provided, but packed into a json structure. Plugin Direct (Analysis) Performing an analysis request with both tools is very similar, as the tools were designed to closely mirror each other so that familiarity with one would easily transfer to the other. The usage statements for each can be examined by invoking each with their -h or --help flag: OliveAnalyze (Java) $ ./OliveAnalyze -h usage: OliveAnalyze --align Perform audio alignment analysis. Must specify the two files to compare using an input list file via the--list argument --apply_update Request the plugin is updated ( if supported ) --box Perform bounding box analysis. Must specify an image or video input --cabundlepass <arg> Specifies the certificate authority passphrase of the certificate authority. --cabundlepath <arg> Specifies the path of the certificate authority --certpass <arg> Specifies the certificate passphrase to unlock the encrypted certificate key --certpath <arg> Specifies the path of the certificate --channel <arg> Process stereo files using channel NUMBER --class_ids <arg> Use Class ( s ) from FILE for scoring. Each line in the file contains a single class, including any white space --compare Perform audio compare analysis. Must specify the two files to compare using an input list file via the--list argument --decoded Send audio file as decoded PCM16 samples instead of sending as serialized buffer. Input file must be a wav file --domain <arg> Use Domain NAME --enhance Perform audio conversion ( enhancement ) --frame Perform frame scoring analysis --global Perform global scoring analysis -h Print this help message -i,--input <arg> NAME of the input file ( audio/video/image as required by the plugin --input_list <arg> Use an input list FILE having multiple filenames/regions or PEM formatted -l,--load load a plugin now, must use --plugin and --domain to specify the plugin/domain to preload --options <arg> options from FILE --output <arg> Write any output to DIR, default is ./ -p,--port <arg> Scenicserver port number. Defauls is 5588 --path Send audio file path instead of a buffer. Server and client must share a filesystem to use this option --plugin <arg> Use Plugin NAME --print <arg> Print all available plugins and domains. Optionally add 'verbose' as a print option to print full plugin details including traits and classes -r,--unload unload a loaded plugin now, must use --plugin and --domain to specify the plugin/domain to unload --region Perform region scoring analysis -s,--server <arg> Scenicserver hostname. Default is localhost --secure Indicates a secure connection should be made. Requires --certpath, --cabundlepath, --certpass, and --cabundlepass to be set. --shutdown Request a clean shutdown of the server --status Print the current status of the server -t,--timeout <arg> timeout ( in seconds ) when waiting for server response. Default is 10 seconds --threshold <arg> Apply threshold NUMBER when scoring --update_status Get the plugin ' s update status --upload_files Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation. -v,--vec <arg> PATH to a serialized AudioVector, for plugins that support audio vectors in addition to wav files --vector Perform audio vectorization olivepyanalyze (Python) $ olivepyanalyze -h usage: olivepyanalyze [ -h ] [ -C CLIENT_ID ] [ -p PLUGIN ] [ -d DOMAIN ] [ -G ] [ -e ] [ -f ] [ -g ] [ -r ] [ -b ] [ -P PORT ] [ --upload_port UPLOAD_PORT ] [ -s SERVER ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --text ] [ --options OPTIONS ] [ --class_ids CLASS_IDS ] [ --debug ] [ --concurrent_requests CONCURRENT_REQUESTS ] [ --path ] [ --upload_files ] [ --secure ] [ --certpath CERTPATH ] [ --keypath KEYPATH ] [ --keypass KEYPASS ] [ --cabundlepath CABUNDLEPATH ] [ --print ] options: -h, --help show this help message and exit -C CLIENT_ID, --client-id CLIENT_ID Experimental: the client_id to use -p PLUGIN, --plugin PLUGIN The plugin to use. -d DOMAIN, --domain DOMAIN The domain to use -G, --guess Experimental: guess the type of analysis to use based on the plugin/domain. -e, --enhance Enhance the audio of a wave file, which must be passed in with the --wav option. -f, --frame Do frame based analysis of a wave file, which must be passed in with the --wav option. -g, --global Do global analysis of a wave file, which must be passed in with the --wav option. -r, --region Do region based analysis of a wave file, which must be passed in with the --wav option. -b, --box Do bounding box based analysis of an input file, which must be passed in with the --wav option. -P PORT, --port PORT The port to use. --upload_port UPLOAD_PORT The upload port to use when specifying --upload_files. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -t TIMEOUT, --timeout TIMEOUT The timeout to use -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --text Indicates that input ( or input list ) is a literal text string to send in the analysis request. --options OPTIONS Optional file containing plugin properties ans name/value pairs. --class_ids CLASS_IDS Optional file containing plugin properties ans name/value pairs. --debug Debug mode --concurrent_requests CONCURRENT_REQUESTS # of concurrent requests to send the server --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option unless --upload_files is also used. --upload_files Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation. --secure Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. --certpath CERTPATH Specifies the path of the certificate --keypath KEYPATH Specifies the path of the certificate key --keypass KEYPASS Specifies the certificate passphrase to unlock the encrypted certificate key --cabundlepath CABUNDLEPATH Specifies the path of the certificate authority --print Print all available plugins and domains To perform a scoring request with these tools, you will need these essential pieces of information: Plugin name ( --plugin ) Domain name ( --domain ) Scoring type to perform ( --region for region-scoring, --global for global-scoring, others for less-common plugins) Input audio file or list of input audio files ( --input for a single file, --input_list for a list of files) The flag for providing each piece of information is the same for both tools, as shown in the list above. For more information on what the difference is between a plugin and a domain, refer to the Plugins Overview . For more information on the domains available for each plugin, refer to documentation page for that specific plugin. To see which plugins and domains you have installed and running in your specific OLIVE environment, refer to the server startup status message, that appears when you start the server: martini.sh start , then martini.sh log once the server is running for martini-based OLIVE packages (most common) ./run.sh for non-martini docker OLIVE packages oliveserver for native linux OLIVE packages Or exercise the --print option for each tool to query the server and print the available plugins and domains: OliveAnalyze (Java) $ ./OliveAnalyze --print olivepyanalyze (python) $ olivepyanalyze --print Example output: 2022-06-14 12:12:25.786 INFO com.sri.speech.olive.api.Server - Connected to localhost - request port: 5588 status_port: 5589 Found 8 plugin(s): Plugin: sad-dnn-v7.0.2 (SAD,Speech) v7.0.2 has 2 domain(s): Domain: fast-multi-v1, Description: Trained with Telephony, PTT and Music data Domain: multi-v1, Description: Trained with Telephony, PTT and Music data Plugin: asr-dynapy-v3.0.0 (ASR,Content) v3.0.0 has 9 domain(s): Domain: english-tdnnChain-tel-v1, Description: Large vocabulary English DNN model for 8K data Domain: farsi-tdnnChain-tel-v1, Description: Large vocabulary Farsi DNN model for 8K data Domain: french-tdnnChain-tel-v2, Description: Large vocabulary African French DNN Chain model for 8K data Domain: iraqiArabic-tdnnChain-tel-v1, Description: Large vocabulary Iraqi Arabic DNN Chain model for 8K data Domain: levantineArabic-tdnnChain-tel-v1, Description: Large vocabulary Levantine Arabic DNN Chain model for 8K data Domain: mandarin-tdnnChain-tel-v1, Description: Large vocabulary Mandarin DNN model for clean CTS 8K data Domain: pashto-tdnnChain-tel-v1, Description: Large vocabulary Pashto DNN Chain model for 8K data Domain: russian-tdnnChain-tel-v2, Description: Large vocabulary Russian DNN model for 8K data Domain: spanish-tdnnChain-tel-v1, Description: Large vocabulary Spanish DNN model for clean CTS 8K data Plugin: sdd-diarizeEmbedSmolive-v1.0.0 (SDD,Speaker) v1.0.0 has 1 domain(s): Domain: telClosetalk-int8-v1, Description: Speaker Embeddings Framework Plugin: tmt-neural-v1.0.0 (TMT,Content) v1.0.0 has 3 domain(s): Domain: cmn-eng-nmt-v1, Description: Mandarin Chinese to English NMT Domain: rus-eng-nmt-v1, Description: Russian to English NMT Domain: spa-eng-nmt-v3, Description: Spanish to English NMT Plugin: ldd-embedplda-v1.0.1 (LDD,Language) v1.0.1 has 1 domain(s): Domain: multi-v1, Description: PNCC bottleneck domain suitable for mixed conditions (tel/mic/compression) Plugin: sdd-diarizeEmbedSmolive-v1.0.2 (SDD,Speaker) v1.0.2 has 1 domain(s): Domain: telClosetalk-smart-v1, Description: Speaker Embeddings Framework Plugin: sid-dplda-v2.0.2 (SID,Speaker) v2.0.2 has 1 domain(s): Domain: multi-v1, Description: Speaker Embeddings DPLDA Plugin: lid-embedplda-v3.0.1 (LID,Language) v3.0.1 has 1 domain(s): Domain: multi-v1, Description: PNCC Bottleneck embeddings suitable for mixed conditions (tel/mic/compression) Examples To perform a global score analysis on a single file with the speaker identification plugin sid-dplda-v2.0.2 , using the multi-v1 domain, the calls for each would look like this: OliveAnalyze (Java) $ ./OliveAnalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav olivepyanalyze (python) $ olivepyanalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav Performing region scoring instead, using a transcription plugin, asr-dynapy-v3.0.0 , via the english domain english-tdnnChain-tel-v1 on a list of audio files would be performed with: OliveAnalyze (Java) $ ./OliveAnalyze --plugin asr-dynapy-v3.0.0 --domain english-tdnnChain-tel-v1 --region --input_list ~/path/to/list-of-audio-files.txt olivepyanalyze (python) $ olivepyanalyze --plugin asr-dynapy-v3.0.0 --domain english-tdnnChain-tel-v1 --region --input_list ~/path/to/list-of-audio-files.txt Where the format of the input list is simply a text file with a path to an audio file on each line. For example: /data/mixed-language/testFile1.wav /data/mixed-language/testFile2.wav /data/mixed-language/testFile3.wav /moreData/test-files/unknown1.wav Workflow (Analysis) OLIVE Workflows provide a simple way of creating a sort of 'recipe' that specifies how to deal with the input data and one or more OLIVE plugins. It allows complex operations to be requested and performed with a single, simple call to the system by allowing complexities and specific knowledge to be encapsulated within the workflow itself, rather than known and implemented by the user at run time. Due to off-loading this burden, operating with workflows is much simpler than calling the plugin(s) directly - typically all that is needed from the user to request an analysis from a workflow client is the workflow itself, and one or more input files. There are more options available to the workflow client, as shown in the usage statement: OliveWorkflow (Java) ./OliveWorkflow -h usage: OliveWorkflow --cabundlepass <arg> Specifies the certificate authority passphrase of the certificate authority. --cabundlepath <arg> Specifies the path of the certificate authority --certpass <arg> Specifies the certificate passphrase to unlock the encrypted certificate key --certpath <arg> Specifies the path of the certificate -h Print this help message -i,--input <arg> The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list <arg> A list of files to analyze. One file per line. --options <arg> options from FILE -p,--port <arg> Scenicserver port number. Defauls is 5588 --print_class_ids Print the class IDs available for analysis in the specified workflow. --print_tasks Print the workflow analysis tasks. -s,--server <arg> Scenicserver hostname. Default is localhost --secure Indicates a secure connection should be made. Requires --certpath, --cabundlepath, --certpass, and --cabundlepass to be set. -t,--timeout <arg> Timeout ( in seconds ) when waiting for server response. Default is 10 seconds --workflow <arg> The workflow definition to use. olivepyworkflow (Python) $ olivepyworkflow -h usage: olivepyworkflow [ -h ] [ --tasks ] [ --print_class_ids ] [ --class_ids CLASS_IDS ] [ --print_actualized ] [ --print_workflow ] [ -s SERVER ] [ -P PORT ] [ --upload_port UPLOAD_PORT ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --text ] [ --options OPTIONS ] [ --path ] [ --upload_files ] [ --secure ] [ --certpath CERTPATH ] [ --keypath KEYPATH ] [ --keypass KEYPASS ] [ --cabundlepath CABUNDLEPATH ] [ --debug ] workflow Perform OLIVE analysis using a Workflow Definition file positional arguments: workflow The workflow definition to use. options: -h, --help show this help message and exit --tasks Print the workflow analysis tasks. --print_class_ids Print the class IDs available for analysis in the specified workflow. --class_ids CLASS_IDS Send class IDs with the analysis request. --print_actualized Print the actualized workflow info. --print_workflow Print the workflow definition file info ( before it is actualized, if requested ) -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. --upload_port UPLOAD_PORT The upload port to use when specifying --upload_files. -t TIMEOUT, --timeout TIMEOUT The timeout ( in seconds ) to wait for a response from the server -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --text Indicates that input ( or input list ) is a literal text string to send in the analysis request. --options OPTIONS A JSON formatted string of workflow options such as [{ \"task\" : \"SAD\" , \"options\" : { \"filter_length\" :99, \"interpolate\" :1.0 }] or { \"filter_length\" :99, \"interpolate\" :1.0, \"name\" : \"midge\" } , where the former options are only applied to the SAD task, and the later are applied to all tasks --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option --upload_files Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation. --secure Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. --certpath CERTPATH Specifies the path of the certificate --keypath KEYPATH Specifies the path of the certificate key --keypass KEYPASS Specifies the certificate passphrase to unlock the encrypted certificate key --cabundlepath CABUNDLEPATH Specifies the path of the certificate authority --debug Debug mode But their use rarely necessary, and is reserved for advanced users or specific system testing. Generically, calling a workflow client will look like this: Python olivepyworkflow - Single Input File $ olivepyworkflow --input ~/path/to/test-file1.wav <workflow> olivepyworkflow - List of Input Files $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt <workflow> Java OliveWorkflow - Single Input File $ ./OliveWorkflow --input ~/path/to/test-file1.wav --workflow <workflow> OliveWorkflow - List of Input Files $ ./OliveWorkflow --input_list ~/path/to/list-of-audio-files.txt --workflow <workflow> As an example of the power of workflows, this request calls the SmartTranscription workflow - that performs Speech Activity Detection (region scoring), Speaker Diarization and Detection (region scoring), Language Detection (region scoring), and then Automatic Speech Recognition (region scoring) on any sections of each input file that are detected to be a language that ASR currently has support for, and returns all of the appropriate results in a JSON structure. Performing this same task by calling plugins directly, this same functionality would be a minimum of 4 separate calls to OLIVE; significantly more if more than one language is detected being spoken in the file. Python Single Input File $ olivepyworkflow --input ~/path/to/test-file1.wav ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json List of Input Files $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json Java OliveWorkflow - Single Input File $ ./OliveWorkflow --input ~/path/to/test-file1.wav --workflow ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json OliveWorkflow - List of Input Files $ ./OliveWorkflow --input_list ~/path/to/list-of-audio-files.txt --workflow ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json Output Format (Workflow) Workflows are generally customer/user-specific and can be quite specialized - the output format and structure will depend heavily on the individual workflow itself and the tasks being performed. All of the information pieces that define each scoring type are still reported for each result, but the results are organized into a single JSON structure for the workflow call. This means that the output of a region scoring plugin within the workflow is still one or more sets of: < region_start_timestamp > < region_end_timestamp > < class_id > < score > But the data is arranged into the JSON structure and will be nested depending on the structure of the workflow itself and how the audio is routed by the workflow. For more detailed information on the structure of this JSON message and the inner-workings of workflows, please refer to the OLIVE Workflow API documentation. A brief, simplified summary to jump start working with workflow output follows. The main skeleton structure of the results output is shown below, along with an actual example. The results are provided as a result for each input file, that lists the job name(s), some metadata about the input audio and how it was processed, and then the returned results (if any) for each task, which is generally a plugin. Simplified/Generic Output Example (single-job workflow) Work fl ow a nal ysis resul ts : [ { \"job_name\" : <work fl ow job na me> , \"data\" : [ { \"data_id\" : <da ta ID , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <da ta label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ], < tas k 2 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ], < tas k 3 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ] } } ] English Transcription Output Example (single-job workflow) Work fl ow a nal ysis resul ts : [ { \"job_name\" : \"SAD, SDD, and ASR English Workflow\" , \"data\" : [ { \"data_id\" : \"z_eng_englishdemo.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 5.932625 , \"number_channels\" : 1 , \"label\" : \"z_eng_englishdemo.wav\" , \"id\" : \"0b04c7497521d53a5d6939533a55c461795f9d685b1bd19fd9031fc6f3997a8f\" } ], \"tasks\" : { \"SAD_REGIONS\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v7.0.2\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 5.93 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"SDD\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SDD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sdd-diarizeEmbedSmolive-v1.0.2\" , \"domain\" : \"telClosetalk-smart-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.1 , \"end_t\" : 5.0 , \"class_id\" : \"unknownspk00\" , \"score\" : 1.4 } ] } } ], \"ASR\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"ASR\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"asr-dynapy-v3.0.0\" , \"domain\" : \"english-tdnnChain-tel-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.15 , \"end_t\" : 0.51 , \"class_id\" : \"hello\" , \"score\" : 100.0 }, { \"start_t\" : 0.54 , \"end_t\" : 0.69 , \"class_id\" : \"my\" , \"score\" : 100.0 }, { \"start_t\" : 0.69 , \"end_t\" : 0.87 , \"class_id\" : \"name\" , \"score\" : 99.0 }, { \"start_t\" : 0.87 , \"end_t\" : 1.05 , \"class_id\" : \"is\" , \"score\" : 99.0 }, { \"start_t\" : 1.05 , \"end_t\" : 1.35 , \"class_id\" : \"evan\" , \"score\" : 88.0 }, { \"start_t\" : 1.35 , \"end_t\" : 1.47 , \"class_id\" : \"this\" , \"score\" : 99.0 }, { \"start_t\" : 1.5 , \"end_t\" : 1.98 , \"class_id\" : \"audio\" , \"score\" : 95.0 }, { \"start_t\" : 1.98 , \"end_t\" : 2.16 , \"class_id\" : \"is\" , \"score\" : 74.0 }, { \"start_t\" : 2.16 , \"end_t\" : 2.31 , \"class_id\" : \"for\" , \"score\" : 99.0 }, { \"start_t\" : 2.31 , \"end_t\" : 2.4 , \"class_id\" : \"the\" , \"score\" : 99.0 }, { \"start_t\" : 2.4 , \"end_t\" : 2.91 , \"class_id\" : \"purposes\" , \"score\" : 100.0 }, { \"start_t\" : 2.91 , \"end_t\" : 3.06 , \"class_id\" : \"of\" , \"score\" : 99.0 }, { \"start_t\" : 3.12 , \"end_t\" : 3.81 , \"class_id\" : \"demonstrating\" , \"score\" : 100.0 }, { \"start_t\" : 3.81 , \"end_t\" : 3.96 , \"class_id\" : \"our\" , \"score\" : 78.0 }, { \"start_t\" : 4.05 , \"end_t\" : 4.44 , \"class_id\" : \"language\" , \"score\" : 100.0 }, { \"start_t\" : 4.44 , \"end_t\" : 4.53 , \"class_id\" : \"and\" , \"score\" : 93.0 }, { \"start_t\" : 4.53 , \"end_t\" : 4.89 , \"class_id\" : \"speaker\" , \"score\" : 100.0 }, { \"start_t\" : 4.89 , \"end_t\" : 5.01 , \"class_id\" : \"i.\" , \"score\" : 99.0 }, { \"start_t\" : 5.01 , \"end_t\" : 5.22 , \"class_id\" : \"d.\" , \"score\" : 99.0 }, { \"start_t\" : 5.22 , \"end_t\" : 5.85 , \"class_id\" : \"capabilities\" , \"score\" : 99.0 } ] } } ] } } ] Each task output will typically be for a single plugin, and will be outputting the information provided by a Region Scorer or Global Scorer or Text Transformer in the case of Machine Translation, depending on how the workflow is using the plugin. The format of each result sub part is: Global Score < tas k na me> : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : < tas k t ype , ge nerall y LID , SID , e t c.> , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"score\" : [ { \"class_id\" : <class 1 > , \"score\" : <class 1 score> }, { \"class_id\" : <class 2 > , \"score\" : <class 2 score> }, ... { \"class_id\" : <class N> , \"score\" : <class N score> } ] } } ] Region Score < tas k na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype , t ypically ASR , SDD , SAD , e t c.> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 de te c te d class> , \"score\" : <regio n 1 score> }, { \"start_t\" : <regio n 2 s tart t ime (s)> , \"end_t\" : <regio n 2 e n d t ime (s)> , \"class_id\" : <regio n 2 de te c te d class> , \"score\" : <regio n 2 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N de te c te d class> , \"score\" : <regio n N score> }, ] } } ] Text Transformer < tas k na me> : [ { \"task_trait\" : \"TEXT_TRANSFORMER\" , \"task_type\" : < tas k t ype , t ypically MT> , \"message_type\" : \"TEXT_TRANSFORM_RESULT\" , \"plugin\" : <plugi n na me> , \"domain\" : <domai n na me> , \"analysis\" : { \"transformation\" : [ { \"class_id\" : \"test_label\" , \"transformed_text\" : < t he translate d/ transf ormed te x t re turne d fr om t he plugi n > } ] } } ] Many workflows consist of a single job , and bundle all plugin tasks into this single job, as seen above. More complex workflows, or what OLIVE calls \"Conditional Workflows\" can pack multiple jobs into a single workflow. This happens when there are certain tasks in the workflow that depend on other tasks in the workflow - for example when OLIVE needs to choose the appropriate Speech Recognition (ASR) language to use, depending on what language is detected being spoken by Language Identification (LID) or Language Detection (LDD). In this case, the LID/LDD is separated into one job, and the ASR into another, that is triggered to run once LID/LDD's decision is known. In this case, the results from each job are grouped accordingly in the results output. Below shows a simplified output from a workflow that includes three jobs; \"job 1\", \"job 2\", \"job 3\", and a real-life example output from the SmartTranscription conditional workflow that also has three jobs; the first performs Speech Activity Detection and Language Identification (LID); Smart Translation SAD and LID Pre-processing the second uses the language decision from Language Identification to choose the appropriate (if any) language and domain for Automatic Speech Recognition (ASR) and runs that, Dynamic ASR the third takes the output transcript from ASR and the language decision from LID and choose the appropriate (if any) language and domain for Text Machine Translation, and runs that. Dynamic MT As you can see below, these jobs are listed separately in the JSON for each result: Simplified/Generic Output Example (multi-job workflow) Work fl ow a nal ysis resul ts : [ { \"job name\" : <job 1 na me> , \"data\" : [ { \"data_id\" : <da ta ide nt i f ier , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <audio label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me (job 1 )> : [ { < tas k 1 resul ts > } ], ... < tas k N na me (job 1 )> : [ { < tas k N resul ts > } ] } }, { \"job name\" : <job 2 na me> , \"data\" : [ { \"data_id\" : <da ta ide nt i f ier , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <audio label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me (job 2 )> : [ { < tas k 1 resul ts > } ], ... < tas k N na me (job 2 )> : [ { < tas k N resul ts > } ] } }, ... <repea t i f more jobs> ] SmartTranslation Output Example Work fl ow a nal ysis resul ts : [ { \"job_name\" : \"Smart Translation SAD and LID Pre-processing\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 8.0 , \"number_channels\" : 1 , \"label\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"id\" : \"68984a7356fa1ea05f8e985868eb93e066ce80a0f4bf848edf55d547cfcbab41\" } ], \"tasks\" : { \"SAD_REGIONS\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v7.0.2\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 8.0 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"LID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"lid-embedplda-v3.0.1\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"Mandarin\" , \"score\" : 3.5306692 }, { \"class_id\" : \"Korean\" , \"score\" : -1.9072952 }, { \"class_id\" : \"Japanese\" , \"score\" : -3.7805116 }, { \"class_id\" : \"Tagalog\" , \"score\" : -7.4819508 }, { \"class_id\" : \"Vietnamese\" , \"score\" : -8.094855 }, { \"class_id\" : \"Iraqi Arabic\" , \"score\" : -10.63325 }, { \"class_id\" : \"Levantine Arabic\" , \"score\" : -10.694491 }, { \"class_id\" : \"French\" , \"score\" : -11.542379 }, { \"class_id\" : \"Pashto\" , \"score\" : -12.11981 }, { \"class_id\" : \"English\" , \"score\" : -12.323014 }, { \"class_id\" : \"Modern Standard Arabic\" , \"score\" : -12.626052 }, { \"class_id\" : \"Spanish\" , \"score\" : -13.469315 }, { \"class_id\" : \"Iranian Persian\" , \"score\" : -13.763366 }, { \"class_id\" : \"Amharic\" , \"score\" : -17.129797 }, { \"class_id\" : \"Portuguese\" , \"score\" : -17.31257 }, { \"class_id\" : \"Russian\" , \"score\" : -18.770994 } ] } } ] } }, { \"job_name\" : \"Dynamic ASR\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 8.0 , \"number_channels\" : 1 , \"label\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"id\" : \"68984a7356fa1ea05f8e985868eb93e066ce80a0f4bf848edf55d547cfcbab41\" } ], \"tasks\" : { \"ASR\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"ASR\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"asr-dynapy-v3.0.0\" , \"domain\" : \"mandarin-tdnnChain-tel-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 0.18 , \"class_id\" : \"\u8ddf\" , \"score\" : 31.0 }, { \"start_t\" : 0.18 , \"end_t\" : 0.36 , \"class_id\" : \"\u4e00\u4e2a\" , \"score\" : 83.0 }, { \"start_t\" : 0.36 , \"end_t\" : 0.66 , \"class_id\" : \"\u80af\u5b9a\" , \"score\" : 100.0 }, { \"start_t\" : 0.66 , \"end_t\" : 0.81 , \"class_id\" : \"\u662f\" , \"score\" : 83.0 }, { \"start_t\" : 0.81 , \"end_t\" : 1.23 , \"class_id\" : \"\u5317\u4eac\" , \"score\" : 95.0 }, { \"start_t\" : 1.23 , \"end_t\" : 1.47 , \"class_id\" : \"\u554a\" , \"score\" : 96.0 }, { \"start_t\" : 2.07 , \"end_t\" : 2.49 , \"class_id\" : \"\u4ed6\u4fe9\" , \"score\" : 96.0 }, { \"start_t\" : 2.7 , \"end_t\" : 3.09 , \"class_id\" : \"\u4e0a\u6d77\" , \"score\" : 99.0 }, { \"start_t\" : 3.09 , \"end_t\" : 3.21 , \"class_id\" : \"\u7684\" , \"score\" : 99.0 }, { \"start_t\" : 3.21 , \"end_t\" : 3.57 , \"class_id\" : \"\u4eba\u53e3\" , \"score\" : 99.0 }, { \"start_t\" : 3.57 , \"end_t\" : 3.87 , \"class_id\" : \"\u597d\u50cf\" , \"score\" : 73.0 }, { \"start_t\" : 3.87 , \"end_t\" : 3.99 , \"class_id\" : \"\u6ca1\" , \"score\" : 54.0 }, { \"start_t\" : 3.99 , \"end_t\" : 4.32 , \"class_id\" : \"\u5317\u4eac\" , \"score\" : 74.0 }, { \"start_t\" : 4.32 , \"end_t\" : 4.68 , \"class_id\" : \"\u591a\" , \"score\" : 99.0 }, { \"start_t\" : 4.86 , \"end_t\" : 5.19 , \"class_id\" : \"\u4f46\u662f\" , \"score\" : 100.0 }, { \"start_t\" : 5.4 , \"end_t\" : 5.91 , \"class_id\" : \"\u4e0d\u77e5\u9053\" , \"score\" : 100.0 }, { \"start_t\" : 6.06 , \"end_t\" : 6.48 , \"class_id\" : \"@reject@\" , \"score\" : 62.0 }, { \"start_t\" : 6.69 , \"end_t\" : 7.05 , \"class_id\" : \"\u5176\u4ed6\" , \"score\" : 93.0 }, { \"start_t\" : 7.05 , \"end_t\" : 7.2 , \"class_id\" : \"\u7684\" , \"score\" : 97.0 }, { \"start_t\" : 7.65 , \"end_t\" : 7.89 , \"class_id\" : \"\u4e0a\" , \"score\" : 32.0 }, { \"start_t\" : 7.89 , \"end_t\" : 7.95 , \"class_id\" : \"\u554a\" , \"score\" : 67.0 } ] } } ] } }, { \"job_name\" : \"Dynamic MT\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"WORKFlOW_TEXT_RESULT\" , \"text\" : \"\u8ddf \u4e00\u4e2a \u80af\u5b9a \u662f \u5317\u4eac \u554a \u4ed6\u4fe9 \u4e0a\u6d77 \u7684 \u4eba\u53e3 \u597d\u50cf \u6ca1 \u5317\u4eac \u591a \u4f46\u662f \u4e0d\u77e5\u9053 @reject@ \u5176\u4ed6 \u7684 \u4e0a \u554a\" } ], \"tasks\" : { \"MT\" : [ { \"task_trait\" : \"TEXT_TRANSFORMER\" , \"task_type\" : \"MT\" , \"message_type\" : \"TEXT_TRANSFORM_RESULT\" , \"plugin\" : \"tmt-neural-v1.0.0\" , \"domain\" : \"cmn-eng-nmt-v1\" , \"analysis\" : { \"transformation\" : [ { \"class_id\" : \"test_label\" , \"transformed_text\" : \"with someone in beijing they don't seem to have a population in shanghai but we don't know what else to do\" } ] } } ] } } ] Enrollment Requests Enrollments are a sub-set of classes that the user can create and/or modify. These are used for classes that cannot be known ahead of time and therefore can't be pre-loaded into the system, such as specific speakers or keywords of interest. To determine if a plugin supports or requires enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page, linked from the navigation or the Release Plugins page. Enrollment list format As with analysis, both the Java and Python tools were designed to share as much of a common interface as possible, and as such share an input list format when providing exemplars for enrollment. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TPD, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in each plugin's documentation. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7 Plugin Direct (Enrollment) Performing an enrollment request is similar to an analysis request and is again very similar between the two tools. The usage statements for each can be examined by invoking each with their -h or --help flag: OliveEnroll (Java) $ ./OliveEnroll -h usage: OliveEnroll --cabundlepass <arg> Specifies the certificate authority passphrase of the certificate authority. --cabundlepath <arg> Specifies the path of the certificate authority --certpass <arg> Specifies the certificate passphrase to unlock the encrypted certificate key --certpath <arg> Specifies the path of the certificate --channel <arg> Process stereo files using channel NUMBER --classes Print class names if also printing plugin/domain names. Must use with --print option. Default is to not print class IDs --decoded Sennd audio file as a decoded PCM16 sample buffer instead of a serialized buffer. The file must be a WAV file --domain <arg> Use Domain NAME --enroll <arg> Enroll speaker NAME. If no name specified then , the pem or list option must specify an input file --export <arg> Export speaker NAME to an EnrollmentModel ( enrollment.tar.gz ) -h Print this help message -i,--input <arg> NAME of the input file ( input varies by plugin: audio, image, or video ) --import <arg> Import speaker from EnrollmentModel FILE --input_list <arg> Batch enroll using this input list FILE having multiple filenames/class IDs or PEM formmated file --nobatch Disable batch enrollment when using pem or list input files, so that files are processed serially --options <arg> Enrollment options from FILE --output <arg> Write any output to DIR, default is ./ -p,--port <arg> Scenicserver port number. Defauls is 5588 --path Send the path to the audio file instead of a ( serialized ) buffer. The server must have access to this path. --plugin <arg> Use Plugin NAME --print Print all plugins and domains that suport enrollment and/or class import and export --remove <arg> Remove audio enrollment for NAME -s,--server <arg> Scenicserver hostname. Default is localhost --secure Indicates a secure connection should be made. Requires --certpath, --cabundlepath, --certpass, and --cabundlepass to be set. -t,--timeout <arg> timeout ( in seconds ) when waiting for server response. Default is 10 seconds --unenroll <arg> Un-enroll all enrollments for speaker NAME -v,--vec <arg> PATH to a serialized AudioVector, for plugins that support audio vectors in addition to wav files olivepyenroll (Python) $ olivepyenroll -h usage: olivepyenroll [ -h ] [ -C CLIENT_ID ] [ -D ] [ -p PLUGIN ] [ -d DOMAIN ] [ -e ENROLL ] [ -u UNENROLL ] [ -s SERVER ] [ -P PORT ] [ --upload_port UPLOAD_PORT ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --nobatch NOBATCH ] [ --path ] [ --upload_files ] [ --secure ] [ --certpath CERTPATH ] [ --keypath KEYPATH ] [ --keypass KEYPASS ] [ --cabundlepath CABUNDLEPATH ] options: -h, --help show this help message and exit -C CLIENT_ID, --client-id CLIENT_ID Experimental: the client_id to use -D, --debug The domain to use -p PLUGIN, --plugin PLUGIN The plugin to use. -d DOMAIN, --domain DOMAIN The domain to use -e ENROLL, --enroll ENROLL Enroll with this name. -u UNENROLL, --unenroll UNENROLL Uneroll with this name. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. --upload_port UPLOAD_PORT The upload port to use when specifying --upload_files. -t TIMEOUT, --timeout TIMEOUT The timeout to use -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --nobatch NOBATCH Disable batch enrollment when using pem or list input files, so that files are processed individually. --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option --upload_files Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation. --secure Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. --certpath CERTPATH Specifies the path of the certificate --keypath KEYPATH Specifies the path of the certificate key --keypass KEYPASS Specifies the certificate passphrase to unlock the encrypted certificate key --cabundlepath CABUNDLEPATH Specifies the path of the certificate authority To perform an enrollment request with these tools, you will need these essential pieces of information: Plugin name ( --plugin ) Domain name ( --domain ) One of: A properly formatted enrollment list ( --input_list ), if providing multiple files at once (see below for formatting) An input audio file ( --input ) AND the name of the class you wish to enroll ( --enroll for OliveEnroll , -e or --enroll for olivepyanalyze ) Generically, this looks like this for a single file input: OliveEnroll (Java) $ ./OliveEnroll --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> olivepyenroll (python) $ olivepyenroll --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> A more specific example: OliveEnroll (Java) $ ./OliveEnroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" olivepyenroll (python) $ olivepyenroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" Or if providing the enrollment list format shown above, the call is even simpler. Generically: OliveEnroll (Java) $ ./OliveEnroll --plugin <plugin> --domain <domain> --input_list <path to enrollment text file> olivepyenroll (python) $ olivepyenroll --plugin <plugin> --domain <domain> --input_list <path to enrollment text file> Specific: OliveEnroll (Java) $ ./OliveEnroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input_list ~/path/to/enrollment_input.txt olivepyenroll (python) $ olivepyenroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input_list ~/path/to/enrollment_input.txt Where the enrollment_input.txt file might look like: /some/data/somewhere/inputFile1.wav Logan /some/other/data/somewhere/else/LoganPodcast.wav Logan /yet/another/data/directory/charlie-speaks.wav Charlie Workflow (Enrollment) In the most basic case, enrollment using a workflow is just as simple as scoring with a workflow. This is becuase most workflows will only have a single enrollment-capable job; a job is a subset of the the tasks a workflow is performing, typically linked to a single plugin. In the rare case that you're using a workflow with multiple supported enrollment jobs, you will need to specify which job to enroll to. See the Advanced Workflow Enrollment section below. Workflow enrollment is performed by using the olivepyworkflowenroll utility, whose help/usage statement is: olivepyworkflowenroll usage usage: olivepyworkflowenroll [ -h ] [ --print_jobs ] [ --job JOB ] [ --enroll ENROLL ] [ --unenroll UNENROLL ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --path ] [ -s SERVER ] [ -P PORT ] [ -t TIMEOUT ] [ --secure ] [ --certpath CERTPATH ] [ --keypath KEYPATH ] [ --keypass KEYPASS ] [ --cabundlepath CABUNDLEPATH ] workflow Perform OLIVE enrollment using a Workflow Definition file positional arguments: workflow The workflow definition to use. options: -h, --help show this help message and exit --print_jobs Print the supported workflow enrollment jobs. --job JOB Enroll/Unenroll an Class ID for a job ( s ) in the specified workflow. If not specified enroll or unenroll for ALL enrollment/unenrollment jobst --enroll ENROLL Enroll using this ( class ) name. Should be used with the job argument to specify a target job to enroll with ( if there are more than one enrollment jobs ) --unenroll UNENROLL Enroll using this ( class ) name. Should be used with the job argument to specify a job to unenroll ( if there are more than one unenrollment jobs ) -i INPUT, --input INPUT The data input to enroll. Either a pathname to an audio/image/video file or a string for text input --input_list INPUT_LIST A list of files to enroll. One file per line plus the class id to enroll. --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. -t TIMEOUT, --timeout TIMEOUT The timeout ( in seconds ) to wait for a response from the server --secure Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. --certpath CERTPATH Specifies the path of the certificate --keypath KEYPATH Specifies the path of the certificate key --keypass KEYPASS Specifies the certificate passphrase to unlock the encrypted certificate key --cabundlepath CABUNDLEPATH Specifies the path of the certificate authority If there is only one supported enrollment job in the workflow, using this utility for enrollment is very similar to the enrollment utilities above; but a workflow is provided instead of a plugin and domain combination. As with the other enrollment utilities, olivepyworkflowenroll supports both single-file enrollment and batch enrollment using an enrollment-formatted text file. Generically, this looks like: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input <path to audio file> --enroll <class name> <workflow> olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list <path to enrollment file> <workflow> And a specific example of each: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input ~/path/to/enroll-file1.wav --enroll \"Logan\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list ~/path/to/enrollment_input.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json Important: Note that in OLIVE, when you enroll a class, you are enrolling to a plugin and domain , and enrollments are shared server-wide. Even when you enroll using a workflow and the olivepyworkflow utility, enrollments are associated with the specific plugin/domain that the workflow is using under the hood. Any enrollments made to a workflow will be available to anyone else who may be using that server instance, and will also be made available to anyone interacting with the individual plugin - whether directly or via a workflow. As a more concrete example of this, the \"SmartTranscription\" workflow that is sometimes provided with OLIVE, that performs Speech Activity Detection, Speaker Detection, Language Detection, and Speech Recognition on supported languages has a single plugin that supports enrollments; Speaker Detection. As a result, the workflow is set up to have a single enrollment job, to allow workflow users to enroll new speakers to be detected by this plugin. When enrollment is performed with this workflow, the newly created speaker model is created by and for the Speaker Detection plugin itself, and goes into the global OLIVE enrollment space. If a file is analyzed by directly calling this Speaker Detection plugin, the new enrollment will be part of the pool of target speakers the plugin will search for. More information on this concept of \"Workflow Enrollment Jobs\" is provided in the next section. Advanced Workflow Enrollment - Jobs It's rare, but possible for a workflow to bundle multiple enrollment-capable plugin capabilities into one. One example could be combining Speaker Detection in a workflow that also runs Query-by-Example Keyword Spotting , both of which rely on user enrollments to define their target classes. When this happens, if a user wishes to maintain the ability to enroll separate classes into each enrollable plugin, the workflow needs to expose these different enrollment tasks as separate jobs in the workflow enrollment capabilities. If this is necessary, the workflow will come from SRI configured appropriately - the user need only be concerned with how to specify which job to enroll with. To query which enrollment jobs are available to a workflow, use the olivepyworkflowenroll tool with the --print_jobs flag: $ olivepyworkflowenroll --print_jobs <workflow> Investigating the \"SmartTranscription\" workflow we briefly mentioned above: $ olivepyworkflowenroll --print_jobs SmartTranscriptionFull.workflow.json enrolling 0 files Enrollment jobs '[' SDD Enrollment ']' Un-Enrollment jobs '[' SDD Unenrollment ']' We see that there is only a single Enrollment job available; SDD Enrollment . If there were others, they would be listed in this output. Now that the desired job name is known, enrolling with the specified job is done by supplying that job name to the --job flag; in this case: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input ~/path/to/enroll-file1.wav --enroll \"Logan\" --job \"SDD Enrollment\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list ~/path/to/enrollment_input.txt --job \"SDD Enrollment\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json Stateless Enrollment and Scoring With OLIVE 6.0.0, the ability to perform Stateless Enrollment or Vectorization was introduced, to allow decoupling of target class enrollments from being managed by the OLIVE server. The page linked here covers more details on why a user may want to use Stateless Enrollment, but the key difference here is that users will need to provide the models, also called vectors of the classes they wish to score against, at score time. The olivepy CLI utility has added support for stateless enrollment, and an example follows. Retrieving an Enrollment Vector Creating an enrollment vector for use with stateless enrollment is very similar to creating an OLIVE server managed enrollment. From an olivepy utility standpoint, the main difference is the --vectorize flag provided to olivepyenroll , which informs the utility to retrieve the vector and store it locally, rather than have the server retain it. A generic example: $ olivepyenroll --vectorize --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> A more specific example: $ olivepyenroll --vectorize --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" In the specific case, this should create a directory structure that looks like this: user@user-computer:~/workspace/sid-dplda-v2.0.2 $ tree . \u2514\u2500\u2500 multi-v1 \u2514\u2500\u2500 enrollments \u2514\u2500\u2500 Logan \u2514\u2500\u2500 5a169f97-504f-4819-ad71-fab3faa8e4db You can use input_list as an option to olivepyenroll as well, and it will create, retrieve, and write all of the vectors corresponding to the input data and labels. Warning Saving to disk is FOR DEMO/TESTING PURPOSES ONLY . A production system should consider using SQLite or similar technology which allows associating a speaker label String to binary data blobs rather than storing on disk if they require long-term persistent storage. The examples are to illustrate functionality and proof of concept of the audio vectors. Scoring using Stored Enrollment Vectors Like olivepyenroll , olivepyanalyze has been modified to support scoring with offline or stateless enrollment vectors. To do this, the directories vectors corresponding to the classes a user would like to score must be passed in as part of a list sent to the --class_vectors flag. The format for this list is: <path to>/class1vectors <path to>/class2vectors ... <path to>/classNvectors An example of what this might look like, if the olivepyenroll tool was used to create the enrollment vectors, could look like: /home/user/workspace/sid-dplda/multi-v1/enrollments/Bob /home/user/workspace/sid-dplda/multi-v1/enrollments/Logan /home/user/workspace/sid-dplda/multi-v1/enrollments/Pierre /home/user/workspace/sid-dplda/multi-v1/enrollments/Orla Where each of these (i.e. Bob, logan, Pierre, Orla) are directories created above, and containing individual enrollment vectors, which can then be provided to olivepyanalyze generically as: $ olivepyanalyze --class_vectors <file list of enrollment vector directories> --global --plugin <plugin> --domain <domain> --input <path to audio file> Or more specifically: $ olivepyanalyze --class_vectors input_stateless.txt --global --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/test-file1.wav TLS/Secure Mode Note that when using OLIVE with TLS enabled, additional arguments are required to pass the appropriate certificates and other info necessary for the TLS configuration. In addition to what was shown above, these parameters must also be supplied for Java : secure - Enables TLS/secure connection mode. certpath - Specifies the path of the certificate. certpass - Specifies the certificate passphrase to unlock the encrypted certificate key. cabundlepath - Specifies the path of the certificate authority. cabundlepass - Specifies the certificate authority passphrase of the certificate authority. And these must also be supplied for Python : secure - Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. certpath - Specifies the path of the certificate. keypath - Specifies the path of the certificate key. keypass - Specifies the certificate passphrase to unlock the encrypted certificate key. cabundlepath - Specifies the path of the certificate authority. Plugin Direct TLS Examples Borrowing the global scoring examples from above, if TLS is enabled, they would need to be modified as follows: OliveAnalyze (Java) $ ./OliveAnalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav \\ --secure \\ --certpath /home/username/cert-directory/test-certs/client.p12 \\ --certpass \"my password\" \\ --cabundlepath /home/username/cert-directory/test-certs/root-ca.p12 \\ --cabundlepass \"another password\" olivepyanalyze (python) $ olivepyanalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav \\ --secure \\ --certpath /home/username/cert-directory/test-certs/client.crt \\ --keypath /home/username/cert-directory/test-certs/client.key \\ --keypass \"my password\" \\ --cabundlepath /home/username/cert-directory/test-certs/root-ca.crt Note that the same flags and info wil need to be provided for enrollment requests and any other communication or job submissions to a TLS-enabled OLIVE server. Workflow TLS Examples Modifying the workflow analysis examples above to enable TLS/secure communication: OliveWorkflow (Java) $ ./OliveWorkflow --input_list ~/path/to/list-of-audio-files.txt ~/olive5.4.0/oliveAppData/ --workflow workflows/SmartTranscription.workflow.json \\ --secure \\ --certpath /home/username/cert-directory/test-certs/client.p12 \\ --certpass \"my password\" \\ --cabundlepath /home/username/cert-directory/test-certs/root-ca.p12 \\ --cabundlepass \"another password\" olivepyworkflow (python) $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json \\ --secure \\ --certpath /home/username/cert-directory/test-certs/client.crt \\ --keypath /home/username/cert-directory/test-certs/client.key \\ --keypass \"my password\" \\ --cabundlepath /home/username/cert-directory/test-certs/root-ca.crt Note that the same flags and info wil need to be provided for workflow enrollment requests and any other communication or job submissions to a TLS-enabled OLIVE server. Job Cancellation Request This utility submits a request to the OLIVE server to cancel all jobs currently in progress or pending - this allows the user to recover from submitting a large job or batch of jobs that is no longer relevant, or was perhaps submitted to the wrong plugin(s) or workflow(s), without restarting the entire server. This functionality is currently only offered in the Python olivepy suite, and will be coming to the Java client in the future. This request is accomplished with the olivepycancel utility. Its usage is: olivepycancel (python) Usage usage: olivepycancel [ -h ] [ -P PORT ] [ -s SERVER ] [ -t TIMEOUT ] options: -h, --help show this help message and exit -P PORT, --port PORT The port to use. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -t TIMEOUT, --timeout TIMEOUT The timeout to use This utility is very simple - an example call with the default OLIVE server settings: olivepycancel (python) Example olivepycancel Adaption The adaption process is complicated, time-consuming, and plugin/domain specific. Use the SRI provided Python client (olivepylearn) or Java client (OliveLearn) to perform adaptation. To adapt using the olivepylearn utility: olivepylearn --plugin sad-dnn --domain multi-v1 -a TEST_NEW_DOMAIN -i /olive/sadRegression/lists/adapt_s.lst Where that adapt_s.lst looks like this: /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav S 20.469 21.719 /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav NS 10.8000 10.8229 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav S 72.898 73.748 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav NS 42.754 43.010 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav S 79.437 80.427 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav NS 61.459 62.003 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav S 11.0438 111.638 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav NS 69.058 73.090 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav S 112.936 113.656 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav NS 83.046 83.114 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav S 16.940 20.050 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav NS 59.794 59.858 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav S 87.280 88.651 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav NS 82.229 82.461 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav S 111.346 111.936 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav NS 83.736 84.446 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav S 77.291 78.421 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav NS 0 4.951 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav S 30.349 32.429 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav NS 100.299 101.647 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav S 46.527 49.147 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav NS 44.747 46.148 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav S 24.551 25.471 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav NS 52.033 52.211 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav S 88.358 93.418 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav NS 46.564 46.788 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav S 10.507 11.077 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav NS 41.099 41.227 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav S 61.072 63.002 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav NS 19.108 19.460 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav S 97.182 97.789 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav NS 71.711 71.732 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav S 114.312 117.115 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav NS 31.065 31.154 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav S 90.346 91.608 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav NS 50.028 51.377 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav S 75.986 76.596 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav NS 12.485 12.709 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav S 72.719 73.046 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav NS 51.923 53.379 /olive//sadRegression/audio/adapt/20131213T104502UTC_18520_A.wav NS 11.1192 112.761 /olive//sadRegression/audio/adapt/20131213T121501UTC_18530_A.wav NS 81.277 82.766 /olive//sadRegression/audio/adapt/20131213T124501UTC_18533_A.wav NS 83.702 84.501 /olive//sadRegression/audio/adapt/20131213T134502UTC_18567_A.wav NS 69.379 72.258 /olive//sadRegression/audio/adapt/20131217T015001UTC_18707_A.wav NS 5.099 10.507","title":"Java and Python CLI Client Utilities"},{"location":"clients.html#olive-java-and-python-clients","text":"","title":"OLIVE Java and Python Clients"},{"location":"clients.html#introduction","text":"Each OLIVE delivery includes two OLIVE client utilities - one written in Java, one written in Python. Out of the box, these tools allow a user to jump right in with running OLIVE if the GUI is not desired. These can also serve as code examples for integrating with OLIVE. This page primarily covers using these clients for processing audio, rather than integrating with the OLIVE API. For more information on integration, the nitty-gritty details of the OLIVE Enterprise API, and code examples, refer to these integration-focused pages instead: OLIVE Enterprise API Primer OLIVE Python Client API Documentation Integrating a Client API with OLIVE Building an OLIVE API Reference Implementation As far as the usage and capabilities of these tools, they were meant to mirror the Legacy CLI Tools as closely as possible, and shares many input/output formats and assumptions with those tools. As this document is still under construction, referring to this older guide may help fill in some useful information that may currently be missing from this page. Note that unlike the Legacy CLI tools, that are calling plugin code directly, these client tools require a running OLIVE Server. They are client utilities that are queueing and submitting job requests to the OLIVE server, which then manages the plugins themselves and actual audio processing. If you haven't already, please refer to the appropriate guide for setting up and starting an OLIVE server depending on your installation type: OLIVE Martini Docker-based Installation OLIVE Standalone Docker-based Installation Redhat/CentOS 7 Native Linux Installation OLIVE Server Guide","title":"Introduction"},{"location":"clients.html#client-setup-installation-requirements","text":"As a quick review, the contents of an OLIVE package typically look like this: olive6.0.0/ api/ java/ python/ docs/ martini/ -or- docker/ -or- runtime/ OliveGUI/ - (Optional) The OLIVE Nightingale GUI (not included in all deliveries) oliveAppData/ The clients this page describes are contained in the bolded api/ directory above.","title":"Client Setup, Installation, Requirements"},{"location":"clients.html#java-oliveanalyze","text":"The Java tools are the most full-featured with respect to tasking individual plugins. They are asynchronous, and better able to deal with large amounts of file submissions by parallelizing the submission of large lists of files. If the primary task is enrolling and scoring audio files with individual plugins, the Java tools, what we call the OliveAnalyze suite. The tools themselves do not need to be 'installed'. For convenience, their directory can be added to your $PATH environment variable, so that they can be called from anywhere: $ export PATH =$ PATH : < path >/ olive6 . 0.0 / api / java / bin / $ OliveAnalyze - h But they can also be left alone and called directly, as long as their full or relative path is present: # From inside olive6.0.0/api/java/bin: $ ./OliveAnalyze -h # From inside olive6.0.0/: $ ./api/java/bin/OliveAnalyze -h # From elsewhere: $ <path>/olive6.0.0/api/java/bin/OliveAnalyze -h These tools depend on OpenJDK 11 or newer being installed. Refer to OpenJDK for more information on downloading and installing this for your operating system. The full list of utilities in this suite are as follows: OliveAnalyze OliveAnalyzeText OliveEnroll OliveLearn (rarely used) OliveWorkflow But the most commonly used are OliveAnalyze for scoring requests, and OliveEnroll for enrollment requests. Examples are provided for each of these below, and for more advanced users that need different tools, each utility has its own help statement that can be accessed with the -h flag: $ OliveAnalyzeText -h The arguments and formatting for each tool is very similar, so familiarity with the OliveAnalyze and OliveEnroll examples below should allow use of most of these tools.","title":"Java (OliveAnalyze)"},{"location":"clients.html#python-olivepyanalyze","text":"The Python client, what we call the olivepyanalyze suite, is not as fully-featured with respect to batch-processing of audio files. It performs synchronous requests to the OLIVE server, and so it will sequentially score each provided audio file, rather than submitting jobs in parallel. For this reason, the Java OliveAnalyze tools are recommended for batch processing of individual plugin tasks. The python client tools require Python 3.8 or newer - please refer to Python for downloading and installing Python. Installing these tools has been simplified by providing them in the form of a Python wheel, that can be easily installed with pip . Standard $ cd olive6.0.0/api/python $ ls olivepy-6.0.0-py3-none-any.whl olivepy-6.0.0.tar.gz $ python3 -m pip install --upgrade pip setuptools wheel $ python3 -m pip install olivepy-6.0.0-py3-none-any.whl Virtual Environment Install (optional) $ cd olive6.0.0/api/python $ ls olivepy-6.0.0-py3-none-any.whl olivepy-6.0.0.tar.gz $ python3 -m venv olivepy-virtualenvironment $ source olivepy-virtualenvironment/bin/activate ( olivepy-virtualenvironment ) $ python3 -m pip install --upgrade pip setuptools wheel ( olivepy-virtualenvironment ) $ python3 -m pip install olivepy-6.0.0-py3-none-any.whl This will fetch and install (if necessary) the olivepy dependencies, and install the olivepy tools. Those dependencies are: protobuf soundfile numpy zmq The olivepy utilities closely mirror the Java utilities, with the addition of the workflow tool, and are as follows: olivepyanalyze olivepyenroll olivepylearn (rarely used) olivepyworkflow olivepyworkflowenroll The olivepyworkflow tools are the most important, and examples are provided below for both scoring with olivepyworkflow and enrollment with olivepyworkflowenroll . We also provide examples for olivepyanalyze and olivepyenroll that mirror the Java examples.","title":"Python (olivepyanalyze)"},{"location":"clients.html#scoringanalysis-requests","text":"","title":"Scoring/Analysis Requests"},{"location":"clients.html#background-plugin-scoring-types","text":"In general, the output format will depend on the type of \u2018scorer\u2019 the plugin being used is. For a deeper dive into OLIVE scoring types, please refer to the appropriate section in the OLIVE Plugin Traits Guide , but a brief overview follows. The most common types of plugins in OLIVE are:","title":"Background: Plugin Scoring Types"},{"location":"clients.html#global-scorer","text":"Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Speaker and Language Identification are examples of global scorers. OLIVE typically calls a global scoring plugin an \"Identification\" plugin, whereas a region scoring plugin to pinpoint the same class types would instead be called a \"Detection\" plugin. For example, Speaker Identification versus Speaker Detection; the former assumes the entire audio contains a single speaker, where the latter makes no such assumption, and attempts to localize any detected speakers of interest.","title":"Global Scorer"},{"location":"clients.html#global-scorer-output","text":"In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> For example, a Speaker Identification analysis run, with three enrolled speakers (Alex, Taylor, Blake) might return: /data/sid/audio/file1.wav Alex -0.5348 /data/sid/audio/file1.wav Taylor 3.2122 /data/sid/audio/file1.wav Blake -5.5340 /data/sid/audio/file2.wav Alex 0.5333 /data/sid/audio/file2.wav Taylor -4.9444 /data/sid/audio/file2.wav Blake -2.6564 Note the actual meanings of the scores and available classes will vary from plugin-to-plugin. Please refer to individual plugin documentation for more guidance on what the scores mean and what ranges are acceptable. Also note that the output format described here is literally what will be returned when calling a plugin directly with OliveAnalyze or olivepyanalyze - but when performing a global-scoring task as part of analysis with a workflow,these same informational pieces (audio_path or object, class_id, score) are still provided, but packed into a json structure.","title":"Global Scorer Output"},{"location":"clients.html#region-scorer","text":"Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. Automatic Speech Recognition (ASR), Language Detection (LDD), and Speaker Detection (SDD) are all region scorers. OLIVE typically calls a global scoring plugin an \"Identification\" plugin, whereas a region scoring plugin to pinpoint the same class types would instead be called a \"Detection\" plugin. For example, Speaker Identification versus Speaker Detection; the former assumes the entire audio contains a single speaker, where the latter makes no such assumption, and attempts to localize any detected speakers of interest.","title":"Region scorer"},{"location":"clients.html#region-scorer-output","text":"Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: < audio_path > < region_start_timestamp > < region_end_timestamp > < class_id > < score > For example, a language detection plugin might output something like this: /data/mixed-language/testFile1.wav 2.170 9.570 Arabic 0.912 /data/mixed-language/testFile1.wav 10.390 15.930 French 0.693 /data/mixed-language/testFile1.wav 17.639 22.549 English 0.832 /data/mixed-language/testFile2.wav 0.142 35.223 Pashto 0.977 Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. More specific examples can be found in the respective plugin-specific documentation pages. As with global scoring, note that the output format described here is literally what will be returned when calling a plugin directly with OliveAnalyze or olivepyanalyze - but when performing a region-scoring task as part of analysis with a workflow,these same informational pieces (audio_path or object, region start and end timestamps, class_id, score) are still provided, but packed into a json structure.","title":"Region Scorer Output"},{"location":"clients.html#plugin-direct-analysis","text":"Performing an analysis request with both tools is very similar, as the tools were designed to closely mirror each other so that familiarity with one would easily transfer to the other. The usage statements for each can be examined by invoking each with their -h or --help flag: OliveAnalyze (Java) $ ./OliveAnalyze -h usage: OliveAnalyze --align Perform audio alignment analysis. Must specify the two files to compare using an input list file via the--list argument --apply_update Request the plugin is updated ( if supported ) --box Perform bounding box analysis. Must specify an image or video input --cabundlepass <arg> Specifies the certificate authority passphrase of the certificate authority. --cabundlepath <arg> Specifies the path of the certificate authority --certpass <arg> Specifies the certificate passphrase to unlock the encrypted certificate key --certpath <arg> Specifies the path of the certificate --channel <arg> Process stereo files using channel NUMBER --class_ids <arg> Use Class ( s ) from FILE for scoring. Each line in the file contains a single class, including any white space --compare Perform audio compare analysis. Must specify the two files to compare using an input list file via the--list argument --decoded Send audio file as decoded PCM16 samples instead of sending as serialized buffer. Input file must be a wav file --domain <arg> Use Domain NAME --enhance Perform audio conversion ( enhancement ) --frame Perform frame scoring analysis --global Perform global scoring analysis -h Print this help message -i,--input <arg> NAME of the input file ( audio/video/image as required by the plugin --input_list <arg> Use an input list FILE having multiple filenames/regions or PEM formatted -l,--load load a plugin now, must use --plugin and --domain to specify the plugin/domain to preload --options <arg> options from FILE --output <arg> Write any output to DIR, default is ./ -p,--port <arg> Scenicserver port number. Defauls is 5588 --path Send audio file path instead of a buffer. Server and client must share a filesystem to use this option --plugin <arg> Use Plugin NAME --print <arg> Print all available plugins and domains. Optionally add 'verbose' as a print option to print full plugin details including traits and classes -r,--unload unload a loaded plugin now, must use --plugin and --domain to specify the plugin/domain to unload --region Perform region scoring analysis -s,--server <arg> Scenicserver hostname. Default is localhost --secure Indicates a secure connection should be made. Requires --certpath, --cabundlepath, --certpass, and --cabundlepass to be set. --shutdown Request a clean shutdown of the server --status Print the current status of the server -t,--timeout <arg> timeout ( in seconds ) when waiting for server response. Default is 10 seconds --threshold <arg> Apply threshold NUMBER when scoring --update_status Get the plugin ' s update status --upload_files Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation. -v,--vec <arg> PATH to a serialized AudioVector, for plugins that support audio vectors in addition to wav files --vector Perform audio vectorization olivepyanalyze (Python) $ olivepyanalyze -h usage: olivepyanalyze [ -h ] [ -C CLIENT_ID ] [ -p PLUGIN ] [ -d DOMAIN ] [ -G ] [ -e ] [ -f ] [ -g ] [ -r ] [ -b ] [ -P PORT ] [ --upload_port UPLOAD_PORT ] [ -s SERVER ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --text ] [ --options OPTIONS ] [ --class_ids CLASS_IDS ] [ --debug ] [ --concurrent_requests CONCURRENT_REQUESTS ] [ --path ] [ --upload_files ] [ --secure ] [ --certpath CERTPATH ] [ --keypath KEYPATH ] [ --keypass KEYPASS ] [ --cabundlepath CABUNDLEPATH ] [ --print ] options: -h, --help show this help message and exit -C CLIENT_ID, --client-id CLIENT_ID Experimental: the client_id to use -p PLUGIN, --plugin PLUGIN The plugin to use. -d DOMAIN, --domain DOMAIN The domain to use -G, --guess Experimental: guess the type of analysis to use based on the plugin/domain. -e, --enhance Enhance the audio of a wave file, which must be passed in with the --wav option. -f, --frame Do frame based analysis of a wave file, which must be passed in with the --wav option. -g, --global Do global analysis of a wave file, which must be passed in with the --wav option. -r, --region Do region based analysis of a wave file, which must be passed in with the --wav option. -b, --box Do bounding box based analysis of an input file, which must be passed in with the --wav option. -P PORT, --port PORT The port to use. --upload_port UPLOAD_PORT The upload port to use when specifying --upload_files. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -t TIMEOUT, --timeout TIMEOUT The timeout to use -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --text Indicates that input ( or input list ) is a literal text string to send in the analysis request. --options OPTIONS Optional file containing plugin properties ans name/value pairs. --class_ids CLASS_IDS Optional file containing plugin properties ans name/value pairs. --debug Debug mode --concurrent_requests CONCURRENT_REQUESTS # of concurrent requests to send the server --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option unless --upload_files is also used. --upload_files Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation. --secure Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. --certpath CERTPATH Specifies the path of the certificate --keypath KEYPATH Specifies the path of the certificate key --keypass KEYPASS Specifies the certificate passphrase to unlock the encrypted certificate key --cabundlepath CABUNDLEPATH Specifies the path of the certificate authority --print Print all available plugins and domains To perform a scoring request with these tools, you will need these essential pieces of information: Plugin name ( --plugin ) Domain name ( --domain ) Scoring type to perform ( --region for region-scoring, --global for global-scoring, others for less-common plugins) Input audio file or list of input audio files ( --input for a single file, --input_list for a list of files) The flag for providing each piece of information is the same for both tools, as shown in the list above. For more information on what the difference is between a plugin and a domain, refer to the Plugins Overview . For more information on the domains available for each plugin, refer to documentation page for that specific plugin. To see which plugins and domains you have installed and running in your specific OLIVE environment, refer to the server startup status message, that appears when you start the server: martini.sh start , then martini.sh log once the server is running for martini-based OLIVE packages (most common) ./run.sh for non-martini docker OLIVE packages oliveserver for native linux OLIVE packages Or exercise the --print option for each tool to query the server and print the available plugins and domains: OliveAnalyze (Java) $ ./OliveAnalyze --print olivepyanalyze (python) $ olivepyanalyze --print Example output: 2022-06-14 12:12:25.786 INFO com.sri.speech.olive.api.Server - Connected to localhost - request port: 5588 status_port: 5589 Found 8 plugin(s): Plugin: sad-dnn-v7.0.2 (SAD,Speech) v7.0.2 has 2 domain(s): Domain: fast-multi-v1, Description: Trained with Telephony, PTT and Music data Domain: multi-v1, Description: Trained with Telephony, PTT and Music data Plugin: asr-dynapy-v3.0.0 (ASR,Content) v3.0.0 has 9 domain(s): Domain: english-tdnnChain-tel-v1, Description: Large vocabulary English DNN model for 8K data Domain: farsi-tdnnChain-tel-v1, Description: Large vocabulary Farsi DNN model for 8K data Domain: french-tdnnChain-tel-v2, Description: Large vocabulary African French DNN Chain model for 8K data Domain: iraqiArabic-tdnnChain-tel-v1, Description: Large vocabulary Iraqi Arabic DNN Chain model for 8K data Domain: levantineArabic-tdnnChain-tel-v1, Description: Large vocabulary Levantine Arabic DNN Chain model for 8K data Domain: mandarin-tdnnChain-tel-v1, Description: Large vocabulary Mandarin DNN model for clean CTS 8K data Domain: pashto-tdnnChain-tel-v1, Description: Large vocabulary Pashto DNN Chain model for 8K data Domain: russian-tdnnChain-tel-v2, Description: Large vocabulary Russian DNN model for 8K data Domain: spanish-tdnnChain-tel-v1, Description: Large vocabulary Spanish DNN model for clean CTS 8K data Plugin: sdd-diarizeEmbedSmolive-v1.0.0 (SDD,Speaker) v1.0.0 has 1 domain(s): Domain: telClosetalk-int8-v1, Description: Speaker Embeddings Framework Plugin: tmt-neural-v1.0.0 (TMT,Content) v1.0.0 has 3 domain(s): Domain: cmn-eng-nmt-v1, Description: Mandarin Chinese to English NMT Domain: rus-eng-nmt-v1, Description: Russian to English NMT Domain: spa-eng-nmt-v3, Description: Spanish to English NMT Plugin: ldd-embedplda-v1.0.1 (LDD,Language) v1.0.1 has 1 domain(s): Domain: multi-v1, Description: PNCC bottleneck domain suitable for mixed conditions (tel/mic/compression) Plugin: sdd-diarizeEmbedSmolive-v1.0.2 (SDD,Speaker) v1.0.2 has 1 domain(s): Domain: telClosetalk-smart-v1, Description: Speaker Embeddings Framework Plugin: sid-dplda-v2.0.2 (SID,Speaker) v2.0.2 has 1 domain(s): Domain: multi-v1, Description: Speaker Embeddings DPLDA Plugin: lid-embedplda-v3.0.1 (LID,Language) v3.0.1 has 1 domain(s): Domain: multi-v1, Description: PNCC Bottleneck embeddings suitable for mixed conditions (tel/mic/compression)","title":"Plugin Direct (Analysis)"},{"location":"clients.html#examples","text":"To perform a global score analysis on a single file with the speaker identification plugin sid-dplda-v2.0.2 , using the multi-v1 domain, the calls for each would look like this: OliveAnalyze (Java) $ ./OliveAnalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav olivepyanalyze (python) $ olivepyanalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav Performing region scoring instead, using a transcription plugin, asr-dynapy-v3.0.0 , via the english domain english-tdnnChain-tel-v1 on a list of audio files would be performed with: OliveAnalyze (Java) $ ./OliveAnalyze --plugin asr-dynapy-v3.0.0 --domain english-tdnnChain-tel-v1 --region --input_list ~/path/to/list-of-audio-files.txt olivepyanalyze (python) $ olivepyanalyze --plugin asr-dynapy-v3.0.0 --domain english-tdnnChain-tel-v1 --region --input_list ~/path/to/list-of-audio-files.txt Where the format of the input list is simply a text file with a path to an audio file on each line. For example: /data/mixed-language/testFile1.wav /data/mixed-language/testFile2.wav /data/mixed-language/testFile3.wav /moreData/test-files/unknown1.wav","title":"Examples"},{"location":"clients.html#workflow-analysis","text":"OLIVE Workflows provide a simple way of creating a sort of 'recipe' that specifies how to deal with the input data and one or more OLIVE plugins. It allows complex operations to be requested and performed with a single, simple call to the system by allowing complexities and specific knowledge to be encapsulated within the workflow itself, rather than known and implemented by the user at run time. Due to off-loading this burden, operating with workflows is much simpler than calling the plugin(s) directly - typically all that is needed from the user to request an analysis from a workflow client is the workflow itself, and one or more input files. There are more options available to the workflow client, as shown in the usage statement: OliveWorkflow (Java) ./OliveWorkflow -h usage: OliveWorkflow --cabundlepass <arg> Specifies the certificate authority passphrase of the certificate authority. --cabundlepath <arg> Specifies the path of the certificate authority --certpass <arg> Specifies the certificate passphrase to unlock the encrypted certificate key --certpath <arg> Specifies the path of the certificate -h Print this help message -i,--input <arg> The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list <arg> A list of files to analyze. One file per line. --options <arg> options from FILE -p,--port <arg> Scenicserver port number. Defauls is 5588 --print_class_ids Print the class IDs available for analysis in the specified workflow. --print_tasks Print the workflow analysis tasks. -s,--server <arg> Scenicserver hostname. Default is localhost --secure Indicates a secure connection should be made. Requires --certpath, --cabundlepath, --certpass, and --cabundlepass to be set. -t,--timeout <arg> Timeout ( in seconds ) when waiting for server response. Default is 10 seconds --workflow <arg> The workflow definition to use. olivepyworkflow (Python) $ olivepyworkflow -h usage: olivepyworkflow [ -h ] [ --tasks ] [ --print_class_ids ] [ --class_ids CLASS_IDS ] [ --print_actualized ] [ --print_workflow ] [ -s SERVER ] [ -P PORT ] [ --upload_port UPLOAD_PORT ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --text ] [ --options OPTIONS ] [ --path ] [ --upload_files ] [ --secure ] [ --certpath CERTPATH ] [ --keypath KEYPATH ] [ --keypass KEYPASS ] [ --cabundlepath CABUNDLEPATH ] [ --debug ] workflow Perform OLIVE analysis using a Workflow Definition file positional arguments: workflow The workflow definition to use. options: -h, --help show this help message and exit --tasks Print the workflow analysis tasks. --print_class_ids Print the class IDs available for analysis in the specified workflow. --class_ids CLASS_IDS Send class IDs with the analysis request. --print_actualized Print the actualized workflow info. --print_workflow Print the workflow definition file info ( before it is actualized, if requested ) -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. --upload_port UPLOAD_PORT The upload port to use when specifying --upload_files. -t TIMEOUT, --timeout TIMEOUT The timeout ( in seconds ) to wait for a response from the server -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --text Indicates that input ( or input list ) is a literal text string to send in the analysis request. --options OPTIONS A JSON formatted string of workflow options such as [{ \"task\" : \"SAD\" , \"options\" : { \"filter_length\" :99, \"interpolate\" :1.0 }] or { \"filter_length\" :99, \"interpolate\" :1.0, \"name\" : \"midge\" } , where the former options are only applied to the SAD task, and the later are applied to all tasks --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option --upload_files Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation. --secure Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. --certpath CERTPATH Specifies the path of the certificate --keypath KEYPATH Specifies the path of the certificate key --keypass KEYPASS Specifies the certificate passphrase to unlock the encrypted certificate key --cabundlepath CABUNDLEPATH Specifies the path of the certificate authority --debug Debug mode But their use rarely necessary, and is reserved for advanced users or specific system testing. Generically, calling a workflow client will look like this: Python olivepyworkflow - Single Input File $ olivepyworkflow --input ~/path/to/test-file1.wav <workflow> olivepyworkflow - List of Input Files $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt <workflow> Java OliveWorkflow - Single Input File $ ./OliveWorkflow --input ~/path/to/test-file1.wav --workflow <workflow> OliveWorkflow - List of Input Files $ ./OliveWorkflow --input_list ~/path/to/list-of-audio-files.txt --workflow <workflow> As an example of the power of workflows, this request calls the SmartTranscription workflow - that performs Speech Activity Detection (region scoring), Speaker Diarization and Detection (region scoring), Language Detection (region scoring), and then Automatic Speech Recognition (region scoring) on any sections of each input file that are detected to be a language that ASR currently has support for, and returns all of the appropriate results in a JSON structure. Performing this same task by calling plugins directly, this same functionality would be a minimum of 4 separate calls to OLIVE; significantly more if more than one language is detected being spoken in the file. Python Single Input File $ olivepyworkflow --input ~/path/to/test-file1.wav ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json List of Input Files $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json Java OliveWorkflow - Single Input File $ ./OliveWorkflow --input ~/path/to/test-file1.wav --workflow ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json OliveWorkflow - List of Input Files $ ./OliveWorkflow --input_list ~/path/to/list-of-audio-files.txt --workflow ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json","title":"Workflow (Analysis)"},{"location":"clients.html#output-format-workflow","text":"Workflows are generally customer/user-specific and can be quite specialized - the output format and structure will depend heavily on the individual workflow itself and the tasks being performed. All of the information pieces that define each scoring type are still reported for each result, but the results are organized into a single JSON structure for the workflow call. This means that the output of a region scoring plugin within the workflow is still one or more sets of: < region_start_timestamp > < region_end_timestamp > < class_id > < score > But the data is arranged into the JSON structure and will be nested depending on the structure of the workflow itself and how the audio is routed by the workflow. For more detailed information on the structure of this JSON message and the inner-workings of workflows, please refer to the OLIVE Workflow API documentation. A brief, simplified summary to jump start working with workflow output follows. The main skeleton structure of the results output is shown below, along with an actual example. The results are provided as a result for each input file, that lists the job name(s), some metadata about the input audio and how it was processed, and then the returned results (if any) for each task, which is generally a plugin. Simplified/Generic Output Example (single-job workflow) Work fl ow a nal ysis resul ts : [ { \"job_name\" : <work fl ow job na me> , \"data\" : [ { \"data_id\" : <da ta ID , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <da ta label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ], < tas k 2 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ], < tas k 3 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ] } } ] English Transcription Output Example (single-job workflow) Work fl ow a nal ysis resul ts : [ { \"job_name\" : \"SAD, SDD, and ASR English Workflow\" , \"data\" : [ { \"data_id\" : \"z_eng_englishdemo.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 5.932625 , \"number_channels\" : 1 , \"label\" : \"z_eng_englishdemo.wav\" , \"id\" : \"0b04c7497521d53a5d6939533a55c461795f9d685b1bd19fd9031fc6f3997a8f\" } ], \"tasks\" : { \"SAD_REGIONS\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v7.0.2\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 5.93 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"SDD\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SDD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sdd-diarizeEmbedSmolive-v1.0.2\" , \"domain\" : \"telClosetalk-smart-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.1 , \"end_t\" : 5.0 , \"class_id\" : \"unknownspk00\" , \"score\" : 1.4 } ] } } ], \"ASR\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"ASR\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"asr-dynapy-v3.0.0\" , \"domain\" : \"english-tdnnChain-tel-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.15 , \"end_t\" : 0.51 , \"class_id\" : \"hello\" , \"score\" : 100.0 }, { \"start_t\" : 0.54 , \"end_t\" : 0.69 , \"class_id\" : \"my\" , \"score\" : 100.0 }, { \"start_t\" : 0.69 , \"end_t\" : 0.87 , \"class_id\" : \"name\" , \"score\" : 99.0 }, { \"start_t\" : 0.87 , \"end_t\" : 1.05 , \"class_id\" : \"is\" , \"score\" : 99.0 }, { \"start_t\" : 1.05 , \"end_t\" : 1.35 , \"class_id\" : \"evan\" , \"score\" : 88.0 }, { \"start_t\" : 1.35 , \"end_t\" : 1.47 , \"class_id\" : \"this\" , \"score\" : 99.0 }, { \"start_t\" : 1.5 , \"end_t\" : 1.98 , \"class_id\" : \"audio\" , \"score\" : 95.0 }, { \"start_t\" : 1.98 , \"end_t\" : 2.16 , \"class_id\" : \"is\" , \"score\" : 74.0 }, { \"start_t\" : 2.16 , \"end_t\" : 2.31 , \"class_id\" : \"for\" , \"score\" : 99.0 }, { \"start_t\" : 2.31 , \"end_t\" : 2.4 , \"class_id\" : \"the\" , \"score\" : 99.0 }, { \"start_t\" : 2.4 , \"end_t\" : 2.91 , \"class_id\" : \"purposes\" , \"score\" : 100.0 }, { \"start_t\" : 2.91 , \"end_t\" : 3.06 , \"class_id\" : \"of\" , \"score\" : 99.0 }, { \"start_t\" : 3.12 , \"end_t\" : 3.81 , \"class_id\" : \"demonstrating\" , \"score\" : 100.0 }, { \"start_t\" : 3.81 , \"end_t\" : 3.96 , \"class_id\" : \"our\" , \"score\" : 78.0 }, { \"start_t\" : 4.05 , \"end_t\" : 4.44 , \"class_id\" : \"language\" , \"score\" : 100.0 }, { \"start_t\" : 4.44 , \"end_t\" : 4.53 , \"class_id\" : \"and\" , \"score\" : 93.0 }, { \"start_t\" : 4.53 , \"end_t\" : 4.89 , \"class_id\" : \"speaker\" , \"score\" : 100.0 }, { \"start_t\" : 4.89 , \"end_t\" : 5.01 , \"class_id\" : \"i.\" , \"score\" : 99.0 }, { \"start_t\" : 5.01 , \"end_t\" : 5.22 , \"class_id\" : \"d.\" , \"score\" : 99.0 }, { \"start_t\" : 5.22 , \"end_t\" : 5.85 , \"class_id\" : \"capabilities\" , \"score\" : 99.0 } ] } } ] } } ] Each task output will typically be for a single plugin, and will be outputting the information provided by a Region Scorer or Global Scorer or Text Transformer in the case of Machine Translation, depending on how the workflow is using the plugin. The format of each result sub part is: Global Score < tas k na me> : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : < tas k t ype , ge nerall y LID , SID , e t c.> , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"score\" : [ { \"class_id\" : <class 1 > , \"score\" : <class 1 score> }, { \"class_id\" : <class 2 > , \"score\" : <class 2 score> }, ... { \"class_id\" : <class N> , \"score\" : <class N score> } ] } } ] Region Score < tas k na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype , t ypically ASR , SDD , SAD , e t c.> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 de te c te d class> , \"score\" : <regio n 1 score> }, { \"start_t\" : <regio n 2 s tart t ime (s)> , \"end_t\" : <regio n 2 e n d t ime (s)> , \"class_id\" : <regio n 2 de te c te d class> , \"score\" : <regio n 2 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N de te c te d class> , \"score\" : <regio n N score> }, ] } } ] Text Transformer < tas k na me> : [ { \"task_trait\" : \"TEXT_TRANSFORMER\" , \"task_type\" : < tas k t ype , t ypically MT> , \"message_type\" : \"TEXT_TRANSFORM_RESULT\" , \"plugin\" : <plugi n na me> , \"domain\" : <domai n na me> , \"analysis\" : { \"transformation\" : [ { \"class_id\" : \"test_label\" , \"transformed_text\" : < t he translate d/ transf ormed te x t re turne d fr om t he plugi n > } ] } } ] Many workflows consist of a single job , and bundle all plugin tasks into this single job, as seen above. More complex workflows, or what OLIVE calls \"Conditional Workflows\" can pack multiple jobs into a single workflow. This happens when there are certain tasks in the workflow that depend on other tasks in the workflow - for example when OLIVE needs to choose the appropriate Speech Recognition (ASR) language to use, depending on what language is detected being spoken by Language Identification (LID) or Language Detection (LDD). In this case, the LID/LDD is separated into one job, and the ASR into another, that is triggered to run once LID/LDD's decision is known. In this case, the results from each job are grouped accordingly in the results output. Below shows a simplified output from a workflow that includes three jobs; \"job 1\", \"job 2\", \"job 3\", and a real-life example output from the SmartTranscription conditional workflow that also has three jobs; the first performs Speech Activity Detection and Language Identification (LID); Smart Translation SAD and LID Pre-processing the second uses the language decision from Language Identification to choose the appropriate (if any) language and domain for Automatic Speech Recognition (ASR) and runs that, Dynamic ASR the third takes the output transcript from ASR and the language decision from LID and choose the appropriate (if any) language and domain for Text Machine Translation, and runs that. Dynamic MT As you can see below, these jobs are listed separately in the JSON for each result: Simplified/Generic Output Example (multi-job workflow) Work fl ow a nal ysis resul ts : [ { \"job name\" : <job 1 na me> , \"data\" : [ { \"data_id\" : <da ta ide nt i f ier , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <audio label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me (job 1 )> : [ { < tas k 1 resul ts > } ], ... < tas k N na me (job 1 )> : [ { < tas k N resul ts > } ] } }, { \"job name\" : <job 2 na me> , \"data\" : [ { \"data_id\" : <da ta ide nt i f ier , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <audio label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me (job 2 )> : [ { < tas k 1 resul ts > } ], ... < tas k N na me (job 2 )> : [ { < tas k N resul ts > } ] } }, ... <repea t i f more jobs> ] SmartTranslation Output Example Work fl ow a nal ysis resul ts : [ { \"job_name\" : \"Smart Translation SAD and LID Pre-processing\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 8.0 , \"number_channels\" : 1 , \"label\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"id\" : \"68984a7356fa1ea05f8e985868eb93e066ce80a0f4bf848edf55d547cfcbab41\" } ], \"tasks\" : { \"SAD_REGIONS\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v7.0.2\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 8.0 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"LID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"lid-embedplda-v3.0.1\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"Mandarin\" , \"score\" : 3.5306692 }, { \"class_id\" : \"Korean\" , \"score\" : -1.9072952 }, { \"class_id\" : \"Japanese\" , \"score\" : -3.7805116 }, { \"class_id\" : \"Tagalog\" , \"score\" : -7.4819508 }, { \"class_id\" : \"Vietnamese\" , \"score\" : -8.094855 }, { \"class_id\" : \"Iraqi Arabic\" , \"score\" : -10.63325 }, { \"class_id\" : \"Levantine Arabic\" , \"score\" : -10.694491 }, { \"class_id\" : \"French\" , \"score\" : -11.542379 }, { \"class_id\" : \"Pashto\" , \"score\" : -12.11981 }, { \"class_id\" : \"English\" , \"score\" : -12.323014 }, { \"class_id\" : \"Modern Standard Arabic\" , \"score\" : -12.626052 }, { \"class_id\" : \"Spanish\" , \"score\" : -13.469315 }, { \"class_id\" : \"Iranian Persian\" , \"score\" : -13.763366 }, { \"class_id\" : \"Amharic\" , \"score\" : -17.129797 }, { \"class_id\" : \"Portuguese\" , \"score\" : -17.31257 }, { \"class_id\" : \"Russian\" , \"score\" : -18.770994 } ] } } ] } }, { \"job_name\" : \"Dynamic ASR\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 8.0 , \"number_channels\" : 1 , \"label\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"id\" : \"68984a7356fa1ea05f8e985868eb93e066ce80a0f4bf848edf55d547cfcbab41\" } ], \"tasks\" : { \"ASR\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"ASR\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"asr-dynapy-v3.0.0\" , \"domain\" : \"mandarin-tdnnChain-tel-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 0.18 , \"class_id\" : \"\u8ddf\" , \"score\" : 31.0 }, { \"start_t\" : 0.18 , \"end_t\" : 0.36 , \"class_id\" : \"\u4e00\u4e2a\" , \"score\" : 83.0 }, { \"start_t\" : 0.36 , \"end_t\" : 0.66 , \"class_id\" : \"\u80af\u5b9a\" , \"score\" : 100.0 }, { \"start_t\" : 0.66 , \"end_t\" : 0.81 , \"class_id\" : \"\u662f\" , \"score\" : 83.0 }, { \"start_t\" : 0.81 , \"end_t\" : 1.23 , \"class_id\" : \"\u5317\u4eac\" , \"score\" : 95.0 }, { \"start_t\" : 1.23 , \"end_t\" : 1.47 , \"class_id\" : \"\u554a\" , \"score\" : 96.0 }, { \"start_t\" : 2.07 , \"end_t\" : 2.49 , \"class_id\" : \"\u4ed6\u4fe9\" , \"score\" : 96.0 }, { \"start_t\" : 2.7 , \"end_t\" : 3.09 , \"class_id\" : \"\u4e0a\u6d77\" , \"score\" : 99.0 }, { \"start_t\" : 3.09 , \"end_t\" : 3.21 , \"class_id\" : \"\u7684\" , \"score\" : 99.0 }, { \"start_t\" : 3.21 , \"end_t\" : 3.57 , \"class_id\" : \"\u4eba\u53e3\" , \"score\" : 99.0 }, { \"start_t\" : 3.57 , \"end_t\" : 3.87 , \"class_id\" : \"\u597d\u50cf\" , \"score\" : 73.0 }, { \"start_t\" : 3.87 , \"end_t\" : 3.99 , \"class_id\" : \"\u6ca1\" , \"score\" : 54.0 }, { \"start_t\" : 3.99 , \"end_t\" : 4.32 , \"class_id\" : \"\u5317\u4eac\" , \"score\" : 74.0 }, { \"start_t\" : 4.32 , \"end_t\" : 4.68 , \"class_id\" : \"\u591a\" , \"score\" : 99.0 }, { \"start_t\" : 4.86 , \"end_t\" : 5.19 , \"class_id\" : \"\u4f46\u662f\" , \"score\" : 100.0 }, { \"start_t\" : 5.4 , \"end_t\" : 5.91 , \"class_id\" : \"\u4e0d\u77e5\u9053\" , \"score\" : 100.0 }, { \"start_t\" : 6.06 , \"end_t\" : 6.48 , \"class_id\" : \"@reject@\" , \"score\" : 62.0 }, { \"start_t\" : 6.69 , \"end_t\" : 7.05 , \"class_id\" : \"\u5176\u4ed6\" , \"score\" : 93.0 }, { \"start_t\" : 7.05 , \"end_t\" : 7.2 , \"class_id\" : \"\u7684\" , \"score\" : 97.0 }, { \"start_t\" : 7.65 , \"end_t\" : 7.89 , \"class_id\" : \"\u4e0a\" , \"score\" : 32.0 }, { \"start_t\" : 7.89 , \"end_t\" : 7.95 , \"class_id\" : \"\u554a\" , \"score\" : 67.0 } ] } } ] } }, { \"job_name\" : \"Dynamic MT\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"WORKFlOW_TEXT_RESULT\" , \"text\" : \"\u8ddf \u4e00\u4e2a \u80af\u5b9a \u662f \u5317\u4eac \u554a \u4ed6\u4fe9 \u4e0a\u6d77 \u7684 \u4eba\u53e3 \u597d\u50cf \u6ca1 \u5317\u4eac \u591a \u4f46\u662f \u4e0d\u77e5\u9053 @reject@ \u5176\u4ed6 \u7684 \u4e0a \u554a\" } ], \"tasks\" : { \"MT\" : [ { \"task_trait\" : \"TEXT_TRANSFORMER\" , \"task_type\" : \"MT\" , \"message_type\" : \"TEXT_TRANSFORM_RESULT\" , \"plugin\" : \"tmt-neural-v1.0.0\" , \"domain\" : \"cmn-eng-nmt-v1\" , \"analysis\" : { \"transformation\" : [ { \"class_id\" : \"test_label\" , \"transformed_text\" : \"with someone in beijing they don't seem to have a population in shanghai but we don't know what else to do\" } ] } } ] } } ]","title":"Output Format (Workflow)"},{"location":"clients.html#enrollment-requests","text":"Enrollments are a sub-set of classes that the user can create and/or modify. These are used for classes that cannot be known ahead of time and therefore can't be pre-loaded into the system, such as specific speakers or keywords of interest. To determine if a plugin supports or requires enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page, linked from the navigation or the Release Plugins page.","title":"Enrollment Requests"},{"location":"clients.html#enrollment-list-format","text":"As with analysis, both the Java and Python tools were designed to share as much of a common interface as possible, and as such share an input list format when providing exemplars for enrollment. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TPD, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in each plugin's documentation. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7","title":"Enrollment list format"},{"location":"clients.html#plugin-direct-enrollment","text":"Performing an enrollment request is similar to an analysis request and is again very similar between the two tools. The usage statements for each can be examined by invoking each with their -h or --help flag: OliveEnroll (Java) $ ./OliveEnroll -h usage: OliveEnroll --cabundlepass <arg> Specifies the certificate authority passphrase of the certificate authority. --cabundlepath <arg> Specifies the path of the certificate authority --certpass <arg> Specifies the certificate passphrase to unlock the encrypted certificate key --certpath <arg> Specifies the path of the certificate --channel <arg> Process stereo files using channel NUMBER --classes Print class names if also printing plugin/domain names. Must use with --print option. Default is to not print class IDs --decoded Sennd audio file as a decoded PCM16 sample buffer instead of a serialized buffer. The file must be a WAV file --domain <arg> Use Domain NAME --enroll <arg> Enroll speaker NAME. If no name specified then , the pem or list option must specify an input file --export <arg> Export speaker NAME to an EnrollmentModel ( enrollment.tar.gz ) -h Print this help message -i,--input <arg> NAME of the input file ( input varies by plugin: audio, image, or video ) --import <arg> Import speaker from EnrollmentModel FILE --input_list <arg> Batch enroll using this input list FILE having multiple filenames/class IDs or PEM formmated file --nobatch Disable batch enrollment when using pem or list input files, so that files are processed serially --options <arg> Enrollment options from FILE --output <arg> Write any output to DIR, default is ./ -p,--port <arg> Scenicserver port number. Defauls is 5588 --path Send the path to the audio file instead of a ( serialized ) buffer. The server must have access to this path. --plugin <arg> Use Plugin NAME --print Print all plugins and domains that suport enrollment and/or class import and export --remove <arg> Remove audio enrollment for NAME -s,--server <arg> Scenicserver hostname. Default is localhost --secure Indicates a secure connection should be made. Requires --certpath, --cabundlepath, --certpass, and --cabundlepass to be set. -t,--timeout <arg> timeout ( in seconds ) when waiting for server response. Default is 10 seconds --unenroll <arg> Un-enroll all enrollments for speaker NAME -v,--vec <arg> PATH to a serialized AudioVector, for plugins that support audio vectors in addition to wav files olivepyenroll (Python) $ olivepyenroll -h usage: olivepyenroll [ -h ] [ -C CLIENT_ID ] [ -D ] [ -p PLUGIN ] [ -d DOMAIN ] [ -e ENROLL ] [ -u UNENROLL ] [ -s SERVER ] [ -P PORT ] [ --upload_port UPLOAD_PORT ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --nobatch NOBATCH ] [ --path ] [ --upload_files ] [ --secure ] [ --certpath CERTPATH ] [ --keypath KEYPATH ] [ --keypass KEYPASS ] [ --cabundlepath CABUNDLEPATH ] options: -h, --help show this help message and exit -C CLIENT_ID, --client-id CLIENT_ID Experimental: the client_id to use -D, --debug The domain to use -p PLUGIN, --plugin PLUGIN The plugin to use. -d DOMAIN, --domain DOMAIN The domain to use -e ENROLL, --enroll ENROLL Enroll with this name. -u UNENROLL, --unenroll UNENROLL Uneroll with this name. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. --upload_port UPLOAD_PORT The upload port to use when specifying --upload_files. -t TIMEOUT, --timeout TIMEOUT The timeout to use -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --nobatch NOBATCH Disable batch enrollment when using pem or list input files, so that files are processed individually. --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option --upload_files Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation. --secure Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. --certpath CERTPATH Specifies the path of the certificate --keypath KEYPATH Specifies the path of the certificate key --keypass KEYPASS Specifies the certificate passphrase to unlock the encrypted certificate key --cabundlepath CABUNDLEPATH Specifies the path of the certificate authority To perform an enrollment request with these tools, you will need these essential pieces of information: Plugin name ( --plugin ) Domain name ( --domain ) One of: A properly formatted enrollment list ( --input_list ), if providing multiple files at once (see below for formatting) An input audio file ( --input ) AND the name of the class you wish to enroll ( --enroll for OliveEnroll , -e or --enroll for olivepyanalyze ) Generically, this looks like this for a single file input: OliveEnroll (Java) $ ./OliveEnroll --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> olivepyenroll (python) $ olivepyenroll --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> A more specific example: OliveEnroll (Java) $ ./OliveEnroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" olivepyenroll (python) $ olivepyenroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" Or if providing the enrollment list format shown above, the call is even simpler. Generically: OliveEnroll (Java) $ ./OliveEnroll --plugin <plugin> --domain <domain> --input_list <path to enrollment text file> olivepyenroll (python) $ olivepyenroll --plugin <plugin> --domain <domain> --input_list <path to enrollment text file> Specific: OliveEnroll (Java) $ ./OliveEnroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input_list ~/path/to/enrollment_input.txt olivepyenroll (python) $ olivepyenroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input_list ~/path/to/enrollment_input.txt Where the enrollment_input.txt file might look like: /some/data/somewhere/inputFile1.wav Logan /some/other/data/somewhere/else/LoganPodcast.wav Logan /yet/another/data/directory/charlie-speaks.wav Charlie","title":"Plugin Direct (Enrollment)"},{"location":"clients.html#workflow-enrollment","text":"In the most basic case, enrollment using a workflow is just as simple as scoring with a workflow. This is becuase most workflows will only have a single enrollment-capable job; a job is a subset of the the tasks a workflow is performing, typically linked to a single plugin. In the rare case that you're using a workflow with multiple supported enrollment jobs, you will need to specify which job to enroll to. See the Advanced Workflow Enrollment section below. Workflow enrollment is performed by using the olivepyworkflowenroll utility, whose help/usage statement is: olivepyworkflowenroll usage usage: olivepyworkflowenroll [ -h ] [ --print_jobs ] [ --job JOB ] [ --enroll ENROLL ] [ --unenroll UNENROLL ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --path ] [ -s SERVER ] [ -P PORT ] [ -t TIMEOUT ] [ --secure ] [ --certpath CERTPATH ] [ --keypath KEYPATH ] [ --keypass KEYPASS ] [ --cabundlepath CABUNDLEPATH ] workflow Perform OLIVE enrollment using a Workflow Definition file positional arguments: workflow The workflow definition to use. options: -h, --help show this help message and exit --print_jobs Print the supported workflow enrollment jobs. --job JOB Enroll/Unenroll an Class ID for a job ( s ) in the specified workflow. If not specified enroll or unenroll for ALL enrollment/unenrollment jobst --enroll ENROLL Enroll using this ( class ) name. Should be used with the job argument to specify a target job to enroll with ( if there are more than one enrollment jobs ) --unenroll UNENROLL Enroll using this ( class ) name. Should be used with the job argument to specify a job to unenroll ( if there are more than one unenrollment jobs ) -i INPUT, --input INPUT The data input to enroll. Either a pathname to an audio/image/video file or a string for text input --input_list INPUT_LIST A list of files to enroll. One file per line plus the class id to enroll. --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. -t TIMEOUT, --timeout TIMEOUT The timeout ( in seconds ) to wait for a response from the server --secure Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. --certpath CERTPATH Specifies the path of the certificate --keypath KEYPATH Specifies the path of the certificate key --keypass KEYPASS Specifies the certificate passphrase to unlock the encrypted certificate key --cabundlepath CABUNDLEPATH Specifies the path of the certificate authority If there is only one supported enrollment job in the workflow, using this utility for enrollment is very similar to the enrollment utilities above; but a workflow is provided instead of a plugin and domain combination. As with the other enrollment utilities, olivepyworkflowenroll supports both single-file enrollment and batch enrollment using an enrollment-formatted text file. Generically, this looks like: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input <path to audio file> --enroll <class name> <workflow> olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list <path to enrollment file> <workflow> And a specific example of each: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input ~/path/to/enroll-file1.wav --enroll \"Logan\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list ~/path/to/enrollment_input.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json Important: Note that in OLIVE, when you enroll a class, you are enrolling to a plugin and domain , and enrollments are shared server-wide. Even when you enroll using a workflow and the olivepyworkflow utility, enrollments are associated with the specific plugin/domain that the workflow is using under the hood. Any enrollments made to a workflow will be available to anyone else who may be using that server instance, and will also be made available to anyone interacting with the individual plugin - whether directly or via a workflow. As a more concrete example of this, the \"SmartTranscription\" workflow that is sometimes provided with OLIVE, that performs Speech Activity Detection, Speaker Detection, Language Detection, and Speech Recognition on supported languages has a single plugin that supports enrollments; Speaker Detection. As a result, the workflow is set up to have a single enrollment job, to allow workflow users to enroll new speakers to be detected by this plugin. When enrollment is performed with this workflow, the newly created speaker model is created by and for the Speaker Detection plugin itself, and goes into the global OLIVE enrollment space. If a file is analyzed by directly calling this Speaker Detection plugin, the new enrollment will be part of the pool of target speakers the plugin will search for. More information on this concept of \"Workflow Enrollment Jobs\" is provided in the next section.","title":"Workflow (Enrollment)"},{"location":"clients.html#advanced-workflow-enrollment-jobs","text":"It's rare, but possible for a workflow to bundle multiple enrollment-capable plugin capabilities into one. One example could be combining Speaker Detection in a workflow that also runs Query-by-Example Keyword Spotting , both of which rely on user enrollments to define their target classes. When this happens, if a user wishes to maintain the ability to enroll separate classes into each enrollable plugin, the workflow needs to expose these different enrollment tasks as separate jobs in the workflow enrollment capabilities. If this is necessary, the workflow will come from SRI configured appropriately - the user need only be concerned with how to specify which job to enroll with. To query which enrollment jobs are available to a workflow, use the olivepyworkflowenroll tool with the --print_jobs flag: $ olivepyworkflowenroll --print_jobs <workflow> Investigating the \"SmartTranscription\" workflow we briefly mentioned above: $ olivepyworkflowenroll --print_jobs SmartTranscriptionFull.workflow.json enrolling 0 files Enrollment jobs '[' SDD Enrollment ']' Un-Enrollment jobs '[' SDD Unenrollment ']' We see that there is only a single Enrollment job available; SDD Enrollment . If there were others, they would be listed in this output. Now that the desired job name is known, enrolling with the specified job is done by supplying that job name to the --job flag; in this case: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input ~/path/to/enroll-file1.wav --enroll \"Logan\" --job \"SDD Enrollment\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list ~/path/to/enrollment_input.txt --job \"SDD Enrollment\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json","title":"Advanced Workflow Enrollment - Jobs"},{"location":"clients.html#stateless-enrollment-and-scoring","text":"With OLIVE 6.0.0, the ability to perform Stateless Enrollment or Vectorization was introduced, to allow decoupling of target class enrollments from being managed by the OLIVE server. The page linked here covers more details on why a user may want to use Stateless Enrollment, but the key difference here is that users will need to provide the models, also called vectors of the classes they wish to score against, at score time. The olivepy CLI utility has added support for stateless enrollment, and an example follows.","title":"Stateless Enrollment and Scoring"},{"location":"clients.html#retrieving-an-enrollment-vector","text":"Creating an enrollment vector for use with stateless enrollment is very similar to creating an OLIVE server managed enrollment. From an olivepy utility standpoint, the main difference is the --vectorize flag provided to olivepyenroll , which informs the utility to retrieve the vector and store it locally, rather than have the server retain it. A generic example: $ olivepyenroll --vectorize --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> A more specific example: $ olivepyenroll --vectorize --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" In the specific case, this should create a directory structure that looks like this: user@user-computer:~/workspace/sid-dplda-v2.0.2 $ tree . \u2514\u2500\u2500 multi-v1 \u2514\u2500\u2500 enrollments \u2514\u2500\u2500 Logan \u2514\u2500\u2500 5a169f97-504f-4819-ad71-fab3faa8e4db You can use input_list as an option to olivepyenroll as well, and it will create, retrieve, and write all of the vectors corresponding to the input data and labels. Warning Saving to disk is FOR DEMO/TESTING PURPOSES ONLY . A production system should consider using SQLite or similar technology which allows associating a speaker label String to binary data blobs rather than storing on disk if they require long-term persistent storage. The examples are to illustrate functionality and proof of concept of the audio vectors.","title":"Retrieving an Enrollment Vector"},{"location":"clients.html#scoring-using-stored-enrollment-vectors","text":"Like olivepyenroll , olivepyanalyze has been modified to support scoring with offline or stateless enrollment vectors. To do this, the directories vectors corresponding to the classes a user would like to score must be passed in as part of a list sent to the --class_vectors flag. The format for this list is: <path to>/class1vectors <path to>/class2vectors ... <path to>/classNvectors An example of what this might look like, if the olivepyenroll tool was used to create the enrollment vectors, could look like: /home/user/workspace/sid-dplda/multi-v1/enrollments/Bob /home/user/workspace/sid-dplda/multi-v1/enrollments/Logan /home/user/workspace/sid-dplda/multi-v1/enrollments/Pierre /home/user/workspace/sid-dplda/multi-v1/enrollments/Orla Where each of these (i.e. Bob, logan, Pierre, Orla) are directories created above, and containing individual enrollment vectors, which can then be provided to olivepyanalyze generically as: $ olivepyanalyze --class_vectors <file list of enrollment vector directories> --global --plugin <plugin> --domain <domain> --input <path to audio file> Or more specifically: $ olivepyanalyze --class_vectors input_stateless.txt --global --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/test-file1.wav","title":"Scoring using Stored Enrollment Vectors"},{"location":"clients.html#tlssecure-mode","text":"Note that when using OLIVE with TLS enabled, additional arguments are required to pass the appropriate certificates and other info necessary for the TLS configuration. In addition to what was shown above, these parameters must also be supplied for Java : secure - Enables TLS/secure connection mode. certpath - Specifies the path of the certificate. certpass - Specifies the certificate passphrase to unlock the encrypted certificate key. cabundlepath - Specifies the path of the certificate authority. cabundlepass - Specifies the certificate authority passphrase of the certificate authority. And these must also be supplied for Python : secure - Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set. certpath - Specifies the path of the certificate. keypath - Specifies the path of the certificate key. keypass - Specifies the certificate passphrase to unlock the encrypted certificate key. cabundlepath - Specifies the path of the certificate authority.","title":"TLS/Secure Mode"},{"location":"clients.html#plugin-direct-tls-examples","text":"Borrowing the global scoring examples from above, if TLS is enabled, they would need to be modified as follows: OliveAnalyze (Java) $ ./OliveAnalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav \\ --secure \\ --certpath /home/username/cert-directory/test-certs/client.p12 \\ --certpass \"my password\" \\ --cabundlepath /home/username/cert-directory/test-certs/root-ca.p12 \\ --cabundlepass \"another password\" olivepyanalyze (python) $ olivepyanalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav \\ --secure \\ --certpath /home/username/cert-directory/test-certs/client.crt \\ --keypath /home/username/cert-directory/test-certs/client.key \\ --keypass \"my password\" \\ --cabundlepath /home/username/cert-directory/test-certs/root-ca.crt Note that the same flags and info wil need to be provided for enrollment requests and any other communication or job submissions to a TLS-enabled OLIVE server.","title":"Plugin Direct TLS Examples"},{"location":"clients.html#workflow-tls-examples","text":"Modifying the workflow analysis examples above to enable TLS/secure communication: OliveWorkflow (Java) $ ./OliveWorkflow --input_list ~/path/to/list-of-audio-files.txt ~/olive5.4.0/oliveAppData/ --workflow workflows/SmartTranscription.workflow.json \\ --secure \\ --certpath /home/username/cert-directory/test-certs/client.p12 \\ --certpass \"my password\" \\ --cabundlepath /home/username/cert-directory/test-certs/root-ca.p12 \\ --cabundlepass \"another password\" olivepyworkflow (python) $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json \\ --secure \\ --certpath /home/username/cert-directory/test-certs/client.crt \\ --keypath /home/username/cert-directory/test-certs/client.key \\ --keypass \"my password\" \\ --cabundlepath /home/username/cert-directory/test-certs/root-ca.crt Note that the same flags and info wil need to be provided for workflow enrollment requests and any other communication or job submissions to a TLS-enabled OLIVE server.","title":"Workflow TLS Examples"},{"location":"clients.html#job-cancellation-request","text":"This utility submits a request to the OLIVE server to cancel all jobs currently in progress or pending - this allows the user to recover from submitting a large job or batch of jobs that is no longer relevant, or was perhaps submitted to the wrong plugin(s) or workflow(s), without restarting the entire server. This functionality is currently only offered in the Python olivepy suite, and will be coming to the Java client in the future. This request is accomplished with the olivepycancel utility. Its usage is: olivepycancel (python) Usage usage: olivepycancel [ -h ] [ -P PORT ] [ -s SERVER ] [ -t TIMEOUT ] options: -h, --help show this help message and exit -P PORT, --port PORT The port to use. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -t TIMEOUT, --timeout TIMEOUT The timeout to use This utility is very simple - an example call with the default OLIVE server settings: olivepycancel (python) Example olivepycancel","title":"Job Cancellation Request"},{"location":"clients.html#adaption","text":"The adaption process is complicated, time-consuming, and plugin/domain specific. Use the SRI provided Python client (olivepylearn) or Java client (OliveLearn) to perform adaptation. To adapt using the olivepylearn utility: olivepylearn --plugin sad-dnn --domain multi-v1 -a TEST_NEW_DOMAIN -i /olive/sadRegression/lists/adapt_s.lst Where that adapt_s.lst looks like this: /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav S 20.469 21.719 /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav NS 10.8000 10.8229 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav S 72.898 73.748 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav NS 42.754 43.010 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav S 79.437 80.427 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav NS 61.459 62.003 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav S 11.0438 111.638 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav NS 69.058 73.090 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav S 112.936 113.656 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav NS 83.046 83.114 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav S 16.940 20.050 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav NS 59.794 59.858 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav S 87.280 88.651 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav NS 82.229 82.461 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav S 111.346 111.936 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav NS 83.736 84.446 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav S 77.291 78.421 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav NS 0 4.951 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav S 30.349 32.429 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav NS 100.299 101.647 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav S 46.527 49.147 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav NS 44.747 46.148 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav S 24.551 25.471 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav NS 52.033 52.211 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav S 88.358 93.418 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav NS 46.564 46.788 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav S 10.507 11.077 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav NS 41.099 41.227 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav S 61.072 63.002 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav NS 19.108 19.460 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav S 97.182 97.789 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav NS 71.711 71.732 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav S 114.312 117.115 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav NS 31.065 31.154 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav S 90.346 91.608 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav NS 50.028 51.377 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav S 75.986 76.596 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav NS 12.485 12.709 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav S 72.719 73.046 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav NS 51.923 53.379 /olive//sadRegression/audio/adapt/20131213T104502UTC_18520_A.wav NS 11.1192 112.761 /olive//sadRegression/audio/adapt/20131213T121501UTC_18530_A.wav NS 81.277 82.766 /olive//sadRegression/audio/adapt/20131213T124501UTC_18533_A.wav NS 83.702 84.501 /olive//sadRegression/audio/adapt/20131213T134502UTC_18567_A.wav NS 69.379 72.258 /olive//sadRegression/audio/adapt/20131217T015001UTC_18707_A.wav NS 5.099 10.507","title":"Adaption"},{"location":"contact.html","text":"The Team The OLIVE Software and Plugins are developed and maintained by the engineers, scientists, and linguists of the Speech Technology and Research (STAR) Laboratory at SRI International. OLIVE Page at SRI.com For more information about STAR Lab and current research or development, please visit the STAR Lab SRI International page, or email olive-support@sri.com .","title":"Contact"},{"location":"contact.html#the-team","text":"The OLIVE Software and Plugins are developed and maintained by the engineers, scientists, and linguists of the Speech Technology and Research (STAR) Laboratory at SRI International. OLIVE Page at SRI.com For more information about STAR Lab and current research or development, please visit the STAR Lab SRI International page, or email olive-support@sri.com .","title":"The Team"},{"location":"docker.html","text":"OLIVE Installation for container-based Deliveries These instructions cover the traditional Docker container based OLIVE delivery where the container only includes the OLIVE server alone. For the new, multi-server delivery packaging that includes the Raven Web GUI , please refer to the OLIVE Martini Setup Guide. Deploying OLIVE in an Existing Multi-Container Application If deploying the OLIVE Docker image as a single container in a conventional system then skip to the next section . Otherwise, for users deploying OLIVE in an existing multi-container application managed by Docker Compose or Kubernetes, be sure to create OLIVE containers that meet these requirements: The oliveAppData directory, distributed with an OLIVE release, must be mounted as /home/olive/olive inside the docker container. The oliveAppData is the directory that contains the 'plugins' folder with one or more OLIVE plugins. OLIVE will write logs and persistent enrollments to the oliveAppData directory. The plugins folder mounted in /home/olive/olive can vary by container if specific tasks need to be supported by a container. For example, an olive data directory can be mounted that contains only one plugin. The OLIVE server is executed as 'oliveserver' The OLIVE server is accessible via ports 5588 and 5589 (and internally uses port 5590). External clients must have access to these ports on your OLIVE container(s). You can change the default ports used by the OLIVE server via the --port argument. For example, this command will start the OLIVE server using client accessible ports 5591 and 5592: oliveserver --port 5591 Starting the server with different ports is only necessary if the default OLIVE ports are already used by an existing application, or multiple OLIVE containers are addressable at the same hostname. The number of concurrent jobs supported by an OLIVE server can be restricted using the -j argument. For example, to limit the server to one job at a time: oliveserver -j 1 If audio needs to be submitted to OLIVE as a path instead of sent as buffer then please contact SRI for some advanced configuration options. Download, Install, and Launch Docker First you will need to obtain and install Docker - this can be done in a number of ways, the easiest of which is likely through the Docker website. The Community Edition is free (though you now need to create an account with Docker to obtain it), should fulfill most or all OLIVE needs, and is available for several operating systems, including Windows 10. Docker is available here - https://www.docker.com/community-edition and up-to-date installation instructions can be found from Docker. Once Docker is installed, launch it to proceed. When running Docker for the first time, if you are using a Windows OS, you may be prompted to enable some Windows features that are not enabled by default, but are required by Docker to run; Hyper-V and Containers. Select \u201cOk\u201d and wait for the computer to restart to enable these features. Docker should automatically start up once the computer has restarted. Adjust Docker settings (RAM, Cores) If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS Download OLIVE Docker Package Now that Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: olive2.0-docker-19Sep2020.tar.gz You should find similar content to below unless told otherwise: olive6.0.0 docs/ - Directory containing the OLIVE documentation oliveDocker/ olive-6.0.0-docker.tar \u2013 Docker image containing OLIVE run.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and oliveserver. run.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and oliveserver. run-shell.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. run-shell.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ sad-dnn-v7.0.0 \u2013 Speech Activity Detection plugin Actual plugins included will depend on the customer, mission, and delivery --optional-- oliveAppDataMT/ plugins/ tmt-statistical-v1.0.0 You may optionally be provided with Machine Translation capabilities, as shown at the end of the list - this plugin has some special runtime constraints, and will be contained in a separate oliveAppData directory as a result. Load the OLIVE Docker Image The first setup step is to load the OLIVE Docker image. Loading the image is as simple as opening a command prompt like PowerShell in Windows or Terminal in MacOS, navigating to the directory containing the OLIVE Docker image, and following the examples below. Loading the docker image (only necessary once). Windows $ cd C : \\ Users \\ < username > \\ olive2 . 0 \\ oliveDocker $ docker load - i olive - 6.0 . 0 - docker . tar macOS / linux $ cd / home /< username >/ olive6 . 0.0 / oliveDocker $ docker load - i olive - 6.0 . 0 - docker . tar This operation can take some time; quite a few minutes, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed. Set up run and run-shell scripts Note that we include two (optionally three, if translation is provided) 'run' scripts in this release. run-shell.sh will open a shell within the container, with the OLIVE environment properly set up, allowing a user to run CLI tasks using localenroll and localanalyze. This script is mainly used for troubleshooting and debugging. The other script(s) will launch an OLIVE server using the olive-6.0.0 container, and vary only slightly. run.sh is a general-purpose script currently configured to launch an OLIVE server that will only restrict the number of jobs/workers based on the number of cores available on the host machine, and will allow parallel processing to scale based on this. This should be used for most purposes. (optional) run_mt.sh is a specialized script that is meant to be used to launch an OLIVE server that only points to a Machine Translation plugin. It is configured to limit the number of jobs/workers to one and does not allow parallel processing. It is also configured to have this server listen on different ports than a standard OLIVE server would, in case you need both OLIVE servers to run on the same host. Note that this distinction is the reason that the plugins for deliveries that include translation are split between oliveAppData and oliveAppDataMT - you should be able to support running two containers; one for all plugins that support parallel processing and one for MT - without having to shuffle plugin directories around. The run.cmd / run.sh and run-shell.cmd / run-shell.sh scripts do most of the heavy-lifting for starting the OLIVE docker and associated processes, but before you can run them, you will need to either set an environment variable, or for a more permanent change, open the desired script with a text editor and edit one or two lines to allow them to establish a shared file location that both the host file system (Windows, Linux, or MacOS) and the OLIVE Docker container can access. Both processes are shown below. Environment Variable The run and run-shell scripts are shipped configured to pull the location of the plugins from an environment variable, so that the scripts don't need to be edited out of the box. This does require these variables to be set each time you will need to run If you would like to permanently edit the scripts instead, refer to the next section, direct script editing . To set this location, you will need to set the OLIVE_APP_DATA environment variable, that will adjust what the OLIVE_DATA variable inside the script is set to. This variable needs to point to the path on the host machine that contains the 'plugins' directory you wish for it to use - for typical OLIVE deliveries, this will be [REPLACE-WITH-LOCAL-PATH]/olive2.0/oliveAppData/. An example of this setup step for the non-translation server: $ export OLIVE_APP_DATA =[ REPLACE-WITH-LOCAL-PATH ] /olive2.0/oliveAppData/ $ ./run.sh Docker will get access to directory: /Users/allen/oliveAppData/ as /home/olive/olive and as /olive-data. Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- ASR asr-dynapy-v2.0.2 2 .0.2 [ 'cmn-tdnnChain-tel-v1' , 'eng-tdnnChain-tel-v1' , 'fas-tdnn-tel-v1' , 'rus-tdnnChain-tel-v1' , 'spa-tdnnChain-tel-v1' ] LDD ldd-sbcEmbed-v1.0.1 1 .0.1 [ 'multi-v1' ] LID lid-embedplda-v2.0.1 2 .0.1 [ 'multi-v1' ] ... etc. --------- Server ready Tue Feb 23 04 :03:13 2021 --------- -- Optional step for Machine Translation Only -- This same step described above is necessary for the run-mt.sh script that will launch the job-limited server that is only running Machine Translation. The only difference is that the environment variable is instead $OLIVE_APP_DATA_MT , and the default location for most OLIVE deliveries will likely be [REPLACE-WITH-LOCAL-PATH]/olive6.0.0/oliveAppDataMT/. $ export OLIVE_APP_DATA_MT= [ REPLACE - WITH - LOCAL - PATH ] /olive6 . 0 . 0/oliveAppDataMT/ $ . /run - mt . sh Docker will get access to directory: /Users/allen/oliveAppDataMT/ as /home/olive/olive and as /olive - data . Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- TMT tmt - statistical - v1 . 0 . 1 1 . 0 . 1 [ 'spa - eng - generic - v2' , 'fre - eng - generic - v1' ] --------- Server ready Tue Feb 23 04:03:13 2021 --------- NOTE that it is still necessary to set the LOCAL_OLIVE_DATA variable within the run-shell script if you wish to have access to shared audio files. On top of setting OLIVE_APP_DATA as above (which will mount plugins inside the container /home/olive/olive/plugins ), you will also need to set the LOCAL_OLIVE_DATA variable within the script. LOCAL_OLIVE_DATA must be set to a location that ends with a directory called olive-data , and can contain scripts, text files, audio files, and any other utilities you might wish to use with the shell while inside your container. To set this variable, you will need to edit this line in side run-shell.sh : LOCAL_OLIVE_DATA=`cd \"$THISDIR/../../../../../../olive-data\" && pwd` So that the path matches where your audio and scripts are stored, for example: LOCAL_OLIVE_DATA=/home/user1/audio/olive-data/ Direct Script Editing The first of these shared file locations, stored as the variable OLIVE_DATA within the run/run-shell scripts, is where the container will write information such as model enrollments and log files, as well as where the server will find plugins. Plugins must be contained in a directory called plugins, located directly within the directory that LOCAL_OLIVE is assigned to. In the examples below, C:\\Users\\<username>\\olive6.0.0\\oliveAppData for the Windows example, and /home/<username>/olive2.0/oliveAppData for the macOS/linux example has a child directory called plugins: C:\\Users\\<username>\\olive6.0.0\\oliveAppData\\plugins or /home/<username>/olive6.0.0/oliveAppData/plugins respectively. The second location is saved into the LOCAL_OLIVE_DATA variable, and should be assigned to a folder that contains audio you wish to process with OLIVE, or where you plan on placing this audio. It can be any directory on the host file system (Windows or MacOS) as long as the current user has access to it and the last directory in the path is a directory named \u2018olive-data\u2019. The files and folders contained within the directory assigned to LOCAL_OLIVE_DATA will be mounted to /olive-data/ within the OLIVE Docker container, and accessible at that location when operating within the container using the run or run-shell scripts. run-shell.sh and run.sh (macOS and linux) # MUST set OLIVE_DATA - For convenience we set it to the value of $OLIVE_APP_DATA , but one could also manually edit if # you don't normally set OLIVE_APP_DATA. The path at OLIVE_DATA must include your OLIVE 'plugins' folder : \" ${ OLIVE_APP_DATA : ? \"ERROR: OLIVE_APP_DATA is not set. Set this to the location of your OLIVE plugins folder\" } \" OLIVE_DATA= $OLIVE_APP_DATA # Optionally set LOCAL_OLIVE_DATA to a path on the host that ends with a directory named 'olive-data' LOCAL_OLIVE_DATA=\"/home/ <username> /olive-data\" run-shell.cmd and run.cmd (Windows) REM You must set LOCAL_OLIVE below to the directory on your local host that contains the OLIVE 'plugins' directory set OLIVE_DATA=C:\\Users\\<username>\\olive2.0\\oliveAppData set LOCAL_OLIVE_DATA=C:\\Users\\<username>\\olive-data Note that for macOS and linux hosts, the OLIVE_DATA variable is automatically assigned to the value of the $OLIVE_APP_DATA environment variable, if it is preferable to set this before calling the run scripts, instead of editing the scripts themselves. Your delivered run.sh and/or run-shell.sh scripts may have had the line that sets OLIVE_DATA modified to match your specific software installation package more closely and make the necessary changes clearer, especially if there are special cases to consider for your delivery, and may look more similar to this: OLIVE_DATA=[REPLACE-WITH-LOCAL-PATH]/olive6.0.0/oliveAppDataMT/ An example of what this might look like after adapting it to your local environment: OLIVE_DATA=/home/users/allen/olive6.0.0/oliveAppDataMT/ or OLIVE_DATA=/Users/allen/olive6.0.0/oliveAppDataMT/ Note that if your run.sh or run-shell.sh scripts look like this, OLIVE_DATA will not be set automatically based on your OLIVE_APP_DATA environment variable, but must be manually changed within the script before running. Run OLIVE scripts Now that all of the prep work has been done, you are ready to launch OLIVE and start running jobs. Windows hosts Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd C:\\Users\\<username>\\olive6.0.0\\oliveDocker $ .\\run.cmd -or- $ .\\run-shell.cmd macOS and linux hosts Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd / home /< username >/ olive6 .0.0 / oliveDocker $ export OLIVE_APP_DATA =/ home /< username >/ olive6 .0.0 / oliveAppData / ( Optional step if you haven 't edited the specified line in the `run` scripts to point to your plugins) $ ./run.sh -or- $ ./run-shell.sh Which script you run will depend on what task you are hoping to complete. The run.cmd or run.sh script will fire up the OLIVE Docker container and immediately load the oliveserver process. This is to be used alongside the OLIVE GUI or another tool that interacts with oliveserver through the OLIVE API. For command line processing, the run-shell.cmd or run-shell.sh scripts will launch the OLIVE Docker container and open a command line prompt with the OLIVE environment properly set up for running localenroll , localanalyze , and other command-line based tasks. Unload OLIVE Docker Container Image To remove/unload a docker image, whether to upgrade OLIVE containers or for any other reason, first, check that the container is actually still loaded. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE olive-6.0.0-docker latest d00396687de1 5 days ago 3.56GB And then unload it. $ docker rmi olive-6.0.0-docker [Optional] Install, set up, and launch OLIVE GUI An installation of OpenJDK 11 is required for running the OLIVE GUI. Instructions for obtaining and installing OpenJDK 11 will depend on your host's OS and can be found here Once you have installed Open JDK 11, you may need to set the JAVA_HOME environment variable, so that the host operating system knows where to find the appropriate JDK. On macOS and linux, this should be done automatically, or can be done as simply as exporting the JAVA_HOME environment variable to the appropriate location for your machine and OS, and/or with the correct settings: $ export JAVA_HOME =/ usr / libexec / java_home - v 11 You should then be free to launch the OLIVE GUI by simply executing the launcher script included in On Windows this process is slightly more complicated, but can be done by right-clicking This PC and selecting Properties. Before continuing, please refer to any additional instructions you may have been given when provided with access to the OLIVE software delivery. This process outlined below is currently being revised to be simpler, and may not be necessary for you to perform, especially for macOS or Windows 10 hosts. From there, select Advanced system settings in the left-side navigation menu. Navigate to the Advanced tab and select the Environment Variables button on the lower right portion of the dialog. From the Environment Variables window, select New from the lower System variables section, and create a new variable named JAVA_HOME that points to the full path of the JDK installation path. Typically, this is C:\\Program Files\\Java\\jdk11\\bin. Finally, you are ready to launch the GUI, either by navigating in File Explorer to the location where the package was uncompressed, then OliveGui/olive-ui/bin/, and double-clicking Nightingale.bat. You can create a shortcut to this file that you can then place on your desktop for more convenient access. Alternatively, it can be launched from the Windows PowerShell by navigating to the same location, then invoking Nightingale.bat. When launching this way, it is possible to provide a config file if desired. $ cd C:\\Users\\<username>\\olive6.0.0\\oliveGui $ .\\Nightingale.bat -c nightingale_config.xml [Optional] If you wish to use the Batch mode of the OLIVE GUI, an additional step is required to allow the Batch GUI to access audio files stored on the host operating system. Once you\u2019ve installed and run the GUI at least once, open and edit the .scenic-properties file that was created in the user\u2019s home folder in the host OS. For example, for a user named \u2018olive\u2019, this file will be created at the location C:\\Users\\olive.scenic-properties. Once you\u2019ve located and opened this file (it is a text file), add the following lines: use_scenic_data=true olive_data_directory=C:\\\\Users\\\\<username>\\\\olive-data Note that it is very important that the directory assigned to olive_data_directory matches the directory assigned to LOCAL_OLIVE_DATA in the step above. Note also that it is necessary to include double backslashes. If you don\u2019t, the Java properties reader strips them from the variable, and the path will not be valid.","title":"OLIVE Installation for container-based Deliveries"},{"location":"docker.html#olive-installation-for-container-based-deliveries","text":"These instructions cover the traditional Docker container based OLIVE delivery where the container only includes the OLIVE server alone. For the new, multi-server delivery packaging that includes the Raven Web GUI , please refer to the OLIVE Martini Setup Guide.","title":"OLIVE Installation for container-based Deliveries"},{"location":"docker.html#deploying-olive-in-an-existing-multi-container-application","text":"If deploying the OLIVE Docker image as a single container in a conventional system then skip to the next section . Otherwise, for users deploying OLIVE in an existing multi-container application managed by Docker Compose or Kubernetes, be sure to create OLIVE containers that meet these requirements: The oliveAppData directory, distributed with an OLIVE release, must be mounted as /home/olive/olive inside the docker container. The oliveAppData is the directory that contains the 'plugins' folder with one or more OLIVE plugins. OLIVE will write logs and persistent enrollments to the oliveAppData directory. The plugins folder mounted in /home/olive/olive can vary by container if specific tasks need to be supported by a container. For example, an olive data directory can be mounted that contains only one plugin. The OLIVE server is executed as 'oliveserver' The OLIVE server is accessible via ports 5588 and 5589 (and internally uses port 5590). External clients must have access to these ports on your OLIVE container(s). You can change the default ports used by the OLIVE server via the --port argument. For example, this command will start the OLIVE server using client accessible ports 5591 and 5592: oliveserver --port 5591 Starting the server with different ports is only necessary if the default OLIVE ports are already used by an existing application, or multiple OLIVE containers are addressable at the same hostname. The number of concurrent jobs supported by an OLIVE server can be restricted using the -j argument. For example, to limit the server to one job at a time: oliveserver -j 1 If audio needs to be submitted to OLIVE as a path instead of sent as buffer then please contact SRI for some advanced configuration options.","title":"Deploying OLIVE in an Existing Multi-Container Application"},{"location":"docker.html#download-install-and-launch-docker","text":"First you will need to obtain and install Docker - this can be done in a number of ways, the easiest of which is likely through the Docker website. The Community Edition is free (though you now need to create an account with Docker to obtain it), should fulfill most or all OLIVE needs, and is available for several operating systems, including Windows 10. Docker is available here - https://www.docker.com/community-edition and up-to-date installation instructions can be found from Docker. Once Docker is installed, launch it to proceed. When running Docker for the first time, if you are using a Windows OS, you may be prompted to enable some Windows features that are not enabled by default, but are required by Docker to run; Hyper-V and Containers. Select \u201cOk\u201d and wait for the computer to restart to enable these features. Docker should automatically start up once the computer has restarted.","title":"Download, Install, and Launch Docker"},{"location":"docker.html#adjust-docker-settings-ram-cores","text":"If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS","title":"Adjust Docker settings (RAM, Cores)"},{"location":"docker.html#download-olive-docker-package","text":"Now that Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: olive2.0-docker-19Sep2020.tar.gz You should find similar content to below unless told otherwise: olive6.0.0 docs/ - Directory containing the OLIVE documentation oliveDocker/ olive-6.0.0-docker.tar \u2013 Docker image containing OLIVE run.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and oliveserver. run.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and oliveserver. run-shell.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. run-shell.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ sad-dnn-v7.0.0 \u2013 Speech Activity Detection plugin Actual plugins included will depend on the customer, mission, and delivery --optional-- oliveAppDataMT/ plugins/ tmt-statistical-v1.0.0 You may optionally be provided with Machine Translation capabilities, as shown at the end of the list - this plugin has some special runtime constraints, and will be contained in a separate oliveAppData directory as a result.","title":"Download OLIVE Docker Package"},{"location":"docker.html#load-the-olive-docker-image","text":"The first setup step is to load the OLIVE Docker image. Loading the image is as simple as opening a command prompt like PowerShell in Windows or Terminal in MacOS, navigating to the directory containing the OLIVE Docker image, and following the examples below. Loading the docker image (only necessary once). Windows $ cd C : \\ Users \\ < username > \\ olive2 . 0 \\ oliveDocker $ docker load - i olive - 6.0 . 0 - docker . tar macOS / linux $ cd / home /< username >/ olive6 . 0.0 / oliveDocker $ docker load - i olive - 6.0 . 0 - docker . tar This operation can take some time; quite a few minutes, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed.","title":"Load the OLIVE Docker Image"},{"location":"docker.html#set-up-run-and-run-shell-scripts","text":"Note that we include two (optionally three, if translation is provided) 'run' scripts in this release. run-shell.sh will open a shell within the container, with the OLIVE environment properly set up, allowing a user to run CLI tasks using localenroll and localanalyze. This script is mainly used for troubleshooting and debugging. The other script(s) will launch an OLIVE server using the olive-6.0.0 container, and vary only slightly. run.sh is a general-purpose script currently configured to launch an OLIVE server that will only restrict the number of jobs/workers based on the number of cores available on the host machine, and will allow parallel processing to scale based on this. This should be used for most purposes. (optional) run_mt.sh is a specialized script that is meant to be used to launch an OLIVE server that only points to a Machine Translation plugin. It is configured to limit the number of jobs/workers to one and does not allow parallel processing. It is also configured to have this server listen on different ports than a standard OLIVE server would, in case you need both OLIVE servers to run on the same host. Note that this distinction is the reason that the plugins for deliveries that include translation are split between oliveAppData and oliveAppDataMT - you should be able to support running two containers; one for all plugins that support parallel processing and one for MT - without having to shuffle plugin directories around. The run.cmd / run.sh and run-shell.cmd / run-shell.sh scripts do most of the heavy-lifting for starting the OLIVE docker and associated processes, but before you can run them, you will need to either set an environment variable, or for a more permanent change, open the desired script with a text editor and edit one or two lines to allow them to establish a shared file location that both the host file system (Windows, Linux, or MacOS) and the OLIVE Docker container can access. Both processes are shown below.","title":"Set up run and run-shell scripts"},{"location":"docker.html#environment-variable","text":"The run and run-shell scripts are shipped configured to pull the location of the plugins from an environment variable, so that the scripts don't need to be edited out of the box. This does require these variables to be set each time you will need to run If you would like to permanently edit the scripts instead, refer to the next section, direct script editing . To set this location, you will need to set the OLIVE_APP_DATA environment variable, that will adjust what the OLIVE_DATA variable inside the script is set to. This variable needs to point to the path on the host machine that contains the 'plugins' directory you wish for it to use - for typical OLIVE deliveries, this will be [REPLACE-WITH-LOCAL-PATH]/olive2.0/oliveAppData/. An example of this setup step for the non-translation server: $ export OLIVE_APP_DATA =[ REPLACE-WITH-LOCAL-PATH ] /olive2.0/oliveAppData/ $ ./run.sh Docker will get access to directory: /Users/allen/oliveAppData/ as /home/olive/olive and as /olive-data. Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- ASR asr-dynapy-v2.0.2 2 .0.2 [ 'cmn-tdnnChain-tel-v1' , 'eng-tdnnChain-tel-v1' , 'fas-tdnn-tel-v1' , 'rus-tdnnChain-tel-v1' , 'spa-tdnnChain-tel-v1' ] LDD ldd-sbcEmbed-v1.0.1 1 .0.1 [ 'multi-v1' ] LID lid-embedplda-v2.0.1 2 .0.1 [ 'multi-v1' ] ... etc. --------- Server ready Tue Feb 23 04 :03:13 2021 --------- -- Optional step for Machine Translation Only -- This same step described above is necessary for the run-mt.sh script that will launch the job-limited server that is only running Machine Translation. The only difference is that the environment variable is instead $OLIVE_APP_DATA_MT , and the default location for most OLIVE deliveries will likely be [REPLACE-WITH-LOCAL-PATH]/olive6.0.0/oliveAppDataMT/. $ export OLIVE_APP_DATA_MT= [ REPLACE - WITH - LOCAL - PATH ] /olive6 . 0 . 0/oliveAppDataMT/ $ . /run - mt . sh Docker will get access to directory: /Users/allen/oliveAppDataMT/ as /home/olive/olive and as /olive - data . Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- TMT tmt - statistical - v1 . 0 . 1 1 . 0 . 1 [ 'spa - eng - generic - v2' , 'fre - eng - generic - v1' ] --------- Server ready Tue Feb 23 04:03:13 2021 --------- NOTE that it is still necessary to set the LOCAL_OLIVE_DATA variable within the run-shell script if you wish to have access to shared audio files. On top of setting OLIVE_APP_DATA as above (which will mount plugins inside the container /home/olive/olive/plugins ), you will also need to set the LOCAL_OLIVE_DATA variable within the script. LOCAL_OLIVE_DATA must be set to a location that ends with a directory called olive-data , and can contain scripts, text files, audio files, and any other utilities you might wish to use with the shell while inside your container. To set this variable, you will need to edit this line in side run-shell.sh : LOCAL_OLIVE_DATA=`cd \"$THISDIR/../../../../../../olive-data\" && pwd` So that the path matches where your audio and scripts are stored, for example: LOCAL_OLIVE_DATA=/home/user1/audio/olive-data/","title":"Environment Variable"},{"location":"docker.html#direct-script-editing","text":"The first of these shared file locations, stored as the variable OLIVE_DATA within the run/run-shell scripts, is where the container will write information such as model enrollments and log files, as well as where the server will find plugins. Plugins must be contained in a directory called plugins, located directly within the directory that LOCAL_OLIVE is assigned to. In the examples below, C:\\Users\\<username>\\olive6.0.0\\oliveAppData for the Windows example, and /home/<username>/olive2.0/oliveAppData for the macOS/linux example has a child directory called plugins: C:\\Users\\<username>\\olive6.0.0\\oliveAppData\\plugins or /home/<username>/olive6.0.0/oliveAppData/plugins respectively. The second location is saved into the LOCAL_OLIVE_DATA variable, and should be assigned to a folder that contains audio you wish to process with OLIVE, or where you plan on placing this audio. It can be any directory on the host file system (Windows or MacOS) as long as the current user has access to it and the last directory in the path is a directory named \u2018olive-data\u2019. The files and folders contained within the directory assigned to LOCAL_OLIVE_DATA will be mounted to /olive-data/ within the OLIVE Docker container, and accessible at that location when operating within the container using the run or run-shell scripts. run-shell.sh and run.sh (macOS and linux) # MUST set OLIVE_DATA - For convenience we set it to the value of $OLIVE_APP_DATA , but one could also manually edit if # you don't normally set OLIVE_APP_DATA. The path at OLIVE_DATA must include your OLIVE 'plugins' folder : \" ${ OLIVE_APP_DATA : ? \"ERROR: OLIVE_APP_DATA is not set. Set this to the location of your OLIVE plugins folder\" } \" OLIVE_DATA= $OLIVE_APP_DATA # Optionally set LOCAL_OLIVE_DATA to a path on the host that ends with a directory named 'olive-data' LOCAL_OLIVE_DATA=\"/home/ <username> /olive-data\" run-shell.cmd and run.cmd (Windows) REM You must set LOCAL_OLIVE below to the directory on your local host that contains the OLIVE 'plugins' directory set OLIVE_DATA=C:\\Users\\<username>\\olive2.0\\oliveAppData set LOCAL_OLIVE_DATA=C:\\Users\\<username>\\olive-data Note that for macOS and linux hosts, the OLIVE_DATA variable is automatically assigned to the value of the $OLIVE_APP_DATA environment variable, if it is preferable to set this before calling the run scripts, instead of editing the scripts themselves. Your delivered run.sh and/or run-shell.sh scripts may have had the line that sets OLIVE_DATA modified to match your specific software installation package more closely and make the necessary changes clearer, especially if there are special cases to consider for your delivery, and may look more similar to this: OLIVE_DATA=[REPLACE-WITH-LOCAL-PATH]/olive6.0.0/oliveAppDataMT/ An example of what this might look like after adapting it to your local environment: OLIVE_DATA=/home/users/allen/olive6.0.0/oliveAppDataMT/ or OLIVE_DATA=/Users/allen/olive6.0.0/oliveAppDataMT/ Note that if your run.sh or run-shell.sh scripts look like this, OLIVE_DATA will not be set automatically based on your OLIVE_APP_DATA environment variable, but must be manually changed within the script before running.","title":"Direct Script Editing"},{"location":"docker.html#run-olive-scripts","text":"Now that all of the prep work has been done, you are ready to launch OLIVE and start running jobs.","title":"Run OLIVE scripts"},{"location":"docker.html#windows-hosts","text":"Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd C:\\Users\\<username>\\olive6.0.0\\oliveDocker $ .\\run.cmd -or- $ .\\run-shell.cmd","title":"Windows hosts"},{"location":"docker.html#macos-and-linux-hosts","text":"Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd / home /< username >/ olive6 .0.0 / oliveDocker $ export OLIVE_APP_DATA =/ home /< username >/ olive6 .0.0 / oliveAppData / ( Optional step if you haven 't edited the specified line in the `run` scripts to point to your plugins) $ ./run.sh -or- $ ./run-shell.sh Which script you run will depend on what task you are hoping to complete. The run.cmd or run.sh script will fire up the OLIVE Docker container and immediately load the oliveserver process. This is to be used alongside the OLIVE GUI or another tool that interacts with oliveserver through the OLIVE API. For command line processing, the run-shell.cmd or run-shell.sh scripts will launch the OLIVE Docker container and open a command line prompt with the OLIVE environment properly set up for running localenroll , localanalyze , and other command-line based tasks.","title":"macOS and linux hosts"},{"location":"docker.html#unload-olive-docker-container-image","text":"To remove/unload a docker image, whether to upgrade OLIVE containers or for any other reason, first, check that the container is actually still loaded. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE olive-6.0.0-docker latest d00396687de1 5 days ago 3.56GB And then unload it. $ docker rmi olive-6.0.0-docker","title":"Unload OLIVE Docker Container Image"},{"location":"docker.html#optional-install-set-up-and-launch-olive-gui","text":"An installation of OpenJDK 11 is required for running the OLIVE GUI. Instructions for obtaining and installing OpenJDK 11 will depend on your host's OS and can be found here Once you have installed Open JDK 11, you may need to set the JAVA_HOME environment variable, so that the host operating system knows where to find the appropriate JDK. On macOS and linux, this should be done automatically, or can be done as simply as exporting the JAVA_HOME environment variable to the appropriate location for your machine and OS, and/or with the correct settings: $ export JAVA_HOME =/ usr / libexec / java_home - v 11 You should then be free to launch the OLIVE GUI by simply executing the launcher script included in On Windows this process is slightly more complicated, but can be done by right-clicking This PC and selecting Properties. Before continuing, please refer to any additional instructions you may have been given when provided with access to the OLIVE software delivery. This process outlined below is currently being revised to be simpler, and may not be necessary for you to perform, especially for macOS or Windows 10 hosts. From there, select Advanced system settings in the left-side navigation menu. Navigate to the Advanced tab and select the Environment Variables button on the lower right portion of the dialog. From the Environment Variables window, select New from the lower System variables section, and create a new variable named JAVA_HOME that points to the full path of the JDK installation path. Typically, this is C:\\Program Files\\Java\\jdk11\\bin. Finally, you are ready to launch the GUI, either by navigating in File Explorer to the location where the package was uncompressed, then OliveGui/olive-ui/bin/, and double-clicking Nightingale.bat. You can create a shortcut to this file that you can then place on your desktop for more convenient access. Alternatively, it can be launched from the Windows PowerShell by navigating to the same location, then invoking Nightingale.bat. When launching this way, it is possible to provide a config file if desired. $ cd C:\\Users\\<username>\\olive6.0.0\\oliveGui $ .\\Nightingale.bat -c nightingale_config.xml [Optional] If you wish to use the Batch mode of the OLIVE GUI, an additional step is required to allow the Batch GUI to access audio files stored on the host operating system. Once you\u2019ve installed and run the GUI at least once, open and edit the .scenic-properties file that was created in the user\u2019s home folder in the host OS. For example, for a user named \u2018olive\u2019, this file will be created at the location C:\\Users\\olive.scenic-properties. Once you\u2019ve located and opened this file (it is a text file), add the following lines: use_scenic_data=true olive_data_directory=C:\\\\Users\\\\<username>\\\\olive-data Note that it is very important that the directory assigned to olive_data_directory matches the directory assigned to LOCAL_OLIVE_DATA in the step above. Note also that it is necessary to include double backslashes. If you don\u2019t, the Java properties reader strips them from the variable, and the path will not be valid.","title":"[Optional] Install, set up, and launch OLIVE GUI"},{"location":"glossary.html","text":"Glossary / Appendix Below you will find definitions of terms commonly used throughout this documentation. If anything is unclear, please reach out for clarification . General Terms Plugin A module that encapsulates a process designed to perform a specific task (detect speakers, identify languages, find keywords) in a specific way (using a deep neural network embeddings, i-vectors+DNN bottleneck, etc.). A plugin thus contains the plan that links together a series of components (acoustic front-end, representation, classifier, fusion, calibration) into a pipeline. This generally captures a specific approach (algorithm) or process that uses a data model in the domain to perform the task, though the algorithm is generally independent of the data or audio condition specialization in a plugin's domain(s). For more information on the different types and capabilities, see the Plugins information page and domain , below. Domain A domain always resides within a plugin. The domain contains specific information (trained models, parameters, etc.) needed to prepare the plugin for specific operating conditions. Every plugin must have at least one domain, but may have many. Examples include telephone (tel), analog and/or digital push-to-talk (ptt), or distant microphone. Some plugins have a general domain trained on many data types, commonly-called \"multi-condition\". Task In the context of OLIVE, Task typically refers to the goal of a plugin, or the problem it is designed to address. For example, the plugin sad-dnn-v6 has the Task of SAD, or speech activity detection. For more information on the range of Tasks we currently have plugins for, please refer to the Plugins documentation. Class A specific target category of interest to be identified or detected by the system. A class can refer to a variety of things, depending on the respective plugin type it belongs to. For example, a 'class' in the context of a speaker identification plugin is an individual speaker of interest; it is a language or dialect in a LID plugin, a keyword in a KWS or QBE plugin, or a topic when referring to a TPD plugin. Classes can be pre-enrolled within a plugin, as is often the case with language identification plugins, but it is often necessary for end users to enroll their own classes of interest, as in the case of speaker identification plugins. Frame A frame of audio is a very short, typically 10ms slice of audio. FrameScoring plugins will report a score for each frame of audio submitted. Plugin Traits (Common API Processes) A plugin's functionality is defined by the Traits that it implements. Each plugin trait is associated with the set of messages that it is allowed to send, and that must be implemented for proper functionality. Below, these traits and their associated messages are defined. FrameScorer A frame scorer provides a score output for every X ms of an audio file or buffer, generally 10 ms. SAD and VTD are currently the only frame scorers in OLIVE. RegionScorer A region scorer provides scores for audio sub-segments detected within an audio file or buffer. For example, a KWS plugin would provide a keyword detection, its boundaries in time and score. GlobalScorer A global scorer assumes that an audio file or buffer is all of the same class and scores it as a unit. Examples include language identification and speaker verification. Common API Processes Adaptable / Adaptation Adaptation typically uses in-domain data from a specific operational environment to alter the core behavior of the system such that it functions more effectively. Unlike adding data to a class, adaptation is altering the system as a whole and thus produces a new domain. Plugins that are adaptable support either supervised or unsupervised adaptation. Unsupervised adaptation improves performance without human input, using audio examples provided by users or accrued from use in a mission's audio conditions. This type of adaptation is \"triggered\" either when a user-specific amount of data is accrued or explicitly called by the end user application and applied to the plugin. Unsupervised adaptation does not create a new domain, it alters an existing domain, but it is reversible. Supervised adaptation, however, requires human input. Generally data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech/non-speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. Supervised adaptation creates a new domain in most cases. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate. Supervised Adaptation Human assisted improvement of the plugin, generally with feedback to the system in the form of annotations of target phenomena, or error corrections. Unsupervised Adaptation Autonomous adaptation using unlabelled data; requires no human labelling or feedback. This is not currently supported by any OLIVE plugins in the traditional sense, but some plugins do support the ability to perform an Update , which is a form of unsupervised adaptation. Enrollable / Enrollment Enrollment is the mechanism by which target classes are added to a plugin domain. An enroll able plugin allows users to add new classes and augment existing classes. Examples include speaker detection, language recognition and keyword spotting. Stateless Enrollment Some enrollable plugins also support Stateless Enrollment . This is a special use-case method of managing models for classes of interest that allows the server to remain stateless. For more details on where this technique might apply, refer to the Stateless Enrollment info page . Augmentable / Augmentation Plugins that support enrollment also support augmentation. Augmentation is simply the process of adding additional data to an existing class. Updateable / Update Updating occurs when a user invokes unsupervised adaptation on a plugin/domain by requesting that the plugin use the operational data examples it has accrued throughout normal usage to update the plugin's parameters and models to better fit the usage environment. Diarization Diarization is the process of automatically segmenting an audio file or stream based on a set of target phenomena. SAD is diarization based on speech and non-speech segments. Speaker diarization segments a files based on speaker changes. Audio Vector An audio vector is a representation of an audio file in a form pre-processed for a specific task. For example, an audio files stored as a speaker vector representation for a speaker detection plugin. This is useful since it is a very compact form of the file that is very small and quick to read into memory and very fast to score against versus reading in a file from disk.","title":"Glossary"},{"location":"glossary.html#glossary-appendix","text":"Below you will find definitions of terms commonly used throughout this documentation. If anything is unclear, please reach out for clarification .","title":"Glossary / Appendix"},{"location":"glossary.html#general-terms","text":"","title":"General Terms"},{"location":"glossary.html#plugin","text":"A module that encapsulates a process designed to perform a specific task (detect speakers, identify languages, find keywords) in a specific way (using a deep neural network embeddings, i-vectors+DNN bottleneck, etc.). A plugin thus contains the plan that links together a series of components (acoustic front-end, representation, classifier, fusion, calibration) into a pipeline. This generally captures a specific approach (algorithm) or process that uses a data model in the domain to perform the task, though the algorithm is generally independent of the data or audio condition specialization in a plugin's domain(s). For more information on the different types and capabilities, see the Plugins information page and domain , below.","title":"Plugin"},{"location":"glossary.html#domain","text":"A domain always resides within a plugin. The domain contains specific information (trained models, parameters, etc.) needed to prepare the plugin for specific operating conditions. Every plugin must have at least one domain, but may have many. Examples include telephone (tel), analog and/or digital push-to-talk (ptt), or distant microphone. Some plugins have a general domain trained on many data types, commonly-called \"multi-condition\".","title":"Domain"},{"location":"glossary.html#task","text":"In the context of OLIVE, Task typically refers to the goal of a plugin, or the problem it is designed to address. For example, the plugin sad-dnn-v6 has the Task of SAD, or speech activity detection. For more information on the range of Tasks we currently have plugins for, please refer to the Plugins documentation.","title":"Task"},{"location":"glossary.html#class","text":"A specific target category of interest to be identified or detected by the system. A class can refer to a variety of things, depending on the respective plugin type it belongs to. For example, a 'class' in the context of a speaker identification plugin is an individual speaker of interest; it is a language or dialect in a LID plugin, a keyword in a KWS or QBE plugin, or a topic when referring to a TPD plugin. Classes can be pre-enrolled within a plugin, as is often the case with language identification plugins, but it is often necessary for end users to enroll their own classes of interest, as in the case of speaker identification plugins.","title":"Class"},{"location":"glossary.html#frame","text":"A frame of audio is a very short, typically 10ms slice of audio. FrameScoring plugins will report a score for each frame of audio submitted.","title":"Frame"},{"location":"glossary.html#plugin-traits-common-api-processes","text":"A plugin's functionality is defined by the Traits that it implements. Each plugin trait is associated with the set of messages that it is allowed to send, and that must be implemented for proper functionality. Below, these traits and their associated messages are defined.","title":"Plugin Traits (Common API Processes)"},{"location":"glossary.html#framescorer","text":"A frame scorer provides a score output for every X ms of an audio file or buffer, generally 10 ms. SAD and VTD are currently the only frame scorers in OLIVE.","title":"FrameScorer"},{"location":"glossary.html#regionscorer","text":"A region scorer provides scores for audio sub-segments detected within an audio file or buffer. For example, a KWS plugin would provide a keyword detection, its boundaries in time and score.","title":"RegionScorer"},{"location":"glossary.html#globalscorer","text":"A global scorer assumes that an audio file or buffer is all of the same class and scores it as a unit. Examples include language identification and speaker verification.","title":"GlobalScorer"},{"location":"glossary.html#common-api-processes","text":"","title":"Common API Processes"},{"location":"glossary.html#adaptable-adaptation","text":"Adaptation typically uses in-domain data from a specific operational environment to alter the core behavior of the system such that it functions more effectively. Unlike adding data to a class, adaptation is altering the system as a whole and thus produces a new domain. Plugins that are adaptable support either supervised or unsupervised adaptation. Unsupervised adaptation improves performance without human input, using audio examples provided by users or accrued from use in a mission's audio conditions. This type of adaptation is \"triggered\" either when a user-specific amount of data is accrued or explicitly called by the end user application and applied to the plugin. Unsupervised adaptation does not create a new domain, it alters an existing domain, but it is reversible. Supervised adaptation, however, requires human input. Generally data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech/non-speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. Supervised adaptation creates a new domain in most cases. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate.","title":"Adaptable / Adaptation"},{"location":"glossary.html#supervised-adaptation","text":"Human assisted improvement of the plugin, generally with feedback to the system in the form of annotations of target phenomena, or error corrections.","title":"Supervised Adaptation"},{"location":"glossary.html#unsupervised-adaptation","text":"Autonomous adaptation using unlabelled data; requires no human labelling or feedback. This is not currently supported by any OLIVE plugins in the traditional sense, but some plugins do support the ability to perform an Update , which is a form of unsupervised adaptation.","title":"Unsupervised Adaptation"},{"location":"glossary.html#enrollable-enrollment","text":"Enrollment is the mechanism by which target classes are added to a plugin domain. An enroll able plugin allows users to add new classes and augment existing classes. Examples include speaker detection, language recognition and keyword spotting.","title":"Enrollable / Enrollment"},{"location":"glossary.html#stateless-enrollment","text":"Some enrollable plugins also support Stateless Enrollment . This is a special use-case method of managing models for classes of interest that allows the server to remain stateless. For more details on where this technique might apply, refer to the Stateless Enrollment info page .","title":"Stateless Enrollment"},{"location":"glossary.html#augmentable-augmentation","text":"Plugins that support enrollment also support augmentation. Augmentation is simply the process of adding additional data to an existing class.","title":"Augmentable / Augmentation"},{"location":"glossary.html#updateable-update","text":"Updating occurs when a user invokes unsupervised adaptation on a plugin/domain by requesting that the plugin use the operational data examples it has accrued throughout normal usage to update the plugin's parameters and models to better fit the usage environment.","title":"Updateable / Update"},{"location":"glossary.html#diarization","text":"Diarization is the process of automatically segmenting an audio file or stream based on a set of target phenomena. SAD is diarization based on speech and non-speech segments. Speaker diarization segments a files based on speaker changes.","title":"Diarization"},{"location":"glossary.html#audio-vector","text":"An audio vector is a representation of an audio file in a form pre-processed for a specific task. For example, an audio files stored as a speaker vector representation for a speaker detection plugin. This is useful since it is a very compact form of the file that is very small and quick to read into memory and very fast to score against versus reading in a file from disk.","title":"Audio Vector"},{"location":"gpu-config.html","text":"GPU-Capable OLIVE Plugin / Domain Configuration Introduction With the release of OLIVE 5.5.0, certain plugin capabilities are now able to leverage GPU hardware to enhance the speed of certain operations and algorithms. This allows us to either use advanced technologies that were previously infeasible to deploy without the GPU speed bump, or to sometimes dramatically increase the speed performance of existing technologies. Supported GPU Hardware, Software, and Drivers Refer to the OLIVE Hardware Requirements for information on what versions of CUDA and Nvidia drivers are currently supported. Currently supported GPU plugins These plugins currently support GPU operation, when configured as outlined above: sad-dnn-v8.0.0+ (Speech Activity Detection) sid-dplda-v3.0.0+ (Speaker Identification) sdd-embed-v2.0.0+ (Speaker Diarization and Detection) lid-hdplda-v2.0.0+ (Language Identification) tmt-ctranslate-v1.0.0+ (Text Machine Translation) asr-end2end-v4.0.0+ (Speech Recognition) asr-whisper-v2.0.0+ (Speech Recognition) fdi-pyEmbed-v1.2.0+ (Face Detection in Imagery) fri-pyEmbed-v1.2.0+ (Face Recognition in Imagery) fdv-pyEmbed-v1.2.0+ (Face Detection in Video) frv-pyEmbed-v1.2.0+ (Face Recognition in Video) dfa-end2end-v1.0.0+ (Deep Fake Audio Detection) All are capable of running on CPU as well, though in some cases (notably asr-end2end-v1 and asr-whisper-v1) at a drastically reduced speed. GPU Configuration Requirements In order to use GPUs with an OLIVE software instance, three things need to happen; The hardware OLIVE is being installed and run on must have a compatible GPU or GPUs installed. Currently, OLIVE can only use Nvidia GPUs with CUDA cores and properly installed Nvidia drivers. If applicable, you will also need the Nvidia Docker toolkit. Refer to Nvidia's documentation for installation of these . OLIVE itself must be configured and launched so that it has access to these GPUs. Refer to the documentation specific to your delivery type for information on how to do this. The most likely appropriate reference for this is the Martini Startup Instructions . Each domain of each plugin that you wish to run on the GPU must be configured to specify this information. This is covered below . Plugin Domain Configuration for GPU Use By default, each OLIVE plugin domain is typically configured internally to run solely on the CPU. OLIVE 5.6.0 introduces a new, centralized, more convenient method for configuring the device each domain will use for processing. CPU/GPU device configuration is now accomplished in an OLIVE Configuration file, called olive.conf , that can be found if a default is provided with your delivery or created if one is not in: .../olive6.0.0/oliveAppData/ The format of this file follows: olive.conf Generic [ device.<plugin name 1 > ] <domain 1 > = <desired device id> <domain 2 > = <desired device id> ... <domain N> = <desired device id> [ device.<plugin name 2 > ] <domain 1 > = <desired device id> <domain 2 > = <desired device id> ... <domain N> = <desired device id> Device assignments are made on a per domain basis, not on a plugin-basis. So each domain must be configured separately if wishing to override the default device being used. By default they will still run on CPU, which for most GPU-capable plugins will run significantly more slowly. Each domain can be independently assigned a device; just because one domain of a plugin is using a GPU doesn't mean the others automatically will or need to. By extension, it is not necessary for all domains of a plugin to run on the same GPU.\" Having this device assignment at the domain level allows the distribution of domains across multiple GPUs. For example, if multiple GPUs are available on a system, lower-memory-usage plugins like SAD, SID, and LID may share a single GPU, while heavier plugins like ASR or MT can be flexible enough to assign different language domains of each plugin across multiple GPUs, spreading the memory load to minimize the chance of exhausting GPU memory, while also saving lost time frequently loading and unloading models. The ideal configuration may vary greatly depending on specific customer use case, hardware, and mission needs. As many or few plugins and domains as desired can be re-assigned in this config file. It is not necessary to include all available plugins in this file; any omitted will use the default, internally assigned device (most often cpu ). Multiple domains can be assigned to the same GPU device, as long as that device's available resources can support each domain. The available \"device IDs\" will vary from system to system depending on how many GPUs are available and how they are exposed. Only GPU device(s) that are properly exposed to the server by configuring/launching the server appropriately In general, the valid options are: cpu gpu0 gpu1 ... gpuN Many/most systems will only have a single GPU, exposed as gpu0 - the digit required appended to gpu is the device ID reported by Nvidia with the as reported by NVIDIA System Management Interface, or nvidia-smi tool (more info here . nvidia-smi Example Output (Click to expand) +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.46 Driver Version: 495.46 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:04:00.0 Off | N/A | | 20% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:05:00.0 Off | N/A | | 20% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce ... Off | 00000000:08:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 NVIDIA GeForce ... Off | 00000000:84:00.0 Off | N/A | | 18% 26C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 NVIDIA GeForce ... Off | 00000000:85:00.0 Off | N/A | | 19% 27C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 NVIDIA GeForce ... Off | 00000000:88:00.0 Off | N/A | | 18% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 NVIDIA GeForce ... Off | 00000000:89:00.0 Off | N/A | | 17% 27C P0 73W / 250W | 0MiB / 12212MiB | 1% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ An example of what this config file could look like filled in: olive.conf Example [ device.asr-end2end-v1.2.0 ] english-augmented-v1 = gpu0 french-v1 = cpu korean-augmented-v1 = gpu1 [ device.sad-dnn-v8.1.0 ] multi-v1 = gpu0 OLIVE GPU Restrictions and Configuration Notes GPU Compute Mode: \"Default\" vs \"Exclusive\" The OLIVE software currently assumes that any available GPUs are in the \"default\" mode. In testing, some configurations of number of OLIVE server workers have been found to cause unexpected issues if the GPUs are configured to be running in \"Exclusive\" mode. If possible, please configure GPUs that OLIVE will be using in \"Default\" mode, and if this is not possible, please ensure that the number of workers for the GPU-enabled server is specified to be 1. This is configured in the provided docker-compose.yml file. To check the mode of your GPU, you can view the Compute Mode field of the nvidia-smi command output. Please refer to Nvidia's instructions for Nvidia Control Panel or the Usage instructions for nvidia-smi for more information on how to set these modes.","title":"GPU Plugin Configuration"},{"location":"gpu-config.html#gpu-capable-olive-plugin-domain-configuration","text":"","title":"GPU-Capable OLIVE Plugin / Domain Configuration"},{"location":"gpu-config.html#introduction","text":"With the release of OLIVE 5.5.0, certain plugin capabilities are now able to leverage GPU hardware to enhance the speed of certain operations and algorithms. This allows us to either use advanced technologies that were previously infeasible to deploy without the GPU speed bump, or to sometimes dramatically increase the speed performance of existing technologies.","title":"Introduction"},{"location":"gpu-config.html#supported-gpu-hardware-software-and-drivers","text":"Refer to the OLIVE Hardware Requirements for information on what versions of CUDA and Nvidia drivers are currently supported.","title":"Supported GPU Hardware, Software, and Drivers"},{"location":"gpu-config.html#currently-supported-gpu-plugins","text":"These plugins currently support GPU operation, when configured as outlined above: sad-dnn-v8.0.0+ (Speech Activity Detection) sid-dplda-v3.0.0+ (Speaker Identification) sdd-embed-v2.0.0+ (Speaker Diarization and Detection) lid-hdplda-v2.0.0+ (Language Identification) tmt-ctranslate-v1.0.0+ (Text Machine Translation) asr-end2end-v4.0.0+ (Speech Recognition) asr-whisper-v2.0.0+ (Speech Recognition) fdi-pyEmbed-v1.2.0+ (Face Detection in Imagery) fri-pyEmbed-v1.2.0+ (Face Recognition in Imagery) fdv-pyEmbed-v1.2.0+ (Face Detection in Video) frv-pyEmbed-v1.2.0+ (Face Recognition in Video) dfa-end2end-v1.0.0+ (Deep Fake Audio Detection) All are capable of running on CPU as well, though in some cases (notably asr-end2end-v1 and asr-whisper-v1) at a drastically reduced speed.","title":"Currently supported GPU plugins"},{"location":"gpu-config.html#gpu-configuration-requirements","text":"In order to use GPUs with an OLIVE software instance, three things need to happen; The hardware OLIVE is being installed and run on must have a compatible GPU or GPUs installed. Currently, OLIVE can only use Nvidia GPUs with CUDA cores and properly installed Nvidia drivers. If applicable, you will also need the Nvidia Docker toolkit. Refer to Nvidia's documentation for installation of these . OLIVE itself must be configured and launched so that it has access to these GPUs. Refer to the documentation specific to your delivery type for information on how to do this. The most likely appropriate reference for this is the Martini Startup Instructions . Each domain of each plugin that you wish to run on the GPU must be configured to specify this information. This is covered below .","title":"GPU Configuration Requirements"},{"location":"gpu-config.html#plugin-domain-configuration-for-gpu-use","text":"By default, each OLIVE plugin domain is typically configured internally to run solely on the CPU. OLIVE 5.6.0 introduces a new, centralized, more convenient method for configuring the device each domain will use for processing. CPU/GPU device configuration is now accomplished in an OLIVE Configuration file, called olive.conf , that can be found if a default is provided with your delivery or created if one is not in: .../olive6.0.0/oliveAppData/ The format of this file follows: olive.conf Generic [ device.<plugin name 1 > ] <domain 1 > = <desired device id> <domain 2 > = <desired device id> ... <domain N> = <desired device id> [ device.<plugin name 2 > ] <domain 1 > = <desired device id> <domain 2 > = <desired device id> ... <domain N> = <desired device id> Device assignments are made on a per domain basis, not on a plugin-basis. So each domain must be configured separately if wishing to override the default device being used. By default they will still run on CPU, which for most GPU-capable plugins will run significantly more slowly. Each domain can be independently assigned a device; just because one domain of a plugin is using a GPU doesn't mean the others automatically will or need to. By extension, it is not necessary for all domains of a plugin to run on the same GPU.\" Having this device assignment at the domain level allows the distribution of domains across multiple GPUs. For example, if multiple GPUs are available on a system, lower-memory-usage plugins like SAD, SID, and LID may share a single GPU, while heavier plugins like ASR or MT can be flexible enough to assign different language domains of each plugin across multiple GPUs, spreading the memory load to minimize the chance of exhausting GPU memory, while also saving lost time frequently loading and unloading models. The ideal configuration may vary greatly depending on specific customer use case, hardware, and mission needs. As many or few plugins and domains as desired can be re-assigned in this config file. It is not necessary to include all available plugins in this file; any omitted will use the default, internally assigned device (most often cpu ). Multiple domains can be assigned to the same GPU device, as long as that device's available resources can support each domain. The available \"device IDs\" will vary from system to system depending on how many GPUs are available and how they are exposed. Only GPU device(s) that are properly exposed to the server by configuring/launching the server appropriately In general, the valid options are: cpu gpu0 gpu1 ... gpuN Many/most systems will only have a single GPU, exposed as gpu0 - the digit required appended to gpu is the device ID reported by Nvidia with the as reported by NVIDIA System Management Interface, or nvidia-smi tool (more info here . nvidia-smi Example Output (Click to expand) +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.46 Driver Version: 495.46 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:04:00.0 Off | N/A | | 20% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:05:00.0 Off | N/A | | 20% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce ... Off | 00000000:08:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 NVIDIA GeForce ... Off | 00000000:84:00.0 Off | N/A | | 18% 26C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 NVIDIA GeForce ... Off | 00000000:85:00.0 Off | N/A | | 19% 27C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 NVIDIA GeForce ... Off | 00000000:88:00.0 Off | N/A | | 18% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 NVIDIA GeForce ... Off | 00000000:89:00.0 Off | N/A | | 17% 27C P0 73W / 250W | 0MiB / 12212MiB | 1% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ An example of what this config file could look like filled in: olive.conf Example [ device.asr-end2end-v1.2.0 ] english-augmented-v1 = gpu0 french-v1 = cpu korean-augmented-v1 = gpu1 [ device.sad-dnn-v8.1.0 ] multi-v1 = gpu0","title":"Plugin Domain Configuration for GPU Use"},{"location":"gpu-config.html#olive-gpu-restrictions-and-configuration-notes","text":"","title":"OLIVE GPU Restrictions and Configuration Notes"},{"location":"gpu-config.html#gpu-compute-mode-default-vs-exclusive","text":"The OLIVE software currently assumes that any available GPUs are in the \"default\" mode. In testing, some configurations of number of OLIVE server workers have been found to cause unexpected issues if the GPUs are configured to be running in \"Exclusive\" mode. If possible, please configure GPUs that OLIVE will be using in \"Default\" mode, and if this is not possible, please ensure that the number of workers for the GPU-enabled server is specified to be 1. This is configured in the provided docker-compose.yml file. To check the mode of your GPU, you can view the Compute Mode field of the nvidia-smi command output. Please refer to Nvidia's instructions for Nvidia Control Panel or the Usage instructions for nvidia-smi for more information on how to set these modes.","title":"GPU Compute Mode: \"Default\" vs \"Exclusive\""},{"location":"gpu.html","text":"GPU-Capable OLIVE Docker Installation and Setup OLIVE Folder Structure Overview The initial release of GPU-enabled OLIVE follows the structure described below. Note that this is similar overall to previous deliveries but differs significantly from both native linux-based packages and docker-based packages in the past regarding how the server is started and managed. The important differences can be seen in the oliveDocker/ directory. If the OLIVE package you were provided does not match this formatting, please refer to the appropriate setup guide for your delivery. The OLIVE delivery typically comes in a single archive: olive6.0.0-DDMonthYYYY.tar.gz That unpacks into a structure resembling: - olive6 . 0 . 0 - api - Java and Python example client API implementation code and CLI client utilities - java - python - docs / - Directory containing the OLIVE documentation - index . html - Open this in a web browser to view - oliveDocker / - olive + runtime - 6 . 0 . 0 - Ubuntu - 20 . 04 - x86_64 . tar . gz - OLIVE Core Software and Runtime Bundle - Dockerfile - docker - compose . yml - OliveGUI / - The OLIVE Nightingale GUI ( not included in all deliveries ) - bin / - Nightingale - oliveAppData / - plugins / - sad - dnn - v7 . 0 . 0 ( example ) \u2013 Speech Activity Detection plugin - Actual plugins included will depend on the customer , mission , and delivery - oliveAppDataGPU / - plugins / - asr - end2end - v1 . 0 . 0 ( example ) - Speech Recognition ( end - to - end ) plugin configured to run on GPU - Actual plugins included will depend on the customer , mission , and delivery The actual plugins included will vary from customer to customer, and may even vary between use case configurations within a customer integration. Install and Start Docker Before you can get started installing and running OLIVE, you'll need to make sure you have fully installed and configured Docker. The proper installation steps vary depending on your host OS, so please refer to the appropriate official Docker installation instructions: Docker Desktop for Windows Docker Desktop for macOS Docker Engine for Ubuntu Docker for Ubuntu is especially important to follow the official steps closely, as there are additional important post-installation steps to perform to make sure docker runs smoothly on your system. Note that if installing into an Ubuntu instance running on WSL2, systemctl is not used on such systems. This means that some of the commands provided in the Docker for Ubuntu instructions above may not succeed as written; notably for starting and stopping the Docker service. Please use service for these commands instead: $ sudo service docker start In addition, if using Docker for Ubuntu, the Nvidia drivers must be installed separately. This doesn't seem to be necessary if using Docker Desktop. Instructions for this installation can be found here, from Nvidia . Before moving on, be sure that the docker service has been started. Build and Launch OLIVE Docker The core of the OLIVE software is contained within oliveDocker/olive+runtime-6.0.0-Ubuntu-20.04-x86_64.tar.gz archive. Each delivery includes a docker-compose.yml and Dockerfile that informs Docker how to build and launch this into a running OLIVE server. The process for building the OLIVE image is: $ cd olive5.5.0/oliveDocker/ $ docker compose build This only needs to be performed once per machine. Once this is complete, launching the server can be done with the following command, from the same location: $ docker compose up This call will start the OLIVE image and launch multiple OLIVE servers according to the configuration contained in docker-compose.yml ; the default configuration is described below. Default OLIVE Server Configuration As delivered, performing the steps above will launch two OLIVE servers. The first will only perform CPU processing, and has access to the plugins contained in: `olive6.0.0/oliveAppData/plugins/` This server is analogous to previous Docker-based OLIVE deliveries. It listens on the same ports as before, and is configured the same way, such that the number of workers (concurrent server jobs) is automatically limited based on the number of threads available on the CPU hardware. The second server has access to the GPU and can use this if the plugin domains have been properly configured, and assumes there is only one GPU available, device 0 according to nvidia-smi . It can run with the plugins contained in: `olive6.0.0/oliveAppDataGPU/plugins/` It listens on different ports (see next section), and is configured to have a single worker to stabilize GPU memory usage. This is a global setting for an OLIVE server, so separating the CPU and GPU plugins into separate OLIVE servers allows us to run the CPU server unthrottled, allowing a number of parallel jobs based on the number of cores or threads available on the host hardware, without being limited by the setting of the GPU server that is relying on a single (very fast) worker thread. By default, this initial delivery only includes a single plugin with domains configured and placed such that it will run on the GPU: asr-end2end-v1.0.0 See below for instructions on configuring others to run on the GPU, and a list of released GPU-capable plugins. Interacting with OLIVE GPU Server Once an OLIVE 6.0.0 GPU-capable server is running, tasking it from a client is largely identical to before; whether from the Java or python client APIs or one of our GUIs. The only salient difference is that the multi-server approach means that each server is listening on a different set of ports. By default, those ports are: Server Server Request Port Server Status Port CPU 5588 5589 GPU 6588 6589 These are the defaults, which can be configured in the docker-compose.yml file as shown below. Be sure to route requests to the appropriate server. If using most of SRI's client utilities or UIs, the default ports used (5588/5589) will contact the CPU-only server. Sending requests to the GPU server instead will require a flag (i.e. --port 6588 if using the Java and Python client utilities ) or other configuration changes. Docker Image Configuration Configuration of the number and setup of OLIVE servers is controlled by settings within the included docker-compose.yml file. The entire file is displayed below, with excerpts showing which section is configuring the CPU server and the GPU server. This is followed by a section for each of the most important and likely to be altered variables. Note that none of this needs to be touched if running the default configuration, on the assumed default hardware (single-GPU machine). docker-compose.yml Whole File version : \"3.9\" x-common-variables : &common-variables FONTCONFIG_PATH : /etc/fonts/ FONTCONFIG_FILE : /etc/fonts/fonts.conf XDG_CACHE_HOME : /tmp MPLCONFIGDIR : /tmp services : runtime-cpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" ports : - \"5588:5588\" - \"5589:5589\" - \"5590:5590\" - \"5591:5591\" volumes : - ../oliveAppData:/home/olive/olive environment : *common-variables shm_size : '4gb' runtime-gpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" - \"--workers=1\" ports : - \"6588:5588\" - \"6589:5589\" - \"6590:5590\" - \"6591:5591\" volumes : - ../oliveAppDataGPU:/home/olive/olive environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' shm_size : '4gb' deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] CPU Server Section ... services : runtime-cpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" ports : - \"5588:5588\" - \"5589:5589\" - \"5590:5590\" - \"5591:5591\" volumes : - ../oliveAppData:/home/olive/olive environment : *common-variables shm_size : '4gb' ... GPU Server Section ... services : ... runtime-gpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" - \"--workers=1\" ports : - \"6588:5588\" - \"6589:5589\" - \"6590:5590\" - \"6591:5591\" volumes : - ../oliveAppDataGPU:/home/olive/olive environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' shm_size : '4gb' deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] Exposing GPU(s) to an OLIVE Server (CUDA_VISIBLE_DEVICES) If your hardware contains multiple GPUs or if your GPU does not identify as device 0 for any reason, you may need to modify the CUDA_VISIBLE_DEVICES environment variable used by OLIVE GPU server and/or the device_ids argument in the resource allocation section of the docker-compose.yml . The former is controlled by: services: runtime-gpu: environment: CUDA_VISIBLE_DEVICES And the latter by: services: runtime-gpu: deploy: resources: reservations: devices: device_ids Both can be seen in the excerpt below: ... services : ... runtime-gpu : ... environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' ... deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] For more information on how and why to set CUDA_VISIBLE_DEVICES , see Nvidia's documentation . This variable is used to control what GPU(s) the server is able to see and lean on during processing. Exposing a GPU device here makes the GPU visible to OLIVE, but does not force anything to run on it. For configuring a plugin/domain to use an available GPU, see the relevant section below. For more information about device_ids , how it should be set, and how this may interact with other settings, see the Docker documentation on GPU support . This page also covers a possible, untested alternative, of using count instead of device_ids . Both of these arguments are controlling the container's GPU access; as such, these should likely match, and both pass the same list of GPUs and in the same order, but as each application and each customer's hardware setup and needs may vary, some testing and tweaking may be necessary on the client end to ensure the behavior is as desired. Note that these parameters identify a GPU by its device ID as reported by nvidia-smi . These can be verified by checking the top left entry for each GPU in the nvidia-smi output; an example is shown below. nvidia-smi Example Output (Click to expand) +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.46 Driver Version: 495.46 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:04:00.0 Off | N/A | | 20% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:05:00.0 Off | N/A | | 20% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce ... Off | 00000000:08:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 NVIDIA GeForce ... Off | 00000000:84:00.0 Off | N/A | | 18% 26C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 NVIDIA GeForce ... Off | 00000000:85:00.0 Off | N/A | | 19% 27C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 NVIDIA GeForce ... Off | 00000000:88:00.0 Off | N/A | | 18% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 NVIDIA GeForce ... Off | 00000000:89:00.0 Off | N/A | | 17% 27C P0 73W / 250W | 0MiB / 12212MiB | 1% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Plugin Location (OLIVE_APP_DATA) As described above, the default configuration separates CPU-configured plugins into olive6.0.0/oliveAppData/plugins/ and GPU-enabled/configured plugins into olive6.0.0/oliveAppDataGPU/plugins/ . If maintaining a two-server setup similar to the delivered configuration, these do not need to be changed. To convert a GPU-capable plugin from CPU, first it must be moved or placed into the oliveAppDataGPU plugins directory. Note that if moving a plugin that has enrollment capability, any enrolled models will not be transferred. Re-enrollment must be performed once the plugin is moved and reconfigured, or for advanced users, the enrollments can be moved separately. If the name or location of the oliveAppData directories need to change for any reason, the docker-compose.yml must be updated to reflect this, specifically the volumes mounted for each server that are mapped to /home/olive/olive/ within the containers. This excerpt shows where this is set for the GPU server: ... services : ... runtime-gpu : ... volumes : - ../oliveAppDataGPU:/home/olive/olive ... GPU Server Concurrent Processing Jobs (--workers) [Experimental] If maximum batch throughput is important for your application, and it is known that memory requirements of your use case won't overwhelm the available memory of the GPU, the number of server workers for the GPU server can be increased. This is experimental and may affect overall server performance (speed) and stability. This setting controls the number of jobs the server can process simultaneously, and for GPU-enabled domains will increase the number of times the associated models are loaded into memory. As the percentage of used GPU memory increases, speed may decrease as less optimal pathways are used. This also increases the likelihood of ungracefully exhausting the available GPU memory, which may cause a system crash. The majority of the OLIVE 6.0.0 testing was performed with --workers set to 1 . To increase the number of server workers, increase the argument passed to the --workers flag in the docker-compose.yml , under services: runtime-gpu: command: ... services : ... runtime-gpu : ... command : ... - \"--workers=1\" Plugin Configuration for GPU Use Enable GPU usage for a plugin's domain To allow a plugin to run on an available GPU, it is crucial that the plugin: Is located in the plugins directory of a GPU enabled server (oliveAppDataGPU/plugins by default) Has each desired domain configured to choose a GPU device in its meta.conf file Only has domains configured to select GPU device(s) that are properly exposed to the server via CUDA_VISIBLE_DEVICES and Docker's device_ids More information for #1 and #3 can be found in the appropriate sections above. Be sure to move the plugin to oliveAppDataGPU/plugins/ when reconfiguring, and restarting any running servers. Be sure to double-check that the device(s) you are configuring each plugin domain to use are actually exposed to the server that will be using that plugin/domain. Configuring a plugin to use a GPU is done at the domain level, by changing the device variable assignment within the domain's meta.conf file. By default, most plugins have this variable assigned to cpu , and as such the domain will run on CPU only, even when running within a GPU-enabled OLIVE server. To assign a domain access to a GPU, change this device assignment from cpu to gpuN where N is the device ID of the GPU to run on, as reported by nvidia-smi (more info here ). As already discussed it is critical that the device assigned to the domain is exposed to the OLIVE server that will be running this domain. Most hardware setups will only have a single GPU available, so enabling this is simply replacing cpu with gpu0 . As an example, the meta.conf for the english-v1 domain of asr-end2end-v1.0.0 in its off-the-shelf format is shown below: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : cpu To instead configure this domain to run on the GPU with ID #0, as it is when delivered with OLIVE 6.0.0, this domain then becomes: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : gpu0 Each domain must be configured separately. If within a single plugin, one domain is configured for GPU, the others don't automatically start using a GPU. By default they will still run on CPU, which for most plugins will run significantly more slowly. By extension, it is not necessary for all domains of a plugin to run on the same GPU. Having this device assignment at the domain level allows the distribution of domains across multiple GPUs. For example, if multiple GPUs are available on a system, lower-memory-usage plugins like SAD, SID, and LID may share a single GPU, while heavier plugins like ASR or MT can be flexible enough to assign different language domains of each plugin across multiple GPUs, spreading the memory load to minimize the chance of exhausting GPU memory, while also saving lost time frequently loading and unloading models. Building off of the example above, if we wanted to run the russian-v1 domain of the same ASR plugin on GPU device #2, so that english-v1 and russian-v1 don't compete with respect to GPU memory, the russian-v1 domain would look like this: label : russian - v1 description : Large vocabulary Russian wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : Russian device : gpu2 Note that in these examples, GPU device 0 and GPU device 2 must both be listed in CUDA_VISIBLE_DEVICES and device_ids as outlined above . Example docker-compose.yml modification ``` yaml ... services: ... runtime-gpu: ... environment: <<: *common-variables CUDA_VISIBLE_DEVICES: '0,2' ... deploy: resources: reservations: devices: - driver: nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids: ['0', '2'] capabilities: [gpu] The exact ideal configuration may vary greatly depending on specific customer use case and mission needs. Currently supported GPU plugins Please refer to the GPU Configuration documentation for a list of currently supported GPU plugins. OLIVE GPU Restrictions and Configuration Notes GPU Compute Mode: \"Default\" vs \"Exclusive\" The OLIVE software currently assumes that any available GPUs are in the \"default\" mode. In testing, some configurations of number of OLIVE server workers have been found to cause unexpected issues if the GPUs are configured to be running in \"Exclusive\" mode. If possible, please configure GPUs that OLIVE will be using in \"Default\" mode, and if this is not possible, please ensure that the number of workers for the GPU-enabled server is specified to be 1. This is configured in the provided docker-compose.yml file. To check the mode of your GPU, you can view the Compute Mode field of the nvidia-smi command output. Please refer to Nvidia's instructions for Nvidia Control Panel or the Usage instructions for nvidia-smi for more information on how to set these modes. Workflow Restrictions Plugins can't currently pass jobs between separate OLIVE servers. This means that if you have a workflow that requires several plugins, each of those plugins must be visible to the OLIVE server that is receiving the workflow job request. If your current configuration separates any of these plugins, your plugins directories must be reconfigured so that all required plugins are colocated within the same server. There are two possible workarounds. The first would be disabling GPU capabilities in some plugins and moving them to the CPU oliveAppData/ directory, which may cause them to run much more slowly, but will allow full job parallelization. An alternative would be to move CPU-only plugins into the GPU-enabled server, leaving them configured to run on the CPU. This would allow the GPU-enabled plugins to run at full speed, but would limit parallel processing due to having a single worker thread. Future work in OLIVE hopes to address these first-release GPU limitations to reduce and eventually eliminate workarounds and restrictions like this.","title":"GPU-Capable OLIVE Docker Installation and Setup"},{"location":"gpu.html#gpu-capable-olive-docker-installation-and-setup","text":"","title":"GPU-Capable OLIVE Docker Installation and Setup"},{"location":"gpu.html#olive-folder-structure-overview","text":"The initial release of GPU-enabled OLIVE follows the structure described below. Note that this is similar overall to previous deliveries but differs significantly from both native linux-based packages and docker-based packages in the past regarding how the server is started and managed. The important differences can be seen in the oliveDocker/ directory. If the OLIVE package you were provided does not match this formatting, please refer to the appropriate setup guide for your delivery. The OLIVE delivery typically comes in a single archive: olive6.0.0-DDMonthYYYY.tar.gz That unpacks into a structure resembling: - olive6 . 0 . 0 - api - Java and Python example client API implementation code and CLI client utilities - java - python - docs / - Directory containing the OLIVE documentation - index . html - Open this in a web browser to view - oliveDocker / - olive + runtime - 6 . 0 . 0 - Ubuntu - 20 . 04 - x86_64 . tar . gz - OLIVE Core Software and Runtime Bundle - Dockerfile - docker - compose . yml - OliveGUI / - The OLIVE Nightingale GUI ( not included in all deliveries ) - bin / - Nightingale - oliveAppData / - plugins / - sad - dnn - v7 . 0 . 0 ( example ) \u2013 Speech Activity Detection plugin - Actual plugins included will depend on the customer , mission , and delivery - oliveAppDataGPU / - plugins / - asr - end2end - v1 . 0 . 0 ( example ) - Speech Recognition ( end - to - end ) plugin configured to run on GPU - Actual plugins included will depend on the customer , mission , and delivery The actual plugins included will vary from customer to customer, and may even vary between use case configurations within a customer integration.","title":"OLIVE Folder Structure Overview"},{"location":"gpu.html#install-and-start-docker","text":"Before you can get started installing and running OLIVE, you'll need to make sure you have fully installed and configured Docker. The proper installation steps vary depending on your host OS, so please refer to the appropriate official Docker installation instructions: Docker Desktop for Windows Docker Desktop for macOS Docker Engine for Ubuntu Docker for Ubuntu is especially important to follow the official steps closely, as there are additional important post-installation steps to perform to make sure docker runs smoothly on your system. Note that if installing into an Ubuntu instance running on WSL2, systemctl is not used on such systems. This means that some of the commands provided in the Docker for Ubuntu instructions above may not succeed as written; notably for starting and stopping the Docker service. Please use service for these commands instead: $ sudo service docker start In addition, if using Docker for Ubuntu, the Nvidia drivers must be installed separately. This doesn't seem to be necessary if using Docker Desktop. Instructions for this installation can be found here, from Nvidia . Before moving on, be sure that the docker service has been started.","title":"Install and Start Docker"},{"location":"gpu.html#build-and-launch-olive-docker","text":"The core of the OLIVE software is contained within oliveDocker/olive+runtime-6.0.0-Ubuntu-20.04-x86_64.tar.gz archive. Each delivery includes a docker-compose.yml and Dockerfile that informs Docker how to build and launch this into a running OLIVE server. The process for building the OLIVE image is: $ cd olive5.5.0/oliveDocker/ $ docker compose build This only needs to be performed once per machine. Once this is complete, launching the server can be done with the following command, from the same location: $ docker compose up This call will start the OLIVE image and launch multiple OLIVE servers according to the configuration contained in docker-compose.yml ; the default configuration is described below.","title":"Build and Launch OLIVE Docker"},{"location":"gpu.html#default-olive-server-configuration","text":"As delivered, performing the steps above will launch two OLIVE servers. The first will only perform CPU processing, and has access to the plugins contained in: `olive6.0.0/oliveAppData/plugins/` This server is analogous to previous Docker-based OLIVE deliveries. It listens on the same ports as before, and is configured the same way, such that the number of workers (concurrent server jobs) is automatically limited based on the number of threads available on the CPU hardware. The second server has access to the GPU and can use this if the plugin domains have been properly configured, and assumes there is only one GPU available, device 0 according to nvidia-smi . It can run with the plugins contained in: `olive6.0.0/oliveAppDataGPU/plugins/` It listens on different ports (see next section), and is configured to have a single worker to stabilize GPU memory usage. This is a global setting for an OLIVE server, so separating the CPU and GPU plugins into separate OLIVE servers allows us to run the CPU server unthrottled, allowing a number of parallel jobs based on the number of cores or threads available on the host hardware, without being limited by the setting of the GPU server that is relying on a single (very fast) worker thread. By default, this initial delivery only includes a single plugin with domains configured and placed such that it will run on the GPU: asr-end2end-v1.0.0 See below for instructions on configuring others to run on the GPU, and a list of released GPU-capable plugins.","title":"Default OLIVE Server Configuration"},{"location":"gpu.html#interacting-with-olive-gpu-server","text":"Once an OLIVE 6.0.0 GPU-capable server is running, tasking it from a client is largely identical to before; whether from the Java or python client APIs or one of our GUIs. The only salient difference is that the multi-server approach means that each server is listening on a different set of ports. By default, those ports are: Server Server Request Port Server Status Port CPU 5588 5589 GPU 6588 6589 These are the defaults, which can be configured in the docker-compose.yml file as shown below. Be sure to route requests to the appropriate server. If using most of SRI's client utilities or UIs, the default ports used (5588/5589) will contact the CPU-only server. Sending requests to the GPU server instead will require a flag (i.e. --port 6588 if using the Java and Python client utilities ) or other configuration changes.","title":"Interacting with OLIVE GPU Server"},{"location":"gpu.html#docker-image-configuration","text":"Configuration of the number and setup of OLIVE servers is controlled by settings within the included docker-compose.yml file. The entire file is displayed below, with excerpts showing which section is configuring the CPU server and the GPU server. This is followed by a section for each of the most important and likely to be altered variables. Note that none of this needs to be touched if running the default configuration, on the assumed default hardware (single-GPU machine).","title":"Docker Image Configuration"},{"location":"gpu.html#docker-composeyml","text":"Whole File version : \"3.9\" x-common-variables : &common-variables FONTCONFIG_PATH : /etc/fonts/ FONTCONFIG_FILE : /etc/fonts/fonts.conf XDG_CACHE_HOME : /tmp MPLCONFIGDIR : /tmp services : runtime-cpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" ports : - \"5588:5588\" - \"5589:5589\" - \"5590:5590\" - \"5591:5591\" volumes : - ../oliveAppData:/home/olive/olive environment : *common-variables shm_size : '4gb' runtime-gpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" - \"--workers=1\" ports : - \"6588:5588\" - \"6589:5589\" - \"6590:5590\" - \"6591:5591\" volumes : - ../oliveAppDataGPU:/home/olive/olive environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' shm_size : '4gb' deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] CPU Server Section ... services : runtime-cpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" ports : - \"5588:5588\" - \"5589:5589\" - \"5590:5590\" - \"5591:5591\" volumes : - ../oliveAppData:/home/olive/olive environment : *common-variables shm_size : '4gb' ... GPU Server Section ... services : ... runtime-gpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" - \"--workers=1\" ports : - \"6588:5588\" - \"6589:5589\" - \"6590:5590\" - \"6591:5591\" volumes : - ../oliveAppDataGPU:/home/olive/olive environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' shm_size : '4gb' deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ]","title":"docker-compose.yml"},{"location":"gpu.html#exposing-gpus-to-an-olive-server-cuda_visible_devices","text":"If your hardware contains multiple GPUs or if your GPU does not identify as device 0 for any reason, you may need to modify the CUDA_VISIBLE_DEVICES environment variable used by OLIVE GPU server and/or the device_ids argument in the resource allocation section of the docker-compose.yml . The former is controlled by: services: runtime-gpu: environment: CUDA_VISIBLE_DEVICES And the latter by: services: runtime-gpu: deploy: resources: reservations: devices: device_ids Both can be seen in the excerpt below: ... services : ... runtime-gpu : ... environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' ... deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] For more information on how and why to set CUDA_VISIBLE_DEVICES , see Nvidia's documentation . This variable is used to control what GPU(s) the server is able to see and lean on during processing. Exposing a GPU device here makes the GPU visible to OLIVE, but does not force anything to run on it. For configuring a plugin/domain to use an available GPU, see the relevant section below. For more information about device_ids , how it should be set, and how this may interact with other settings, see the Docker documentation on GPU support . This page also covers a possible, untested alternative, of using count instead of device_ids . Both of these arguments are controlling the container's GPU access; as such, these should likely match, and both pass the same list of GPUs and in the same order, but as each application and each customer's hardware setup and needs may vary, some testing and tweaking may be necessary on the client end to ensure the behavior is as desired. Note that these parameters identify a GPU by its device ID as reported by nvidia-smi . These can be verified by checking the top left entry for each GPU in the nvidia-smi output; an example is shown below. nvidia-smi Example Output (Click to expand) +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.46 Driver Version: 495.46 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:04:00.0 Off | N/A | | 20% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:05:00.0 Off | N/A | | 20% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce ... Off | 00000000:08:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 NVIDIA GeForce ... Off | 00000000:84:00.0 Off | N/A | | 18% 26C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 NVIDIA GeForce ... Off | 00000000:85:00.0 Off | N/A | | 19% 27C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 NVIDIA GeForce ... Off | 00000000:88:00.0 Off | N/A | | 18% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 NVIDIA GeForce ... Off | 00000000:89:00.0 Off | N/A | | 17% 27C P0 73W / 250W | 0MiB / 12212MiB | 1% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+","title":"Exposing GPU(s) to an OLIVE Server (CUDA_VISIBLE_DEVICES)"},{"location":"gpu.html#plugin-location-olive_app_data","text":"As described above, the default configuration separates CPU-configured plugins into olive6.0.0/oliveAppData/plugins/ and GPU-enabled/configured plugins into olive6.0.0/oliveAppDataGPU/plugins/ . If maintaining a two-server setup similar to the delivered configuration, these do not need to be changed. To convert a GPU-capable plugin from CPU, first it must be moved or placed into the oliveAppDataGPU plugins directory. Note that if moving a plugin that has enrollment capability, any enrolled models will not be transferred. Re-enrollment must be performed once the plugin is moved and reconfigured, or for advanced users, the enrollments can be moved separately. If the name or location of the oliveAppData directories need to change for any reason, the docker-compose.yml must be updated to reflect this, specifically the volumes mounted for each server that are mapped to /home/olive/olive/ within the containers. This excerpt shows where this is set for the GPU server: ... services : ... runtime-gpu : ... volumes : - ../oliveAppDataGPU:/home/olive/olive ...","title":"Plugin Location (OLIVE_APP_DATA)"},{"location":"gpu.html#gpu-server-concurrent-processing-jobs-workers-experimental","text":"If maximum batch throughput is important for your application, and it is known that memory requirements of your use case won't overwhelm the available memory of the GPU, the number of server workers for the GPU server can be increased. This is experimental and may affect overall server performance (speed) and stability. This setting controls the number of jobs the server can process simultaneously, and for GPU-enabled domains will increase the number of times the associated models are loaded into memory. As the percentage of used GPU memory increases, speed may decrease as less optimal pathways are used. This also increases the likelihood of ungracefully exhausting the available GPU memory, which may cause a system crash. The majority of the OLIVE 6.0.0 testing was performed with --workers set to 1 . To increase the number of server workers, increase the argument passed to the --workers flag in the docker-compose.yml , under services: runtime-gpu: command: ... services : ... runtime-gpu : ... command : ... - \"--workers=1\"","title":"GPU Server Concurrent Processing Jobs (--workers) [Experimental]"},{"location":"gpu.html#plugin-configuration-for-gpu-use","text":"","title":"Plugin Configuration for GPU Use"},{"location":"gpu.html#enable-gpu-usage-for-a-plugins-domain","text":"To allow a plugin to run on an available GPU, it is crucial that the plugin: Is located in the plugins directory of a GPU enabled server (oliveAppDataGPU/plugins by default) Has each desired domain configured to choose a GPU device in its meta.conf file Only has domains configured to select GPU device(s) that are properly exposed to the server via CUDA_VISIBLE_DEVICES and Docker's device_ids More information for #1 and #3 can be found in the appropriate sections above. Be sure to move the plugin to oliveAppDataGPU/plugins/ when reconfiguring, and restarting any running servers. Be sure to double-check that the device(s) you are configuring each plugin domain to use are actually exposed to the server that will be using that plugin/domain. Configuring a plugin to use a GPU is done at the domain level, by changing the device variable assignment within the domain's meta.conf file. By default, most plugins have this variable assigned to cpu , and as such the domain will run on CPU only, even when running within a GPU-enabled OLIVE server. To assign a domain access to a GPU, change this device assignment from cpu to gpuN where N is the device ID of the GPU to run on, as reported by nvidia-smi (more info here ). As already discussed it is critical that the device assigned to the domain is exposed to the OLIVE server that will be running this domain. Most hardware setups will only have a single GPU available, so enabling this is simply replacing cpu with gpu0 . As an example, the meta.conf for the english-v1 domain of asr-end2end-v1.0.0 in its off-the-shelf format is shown below: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : cpu To instead configure this domain to run on the GPU with ID #0, as it is when delivered with OLIVE 6.0.0, this domain then becomes: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : gpu0 Each domain must be configured separately. If within a single plugin, one domain is configured for GPU, the others don't automatically start using a GPU. By default they will still run on CPU, which for most plugins will run significantly more slowly. By extension, it is not necessary for all domains of a plugin to run on the same GPU. Having this device assignment at the domain level allows the distribution of domains across multiple GPUs. For example, if multiple GPUs are available on a system, lower-memory-usage plugins like SAD, SID, and LID may share a single GPU, while heavier plugins like ASR or MT can be flexible enough to assign different language domains of each plugin across multiple GPUs, spreading the memory load to minimize the chance of exhausting GPU memory, while also saving lost time frequently loading and unloading models. Building off of the example above, if we wanted to run the russian-v1 domain of the same ASR plugin on GPU device #2, so that english-v1 and russian-v1 don't compete with respect to GPU memory, the russian-v1 domain would look like this: label : russian - v1 description : Large vocabulary Russian wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : Russian device : gpu2 Note that in these examples, GPU device 0 and GPU device 2 must both be listed in CUDA_VISIBLE_DEVICES and device_ids as outlined above . Example docker-compose.yml modification ``` yaml ... services: ... runtime-gpu: ... environment: <<: *common-variables CUDA_VISIBLE_DEVICES: '0,2' ... deploy: resources: reservations: devices: - driver: nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids: ['0', '2'] capabilities: [gpu] The exact ideal configuration may vary greatly depending on specific customer use case and mission needs.","title":"Enable GPU usage for a plugin's domain"},{"location":"gpu.html#currently-supported-gpu-plugins","text":"Please refer to the GPU Configuration documentation for a list of currently supported GPU plugins.","title":"Currently supported GPU plugins"},{"location":"gpu.html#olive-gpu-restrictions-and-configuration-notes","text":"","title":"OLIVE GPU Restrictions and Configuration Notes"},{"location":"gpu.html#gpu-compute-mode-default-vs-exclusive","text":"The OLIVE software currently assumes that any available GPUs are in the \"default\" mode. In testing, some configurations of number of OLIVE server workers have been found to cause unexpected issues if the GPUs are configured to be running in \"Exclusive\" mode. If possible, please configure GPUs that OLIVE will be using in \"Default\" mode, and if this is not possible, please ensure that the number of workers for the GPU-enabled server is specified to be 1. This is configured in the provided docker-compose.yml file. To check the mode of your GPU, you can view the Compute Mode field of the nvidia-smi command output. Please refer to Nvidia's instructions for Nvidia Control Panel or the Usage instructions for nvidia-smi for more information on how to set these modes.","title":"GPU Compute Mode: \"Default\" vs \"Exclusive\""},{"location":"gpu.html#workflow-restrictions","text":"Plugins can't currently pass jobs between separate OLIVE servers. This means that if you have a workflow that requires several plugins, each of those plugins must be visible to the OLIVE server that is receiving the workflow job request. If your current configuration separates any of these plugins, your plugins directories must be reconfigured so that all required plugins are colocated within the same server. There are two possible workarounds. The first would be disabling GPU capabilities in some plugins and moving them to the CPU oliveAppData/ directory, which may cause them to run much more slowly, but will allow full job parallelization. An alternative would be to move CPU-only plugins into the GPU-enabled server, leaving them configured to run on the CPU. This would allow the GPU-enabled plugins to run at full speed, but would limit parallel processing due to having a single worker thread. Future work in OLIVE hopes to address these first-release GPU limitations to reduce and eventually eliminate workarounds and restrictions like this.","title":"Workflow Restrictions"},{"location":"hardware.html","text":"OLIVE Hardware Requirements Processor Hardware Restrictions OLIVE is currently built, tested, and suported for running only on Intel x86_64 processor hardware. This includes consumer Core i-series processors like the i7, i9, etc., as well as the server-line Xeon processors. ARM processors are currently not supported; this includes the new M1 and M2 chips from Apple. Some plugins come with additional restrictions that have stricter requirements. The most notable at this point is for avx2 support from the CPU, this is required by our Neural Machine Translation plugin, as well as most of the low-resource targeted \"SmOlive\" plugins using quantized models. The \"SmOlive\" plugins typically have a \"smart\" domain that can back-off to a less-resource-streamlined set of models if the avx2 support is not discovered and loading the primary quantized model(s) has failed, but there is no back-off for Machine Translation. Typically, processors newer than roughly 2015 have this support, so it should only come into consideration in rare circumstances. Plugins currently requiring avx2 support: tmt-ctranslate-v1 sdd-diarizeEmbedSmolive-v1 (Will back off to non-quantized models on unsupported hardware) asr-end2end-v1 GPU Requirements Some plugins are now capable of performing some operations on compatible GPU hardware, if available, to take advantage of the GPU architecture to increase processing speed. In order to take advantage of this, at least one Nvidia GPU must be properly installed on the system running the OLIVE software. This GPU must support at least CUDA 12.4 or newer, and meet the following installed driver minimum requirements: Linux x86_64: 525.60.13 or newer Windows: 525.60.13 or newer Refer to Nvidia for drivers and more information about CUDA and GPU driver compatibility. GPU Configuration For information on GPU configuration , please refer to the appropriate documentation page. Speed and Memory Requirements Notes and Disclaimer for the Resource Requirement Estimates Provided A few performance-related things that might be important to note: These estimates provided below were recorded on a native linux installation of OLIVE 5.1.0 which may be slightly different than running the equivalent job on Windows or in a Docker based environment. Note that OLIVE 5.2.0 and 5.3.0 have memory improvements that are not yet reflected here; these statistics will be updated when new results are available. Any speed estimates we give are going to be hardware dependent. The numbers reported below should be pessimistic, as they are limited to running on a single core of a low-power computer. Stronger cores will be faster than what's reported, weaker CPU cores will run a bit slower. If you have more than 1 processor core(s) available, which is likely, OLIVE is able to parallelize and run more jobs simultaneously, so the speed should scale accordingly - but we're reporting single-core jobs just to keep everything on the same relative scale so that you can compare plugins to each other. Just as speed will increase/scale as the number of processor cores being used increases (i.e. number of simultaneous jobs), so will memory usage. The provided stats are for a process limited to one job at a time. Memory usage scales sometimes significantly depending on how large the input audio files are. For a plugin like SAD, that has a very small base memory footprint, which only barely increases even if processing many, many small files, can see a much larger memory utilization if you start running 1GB+ audio files through. ASR performance is largely domain-dependent - for example, the Russian domain currently has a much larger language model than other plugins because of how the language is structured, and priorities of the project that funded its development, so its memory usage is quite significant, \\~9+GB per processor core. Another thing to note for ASR speed performance is that because of how sizable the models can be, the overhead of the loading time of this model into memory can really come into play. It may take some time to get an initial response back from the server due to this overhead, but subsequent responses should be much faster as this 'heavy lifting' is already done. Note that if you're just running the CLI tools like localanalyze, this loading must be performed every time, so you won't realize this speedup unless you're running with the OLIVE server. If you are using the OLIVE server, it's possible to send a 'preload' request to load a plugin's models before any audio is submitted for scoring and avoid this initial delay. The models for each ASR/TPD domain/language are disjoint, and take up separate memory footprints. So if you would like to run data through both the Russian and English ASR domains when running the OLIVE server, the models, once loaded, are retained in memory for later processing and you may quickly run out of memory. For example, if you run a Russian job, a minimum of ~9GB of memory will be used. If you run an English job shortly after, this will load another ~6GB or so worth of models into memory. If you have insufficient memory, you will need to either explicity unload plugins/domains using API calls (not a feature currently offered by our GUI), or will need to restart the server to clear out the loaded models. QBE performance will depend on how many queries/keywords are currently enrolled - as more queries are enrolled and need to be considered during the search, the speed of the plugin will decrease. Some of the statistics below may be extra pessimistic because some of these readings will depend on how much of the input audio actually contains speech. If you have a 3 hour file, but only 5 minutes of it is speech, many of these plugins can be much, much faster and use less memory, because the task-specific processing (LID, for example) will only process audio that is identified as speech, and so will be operating on a much smaller piece of the audio than the whole file. The audio used to generate these numbers is pretty packed with speech, so should be close to a 'worst case.' Plugin Resource Requirement Estimates With that out of the way, here is a summary for most of the plugins: plugin / domain speed mem (1 min) mem (2 hr) sad-dnn-v7.0.1 / fast-multi-v1 214.1 105 MB 766 MB sad-dnn-v7.0.1 / multi-v1 90.6 127 MB 775 MB gid-gb-v2.0.0 / clean-v1 354.9 161 MB 1.56 GB ldd-sbcEmbed-v1.0.1 / multi-v1 18.5 582 MB 5.04 GB lid-embedplda-v2.0.1 / multi-v1 29.2 660 MB 3.15 GB qbe-tdnn-v5.0.0 / multi-v1* 28.3 198 MB 2.55 GB sdd-sbcEmbed-v2.0.2 / telClosetalk-v1 42.8 232 MB 1.12 GB sid-dplda-v2.0.1 / multi-v1 42.2 296 MB 2.38 GB asr-dynapy-v2.0.2 / rus-tdnnChain-tel-v1** 10.6 8.94 GB 10.19 GB tpd-dynapy-v3.0.0 / rus-cts-v1** 6.3 7.46 GB 8.66 GB * QBE Note: with 3 enrolled keywords ** ASR/TPD Note - the mem (2 hr) and speed statistics are generated from different data than the other plugins. That is because these plugins are language-dependent, and the data I used for the rest of the tests does not match the language of the domains I was running here. Feeding mismatched data into these plugins can cause both runtime and memory usage to balloon, as the plugin tries very hard to make sense of something that it's never seen before. Instead, 100 files adding to 2.5 hrs were used for the speed test and one 2-hr file was used for the mem (2 hr) test, but a different one than the rest of the plugins. Note also that the Russian models are by far the largest delivered - this will change depending on which language/domain you are using, but these should represent a 'worst case' for ASR/TPD for planning purposes. Speed numbers are reported in terms of \"times faster than real time\", and the numbers were reached by scoring 90 files adding up to approximately 5 hrs of data on a single core of a circa-2016 Gigabyte BRIX Compact PC (Intel i7-5500U 2.40 GHz processor). Higher is better, so for the slower SAD domain, which scores roughly 90 here, that means it can process a 90 second input file in 1 second. Two memory points are provided for each plugin - the memory used to score a single 1 minute file, which should show roughly the baseline usage of the plugin, as well as memory used to score a single 2 hour file, to give a sense of how the usage scales as files grow. Lower is better.","title":"OLIVE System Requirements"},{"location":"hardware.html#olive-hardware-requirements","text":"","title":"OLIVE Hardware Requirements"},{"location":"hardware.html#processor-hardware-restrictions","text":"OLIVE is currently built, tested, and suported for running only on Intel x86_64 processor hardware. This includes consumer Core i-series processors like the i7, i9, etc., as well as the server-line Xeon processors. ARM processors are currently not supported; this includes the new M1 and M2 chips from Apple. Some plugins come with additional restrictions that have stricter requirements. The most notable at this point is for avx2 support from the CPU, this is required by our Neural Machine Translation plugin, as well as most of the low-resource targeted \"SmOlive\" plugins using quantized models. The \"SmOlive\" plugins typically have a \"smart\" domain that can back-off to a less-resource-streamlined set of models if the avx2 support is not discovered and loading the primary quantized model(s) has failed, but there is no back-off for Machine Translation. Typically, processors newer than roughly 2015 have this support, so it should only come into consideration in rare circumstances. Plugins currently requiring avx2 support: tmt-ctranslate-v1 sdd-diarizeEmbedSmolive-v1 (Will back off to non-quantized models on unsupported hardware) asr-end2end-v1","title":"Processor Hardware Restrictions"},{"location":"hardware.html#gpu-requirements","text":"Some plugins are now capable of performing some operations on compatible GPU hardware, if available, to take advantage of the GPU architecture to increase processing speed. In order to take advantage of this, at least one Nvidia GPU must be properly installed on the system running the OLIVE software. This GPU must support at least CUDA 12.4 or newer, and meet the following installed driver minimum requirements: Linux x86_64: 525.60.13 or newer Windows: 525.60.13 or newer Refer to Nvidia for drivers and more information about CUDA and GPU driver compatibility. GPU Configuration For information on GPU configuration , please refer to the appropriate documentation page.","title":"GPU Requirements"},{"location":"hardware.html#speed-and-memory-requirements","text":"","title":"Speed and Memory Requirements"},{"location":"hardware.html#notes-and-disclaimer-for-the-resource-requirement-estimates-provided","text":"A few performance-related things that might be important to note: These estimates provided below were recorded on a native linux installation of OLIVE 5.1.0 which may be slightly different than running the equivalent job on Windows or in a Docker based environment. Note that OLIVE 5.2.0 and 5.3.0 have memory improvements that are not yet reflected here; these statistics will be updated when new results are available. Any speed estimates we give are going to be hardware dependent. The numbers reported below should be pessimistic, as they are limited to running on a single core of a low-power computer. Stronger cores will be faster than what's reported, weaker CPU cores will run a bit slower. If you have more than 1 processor core(s) available, which is likely, OLIVE is able to parallelize and run more jobs simultaneously, so the speed should scale accordingly - but we're reporting single-core jobs just to keep everything on the same relative scale so that you can compare plugins to each other. Just as speed will increase/scale as the number of processor cores being used increases (i.e. number of simultaneous jobs), so will memory usage. The provided stats are for a process limited to one job at a time. Memory usage scales sometimes significantly depending on how large the input audio files are. For a plugin like SAD, that has a very small base memory footprint, which only barely increases even if processing many, many small files, can see a much larger memory utilization if you start running 1GB+ audio files through. ASR performance is largely domain-dependent - for example, the Russian domain currently has a much larger language model than other plugins because of how the language is structured, and priorities of the project that funded its development, so its memory usage is quite significant, \\~9+GB per processor core. Another thing to note for ASR speed performance is that because of how sizable the models can be, the overhead of the loading time of this model into memory can really come into play. It may take some time to get an initial response back from the server due to this overhead, but subsequent responses should be much faster as this 'heavy lifting' is already done. Note that if you're just running the CLI tools like localanalyze, this loading must be performed every time, so you won't realize this speedup unless you're running with the OLIVE server. If you are using the OLIVE server, it's possible to send a 'preload' request to load a plugin's models before any audio is submitted for scoring and avoid this initial delay. The models for each ASR/TPD domain/language are disjoint, and take up separate memory footprints. So if you would like to run data through both the Russian and English ASR domains when running the OLIVE server, the models, once loaded, are retained in memory for later processing and you may quickly run out of memory. For example, if you run a Russian job, a minimum of ~9GB of memory will be used. If you run an English job shortly after, this will load another ~6GB or so worth of models into memory. If you have insufficient memory, you will need to either explicity unload plugins/domains using API calls (not a feature currently offered by our GUI), or will need to restart the server to clear out the loaded models. QBE performance will depend on how many queries/keywords are currently enrolled - as more queries are enrolled and need to be considered during the search, the speed of the plugin will decrease. Some of the statistics below may be extra pessimistic because some of these readings will depend on how much of the input audio actually contains speech. If you have a 3 hour file, but only 5 minutes of it is speech, many of these plugins can be much, much faster and use less memory, because the task-specific processing (LID, for example) will only process audio that is identified as speech, and so will be operating on a much smaller piece of the audio than the whole file. The audio used to generate these numbers is pretty packed with speech, so should be close to a 'worst case.'","title":"Notes and Disclaimer for the Resource Requirement Estimates Provided"},{"location":"hardware.html#plugin-resource-requirement-estimates","text":"With that out of the way, here is a summary for most of the plugins: plugin / domain speed mem (1 min) mem (2 hr) sad-dnn-v7.0.1 / fast-multi-v1 214.1 105 MB 766 MB sad-dnn-v7.0.1 / multi-v1 90.6 127 MB 775 MB gid-gb-v2.0.0 / clean-v1 354.9 161 MB 1.56 GB ldd-sbcEmbed-v1.0.1 / multi-v1 18.5 582 MB 5.04 GB lid-embedplda-v2.0.1 / multi-v1 29.2 660 MB 3.15 GB qbe-tdnn-v5.0.0 / multi-v1* 28.3 198 MB 2.55 GB sdd-sbcEmbed-v2.0.2 / telClosetalk-v1 42.8 232 MB 1.12 GB sid-dplda-v2.0.1 / multi-v1 42.2 296 MB 2.38 GB asr-dynapy-v2.0.2 / rus-tdnnChain-tel-v1** 10.6 8.94 GB 10.19 GB tpd-dynapy-v3.0.0 / rus-cts-v1** 6.3 7.46 GB 8.66 GB * QBE Note: with 3 enrolled keywords ** ASR/TPD Note - the mem (2 hr) and speed statistics are generated from different data than the other plugins. That is because these plugins are language-dependent, and the data I used for the rest of the tests does not match the language of the domains I was running here. Feeding mismatched data into these plugins can cause both runtime and memory usage to balloon, as the plugin tries very hard to make sense of something that it's never seen before. Instead, 100 files adding to 2.5 hrs were used for the speed test and one 2-hr file was used for the mem (2 hr) test, but a different one than the rest of the plugins. Note also that the Russian models are by far the largest delivered - this will change depending on which language/domain you are using, but these should represent a 'worst case' for ASR/TPD for planning purposes. Speed numbers are reported in terms of \"times faster than real time\", and the numbers were reached by scoring 90 files adding up to approximately 5 hrs of data on a single core of a circa-2016 Gigabyte BRIX Compact PC (Intel i7-5500U 2.40 GHz processor). Higher is better, so for the slower SAD domain, which scores roughly 90 here, that means it can process a 90 second input file in 1 second. Two memory points are provided for each plugin - the memory used to score a single 1 minute file, which should show roughly the baseline usage of the plugin, as well as memory used to score a single 2 hour file, to give a sense of how the usage scales as files grow. Lower is better.","title":"Plugin Resource Requirement Estimates"},{"location":"install.html","text":"OLIVE (Native Linux) Installation OLIVE Folder Structure Overview Typical OLIVE installations for native, linux-based operating systems follow the general formats shown below. This is for using OLIVE on the OS the OLIVE runtime has been created on/for. For OLIVE 6.0.0 this is currently Ubuntu 20.04. This may be different for different OLIVE versions. Delivery Type Caution If your delivery is what we call a \"Martini\" package, based on a docker container, please refer to the OLIVE Martini Docker Container Setup . If you instead have a standalone OLIVE docker container (this is unlikely as the delivery method is deprecated), please instead go to the OLIVE Docker Container Setup Guide . We recommend installing the OLIVE Runtime and OLIVE itself within a directory in an easy-to-find location. Typically olive5.x.y/ within $HOME is a reasonable starting place. Plugins we generally store outside of this location, in a directory we name oliveAppData to allow for retention of plugins and audio enrollments if/when a new version of OLIVE is installed. $HOME is again often a reasonable location for this folder. The configuration or relationship between the runtime and the OLIVE package itself generally matches one of the following two examples. In either case, you will need to properly set up the OLIVE environment before performing any operations with OLIVE, and SRI provides a script to make this as straightforward as possible. olive_env.sh setup script This script is included with every release and is generally the recommended and easiest way to properly set up the OLIVE environment, as shown above. It should be possible to use this script to cover environmental setup in even advanced OLIVE deployments. This script will properly set a number of necessary environment variables and paths essential for OLIVE operation all in one go. Depending on how you've chosen to lay out your OLIVE package on disk, you may need to source two of these scripts. As a quick primer, this is the README included at the top of the olive_env.sh script, that will be explained in more detail below: # Sets the Olive / Scenic environment when running from a distribution . # Assumes this will be sourced in the Olive directory , as : # # % source olive_env . sh # # If this script is sourced in the runtime distribution , it will only set # runtime-related values . # # If this script is sourced in the Olive distribution , it will set both # OLIVE ( SCENIC ) and runtime-related values if : # ( 1 ) the Olive directory is a sub-directory of the runtime directory # or ( 2 ) the Olive and runtime directories have been combined . # # If the Olive directory is a subdirectory of the runtime , then you # should only source olive_env . sh from the Olive directory . # # If the runtime and Olive directories are completely separate , then # do the following ( in this order , so that $ OLIVE paths come before # $ OLIVE_RUNTIME paths ): # # ( 1 ) cd / path / to / runtime ; source olive_env . sh # ( 2 ) cd / path / to / olive ; source olive_env . sh One important thing to note is that the olive_env.sh scripts do not cover setting the OLIVE_APP_DATA environment variable, which informs the server of the location of the OLIVE plugins, among other things. More details available here . This is a very important step to make sure the Server can find and utilize the appropriate plugins. Setting this variable is included in each of the examples below. OLIVE/OLIVE-Runtime Layouts Often, the OLIVE Runtime and OLIVE itself are placed side-by-side in the same parent directory, for easier readability. If this configuration is used, it is necessary to source two separate olive_env.sh files to properly establish the OLIVE environment, as outlined in the olive_env.sh README shown above. You should always start by sourcing the olive_env.sh file for the runtime. An example file structure of side-by-side OLIVE and OLIVE runtime directories: $HOME olive6.0.0/ runtime-6.0.0-CentOS_Linux-7-x86_64/ olive-6.0.0-CentOS_Linux-7-x86_64/ documentation/ oliveAppData/ plugins/ sad-dnn-v7.0.0 sid-dplda-v2.0.0/ lid-embedplda-v2.0.0/ Example environment setup with this configuration: $ cd $ HOME / olive6 . 0.0 / runtime - 6.0 . 0 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ cd $ HOME / olive6 . 0.0 / olive - 6.0 . 0 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / Alternately, the OLIVE package can be nested within the runtime directory, a structure shown in the example below. If this arrangement is used, it is only necessary to source the olive_env.sh script within the OLIVE package, as it is aware that its parent directory is the OLIVE runtime, and appropriately establishes the environment with this information. $HOME olive6.0.0/ runtime-6.0.0-CentOS_Linux-7-x86_64/ olive-6.0.0-CentOS_Linux-7-x86_64/ documentation/ oliveAppData/ plugins/ lid-embed-v2/ sad-dnn-v4a/ sid-embed-v2/ Example environment setup with this configuration: $ cd $ HOME / olive6 . 0.0 / runtime - 6.0 . 0 - CentOS_Linux - 7 - x86_64 / olive - 6.0 . 0 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / After performing these installation and setup steps, you should be ready to get started and run your own instance of the OLIVE Server For more information on what variables are being established by this environment setup script and what is needed by OLIVE to run, continue on to the Environment Variables section of the OLIVE Server Page . For more information on getting started with running the OLIVE server, continue on to the Running The Server section. Or skip to the OLIVE Runtime for more details about the purpose and contents of the OLIVE Runtime.","title":"CentOS/RedHat 7 Native Setup (deprecated)"},{"location":"install.html#olive-native-linux-installation","text":"","title":"OLIVE (Native Linux) Installation"},{"location":"install.html#olive-folder-structure-overview","text":"Typical OLIVE installations for native, linux-based operating systems follow the general formats shown below. This is for using OLIVE on the OS the OLIVE runtime has been created on/for. For OLIVE 6.0.0 this is currently Ubuntu 20.04. This may be different for different OLIVE versions. Delivery Type Caution If your delivery is what we call a \"Martini\" package, based on a docker container, please refer to the OLIVE Martini Docker Container Setup . If you instead have a standalone OLIVE docker container (this is unlikely as the delivery method is deprecated), please instead go to the OLIVE Docker Container Setup Guide . We recommend installing the OLIVE Runtime and OLIVE itself within a directory in an easy-to-find location. Typically olive5.x.y/ within $HOME is a reasonable starting place. Plugins we generally store outside of this location, in a directory we name oliveAppData to allow for retention of plugins and audio enrollments if/when a new version of OLIVE is installed. $HOME is again often a reasonable location for this folder. The configuration or relationship between the runtime and the OLIVE package itself generally matches one of the following two examples. In either case, you will need to properly set up the OLIVE environment before performing any operations with OLIVE, and SRI provides a script to make this as straightforward as possible.","title":"OLIVE Folder Structure Overview"},{"location":"install.html#olive_envsh-setup-script","text":"This script is included with every release and is generally the recommended and easiest way to properly set up the OLIVE environment, as shown above. It should be possible to use this script to cover environmental setup in even advanced OLIVE deployments. This script will properly set a number of necessary environment variables and paths essential for OLIVE operation all in one go. Depending on how you've chosen to lay out your OLIVE package on disk, you may need to source two of these scripts. As a quick primer, this is the README included at the top of the olive_env.sh script, that will be explained in more detail below: # Sets the Olive / Scenic environment when running from a distribution . # Assumes this will be sourced in the Olive directory , as : # # % source olive_env . sh # # If this script is sourced in the runtime distribution , it will only set # runtime-related values . # # If this script is sourced in the Olive distribution , it will set both # OLIVE ( SCENIC ) and runtime-related values if : # ( 1 ) the Olive directory is a sub-directory of the runtime directory # or ( 2 ) the Olive and runtime directories have been combined . # # If the Olive directory is a subdirectory of the runtime , then you # should only source olive_env . sh from the Olive directory . # # If the runtime and Olive directories are completely separate , then # do the following ( in this order , so that $ OLIVE paths come before # $ OLIVE_RUNTIME paths ): # # ( 1 ) cd / path / to / runtime ; source olive_env . sh # ( 2 ) cd / path / to / olive ; source olive_env . sh One important thing to note is that the olive_env.sh scripts do not cover setting the OLIVE_APP_DATA environment variable, which informs the server of the location of the OLIVE plugins, among other things. More details available here . This is a very important step to make sure the Server can find and utilize the appropriate plugins. Setting this variable is included in each of the examples below.","title":"olive_env.sh setup script"},{"location":"install.html#oliveolive-runtime-layouts","text":"Often, the OLIVE Runtime and OLIVE itself are placed side-by-side in the same parent directory, for easier readability. If this configuration is used, it is necessary to source two separate olive_env.sh files to properly establish the OLIVE environment, as outlined in the olive_env.sh README shown above. You should always start by sourcing the olive_env.sh file for the runtime. An example file structure of side-by-side OLIVE and OLIVE runtime directories: $HOME olive6.0.0/ runtime-6.0.0-CentOS_Linux-7-x86_64/ olive-6.0.0-CentOS_Linux-7-x86_64/ documentation/ oliveAppData/ plugins/ sad-dnn-v7.0.0 sid-dplda-v2.0.0/ lid-embedplda-v2.0.0/ Example environment setup with this configuration: $ cd $ HOME / olive6 . 0.0 / runtime - 6.0 . 0 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ cd $ HOME / olive6 . 0.0 / olive - 6.0 . 0 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / Alternately, the OLIVE package can be nested within the runtime directory, a structure shown in the example below. If this arrangement is used, it is only necessary to source the olive_env.sh script within the OLIVE package, as it is aware that its parent directory is the OLIVE runtime, and appropriately establishes the environment with this information. $HOME olive6.0.0/ runtime-6.0.0-CentOS_Linux-7-x86_64/ olive-6.0.0-CentOS_Linux-7-x86_64/ documentation/ oliveAppData/ plugins/ lid-embed-v2/ sad-dnn-v4a/ sid-embed-v2/ Example environment setup with this configuration: $ cd $ HOME / olive6 . 0.0 / runtime - 6.0 . 0 - CentOS_Linux - 7 - x86_64 / olive - 6.0 . 0 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / After performing these installation and setup steps, you should be ready to get started and run your own instance of the OLIVE Server For more information on what variables are being established by this environment setup script and what is needed by OLIVE to run, continue on to the Environment Variables section of the OLIVE Server Page . For more information on getting started with running the OLIVE server, continue on to the Running The Server section. Or skip to the OLIVE Runtime for more details about the purpose and contents of the OLIVE Runtime.","title":"OLIVE/OLIVE-Runtime Layouts"},{"location":"martini.html","text":"OLIVE Martini Docker Container Setup Introduction This page guides through the installation, setup, and launching of a docker-container based OLIVE software package featuring the new \"OLIVE Martini\" container setup. This new delivery method includes the usual OLIVE server and plugin capabilities delivered in the past, but includes a new addition - Batch GUI capabilities powered by the new Raven GUI that can be accessed through your web browser. For instructions on how to use this new GUI once you have the software properly installed, check out the Raven Web GUI page. The overall installation process is very similar to previous Docker container based OLIVE deliveries in the past, with the main changes affecting how the OLIVE container is managed for startup, shutdown, etc. This page assumes you already have Docker installed and configured, or that you are already familiar with it. Some sections are collapsed by default to shift the OLIVE-specific info up, but they can be easily expanded for more details if necessary. OLIVE Martini Quick Start This briefly covers the steps necessary to get up and running with an OLIVE Martini to serve as a quick refresher for returning users or those simply wishing to start as quickly as possible. Please refer to the respective linked sections for each step if more information is needed, or if any trouble is encountered. Make sure Docker is installed Unpack the delivery archive Using $ tar -xzf olive6.0.0-martini-2Jun2022.tar.gz Load the OLIVE Martini image $ cd olive6.0.0/martini/ docker load -i olive-martini-container-6.0.0.tar Start the container $ ./martini.sh start or $ .\\martini.ps1 start Use the container When finished, stop the container $ ./martini.sh stop or $ .\\martini.ps1 stop GPU Configuration Note If you wish for OLIVE to use GPUs for supported plugins, you must launch Martini with the --gpu / -gpu flag , and also be sure to configure compatible plugins to use the GPU as outlined in the GPU Plugin/Domain Configuration documentation before launching Martini.\" OLIVE Quick Troubleshooting There are some occasions where Martini does not start up as expected. Often, the problem is displayed by the Martini output in the terminal itself, and the cause can be tracked down directly. Occasionally, a problem is encountered that for some reason isn't, or can't be passed back to the Martini management script, so the cause isn't clear. Two of the most popular occasions of this we're seeing are below. Misconfigured GPU device for one or more plugins OLIVE is sometimes shipped assuming GPU hardware is available for certain plugins; especially plugins that are inefficient at processing on CPU hardware. If a plugin/domain is configured to run on a GPU device, and any one of the following is true: No GPU device is available on the hardware Martini was not launched with the --gpu flag and has no access to GPUs on the machine NVIDIA drivers or NVIDIA container toolkit are not properly installed, or are of an unsupported version Then the container will fail to start. To fix these, please verify that plugin domains are properly configured for the desired devices , be sure to start up Martini with the appropriate flags , and review the prerequisites for GPU processing with OLIVE and supported hardware and driver notes . OLIVE Server error isn't passed back If Martini fails to start and no useful message or information is provided, it's likely that an OLIVE server error was encountered that wasn't passed back to the management script. The most likely location to find useful troubleshooting information is in the oliveserver.log OLIVE server log. This can be found here: olive6.0.0/oliveAppData/server/oliveserver.log Even if the contents don't immediately point to a solution, it will provide very important information to SRI to help troubleshoot. Please provide this whenever possible. This same location ( olive6.0.0/oliveAppData/server/ ) will also contain the logs of failed jobs, with the format: <Job UUID>.failed An example: olive6.0.0/oliveAppData/server/f2b35dba-904e-46b3-abbe-eaf93785a4e6.failed These can also provide very important pointers and information for troubleshooting any problem if you need to reach out for support . Download, Install, and Launch Docker If Docker is already installed and running, continue - if not, please expand the section below for setup instructions. Docker Installation Instructions (Click to expand) Install and Start Docker Before you can get started installing and running OLIVE, you'll need to make sure you have fully installed and configured Docker. The proper installation steps vary depending on your host OS, so please refer to the appropriate official Docker installation instructions: Docker Desktop for Windows Docker Desktop for macOS Docker Engine for Ubuntu Docker for Ubuntu is especially important to follow the official steps closely, as there are additional important post-installation steps to perform to make sure docker runs smoothly on your system. Note that if installing into an Ubuntu instance running on WSL2, systemctl is not used on such systems. This means that some of the commands provided in the Docker for Ubuntu instructions above may not succeed as written; notably for starting and stopping the Docker service. Please use service for these commands instead: $ sudo service docker start In addition, if using Docker for Ubuntu, the Nvidia drivers must be installed separately. This doesn't seem to be necessary if using Docker Desktop. Instructions for this installation can be found here, from Nvidia . Before moving on, be sure that the docker service has been started. Adjust Docker settings (RAM, Cores) Configure Docker-allocated Resources (Windows and macOS only) (Click to expand) If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS Anatomy of an OLIVE Martini Package Once Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: $ tar -xzf olive6.0.0-martini-2Jun2022.tar.gz You should find similar content to below unless told otherwise: olive6.0.0/ api/ - Directory containing the Python and Java OLIVE Client utilities docs/ - OLIVE documentation martini/ olive-martini-container-6.0.0.tar - OLIVE Martini Docker container martini.sh - Multi-purpose utility and management script for the OLIVE Martini Docker container on linux and macOS martini.ps1 / martini.bat - Same as above, but for Windows PowerShell OliveGUI/ - (Optional) The OLIVE Nightingale Forensic GUI oliveAppData/ plugins/ - OLIVE Plugins directory; Actual plugins included will depend on the customer, mission, and delivery workflows/ - (Optional) Directory containing OLIVE workflows; actual included worfklows will depend on the customer, mission, and delivery certs - (Optional) Directory to contain certificates, keys, passphrases if needed. This is where you will place certificates if using Martini with TLS. See the TLS startup flags below for more details. Expand the section below for more details on each component, or scroll to continue setup. OLIVE Martini Component Details (Click to Expand) api/ This directory contains the Python and Java OLIVE Client utilities that interact with and task a running OLIVE/Martini server. The client documentation covers how to get started with these. docs/ Contains the documentation you're currently browsing. Other versions can be viewed online at the OLIVE Software Documentation Page , but the version delivered with OLIVE should be the most appropriate for your specific delivery. To view the documentation, open this page in a web browser: olive6.0.0/docs/index.html Or follow the links given by the martini.sh startup output for the live-hosted local documentation. martini/ Contains the core OLIVE server and utility components for running the OLIVE software. This includes the OLIVE Martini image itself, and the the management scripts to help start, monitor, configure, and stop the container. olive-martini-container-6.0.0.tar Docker image that includes OLIVE , the Raven Web GUI , the OLIVE Web Broker, and various other utilities, including an html server to host this documentation. martini.sh - martini.ps1 / martini.bat Multi-purpose utility and management script for the OLIVE Martini Docker container on linux and macOS (.sh) or Windows (.ps1 and .bat) respectively. OliveGUI/ The OLIVE Nightingale Forensic UI for close file analysis, live streaming tasks, and other OLIVE GUI tasks. oliveAppData/ This is the default location for the all important OLIVE plugins and workflows . Once you start using the system, it is also where the OLIVE server will start saving server logs, in a server/ directory created here. plugins/ Included plugins are installed here - for more information on OLIVE plugins, refer to the OLIVE Plugin Overview , the list of released plugins , or the individual documentation page of the plugin of interest from the navigation on the left. workflows/ If any workflows are included with your OLIVE package, they reside here. Workflows are powerful ways to combine different plugins and capabilities to perform multiple scoring requests at once, and/or perform complex routing operations like using Language ID results to choose whether or not to perform ASR, and which language to use if so. Load the OLIVE Martini Docker Image The first setup step is to load the OLIVE Martini Docker image. This is only necessary once. Open a command prompt (PowerShell in Windows or Terminal in linux/macOS), navigate to the directory containing the OLIVE Docker image, and follow the examples below. linux / macOS $ cd / home /< username >/ olive6 . 0.0 / martini $ docker load - i olive - martini - container - 6.0 . 0. tar Windows $ cd C : \\ Users \\ < username > \\ olive6 . 0.0 \\ martini $ docker load - i olive - martini - container - 6.0 . 0. tar This operation can take some time; quite a few minutes on some machines, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed. Operating Systems Note There are three provided martini management scripts: - martini.sh - martini.bat - martini.ps1 The functionality of each is intended to be identical, and these are provided as different options for different operating systems for convenience. The martini.sh script should be used for linux and for macOS. martini.ps1 should be used when managing the OLIVE Martini container from Windows PowerShell, and martini.bat , which calls martini.ps1 internally, should be used if martini.ps1 cannot execute for permission reasons in PowerShell, and when managing OLIVE Martini from Windows Command Prompt ( cmd ). The examples below attempt to show all three being used, but in the text, they are refered to as simply martini.sh for brevity. Their features and functionality are the same. Windows vs macOS/Linux flag format Note that the one exception is the flag format. The Windows management use only a single '-' for flags, while the linux/macOS script uses two '--'. For example the '--gpu' optional flag for ./martini.sh start --gpu to launch with GPU connections enabled becomes '-gpu' on Windows: .\\martini.ps1 start -gpu \" Controlling the Container Controlling and managing the OLIVE Multi container is made much easier with the provided martini.sh management script. Using this script is as simple as running the script, and providing a command: macOS and linux $ ./martini.sh <command \\> Windows $ . \\m artini.bat <command \\> The list of commands available to martini.sh are: cli - Starts a shell on the container for debugging or operating the OLIVE CLI tools. help - Display the martini.sh help statement (shown below) list - List the running container(s), if any. log - Display a snapshot of the OLIVE Server log in the terminal. Useful for experts for debugging. net - Show the ports on the host that the container is listening on. start - Start the container. stop - Stop the container. status - Show the status of the processes on the container. version - Display the version information for the installed Martini components. More details for each command, how to use it, and the designed functionality can be found below. help Prints out the martini.sh help statement, reminding the user of the available commands: macOS and linux $ . / martini . sh help Supported commands include : cli , help , list , log , net , start , stop , status , and version . martini . sh cli : Starts a shell on the container for debugging . martini . sh help : Prints out help information . martini . sh list : List the running container . martini . sh log : Print out log files from the container . martini . sh net : Shows the ports on the host that the container is listening on . ( requires `netstat` OS utility be installed for proper function ) martini . sh start : Start the container . Optional flags : -- gpu : Enable Martini access to GPUs , if available . Be sure to properly configure any plugin / domains to be used with GPU devices as outlined in the documentation . -- tls_server_only : The server will be configured with a certificate and will only respond to HTTPS requests ; clients aren 't required to send certificate but must use HTTPS protocol (one-way TLS) --tls_server_and_client: Both the server and clients will need to communicate with certificates over HTTPS (two-way TLS) --debug: activate ' debug ' mode for the OLIVE server. This will cause OLIVE to maintain all OLIVE server logs to aid in troubleshooting and debugging. martini.sh status: Shows status of the processes on the container. martini.sh stop: Stop the container. martini.sh version: Prints out version information. Windows $ . \\ martini . bat help Supported commands include : cli , help , list , log , net , start , stop , status , and version . martini . bat cli : Starts a shell on the container for debugging . martini . bat help : Prints out help information . martini . bat list : List the running container . martini . bat log : Print out log files from the container . martini . bat net : Shows the ports on the host that the container is listening on . ( requires `netstat` OS utility be installed for proper function ) martini . bat start : Start the container . Optional flags : - gpu : Enable Martini access to GPUs , if available . Be sure to properly configure any plugin / domains to be used with GPU devices as outlined in the documentation . - tls_server_only : The server will be configured with a certificate and will only respond to HTTPS requests ; clients aren 't required to send certificate but must use HTTPS protocol (one-way TLS) -tls_server_and_client: Both the server and clients will need to communicate with certificates over HTTPS (two-way TLS) -debug: activate ' debug ' mode for the OLIVE server. This will cause OLIVE to maintain all OLIVE server logs to aid in troubleshooting and debugging. martini.bat status: Shows status of the processes on the container. martini.bat stop: Stop the container. martini.bat version: Prints out version information. start Starts up a previously built and loaded container. Note that it can take several seconds for all the servers to start on the container. Usage: macOS and linux $ ./martini.sh start Windows $ . \\m artini.bat start Optional Flags Introduced with OLIVE 5.5.1, the start command now accepts four different optional startup flags. GPU Access This grants the OLIVE Martini access to GPUs, if available. Usage: macOS and linux $ ./martini.sh start --gpu Windows $ . \\m artini.bat start -gpu GPU Availability Note that if no GPUs are available on the machine, or the GPUs are not compatible with the OLIVE 6.0.0 software and this flag is used, startup of the Martini will fail!\" Plugin GPU Configuration This flag allows the Martini container to access GPUs that may be available on the system, but does not reconfigure any plugins to use these GPUs. Please be sure to properly configure any plugin/domains to be used with GPU devices as outlined in the GPU Plugin/Domain Configuration documentation. Without explicit configuration to use a GPU, whether by SRI before delivery or by the end user, no plugin will take advantage of available GPUs. TLS Encryption (Server-side Only) Configures the server to use one-way TLS, relying on a server-side for encrypted communication. The server will only respond to HTTPS requests. Clients aren't required to send a certificate, but must use HTTPS protocol. Usage: macOS and linux $ ./martini.sh start --tls_server_only Windows $ . \\m artini.bat start -tls_server_only Requires additional files for proper function, and for Martini to start at all if this option is enabled. All of these files must be placed in olive6.0.0/oliveAppData/certs/ . The files are: passwords.txt A file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key. The file can be empty if passphrases are not required. server.crt A certificate in the PEM format. If intermediate certificates should be specified in addition to a primary certificate, they should be specified in the same file in the following order: the primary certificate comes first, then the intermediate certificates. NOTE : You need to rename a copy of your certificate and put it in the /certs/ directory mentioned above. server.key The secret key in the PEM format for the server.crt . NOTE : You need to rename a copy of your certificate key and put it in the /certs/ directory. Please work with your IT department to obtain valid certificates. HTTP / HTTPS Note: Note that if using TLS, the access points for the Raven UI and locally hosted documentation must use HTTPS instead of HTTP. For example, to access the Raven UI without TLS, the user can navigate to: http://localhost:5580 But when using TLS, access must be through HTTPS: https://localhost:5580 The links printed by the Martini startup should reflect this setting. TLS Encryption (Server-side and Client-side) Similar to the --tls_server_only flag described above, but instead activates two-way TLS. Both the server and client are required to provide a certificate, and all communication occurs over HTTPS. Usage: macOS and linux $ ./martini.sh start --tls_server_and_client Windows $ . \\m artini.bat start -tls_server_and_client Carries the same file and certificate requirements as --tls_server_only , but with the addition of the client providing their own valid certificate. The additional file requirement for two way TLS: clientCA.crt A file with trusted CA certificates in the PEM format used to verify client certificates. NOTE : You need to rename a copy of your file and put it in the /certs/ directory mentioned above. As always, please work with your IT department to obtain valid certificates. HTTP / HTTPS Note: Note that if using TLS, the access points for the Raven UI and locally hosted documentation must use HTTPS instead of HTTP. For example, to access the Raven UI without TLS, the user can navigate to: http://localhost:5580 But when using TLS, access must be through HTTPS: https://localhost:5580 The links printed by the Martini startup should reflect this setting. OLIVEServer Pass-through Arguments The Martini management script supports passing arguments through to oliveserver at startup time, to support configuration options that advanced users might benefit from. By default, any flags that are unrecognized by the script as Martini options, and that occur after the options and flags for Martini, will be passed on to the oliveserver itself. Some of the more common flags to use with this include enabling server debug mode or configuring the number of OLIVE workers . Examples of each are shown below. For the full list of optional arguments, refer to the OLIVE Server documentation. Server Debug Mode Activates 'debug' mode for the OLIVE server. This causes OLIVE to mantain all OLIVE server logs to aid in troubleshooting and debugging. These logs are stored in olive6.0.0/oliveAppData/server/ by default. Without this flag, \"failed\" logs are retained, but logs for jobs deemed to be successful are deleted as tasks are completed to avoid clutter and confusion. Usage: macOS and linux $ ./martini.sh start --debug Windows $ . \\m artini.bat start -debug Server Number of Workers (Parallel Processing) Allows configuration of the number of workers that are available to the OLIVE server, controlling the number of jobs that the server can have in process at any given time. By default, this is automatically set to be equal to the number of CPU cores are detected on the hardware running the server, to avoid resource conflicts. Usage: macOS and linux $ ./martini.sh start --gpu --workers 4 Windows $ . \\m artini.bat start -gpu -workers 4 stop Stops a running container. macOS and linux $ ./martini.sh stop Windows $ . \\m artini.bat stop status martini.sh status Displays information on the processes running, network ports, workflows and plugins which are active on the container, as well as confirmation of Martini startup options (like GPU access or TLS/encryption) or custom oliveserver passthrough flags (like --debug mode). macOS and linux $ ./martini.sh status Configuration ------------- - GPU Support: Disabled - SSL/TLS Support: Enabled - Custom OLIVE Server arguments: ( none ) - Mounted plugins: - ldd-embedplda-v1.0.1 - lid-embedplda-v4.0.1 - sad-dnn-v8.0.1 - sad-dnn-v8.1.0 - sdd-diarizeEmbedSmolive-v1.0.3 - sid-dplda-v3.0.0 - tmt-ctranslate-v1.3.0 - Mounted workflows: - SAD-SDD-LDD.workflow.json - SAD.workflow.json - SID.workflow.json - english-to-spanish-TMT.workflow.json - russian-to-english-TMT.workflow.json - sid_enrollment.workflow.json Processes --------- - Message Broker ( web API ) : Running - Nginx ( reverse proxy ) : Running - OLIVE Server: Running - Raven UI ( web server ) : Running - Secure Websockets ( SSL/TLS ) : Running Network ------- Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0 .0.0.0:8888 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:8070 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:8080 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5592 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5589 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5588 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5005 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5004 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5006 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:80 0 .0.0.0:* LISTEN Windows $ . \\m artini.bat status Configuration ------------- - GPU Support: Disabled - SSL/TLS Support: Enabled - Custom OLIVE Server arguments: ( none ) - Mounted plugins: - ldd-embedplda-v1.0.1 - lid-embedplda-v4.0.1 - sad-dnn-v8.0.1 - sad-dnn-v8.1.0 - sdd-diarizeEmbedSmolive-v1.0.3 - sid-dplda-v3.0.0 - tmt-ctranslate-v1.3.0 - Mounted workflows: - SAD-SDD-LDD.workflow.json - SAD.workflow.json - SID.workflow.json - english-to-spanish-TMT.workflow.json - russian-to-english-TMT.workflow.json - sid_enrollment.workflow.json Processes --------- - Message Broker ( web API ) : Running - Nginx ( reverse proxy ) : Running - OLIVE Server: Running - Raven UI ( web server ) : Running - Secure Websockets ( SSL/TLS ) : Running Network ------- Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0 .0.0.0:8888 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:8070 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:8080 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5592 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5589 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5588 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5005 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5004 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5006 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:80 0 .0.0.0:* LISTEN cli Starts up a shell within the OLIVE Martini container. The container must already be running. This is typically used for internal testing and troubleshooting and is not meant to be used by end-users. You can use this shell to run Olive CLI commands, such as: macOS and linux $ ./martini.sh cli # Show the plugins installed on the container: $ ls /home/olive/olive/plugins/ # Show the audio files on the container, which you can use as examples: $ ls /olive-data/media/shared/ # Run an analysis $ localanalyze /home/olive/olive/plugins/lid-embedplda-v2.0.1/domains/multi-v1 /olive-data/media/shared/Komodo_dragon_en.wav.lst # See the results $ more output.txt Windows $ . \\m artini.bat cli # Show the plugins installed on the container: $ ls /home/olive/olive/plugins/ # Show the audio files on the container, which you can use as examples: $ ls /olive-data/media/shared/ # Run an analysis $ localanalyze /home/olive/olive/plugins/lid-embedplda-v2.0.1/domains/multi-v1 /olive-data/media/shared/Komodo_dragon_en.wav.lst # See the results $ more output.txt This can also be used for troubleshooting by advanced users. Advanced Martini Troubleshooting using martini.sh cli When running with the cli command, your shell starts in /opt/olive . This is where the runtime is installed, and where things are run within the container. In /opt/olive : Running martini.sh status will print out what is running, and what network connections are active. And looking at this script will tell you what should be running. Do not run martini-process.sh. That is run when the container starts. The /home/olive/olive directory is the other important directory. Here: The martini-process.log file is the output of the script which starts all servers. The env.log file (if it exists) are the shell variables set. The other log files here were started by each server. The plugins/ directory contains all plugins. The server/ directory contains server logs. net Lists the project network ports that are active on the host machine, the machine running the container. macOS and linux $ ./martini.sh net Windows $ . \\m artini.bat net list Lists the project containers that are running. macOS and linux $ ./martini.sh list Windows $ . \\m artini.bat list log martini.sh log Displays the OLIVE Server log; useful for checking on the status of the OLIVE server in case of a malfunction, or troubleshooting unexpected behavior. macOS and linux $ ./martini.sh log Windows $ . \\m artini.bat log This can be saved out to a local file by providing a filename after the log command: macOS and linux $ ./martini.sh log <logfile> Windows $ . \\m artini.bat log <logfile> And the shell can be attached to this log file and updated in real time with the -f flag after a local logfile name: macOS and linux $ ./martini.sh log <logfile> -f Windows $ . \\m artini.bat log <logfile> -f version Diplays the version information for the individual components of OLIVE: macOS and linux $ ./martini.sh version Martini v1.1. Olive v5.3.0. Broker v1.5. Raven v0.2.0. Windows $ . \\m artini.bat version Martini v1.1. Olive v5.3.0. Broker v1.5. Raven v0.2.0. Using The Container Once you have started up the container using the martini.sh start command described above, you are ready to begin using it. Your container has four different applications within it, which you can use - the Raven Web UI , the OLIVE Server itself , the OLIVE Message Broker, and a web server hosting the OLIVE Documentation. In addition, most OLIVE deliveries typically ship with the interactive Nightingale Forensic GUI for performing close file analysis using OLIVE plugins and workflows, as well as both Java and Python OLIVE Client CLI example applications. Note that if you are attempting to connect from the same machine, you can access this feature at localhost:5580 . If you are accessing from a different machine, you must use the full hostname, or IP address. For convenience, the martini.sh script outputs the hostname and corresponding port for several activities: $ martini.sh start \\Started the container. From this machine: Run Nightingale (Olive GUI) using server localhost and port 5588. Use a web browser to localhost:5570 to see the documentation. Use a web browser to localhost:5580 to use the Raven Web UI. If using the REST API point to the server running on localhost:5004. From any other machine: Run Nightingale (Olive GUI) using server stauf-MBP16 and port 5588. Use a web browser to stauf-MBP16:5570 to see the documentation. Use a web browser to stauf-MBP16:5580 to use the Raven Web UI. If using the REST API point to the server running on stauf-MBP16:5004. Installed plugins (mounted from /Users/astauf/olive/olive5.3.0/oliveAppData/plugins) are: asr-dynapy-v3.0.0 map-routerGlobal-v1.0.0 dfa-cplda-v1.0.0 map-routerGlobal-v1.0.1 dfa-speakerSpecific-v1.0.0 map-routerRegion-v1.0.0 dfa-spoofnet-v1.0.0 pim-validateGlobal-v1.0.0 enh-mmse-v2.0.2 qbe-ftdnnSmolive-v1.0.0 env-audioQuality-v2.0.0 qua-analysis-v1.0.0 fdi-pyEmbed-v1.0.0 qua-filter-v1.0.0 fdv-pyEmbed-v1.0.0 red-transform-v1.0.0 fri-pyEmbed-v1.0.0 sad-dnn-v7.0.2 frv-pyEmbed-v1.0.0 sad-dnnSmolive-v1.0.0 gdd-embedplda-v1.0.0 sdd-diarizeembed-v1.0.0 gid-gb-v2.0.1 sdd-sbcEmbed-v2.0.3 ldd-embedplda-v1.0.0 shl-sbcEmbed-v1.0.2 ldd-embedplda-v1.0.1 sid-dplda-v2.0.2 lid-embedplda-v3.0.0 sid-embed-v6.0.2 lid-embedplda-v3.0.1 tpd-dynapy-v5.0.0 lid-embedpldaSmolive-v1.0.0 vtd-dnn-v7.0.2 Installed workflows (mounted from /Users/astauf/olive/olive5.3.0/oliveAppData/workflows) are: /opt/olive-broker/data/workflows/FaceDetection_Image_FDI.workflow.json /opt/olive-broker/data/workflows/FaceDetection_Video_FDV.workflow.json /opt/olive-broker/data/workflows/FaceRecognition_Image_FRI.workflow.json /opt/olive-broker/data/workflows/FaceRecognition_Video_FRV.workflow.json /opt/olive-broker/data/workflows/SID_quality-controlled.workflow.json /opt/olive-broker/data/workflows/SmartTranscriptionFull.workflow.json /opt/olive-broker/data/workflows/TPD-eng.workflow.json /opt/olive-broker/data/workflows/conditional_asr_v2.workflow.json /opt/olive-broker/data/workflows/quality-region-analysis-for-SID.workflow.json Please choose the appropriate hostname and port number for your desired activity and host situation. Raven Web UI To connect to the Raven Web/Batch GUI , open a web browser and navigate to URL host :5580. Once there, follow the on-screen prompts to drag-and-drop one or more audio files to the \"local media\" dropbox section, select one of the available workflows, and click on the action button in the lower right hand corner. More details for the Raven GUI can be found on its dedicated documentation page: Raven Web UI . The OLIVE Documentation When the container is started, it launches a web server serving this documentation. To view the hosted version, navigate a web browser to URL host :5570. The OLIVE Server This is used internally by the Olive Web UI and the Olive Message Broker. You can also write Python programs which make calls directly to the server. See seperate documentation on olivepy API for details. The OLIVE Message Broker This is used internally by the Olive Web UI. Nightingale Forensic UI This GUI provides access to the full suite of available OLIVE plugins and workflows, for performing close file analysis. It offers powerful audio visualization tools, live streaming capabilities, and a number of other useful features. For more information on what Nightingale offers and how to get started using it, please refer to the Nightingale GUI dedicated documentation page. As a quick primer, the Nightingale UI is typically provided in: <...>/olive6.0.0/OliveGUI Nightingale requires OpenJDK Java 11 or newer. Once this is installed, you can run it by navigating to: <...>/olive6.0.0/OliveGUI/bin/ And either double-clicking or running the Nightingale (macOS, linux) or Nightingale.bat (Windows) script appropriate for your OS. macOS / linux $ ./Nightingale Windows $ . \\N ightingale.bat info \"Note that the Nightingale launcher scripts are designed to be launched from this bin/ directory in order to properly link to other folders that it uses within this package, like the included workflows directory.\" OLIVE Example API Client Implementations OLIVE is also generally shipped with example API Client implementations in both Java and python. They are usually included in: <...>/olive6.0.0/api/[java, python] These can provide both a command-line-based method of interacting with the OLIVE server, and as integration examples providing templates or code bases for some integration teams. Advanced and Optional Usage and Features Click to expand for a deeper dive on some Martini capabilities Configuring Ports (Optional) By default the container exposes seven ports on the host machine running the container: 5588 5589 5004 5005 5570 5580 5888 If you want to change them, there are two ways to do this: You can directly edit the martini.sh or martini.ps1 file to change these seven variables near the top of the file: OLIVE_HOST_PORT_OLIVESERVER OLIVE_HOST_PORT_OLIVESECOND OLIVE_HOST_PORT_TEST OLIVE_HOST_PORT_BROKER OLIVE_HOST_PORT_DOCSERVER OLIVE_HOST_PORT_WEBSERVER OLIVE_HOST_PORT_REVERSE_PROXY Note that if you are using Windows Command Prompt (cmd) or Windows PowerShell via martini.bat , edit the martini.ps1 file, as it is called by martini.bat during operation. Doing this will use the new ports every time the container starts. Alternatively, you can set shell variables, with the same names as above, to the port numbers you want to use. This method will temporarily override the ports used to what you have set, but subsequent container starts will revert to the original, if the shell/environment variables are no longer set. Configuring Plugins, Workflows, and Documentation (Optional) If you are using the default installation, then no configuration is required. Your workflows must be in a directory called oliveAppData/workflows/ , your documentation in a directory called docs/ , and your plugins in a directory called oliveAppData/plugins/ , and the martini.sh script in an oliveMartini directory that is adjacent to docs/ and oliveAppData/. By default, OLIVE Martini will store logs, enrollments and other overhead-related files to oliveAppData/server/ If your data is not located in these default locations, then you have two ways to configure the OLIVE Martini container (similar to configuring ports described above): You can directly edit the martini.sh file to change these three variables near the top of the file: OLIVE_HOST_PLUGINS OLIVE_HOST_SERVER OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE Doing this will use the new locations every time the container starts. You can set shell variables to the full path names of the locations of your plugins, workflows, and documentation. Set these shell variables: OLIVE_HOST_PLUGINS OLIVE_HOST_SERVER OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE But remember, neither of these is required, if your plugins, workflows, and documentation are all in the default locations. Alternatively, changing MY_OLIVE_APP_DATA in the martini.sh script or setting the OLIVE_APP_DATA environment variable (by default set to oliveAppData/ ) will change most of these settings at once (excluding documentation) if it is desired to keep these resources together but in a different location. You can also (optionally) edit martini.sh to change the DELAY count. It can take a few to 10 seconds or so for all the servers to start on the container, depending on the hardware performing the operation. Normally the martini.sh returns immediately, even if the servers have not had time to start. If you don't like that behavior, you can set the DELAY variable near the top of the file to the number of seconds it takes to start all the servers, usually 7 or 10 seconds. This would mean that the command will not return until the servers are actually up and running. Testing The Installation There are a few ways to quickly sanity check that the installation is properly configured, up and running. Use a web browser to see what workflows are available. Go to URL host :5004/api/workflows. You should see json text describing the available workflows. Use a web browser to test the Olive Web UI. Go to URL host :5580. You should see a page with \"SRI International\" in the upper left corner, and a section for \"Media\" and a section for \"Workflows\". (See below for getting started with the Olive Web UI.) Use a web browser to look at the documentation. Go to to URL host :5570. You should see a web page titled \"Open Language Interface for Voice Exploitation (OLIVE)\". Final Notes / Troubleshooting There are a couple of known minor issues that may occur the first time you are starting the OLIVE Martini container on a new device. Since Docker will prompt asking for permission to share a few locations on the machine, in order to access the included plugins, workflows, and documentation, the container cannot initially read these and may report no plugins or workflows found at first. Once access is granted for Docker to share these locations, you should be able to check the plugins that are loaded by checking martini log , or in the extreme case, by stopping and restarting the container: macOS / linux $ ./martini.sh stop $ ./martini.sh start Windows $ . \\m artini.bat stop $ . \\m artini.bat start It can also take a moment for the Raven UI to populate the available Workflows on the bottom of the interface screen, especially the first time you are loading the web page. If this space is blank, please wait a moment (may take up to a minute on some systems) and/or refresh the page.","title":"Martini (Docker) Setup"},{"location":"martini.html#olive-martini-docker-container-setup","text":"","title":"OLIVE Martini Docker Container Setup"},{"location":"martini.html#introduction","text":"This page guides through the installation, setup, and launching of a docker-container based OLIVE software package featuring the new \"OLIVE Martini\" container setup. This new delivery method includes the usual OLIVE server and plugin capabilities delivered in the past, but includes a new addition - Batch GUI capabilities powered by the new Raven GUI that can be accessed through your web browser. For instructions on how to use this new GUI once you have the software properly installed, check out the Raven Web GUI page. The overall installation process is very similar to previous Docker container based OLIVE deliveries in the past, with the main changes affecting how the OLIVE container is managed for startup, shutdown, etc. This page assumes you already have Docker installed and configured, or that you are already familiar with it. Some sections are collapsed by default to shift the OLIVE-specific info up, but they can be easily expanded for more details if necessary.","title":"Introduction"},{"location":"martini.html#olive-martini-quick-start","text":"This briefly covers the steps necessary to get up and running with an OLIVE Martini to serve as a quick refresher for returning users or those simply wishing to start as quickly as possible. Please refer to the respective linked sections for each step if more information is needed, or if any trouble is encountered. Make sure Docker is installed Unpack the delivery archive Using $ tar -xzf olive6.0.0-martini-2Jun2022.tar.gz Load the OLIVE Martini image $ cd olive6.0.0/martini/ docker load -i olive-martini-container-6.0.0.tar Start the container $ ./martini.sh start or $ .\\martini.ps1 start Use the container When finished, stop the container $ ./martini.sh stop or $ .\\martini.ps1 stop GPU Configuration Note If you wish for OLIVE to use GPUs for supported plugins, you must launch Martini with the --gpu / -gpu flag , and also be sure to configure compatible plugins to use the GPU as outlined in the GPU Plugin/Domain Configuration documentation before launching Martini.\"","title":"OLIVE Martini Quick Start"},{"location":"martini.html#olive-quick-troubleshooting","text":"There are some occasions where Martini does not start up as expected. Often, the problem is displayed by the Martini output in the terminal itself, and the cause can be tracked down directly. Occasionally, a problem is encountered that for some reason isn't, or can't be passed back to the Martini management script, so the cause isn't clear. Two of the most popular occasions of this we're seeing are below.","title":"OLIVE Quick Troubleshooting"},{"location":"martini.html#misconfigured-gpu-device-for-one-or-more-plugins","text":"OLIVE is sometimes shipped assuming GPU hardware is available for certain plugins; especially plugins that are inefficient at processing on CPU hardware. If a plugin/domain is configured to run on a GPU device, and any one of the following is true: No GPU device is available on the hardware Martini was not launched with the --gpu flag and has no access to GPUs on the machine NVIDIA drivers or NVIDIA container toolkit are not properly installed, or are of an unsupported version Then the container will fail to start. To fix these, please verify that plugin domains are properly configured for the desired devices , be sure to start up Martini with the appropriate flags , and review the prerequisites for GPU processing with OLIVE and supported hardware and driver notes .","title":"Misconfigured GPU device for one or more plugins"},{"location":"martini.html#olive-server-error-isnt-passed-back","text":"If Martini fails to start and no useful message or information is provided, it's likely that an OLIVE server error was encountered that wasn't passed back to the management script. The most likely location to find useful troubleshooting information is in the oliveserver.log OLIVE server log. This can be found here: olive6.0.0/oliveAppData/server/oliveserver.log Even if the contents don't immediately point to a solution, it will provide very important information to SRI to help troubleshoot. Please provide this whenever possible. This same location ( olive6.0.0/oliveAppData/server/ ) will also contain the logs of failed jobs, with the format: <Job UUID>.failed An example: olive6.0.0/oliveAppData/server/f2b35dba-904e-46b3-abbe-eaf93785a4e6.failed These can also provide very important pointers and information for troubleshooting any problem if you need to reach out for support .","title":"OLIVE Server error isn't passed back"},{"location":"martini.html#download-install-and-launch-docker","text":"If Docker is already installed and running, continue - if not, please expand the section below for setup instructions. Docker Installation Instructions (Click to expand)","title":"Download, Install, and Launch Docker"},{"location":"martini.html#install-and-start-docker","text":"Before you can get started installing and running OLIVE, you'll need to make sure you have fully installed and configured Docker. The proper installation steps vary depending on your host OS, so please refer to the appropriate official Docker installation instructions: Docker Desktop for Windows Docker Desktop for macOS Docker Engine for Ubuntu Docker for Ubuntu is especially important to follow the official steps closely, as there are additional important post-installation steps to perform to make sure docker runs smoothly on your system. Note that if installing into an Ubuntu instance running on WSL2, systemctl is not used on such systems. This means that some of the commands provided in the Docker for Ubuntu instructions above may not succeed as written; notably for starting and stopping the Docker service. Please use service for these commands instead: $ sudo service docker start In addition, if using Docker for Ubuntu, the Nvidia drivers must be installed separately. This doesn't seem to be necessary if using Docker Desktop. Instructions for this installation can be found here, from Nvidia . Before moving on, be sure that the docker service has been started.","title":"Install and Start Docker"},{"location":"martini.html#adjust-docker-settings-ram-cores","text":"Configure Docker-allocated Resources (Windows and macOS only) (Click to expand) If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS","title":"Adjust Docker settings (RAM, Cores)"},{"location":"martini.html#anatomy-of-an-olive-martini-package","text":"Once Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: $ tar -xzf olive6.0.0-martini-2Jun2022.tar.gz You should find similar content to below unless told otherwise: olive6.0.0/ api/ - Directory containing the Python and Java OLIVE Client utilities docs/ - OLIVE documentation martini/ olive-martini-container-6.0.0.tar - OLIVE Martini Docker container martini.sh - Multi-purpose utility and management script for the OLIVE Martini Docker container on linux and macOS martini.ps1 / martini.bat - Same as above, but for Windows PowerShell OliveGUI/ - (Optional) The OLIVE Nightingale Forensic GUI oliveAppData/ plugins/ - OLIVE Plugins directory; Actual plugins included will depend on the customer, mission, and delivery workflows/ - (Optional) Directory containing OLIVE workflows; actual included worfklows will depend on the customer, mission, and delivery certs - (Optional) Directory to contain certificates, keys, passphrases if needed. This is where you will place certificates if using Martini with TLS. See the TLS startup flags below for more details. Expand the section below for more details on each component, or scroll to continue setup. OLIVE Martini Component Details (Click to Expand)","title":"Anatomy of an OLIVE Martini Package"},{"location":"martini.html#api","text":"This directory contains the Python and Java OLIVE Client utilities that interact with and task a running OLIVE/Martini server. The client documentation covers how to get started with these.","title":"api/"},{"location":"martini.html#docs","text":"Contains the documentation you're currently browsing. Other versions can be viewed online at the OLIVE Software Documentation Page , but the version delivered with OLIVE should be the most appropriate for your specific delivery. To view the documentation, open this page in a web browser: olive6.0.0/docs/index.html Or follow the links given by the martini.sh startup output for the live-hosted local documentation.","title":"docs/"},{"location":"martini.html#martini","text":"Contains the core OLIVE server and utility components for running the OLIVE software. This includes the OLIVE Martini image itself, and the the management scripts to help start, monitor, configure, and stop the container.","title":"martini/"},{"location":"martini.html#olive-martini-container-600tar","text":"Docker image that includes OLIVE , the Raven Web GUI , the OLIVE Web Broker, and various other utilities, including an html server to host this documentation.","title":"olive-martini-container-6.0.0.tar"},{"location":"martini.html#martinish-martinips1-martinibat","text":"Multi-purpose utility and management script for the OLIVE Martini Docker container on linux and macOS (.sh) or Windows (.ps1 and .bat) respectively.","title":"martini.sh - martini.ps1 / martini.bat"},{"location":"martini.html#olivegui","text":"The OLIVE Nightingale Forensic UI for close file analysis, live streaming tasks, and other OLIVE GUI tasks.","title":"OliveGUI/"},{"location":"martini.html#oliveappdata","text":"This is the default location for the all important OLIVE plugins and workflows . Once you start using the system, it is also where the OLIVE server will start saving server logs, in a server/ directory created here.","title":"oliveAppData/"},{"location":"martini.html#plugins","text":"Included plugins are installed here - for more information on OLIVE plugins, refer to the OLIVE Plugin Overview , the list of released plugins , or the individual documentation page of the plugin of interest from the navigation on the left.","title":"plugins/"},{"location":"martini.html#workflows","text":"If any workflows are included with your OLIVE package, they reside here. Workflows are powerful ways to combine different plugins and capabilities to perform multiple scoring requests at once, and/or perform complex routing operations like using Language ID results to choose whether or not to perform ASR, and which language to use if so.","title":"workflows/"},{"location":"martini.html#load-the-olive-martini-docker-image","text":"The first setup step is to load the OLIVE Martini Docker image. This is only necessary once. Open a command prompt (PowerShell in Windows or Terminal in linux/macOS), navigate to the directory containing the OLIVE Docker image, and follow the examples below. linux / macOS $ cd / home /< username >/ olive6 . 0.0 / martini $ docker load - i olive - martini - container - 6.0 . 0. tar Windows $ cd C : \\ Users \\ < username > \\ olive6 . 0.0 \\ martini $ docker load - i olive - martini - container - 6.0 . 0. tar This operation can take some time; quite a few minutes on some machines, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed.","title":"Load the OLIVE Martini Docker Image"},{"location":"martini.html#operating-systems-note","text":"There are three provided martini management scripts: - martini.sh - martini.bat - martini.ps1 The functionality of each is intended to be identical, and these are provided as different options for different operating systems for convenience. The martini.sh script should be used for linux and for macOS. martini.ps1 should be used when managing the OLIVE Martini container from Windows PowerShell, and martini.bat , which calls martini.ps1 internally, should be used if martini.ps1 cannot execute for permission reasons in PowerShell, and when managing OLIVE Martini from Windows Command Prompt ( cmd ). The examples below attempt to show all three being used, but in the text, they are refered to as simply martini.sh for brevity. Their features and functionality are the same. Windows vs macOS/Linux flag format Note that the one exception is the flag format. The Windows management use only a single '-' for flags, while the linux/macOS script uses two '--'. For example the '--gpu' optional flag for ./martini.sh start --gpu to launch with GPU connections enabled becomes '-gpu' on Windows: .\\martini.ps1 start -gpu \"","title":"Operating Systems Note"},{"location":"martini.html#controlling-the-container","text":"Controlling and managing the OLIVE Multi container is made much easier with the provided martini.sh management script. Using this script is as simple as running the script, and providing a command: macOS and linux $ ./martini.sh <command \\> Windows $ . \\m artini.bat <command \\> The list of commands available to martini.sh are: cli - Starts a shell on the container for debugging or operating the OLIVE CLI tools. help - Display the martini.sh help statement (shown below) list - List the running container(s), if any. log - Display a snapshot of the OLIVE Server log in the terminal. Useful for experts for debugging. net - Show the ports on the host that the container is listening on. start - Start the container. stop - Stop the container. status - Show the status of the processes on the container. version - Display the version information for the installed Martini components. More details for each command, how to use it, and the designed functionality can be found below.","title":"Controlling the Container"},{"location":"martini.html#help","text":"Prints out the martini.sh help statement, reminding the user of the available commands: macOS and linux $ . / martini . sh help Supported commands include : cli , help , list , log , net , start , stop , status , and version . martini . sh cli : Starts a shell on the container for debugging . martini . sh help : Prints out help information . martini . sh list : List the running container . martini . sh log : Print out log files from the container . martini . sh net : Shows the ports on the host that the container is listening on . ( requires `netstat` OS utility be installed for proper function ) martini . sh start : Start the container . Optional flags : -- gpu : Enable Martini access to GPUs , if available . Be sure to properly configure any plugin / domains to be used with GPU devices as outlined in the documentation . -- tls_server_only : The server will be configured with a certificate and will only respond to HTTPS requests ; clients aren 't required to send certificate but must use HTTPS protocol (one-way TLS) --tls_server_and_client: Both the server and clients will need to communicate with certificates over HTTPS (two-way TLS) --debug: activate ' debug ' mode for the OLIVE server. This will cause OLIVE to maintain all OLIVE server logs to aid in troubleshooting and debugging. martini.sh status: Shows status of the processes on the container. martini.sh stop: Stop the container. martini.sh version: Prints out version information. Windows $ . \\ martini . bat help Supported commands include : cli , help , list , log , net , start , stop , status , and version . martini . bat cli : Starts a shell on the container for debugging . martini . bat help : Prints out help information . martini . bat list : List the running container . martini . bat log : Print out log files from the container . martini . bat net : Shows the ports on the host that the container is listening on . ( requires `netstat` OS utility be installed for proper function ) martini . bat start : Start the container . Optional flags : - gpu : Enable Martini access to GPUs , if available . Be sure to properly configure any plugin / domains to be used with GPU devices as outlined in the documentation . - tls_server_only : The server will be configured with a certificate and will only respond to HTTPS requests ; clients aren 't required to send certificate but must use HTTPS protocol (one-way TLS) -tls_server_and_client: Both the server and clients will need to communicate with certificates over HTTPS (two-way TLS) -debug: activate ' debug ' mode for the OLIVE server. This will cause OLIVE to maintain all OLIVE server logs to aid in troubleshooting and debugging. martini.bat status: Shows status of the processes on the container. martini.bat stop: Stop the container. martini.bat version: Prints out version information.","title":"help"},{"location":"martini.html#start","text":"Starts up a previously built and loaded container. Note that it can take several seconds for all the servers to start on the container. Usage: macOS and linux $ ./martini.sh start Windows $ . \\m artini.bat start","title":"start"},{"location":"martini.html#optional-flags","text":"Introduced with OLIVE 5.5.1, the start command now accepts four different optional startup flags.","title":"Optional Flags"},{"location":"martini.html#gpu-access","text":"This grants the OLIVE Martini access to GPUs, if available. Usage: macOS and linux $ ./martini.sh start --gpu Windows $ . \\m artini.bat start -gpu GPU Availability Note that if no GPUs are available on the machine, or the GPUs are not compatible with the OLIVE 6.0.0 software and this flag is used, startup of the Martini will fail!\" Plugin GPU Configuration This flag allows the Martini container to access GPUs that may be available on the system, but does not reconfigure any plugins to use these GPUs. Please be sure to properly configure any plugin/domains to be used with GPU devices as outlined in the GPU Plugin/Domain Configuration documentation. Without explicit configuration to use a GPU, whether by SRI before delivery or by the end user, no plugin will take advantage of available GPUs.","title":"GPU Access"},{"location":"martini.html#tls-encryption-server-side-only","text":"Configures the server to use one-way TLS, relying on a server-side for encrypted communication. The server will only respond to HTTPS requests. Clients aren't required to send a certificate, but must use HTTPS protocol. Usage: macOS and linux $ ./martini.sh start --tls_server_only Windows $ . \\m artini.bat start -tls_server_only Requires additional files for proper function, and for Martini to start at all if this option is enabled. All of these files must be placed in olive6.0.0/oliveAppData/certs/ . The files are: passwords.txt A file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key. The file can be empty if passphrases are not required. server.crt A certificate in the PEM format. If intermediate certificates should be specified in addition to a primary certificate, they should be specified in the same file in the following order: the primary certificate comes first, then the intermediate certificates. NOTE : You need to rename a copy of your certificate and put it in the /certs/ directory mentioned above. server.key The secret key in the PEM format for the server.crt . NOTE : You need to rename a copy of your certificate key and put it in the /certs/ directory. Please work with your IT department to obtain valid certificates. HTTP / HTTPS Note: Note that if using TLS, the access points for the Raven UI and locally hosted documentation must use HTTPS instead of HTTP. For example, to access the Raven UI without TLS, the user can navigate to: http://localhost:5580 But when using TLS, access must be through HTTPS: https://localhost:5580 The links printed by the Martini startup should reflect this setting.","title":"TLS Encryption (Server-side Only)"},{"location":"martini.html#tls-encryption-server-side-and-client-side","text":"Similar to the --tls_server_only flag described above, but instead activates two-way TLS. Both the server and client are required to provide a certificate, and all communication occurs over HTTPS. Usage: macOS and linux $ ./martini.sh start --tls_server_and_client Windows $ . \\m artini.bat start -tls_server_and_client Carries the same file and certificate requirements as --tls_server_only , but with the addition of the client providing their own valid certificate. The additional file requirement for two way TLS: clientCA.crt A file with trusted CA certificates in the PEM format used to verify client certificates. NOTE : You need to rename a copy of your file and put it in the /certs/ directory mentioned above. As always, please work with your IT department to obtain valid certificates. HTTP / HTTPS Note: Note that if using TLS, the access points for the Raven UI and locally hosted documentation must use HTTPS instead of HTTP. For example, to access the Raven UI without TLS, the user can navigate to: http://localhost:5580 But when using TLS, access must be through HTTPS: https://localhost:5580 The links printed by the Martini startup should reflect this setting.","title":"TLS Encryption (Server-side and Client-side)"},{"location":"martini.html#oliveserver-pass-through-arguments","text":"The Martini management script supports passing arguments through to oliveserver at startup time, to support configuration options that advanced users might benefit from. By default, any flags that are unrecognized by the script as Martini options, and that occur after the options and flags for Martini, will be passed on to the oliveserver itself. Some of the more common flags to use with this include enabling server debug mode or configuring the number of OLIVE workers . Examples of each are shown below. For the full list of optional arguments, refer to the OLIVE Server documentation.","title":"OLIVEServer Pass-through Arguments"},{"location":"martini.html#server-debug-mode","text":"Activates 'debug' mode for the OLIVE server. This causes OLIVE to mantain all OLIVE server logs to aid in troubleshooting and debugging. These logs are stored in olive6.0.0/oliveAppData/server/ by default. Without this flag, \"failed\" logs are retained, but logs for jobs deemed to be successful are deleted as tasks are completed to avoid clutter and confusion. Usage: macOS and linux $ ./martini.sh start --debug Windows $ . \\m artini.bat start -debug","title":"Server Debug Mode"},{"location":"martini.html#server-number-of-workers-parallel-processing","text":"Allows configuration of the number of workers that are available to the OLIVE server, controlling the number of jobs that the server can have in process at any given time. By default, this is automatically set to be equal to the number of CPU cores are detected on the hardware running the server, to avoid resource conflicts. Usage: macOS and linux $ ./martini.sh start --gpu --workers 4 Windows $ . \\m artini.bat start -gpu -workers 4","title":"Server Number of Workers (Parallel Processing)"},{"location":"martini.html#stop","text":"Stops a running container. macOS and linux $ ./martini.sh stop Windows $ . \\m artini.bat stop","title":"stop"},{"location":"martini.html#status","text":"martini.sh status Displays information on the processes running, network ports, workflows and plugins which are active on the container, as well as confirmation of Martini startup options (like GPU access or TLS/encryption) or custom oliveserver passthrough flags (like --debug mode). macOS and linux $ ./martini.sh status Configuration ------------- - GPU Support: Disabled - SSL/TLS Support: Enabled - Custom OLIVE Server arguments: ( none ) - Mounted plugins: - ldd-embedplda-v1.0.1 - lid-embedplda-v4.0.1 - sad-dnn-v8.0.1 - sad-dnn-v8.1.0 - sdd-diarizeEmbedSmolive-v1.0.3 - sid-dplda-v3.0.0 - tmt-ctranslate-v1.3.0 - Mounted workflows: - SAD-SDD-LDD.workflow.json - SAD.workflow.json - SID.workflow.json - english-to-spanish-TMT.workflow.json - russian-to-english-TMT.workflow.json - sid_enrollment.workflow.json Processes --------- - Message Broker ( web API ) : Running - Nginx ( reverse proxy ) : Running - OLIVE Server: Running - Raven UI ( web server ) : Running - Secure Websockets ( SSL/TLS ) : Running Network ------- Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0 .0.0.0:8888 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:8070 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:8080 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5592 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5589 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5588 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5005 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5004 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5006 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:80 0 .0.0.0:* LISTEN Windows $ . \\m artini.bat status Configuration ------------- - GPU Support: Disabled - SSL/TLS Support: Enabled - Custom OLIVE Server arguments: ( none ) - Mounted plugins: - ldd-embedplda-v1.0.1 - lid-embedplda-v4.0.1 - sad-dnn-v8.0.1 - sad-dnn-v8.1.0 - sdd-diarizeEmbedSmolive-v1.0.3 - sid-dplda-v3.0.0 - tmt-ctranslate-v1.3.0 - Mounted workflows: - SAD-SDD-LDD.workflow.json - SAD.workflow.json - SID.workflow.json - english-to-spanish-TMT.workflow.json - russian-to-english-TMT.workflow.json - sid_enrollment.workflow.json Processes --------- - Message Broker ( web API ) : Running - Nginx ( reverse proxy ) : Running - OLIVE Server: Running - Raven UI ( web server ) : Running - Secure Websockets ( SSL/TLS ) : Running Network ------- Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0 .0.0.0:8888 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:8070 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:8080 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5592 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5589 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5588 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5005 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5004 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:5006 0 .0.0.0:* LISTEN tcp 0 0 0 .0.0.0:80 0 .0.0.0:* LISTEN","title":"status"},{"location":"martini.html#cli","text":"Starts up a shell within the OLIVE Martini container. The container must already be running. This is typically used for internal testing and troubleshooting and is not meant to be used by end-users. You can use this shell to run Olive CLI commands, such as: macOS and linux $ ./martini.sh cli # Show the plugins installed on the container: $ ls /home/olive/olive/plugins/ # Show the audio files on the container, which you can use as examples: $ ls /olive-data/media/shared/ # Run an analysis $ localanalyze /home/olive/olive/plugins/lid-embedplda-v2.0.1/domains/multi-v1 /olive-data/media/shared/Komodo_dragon_en.wav.lst # See the results $ more output.txt Windows $ . \\m artini.bat cli # Show the plugins installed on the container: $ ls /home/olive/olive/plugins/ # Show the audio files on the container, which you can use as examples: $ ls /olive-data/media/shared/ # Run an analysis $ localanalyze /home/olive/olive/plugins/lid-embedplda-v2.0.1/domains/multi-v1 /olive-data/media/shared/Komodo_dragon_en.wav.lst # See the results $ more output.txt This can also be used for troubleshooting by advanced users. Advanced Martini Troubleshooting using martini.sh cli When running with the cli command, your shell starts in /opt/olive . This is where the runtime is installed, and where things are run within the container. In /opt/olive : Running martini.sh status will print out what is running, and what network connections are active. And looking at this script will tell you what should be running. Do not run martini-process.sh. That is run when the container starts. The /home/olive/olive directory is the other important directory. Here: The martini-process.log file is the output of the script which starts all servers. The env.log file (if it exists) are the shell variables set. The other log files here were started by each server. The plugins/ directory contains all plugins. The server/ directory contains server logs.","title":"cli"},{"location":"martini.html#net","text":"Lists the project network ports that are active on the host machine, the machine running the container. macOS and linux $ ./martini.sh net Windows $ . \\m artini.bat net","title":"net"},{"location":"martini.html#list","text":"Lists the project containers that are running. macOS and linux $ ./martini.sh list Windows $ . \\m artini.bat list","title":"list"},{"location":"martini.html#log","text":"martini.sh log Displays the OLIVE Server log; useful for checking on the status of the OLIVE server in case of a malfunction, or troubleshooting unexpected behavior. macOS and linux $ ./martini.sh log Windows $ . \\m artini.bat log This can be saved out to a local file by providing a filename after the log command: macOS and linux $ ./martini.sh log <logfile> Windows $ . \\m artini.bat log <logfile> And the shell can be attached to this log file and updated in real time with the -f flag after a local logfile name: macOS and linux $ ./martini.sh log <logfile> -f Windows $ . \\m artini.bat log <logfile> -f","title":"log"},{"location":"martini.html#version","text":"Diplays the version information for the individual components of OLIVE: macOS and linux $ ./martini.sh version Martini v1.1. Olive v5.3.0. Broker v1.5. Raven v0.2.0. Windows $ . \\m artini.bat version Martini v1.1. Olive v5.3.0. Broker v1.5. Raven v0.2.0.","title":"version"},{"location":"martini.html#using-the-container","text":"Once you have started up the container using the martini.sh start command described above, you are ready to begin using it. Your container has four different applications within it, which you can use - the Raven Web UI , the OLIVE Server itself , the OLIVE Message Broker, and a web server hosting the OLIVE Documentation. In addition, most OLIVE deliveries typically ship with the interactive Nightingale Forensic GUI for performing close file analysis using OLIVE plugins and workflows, as well as both Java and Python OLIVE Client CLI example applications. Note that if you are attempting to connect from the same machine, you can access this feature at localhost:5580 . If you are accessing from a different machine, you must use the full hostname, or IP address. For convenience, the martini.sh script outputs the hostname and corresponding port for several activities: $ martini.sh start \\Started the container. From this machine: Run Nightingale (Olive GUI) using server localhost and port 5588. Use a web browser to localhost:5570 to see the documentation. Use a web browser to localhost:5580 to use the Raven Web UI. If using the REST API point to the server running on localhost:5004. From any other machine: Run Nightingale (Olive GUI) using server stauf-MBP16 and port 5588. Use a web browser to stauf-MBP16:5570 to see the documentation. Use a web browser to stauf-MBP16:5580 to use the Raven Web UI. If using the REST API point to the server running on stauf-MBP16:5004. Installed plugins (mounted from /Users/astauf/olive/olive5.3.0/oliveAppData/plugins) are: asr-dynapy-v3.0.0 map-routerGlobal-v1.0.0 dfa-cplda-v1.0.0 map-routerGlobal-v1.0.1 dfa-speakerSpecific-v1.0.0 map-routerRegion-v1.0.0 dfa-spoofnet-v1.0.0 pim-validateGlobal-v1.0.0 enh-mmse-v2.0.2 qbe-ftdnnSmolive-v1.0.0 env-audioQuality-v2.0.0 qua-analysis-v1.0.0 fdi-pyEmbed-v1.0.0 qua-filter-v1.0.0 fdv-pyEmbed-v1.0.0 red-transform-v1.0.0 fri-pyEmbed-v1.0.0 sad-dnn-v7.0.2 frv-pyEmbed-v1.0.0 sad-dnnSmolive-v1.0.0 gdd-embedplda-v1.0.0 sdd-diarizeembed-v1.0.0 gid-gb-v2.0.1 sdd-sbcEmbed-v2.0.3 ldd-embedplda-v1.0.0 shl-sbcEmbed-v1.0.2 ldd-embedplda-v1.0.1 sid-dplda-v2.0.2 lid-embedplda-v3.0.0 sid-embed-v6.0.2 lid-embedplda-v3.0.1 tpd-dynapy-v5.0.0 lid-embedpldaSmolive-v1.0.0 vtd-dnn-v7.0.2 Installed workflows (mounted from /Users/astauf/olive/olive5.3.0/oliveAppData/workflows) are: /opt/olive-broker/data/workflows/FaceDetection_Image_FDI.workflow.json /opt/olive-broker/data/workflows/FaceDetection_Video_FDV.workflow.json /opt/olive-broker/data/workflows/FaceRecognition_Image_FRI.workflow.json /opt/olive-broker/data/workflows/FaceRecognition_Video_FRV.workflow.json /opt/olive-broker/data/workflows/SID_quality-controlled.workflow.json /opt/olive-broker/data/workflows/SmartTranscriptionFull.workflow.json /opt/olive-broker/data/workflows/TPD-eng.workflow.json /opt/olive-broker/data/workflows/conditional_asr_v2.workflow.json /opt/olive-broker/data/workflows/quality-region-analysis-for-SID.workflow.json Please choose the appropriate hostname and port number for your desired activity and host situation.","title":"Using The Container"},{"location":"martini.html#raven-web-ui","text":"To connect to the Raven Web/Batch GUI , open a web browser and navigate to URL host :5580. Once there, follow the on-screen prompts to drag-and-drop one or more audio files to the \"local media\" dropbox section, select one of the available workflows, and click on the action button in the lower right hand corner. More details for the Raven GUI can be found on its dedicated documentation page: Raven Web UI .","title":"Raven Web UI"},{"location":"martini.html#the-olive-documentation","text":"When the container is started, it launches a web server serving this documentation. To view the hosted version, navigate a web browser to URL host :5570.","title":"The OLIVE Documentation"},{"location":"martini.html#the-olive-server","text":"This is used internally by the Olive Web UI and the Olive Message Broker. You can also write Python programs which make calls directly to the server. See seperate documentation on olivepy API for details.","title":"The OLIVE Server"},{"location":"martini.html#the-olive-message-broker","text":"This is used internally by the Olive Web UI.","title":"The OLIVE Message Broker"},{"location":"martini.html#nightingale-forensic-ui","text":"This GUI provides access to the full suite of available OLIVE plugins and workflows, for performing close file analysis. It offers powerful audio visualization tools, live streaming capabilities, and a number of other useful features. For more information on what Nightingale offers and how to get started using it, please refer to the Nightingale GUI dedicated documentation page. As a quick primer, the Nightingale UI is typically provided in: <...>/olive6.0.0/OliveGUI Nightingale requires OpenJDK Java 11 or newer. Once this is installed, you can run it by navigating to: <...>/olive6.0.0/OliveGUI/bin/ And either double-clicking or running the Nightingale (macOS, linux) or Nightingale.bat (Windows) script appropriate for your OS. macOS / linux $ ./Nightingale Windows $ . \\N ightingale.bat info \"Note that the Nightingale launcher scripts are designed to be launched from this bin/ directory in order to properly link to other folders that it uses within this package, like the included workflows directory.\"","title":"Nightingale Forensic UI"},{"location":"martini.html#olive-example-api-client-implementations","text":"OLIVE is also generally shipped with example API Client implementations in both Java and python. They are usually included in: <...>/olive6.0.0/api/[java, python] These can provide both a command-line-based method of interacting with the OLIVE server, and as integration examples providing templates or code bases for some integration teams.","title":"OLIVE Example API Client Implementations"},{"location":"martini.html#advanced-and-optional-usage-and-features","text":"Click to expand for a deeper dive on some Martini capabilities","title":"Advanced and Optional Usage and Features"},{"location":"martini.html#configuring-ports-optional","text":"By default the container exposes seven ports on the host machine running the container: 5588 5589 5004 5005 5570 5580 5888 If you want to change them, there are two ways to do this: You can directly edit the martini.sh or martini.ps1 file to change these seven variables near the top of the file: OLIVE_HOST_PORT_OLIVESERVER OLIVE_HOST_PORT_OLIVESECOND OLIVE_HOST_PORT_TEST OLIVE_HOST_PORT_BROKER OLIVE_HOST_PORT_DOCSERVER OLIVE_HOST_PORT_WEBSERVER OLIVE_HOST_PORT_REVERSE_PROXY Note that if you are using Windows Command Prompt (cmd) or Windows PowerShell via martini.bat , edit the martini.ps1 file, as it is called by martini.bat during operation. Doing this will use the new ports every time the container starts. Alternatively, you can set shell variables, with the same names as above, to the port numbers you want to use. This method will temporarily override the ports used to what you have set, but subsequent container starts will revert to the original, if the shell/environment variables are no longer set.","title":"Configuring Ports (Optional)"},{"location":"martini.html#configuring-plugins-workflows-and-documentation-optional","text":"If you are using the default installation, then no configuration is required. Your workflows must be in a directory called oliveAppData/workflows/ , your documentation in a directory called docs/ , and your plugins in a directory called oliveAppData/plugins/ , and the martini.sh script in an oliveMartini directory that is adjacent to docs/ and oliveAppData/. By default, OLIVE Martini will store logs, enrollments and other overhead-related files to oliveAppData/server/ If your data is not located in these default locations, then you have two ways to configure the OLIVE Martini container (similar to configuring ports described above): You can directly edit the martini.sh file to change these three variables near the top of the file: OLIVE_HOST_PLUGINS OLIVE_HOST_SERVER OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE Doing this will use the new locations every time the container starts. You can set shell variables to the full path names of the locations of your plugins, workflows, and documentation. Set these shell variables: OLIVE_HOST_PLUGINS OLIVE_HOST_SERVER OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE But remember, neither of these is required, if your plugins, workflows, and documentation are all in the default locations. Alternatively, changing MY_OLIVE_APP_DATA in the martini.sh script or setting the OLIVE_APP_DATA environment variable (by default set to oliveAppData/ ) will change most of these settings at once (excluding documentation) if it is desired to keep these resources together but in a different location. You can also (optionally) edit martini.sh to change the DELAY count. It can take a few to 10 seconds or so for all the servers to start on the container, depending on the hardware performing the operation. Normally the martini.sh returns immediately, even if the servers have not had time to start. If you don't like that behavior, you can set the DELAY variable near the top of the file to the number of seconds it takes to start all the servers, usually 7 or 10 seconds. This would mean that the command will not return until the servers are actually up and running.","title":"Configuring Plugins, Workflows, and Documentation (Optional)"},{"location":"martini.html#testing-the-installation","text":"There are a few ways to quickly sanity check that the installation is properly configured, up and running. Use a web browser to see what workflows are available. Go to URL host :5004/api/workflows. You should see json text describing the available workflows. Use a web browser to test the Olive Web UI. Go to URL host :5580. You should see a page with \"SRI International\" in the upper left corner, and a section for \"Media\" and a section for \"Workflows\". (See below for getting started with the Olive Web UI.) Use a web browser to look at the documentation. Go to to URL host :5570. You should see a web page titled \"Open Language Interface for Voice Exploitation (OLIVE)\".","title":"Testing The Installation"},{"location":"martini.html#final-notes-troubleshooting","text":"There are a couple of known minor issues that may occur the first time you are starting the OLIVE Martini container on a new device. Since Docker will prompt asking for permission to share a few locations on the machine, in order to access the included plugins, workflows, and documentation, the container cannot initially read these and may report no plugins or workflows found at first. Once access is granted for Docker to share these locations, you should be able to check the plugins that are loaded by checking martini log , or in the extreme case, by stopping and restarting the container: macOS / linux $ ./martini.sh stop $ ./martini.sh start Windows $ . \\m artini.bat stop $ . \\m artini.bat start It can also take a moment for the Raven UI to populate the available Workflows on the bottom of the interface screen, especially the first time you are loading the web page. If this space is blank, please wait a moment (may take up to a minute on some systems) and/or refresh the page.","title":"Final Notes / Troubleshooting"},{"location":"nightingale.html","text":"OLIVE Nightingale GUI Installation and Startup Getting started with the OLIVE Nightingale GUI is a pretty straightforward process. The only prerequisite for running the GUI, apart from a functional and running OLIVE server, is openJDK Java 17 or newer, that can be sourced here: OpenJDK 17 The GUI is configured to automatically detect and connect to an OLIVE server instance running on the same machine as the GUI, as long as the default server ports are still being used. It is possible to connect this GUI to an OLIVE server running on a remote machine, so long as firewall and other settings permit. For details on how to do this if it's needed, get in touch with us for assistance. These instructions assume a typical OLIVE delivery layout as follows: olive6.0.0 docs/ oliveDocker or runtime-6.0.0*/olive-6.0.0* OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ workflows/ To launch the GUI, once you've installed the version of Java linked above, fire up the OLIVE server in your favorite manner, then you can launch the GUI by hopping into the appropriate directory, and starting the Nightingale launcher: $ cd olive6.0.0/OliveGUI/bin $ ./Nightingale This should start up the GUI that should automatically connect to the running server. The GUI should be pointing to your 'workflows' directory by default, so it should already know about the SAD_LID_SID.workflow that's included - but if for some reason this isn't showing up, you can add it using the '+' symbol in the GUI's Workflows panel. TLS / --secure Connection If Nightingale was started with the flag --secure, or the config file included <secure><enabled>true</enabled></secure> , a dialog will pop up to enter details required to connect to OLIVE in secure mode: Certificate paths will be saved in the config file, but any passwords entered will not be saved on disk. If you do not want to connect with TLS, press the \"Connect without TLS\" button. If the OLIVE server has been started in secure mode, Nightingale needs to be started with the flag --secure, or the config file must include <secure><enabled>true</enabled></secure> . OLIVE cannot accept plain-text connections in secure mode. If there are connection errors, a dialog will pop up giving various troubleshooting suggestions. To retry connecting with Olive, press the \"Re-enter Credentials\" button and either enter security credentials or press the button \"Connect without TLS\". Nightingale GUI User Guide Overview Below is a basic diagram of Nightingale with all the major panels highlighted: Panels: Live Stream Playback Layout New Tier Server Status Workflows Plugins Channel List Spectrogram Spectrogram Control Global Score Results Region Score Results Enrollment What's an Enrollment? Enrollment is the process of adding a model to a plugin and is required for several types of plugins. In the case of Speaker Identification, and enrollment involves submitting a portion of audio that contains only the speaker that you want to look for in the future. The enrollment process turns this portion of audio into a model which the speaker identification plugin will use to score future audio against in order to detect additional instances of the speaker you enrolled. How to Enroll Enrollment in the Nightingale User Interface is fast and simple. You need to do the following steps: Select the audio you would like to enroll Select the plugin you would like to enroll it in Give the enrollment a name, or add it to an existing enrollment (this is called augmentation) Click enroll and then wait. Below are images that can be used as reference for each step. In these examples the enrollments are being done using the Speaker Identification plugin on the left side, and the Query By Example plugin on the right side. 1. Select Audio 2a. Select Plugin 2b. Select Plugin contd 3. Give Enrollment Name 4. Enroll Augmentation Augmentation is almost the exact same process as enrollment, but in augmentation instead of creating a new class (eg. speaker, keyword, etc) you'll be adding additional data to an existing class. This is useful for improving models, especially when the amount of audio, or the quality of audio for the initial enrollment is low. To augment a model simple follow the same steps for enrollment, but instead of giving the enrollment a name in Step 3, select an existing class from the following dropdown: Removing an Enrollment Removing an enrollment can be useful to remove clutter if you no longer need a class or classes. Removing an enrolled class removes the entirety of the class, if you had augmented the class with several audio clips, unenrolling will remove everything. There is no way through the Nightingale GUI to remove individual augmentation segments from an enrollment. To remove a class from a plugin follow the steps below: Select the 'Enroll' button to the right of the plugin you want to remove an enrolled model from. Select 'Remove ______'. Select which class you want to remove from the dropdown list. Click 'Submit' Analysis Analysis is the basic operation of most plugins, and mostly falls under 3 general types of scoring; Global Scoring and Region Scoring. Region scorers return score results for one or more classes, and associate them with a specific region (or regions) in time, whereas global scorers return a single score for each class for the entire portion of the audio that was submitted. Some examples of global scorers and region scorers are below; typically \"Identification\" plugins are global scorers, and \"Detection\" plugins are region scorers. Global Scorers: Speaker Identification (SID) Language Identification (LID) Gender Identification etc. Region Scorers: Language Detection (LDD) Speaker Detection (SDD) Query by Example Keyword Spotting (QBE) Acoustic Event Detection (AED) Automatic Speech Recognition (ASR) etc. The procedure for scoring audio is largely the same for both types of plugins, and entails following these steps: First select audio (if you are analyzing a specific region, otherwise just make sure the appropriate file is selected), then do the following steps to run analysis on that audio: Select plugin type from panel (eg. SID, LID, etc) Select from 'Entire File', 'Selection', and 'Tier' (If you are analyzing a specific Tier, you will also need to make sure the correct tier is selected from the dropdown) If you have multiple versions or domains of the plugin you will need to specify which version of the plugin to use from the '___ Plugin:' dropdown menu. Click 'Submit' Viewing Results Global Scoring After you've performed analysis with a global scoring plugin the results of the analysis will appear in the global scoring section in the top right of the Nightingale GUI. Depending on the plugin there will be a list of each of that plugins classes and a corresponding score displayed in this area. Scores that are above 0 are considered a detection and will be shown at the top of the list in large green text, scores that are below 0 are displayed below in grey text. The higher a positive score is the more confident the system is that that particular class was detected in the audio that was submitted, whereas conversely, the lower the score below zero the more confident the system is that that class was NOT detected in the submitted audio. Some plugins that have many classes enrolled will not be able to display all of the scores in the global score results region, instead they will display the top X number of results and will also display a small button that says \"more\" at the bottom of the list. Clicking \"more\" will display the rest of the class scores for that analysis. Global scoring results appear on the right side of the GUI in panel \"11.\" on the Nightingale Overview Region Scoring After you've performed analysis with a region scoring plugin the results will be displayed as a new tier beneath the audio file that was analyzed. If there were detections in the analysis regions with the name of the class that was detected will be displayed in this tier. Region Scoring results will populate below the file that they were run on in panel \"12.\" in the Nightingale Overview Areas in the region score tiers that are marked by a dot/hash pattern indicate that these areas were not included in the region scoring submission. This happens when only a selection or tier is submitted for analysis instead of the entire file. Font/Language Support If characters from any language(s) do not display correctly in the region score tier, it is possible that the system's default font cannot display that language. That might look like this: Missing Text Completely Unsupported Characters You can change the font that the region score tier uses in the global preferences menu (accessed by clicking the settings icon in the top left \"File\" panel). There, you can reset the tier's font to its default by pressing \"Reset\", or choose a new font to use by pressing \"Browse Fonts\" which displays each font's supported character set. The font used should update immediately upon selecting a new one. Workflows Workflows are prebuilt complex tasks that run a segment of audio through a predefined series of HLT algorithms. This can be very useful and timesaving when there is a well-defined use case or mission that requires the same tasks or procedure to be run on all audio. If you have been provided with one or multiple .workflow, .json, or .txt files you can load them into the Nightingale GUI by following the steps below: Once you've successfully loaded one or multiple workflows into the GUI you will see them appear as new buttons in the workflow panel. You can submit audio to a workflow in the same way as submitting audio for analysis, by highlighting a selection/tier/file and clicking the appropriate workflow button as shown below: Depending on how each workflow is configured and which plugins it consists of, the results will be displayed around the waveform as they are returned, and will be displayed either on the Region Scoring Results area below the waveform, or the Global Scoring Results area to the right of the waveform, as is appropriate for each plugin the workflow is running. Nightingale Basic Tools In addition to HLT capabilities Nightingale includes several built-in tools to help users navigate and manipulate audio files for analysis and submission to algorithms. Below are the descriptions of these basic tools and how to use them. Audio Playback Controls Nightingale has two 'Playback Control Panels' that can be used to play, stop, pause, or speed up audio. These panels will play the audio in the active panel, and will default to playing from the start of the active file if no audio is selected, otherwise they will cause playback to start at the beginning of the selected audio. The playback panels are the panels marked \"2.\" in the Nightingale Overview Layout Panel The layout panel, \"3.\" on the Nightingale Overview can be used to customize the number and layout of different channels the user would like to display for simultaneous viewing. This can be a usefull tool to scale Nightingale to the needs of the user. Typically a user focused on close analysis of a audio files might want to use 1-3 panels to display different files, a user focusing on monitoring many live audio streams may wish to use this panel to configure Nightingale to display up to 12 simultaneous audio files or streams. Tier Manipulation and Annotation The 'New Tier' panel marked \"4.\" in the Nightingale Overview allows users to create their own tiers. Additionally it allows for the creation of creating inverse tiers, and creating new tiers from the difference and sum of multiple existing tiers. This is a tool that can be used by advanced users to fine tune the portions of audio that they wish to run tasks on. The 'New Tier' panel also allows users to create an annotation tier, in which they can add comments and notes at different timestamps in the audio. Channel/File List The file list on the left side of the Nightingale GUI marked \"8.\" in the Nightingale Overview can be used to switch between any files (or channels if you are livestreaming) that have been loaded into Nightingale. If there are more files/streams loaded then there are Layout Panels allocated, then you can allocate one of the undisplayed files to one of the layout panels by hovering over it in the Channel/File list, and then clicking the panel on the grid where you would like the file to be displayed. Spectrogram Nightingale's spectrogram, displayed in panel \"9.\" and controlled from panel \"10.\" in the Nightingale Overview can be a powerful tool to analyze audio and pick out signal from the background noise. The spectrogram in panel 9. is the main interface panel for users, and is where the majority of audio selection can be done. The spectrogram control panel in panel 10. allows the user to customize the color schemes, sensitivity, and a host of more advance settings that can be accessed on panel 10. by selecting the \"more...\" button. Spectrogram 1 Spectrogram 2 Changes made here are temporary and will be reset when the GUI is closed.","title":"Nightingale GUI"},{"location":"nightingale.html#olive-nightingale-gui","text":"","title":"OLIVE Nightingale GUI"},{"location":"nightingale.html#installation-and-startup","text":"Getting started with the OLIVE Nightingale GUI is a pretty straightforward process. The only prerequisite for running the GUI, apart from a functional and running OLIVE server, is openJDK Java 17 or newer, that can be sourced here: OpenJDK 17 The GUI is configured to automatically detect and connect to an OLIVE server instance running on the same machine as the GUI, as long as the default server ports are still being used. It is possible to connect this GUI to an OLIVE server running on a remote machine, so long as firewall and other settings permit. For details on how to do this if it's needed, get in touch with us for assistance. These instructions assume a typical OLIVE delivery layout as follows: olive6.0.0 docs/ oliveDocker or runtime-6.0.0*/olive-6.0.0* OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ workflows/ To launch the GUI, once you've installed the version of Java linked above, fire up the OLIVE server in your favorite manner, then you can launch the GUI by hopping into the appropriate directory, and starting the Nightingale launcher: $ cd olive6.0.0/OliveGUI/bin $ ./Nightingale This should start up the GUI that should automatically connect to the running server. The GUI should be pointing to your 'workflows' directory by default, so it should already know about the SAD_LID_SID.workflow that's included - but if for some reason this isn't showing up, you can add it using the '+' symbol in the GUI's Workflows panel.","title":"Installation and Startup"},{"location":"nightingale.html#tls-secure-connection","text":"If Nightingale was started with the flag --secure, or the config file included <secure><enabled>true</enabled></secure> , a dialog will pop up to enter details required to connect to OLIVE in secure mode: Certificate paths will be saved in the config file, but any passwords entered will not be saved on disk. If you do not want to connect with TLS, press the \"Connect without TLS\" button. If the OLIVE server has been started in secure mode, Nightingale needs to be started with the flag --secure, or the config file must include <secure><enabled>true</enabled></secure> . OLIVE cannot accept plain-text connections in secure mode. If there are connection errors, a dialog will pop up giving various troubleshooting suggestions. To retry connecting with Olive, press the \"Re-enter Credentials\" button and either enter security credentials or press the button \"Connect without TLS\".","title":"TLS / --secure Connection"},{"location":"nightingale.html#nightingale-gui-user-guide","text":"","title":"Nightingale GUI User Guide"},{"location":"nightingale.html#overview","text":"Below is a basic diagram of Nightingale with all the major panels highlighted: Panels: Live Stream Playback Layout New Tier Server Status Workflows Plugins Channel List Spectrogram Spectrogram Control Global Score Results Region Score Results","title":"Overview"},{"location":"nightingale.html#enrollment","text":"","title":"Enrollment"},{"location":"nightingale.html#whats-an-enrollment","text":"Enrollment is the process of adding a model to a plugin and is required for several types of plugins. In the case of Speaker Identification, and enrollment involves submitting a portion of audio that contains only the speaker that you want to look for in the future. The enrollment process turns this portion of audio into a model which the speaker identification plugin will use to score future audio against in order to detect additional instances of the speaker you enrolled.","title":"What's an Enrollment?"},{"location":"nightingale.html#how-to-enroll","text":"Enrollment in the Nightingale User Interface is fast and simple. You need to do the following steps: Select the audio you would like to enroll Select the plugin you would like to enroll it in Give the enrollment a name, or add it to an existing enrollment (this is called augmentation) Click enroll and then wait. Below are images that can be used as reference for each step. In these examples the enrollments are being done using the Speaker Identification plugin on the left side, and the Query By Example plugin on the right side. 1. Select Audio 2a. Select Plugin 2b. Select Plugin contd 3. Give Enrollment Name 4. Enroll","title":"How to Enroll"},{"location":"nightingale.html#augmentation","text":"Augmentation is almost the exact same process as enrollment, but in augmentation instead of creating a new class (eg. speaker, keyword, etc) you'll be adding additional data to an existing class. This is useful for improving models, especially when the amount of audio, or the quality of audio for the initial enrollment is low. To augment a model simple follow the same steps for enrollment, but instead of giving the enrollment a name in Step 3, select an existing class from the following dropdown:","title":"Augmentation"},{"location":"nightingale.html#removing-an-enrollment","text":"Removing an enrollment can be useful to remove clutter if you no longer need a class or classes. Removing an enrolled class removes the entirety of the class, if you had augmented the class with several audio clips, unenrolling will remove everything. There is no way through the Nightingale GUI to remove individual augmentation segments from an enrollment. To remove a class from a plugin follow the steps below: Select the 'Enroll' button to the right of the plugin you want to remove an enrolled model from. Select 'Remove ______'. Select which class you want to remove from the dropdown list. Click 'Submit'","title":"Removing an Enrollment"},{"location":"nightingale.html#analysis","text":"Analysis is the basic operation of most plugins, and mostly falls under 3 general types of scoring; Global Scoring and Region Scoring. Region scorers return score results for one or more classes, and associate them with a specific region (or regions) in time, whereas global scorers return a single score for each class for the entire portion of the audio that was submitted. Some examples of global scorers and region scorers are below; typically \"Identification\" plugins are global scorers, and \"Detection\" plugins are region scorers. Global Scorers: Speaker Identification (SID) Language Identification (LID) Gender Identification etc. Region Scorers: Language Detection (LDD) Speaker Detection (SDD) Query by Example Keyword Spotting (QBE) Acoustic Event Detection (AED) Automatic Speech Recognition (ASR) etc. The procedure for scoring audio is largely the same for both types of plugins, and entails following these steps: First select audio (if you are analyzing a specific region, otherwise just make sure the appropriate file is selected), then do the following steps to run analysis on that audio: Select plugin type from panel (eg. SID, LID, etc) Select from 'Entire File', 'Selection', and 'Tier' (If you are analyzing a specific Tier, you will also need to make sure the correct tier is selected from the dropdown) If you have multiple versions or domains of the plugin you will need to specify which version of the plugin to use from the '___ Plugin:' dropdown menu. Click 'Submit'","title":"Analysis"},{"location":"nightingale.html#viewing-results","text":"","title":"Viewing Results"},{"location":"nightingale.html#global-scoring","text":"After you've performed analysis with a global scoring plugin the results of the analysis will appear in the global scoring section in the top right of the Nightingale GUI. Depending on the plugin there will be a list of each of that plugins classes and a corresponding score displayed in this area. Scores that are above 0 are considered a detection and will be shown at the top of the list in large green text, scores that are below 0 are displayed below in grey text. The higher a positive score is the more confident the system is that that particular class was detected in the audio that was submitted, whereas conversely, the lower the score below zero the more confident the system is that that class was NOT detected in the submitted audio. Some plugins that have many classes enrolled will not be able to display all of the scores in the global score results region, instead they will display the top X number of results and will also display a small button that says \"more\" at the bottom of the list. Clicking \"more\" will display the rest of the class scores for that analysis. Global scoring results appear on the right side of the GUI in panel \"11.\" on the Nightingale Overview","title":"Global Scoring"},{"location":"nightingale.html#region-scoring","text":"After you've performed analysis with a region scoring plugin the results will be displayed as a new tier beneath the audio file that was analyzed. If there were detections in the analysis regions with the name of the class that was detected will be displayed in this tier. Region Scoring results will populate below the file that they were run on in panel \"12.\" in the Nightingale Overview Areas in the region score tiers that are marked by a dot/hash pattern indicate that these areas were not included in the region scoring submission. This happens when only a selection or tier is submitted for analysis instead of the entire file.","title":"Region Scoring"},{"location":"nightingale.html#fontlanguage-support","text":"If characters from any language(s) do not display correctly in the region score tier, it is possible that the system's default font cannot display that language. That might look like this: Missing Text Completely Unsupported Characters You can change the font that the region score tier uses in the global preferences menu (accessed by clicking the settings icon in the top left \"File\" panel). There, you can reset the tier's font to its default by pressing \"Reset\", or choose a new font to use by pressing \"Browse Fonts\" which displays each font's supported character set. The font used should update immediately upon selecting a new one.","title":"Font/Language Support"},{"location":"nightingale.html#workflows","text":"Workflows are prebuilt complex tasks that run a segment of audio through a predefined series of HLT algorithms. This can be very useful and timesaving when there is a well-defined use case or mission that requires the same tasks or procedure to be run on all audio. If you have been provided with one or multiple .workflow, .json, or .txt files you can load them into the Nightingale GUI by following the steps below: Once you've successfully loaded one or multiple workflows into the GUI you will see them appear as new buttons in the workflow panel. You can submit audio to a workflow in the same way as submitting audio for analysis, by highlighting a selection/tier/file and clicking the appropriate workflow button as shown below: Depending on how each workflow is configured and which plugins it consists of, the results will be displayed around the waveform as they are returned, and will be displayed either on the Region Scoring Results area below the waveform, or the Global Scoring Results area to the right of the waveform, as is appropriate for each plugin the workflow is running.","title":"Workflows"},{"location":"nightingale.html#nightingale-basic-tools","text":"In addition to HLT capabilities Nightingale includes several built-in tools to help users navigate and manipulate audio files for analysis and submission to algorithms. Below are the descriptions of these basic tools and how to use them.","title":"Nightingale Basic Tools"},{"location":"nightingale.html#audio-playback-controls","text":"Nightingale has two 'Playback Control Panels' that can be used to play, stop, pause, or speed up audio. These panels will play the audio in the active panel, and will default to playing from the start of the active file if no audio is selected, otherwise they will cause playback to start at the beginning of the selected audio. The playback panels are the panels marked \"2.\" in the Nightingale Overview","title":"Audio Playback Controls"},{"location":"nightingale.html#layout-panel","text":"The layout panel, \"3.\" on the Nightingale Overview can be used to customize the number and layout of different channels the user would like to display for simultaneous viewing. This can be a usefull tool to scale Nightingale to the needs of the user. Typically a user focused on close analysis of a audio files might want to use 1-3 panels to display different files, a user focusing on monitoring many live audio streams may wish to use this panel to configure Nightingale to display up to 12 simultaneous audio files or streams.","title":"Layout Panel"},{"location":"nightingale.html#tier-manipulation-and-annotation","text":"The 'New Tier' panel marked \"4.\" in the Nightingale Overview allows users to create their own tiers. Additionally it allows for the creation of creating inverse tiers, and creating new tiers from the difference and sum of multiple existing tiers. This is a tool that can be used by advanced users to fine tune the portions of audio that they wish to run tasks on. The 'New Tier' panel also allows users to create an annotation tier, in which they can add comments and notes at different timestamps in the audio.","title":"Tier Manipulation and Annotation"},{"location":"nightingale.html#channelfile-list","text":"The file list on the left side of the Nightingale GUI marked \"8.\" in the Nightingale Overview can be used to switch between any files (or channels if you are livestreaming) that have been loaded into Nightingale. If there are more files/streams loaded then there are Layout Panels allocated, then you can allocate one of the undisplayed files to one of the layout panels by hovering over it in the Channel/File list, and then clicking the panel on the grid where you would like the file to be displayed.","title":"Channel/File List"},{"location":"nightingale.html#spectrogram","text":"Nightingale's spectrogram, displayed in panel \"9.\" and controlled from panel \"10.\" in the Nightingale Overview can be a powerful tool to analyze audio and pick out signal from the background noise. The spectrogram in panel 9. is the main interface panel for users, and is where the majority of audio selection can be done. The spectrogram control panel in panel 10. allows the user to customize the color schemes, sensitivity, and a host of more advance settings that can be accessed on panel 10. by selecting the \"more...\" button. Spectrogram 1 Spectrogram 2 Changes made here are temporary and will be reset when the GUI is closed.","title":"Spectrogram"},{"location":"olivepy-api.html","text":"olivepy API Reference The olivepy package consists of three modules intended for client usage: api module messaging module utils module Note : Other modules may be used internally but are not intended for client usage.","title":"olivepy API Reference"},{"location":"olivepy-api.html#olivepy-api-reference","text":"The olivepy package consists of three modules intended for client usage: api module messaging module utils module Note : Other modules may be used internally but are not intended for client usage.","title":"olivepy API Reference"},{"location":"olivepyPrimer.html","text":"Integrating with the OLIVE Python API (olivepy) olivepy is an OLIVE Python API that's built on top of the OLIVE Enterprise API. New Python integrators are advised to read through the Enterprise API Primer for an overview of key concepts that also apply to olivepy . Installation olivepy is distributed as a Python wheel package for easy installation. To install, navigate into the api folder distributed with your OLIVE delivery, and install it using your native pip3 : $ pip3 install olivepy-*-py3-none-any.whl A source distribution is also available as olivepy-*.tar.gz , but it should only be needed for unique operating environments and most clients should use the wheel instead. Dependencies olivepy direct dependencies include: protobuf soundfile numpy requests zmq These will be downloaded by pip when the olivepy package is installed. Integration Quickstart Here's a complete example for those in a hurry: from olivepy.api.olive_async_client import AsyncOliveClient from olivepy.messaging.msgutil import package_audio , serialize_file , InputTransferType from olivepy.messaging.olive_pb2 import Audio # connect client = AsyncOliveClient ( \"example client\" ) client . connect () print ( \"Client connected...\" ) print ( \" \\n -- Connection Information --\" ) print ( f \"Client: { client . client_id } \" ) print ( f \"Server: { client . server_address } \" ) print ( f \"Port: { client . server_status_port } \" ) # find a Language ID (LID) plugin plugin_request = client . request_plugins () plugin_response = plugin_request . get_response () for plugin in plugin_response . plugins : if plugin . task == \"LID\" : plugin_id = plugin . id plugin_domain = plugin . domain [ 0 ] . id break # run Language ID (LID) on serialized audio file print ( \" \\n -- Language ID Results --\" ) packaged_audio = package_audio ( Audio (), serialize_file ( \"Edmund_Yeo_voice_ch.wav\" ), mode = InputTransferType . SERIALIZED ) analyze_request = client . analyze_global ( plugin_id , plugin_domain , packaged_audio , None ) analyze_response = analyze_request . get_response () top_scoring = analyze_response . score [ 0 ] for score in analyze_response . score : print ( f \" { score . class_id } = { score . score } \" ) if score . score > top_scoring . score : top_scoring = score print ( f \" \\n Top scoring: { top_scoring . class_id } ( { top_scoring . score } )\" ) # disconnect client . disconnect () print ( \" \\n Client disconnected!\" ) Establish Server Connection Before making any task request, a client must establish a connection with the server. By default, the OLIVE server listens on ports 5588 (request port) and 5589 (status port) for client connection and status requests. These ports are configurable, but if the server has not been instructed to change its listening ports, the code below should establish a connection. from olivepy.api.olive_async_client import AsyncOliveClient client = AsyncOliveClient ( \"example client\" ) client . connect () ... client . disconnect () The request port (5588 by default) is used for call and response messages. Each request message sent to this port is guaranteed a response from the server (this is why the messages in the API Message Reference are often suffixed with 'Request' and 'Result'). There is no need to poll the server for information on a submitted request, as the result/response for the a request is returned to the client as soon as it is available. The status port (5589 by default) is used by the Server to publish health and status messages (Heartbeat) to client(s). Clients can not send requests on this port. TLS Encrypted Connections As of 5.6.0, olivepy supports TLS encrypted communication with the OLIVE Server. Encrypted connections require the OLIVE Server to be configured and started with TLS. For example, OLIVE Martini must be started with the flag --tls_server_only or --tls_server_and_client . By default, OLIVE listens on port 5588 for encrypted connections. To set up a encrypted connection using olivepy, connect to the server with client.secure_connect() function and provide the required PEM ( *.crt and *.key ) certificate arguments: from olivepy.api.olive_async_client import AsyncOliveClient client = AsyncOliveClient ( \"example client\" ) client . secure_connect ( certfile = \"path/to/certificate.crt\" , keyfile = \"/path/to/keyfile.key\" , password = \"keyfilePassword\" , ca_bundle_path = \"path/to/cert/authority.crt\" ) Connecting to the OLIVE Server using TLS encryption takes slightly longer than a standard non-encrypted connection; however, after the initial connection is made, all subsequent requests will run over TLS with no time penalty. Request Available Plugins In order to submit most server requests, the client must specify the plugin and domain to handle the request. To obtain the handle of a targeted plugin, the client first requests a list of all currently available valid plugins from the server. From the returned plugins list, the client looks up the specific plugin handle by the plugin's unique name (id) and its associated trait for the task to be performed. This handle can then be used in a future request message sent to the server. from olivepy.api.olive_async_client import AsyncOliveClient client = AsyncOliveClient ( \"example client\" ) client . connect () plugin_request = client . request_plugins () plugin_response = plugin_request . get_response () for plugin in plugin_response . plugins : print ( f 'Plugin { plugin . id } ( { plugin . task } , { plugin . group } ) { plugin . version } has { len ( plugin . domain ) } domain(s):' ) for domain in plugin . domain : print ( f ' \\t Domain: { domain . id } , Description: { domain . desc } ' ) client . disconnect () The targeted plugin's id and domain can then be used with subsequent requests. Note Each plugin has additional useful information that can be retrieved as needed: task : The type of task (e.g. LID, SID, SAD, KWS, AED, etc.) trait : See Traits group : Allows additional grouping of plugins such as Keyword, Speaker, Language, etc Audio Submission Guidelines One of the core client activities is submitting Audio with a request. olivepy provides three ways for a client to package audio data to send to the OLIVE server: file path buffer of raw audio sample data serialized file buffer object The enum InputTransferType is used to specify they type of audio transfer to use. When the client and the OLIVE server share the same file system, the easiest way for the client to send audio data to the server is by specifying the audio's file path on disk along with InputTransferType.PATH : packaged_audio = package_audio ( Audio (), \"/home/olive/samples/Edmund_Yeo_voice_ch.wav\" , mode = InputTransferType . PATH ) When the client and the server don't share the same file system, as in the case of a client making a remote connection to the OLIVE server running in docker (Martini), it is necessary to send the client's local audio files using InputTransferType.SERIALIZED or InputTransferType.DECODED : packaged_audio = package_audio ( Audio (), serialize_file ( \"Edmund_Yeo_voice_ch.wav\" ), mode = InputTransferType . SERIALIZED ) Serialized Buffer Note Sending audio data as a serialized file buffer ensures that all audio header information is provided intact to the server. This allows the server to properly decode and process the audio once its received, since it can directly access the bit depth, encoding type, sample rate and other necessary information from the header itself. The tradeoff with serialized files is that there may be additional overhead needed to process the audio into a consumable form. If the client and server reside on the same hardware and file system, it is advisable to simply pass filepaths when possible. This saves the memory overhead burden of both the client and server loading audio into memory. When using 16-bit PCM .wav files, or if first processing audio samples to be in one of the formats supported by the OLIVE server , it's also possible to pass a non-serialized buffer of raw samples along with InputTransferType.DECODED : packaged_audio = package_audio ( Audio (), audio_samples , mode = InputTransferType . DECODED , num_channels = 1 , sample_rate = 8000 , num_samples = 1080 ) Buffer of Raw Samples Caution When submitting audio to the server as a buffer of raw samples, it is important to include information characterizing the audio, such as the bit depth, audio encoding, sample rate, and number of channels, to ensure the server knows how to properly treat the buffer it receives. Synchronous vs. Asynchronous Message Submission olivepy allows the client to choose between processing a task request synchronously or asynchronously. Processing a task request synchronously means the client will block and wait for the task result to return before proceeding to other task requests. On the other hand, asynchronous processing means the client will not wait for the result to come back before moving on, allowing several jobs to be submitted in parallel. The olivepy.api.olive_async_client.AsyncOliveClient class supports both synchronous and synchronous submissions. Most requests sent to the server accept a typing.Callable that will be executed when the asynchronous request has been processed. When a synchronous request is desired (which is the case in most of the examples in this reference guide for conveience), then None needs to be explicitly passed in place of the argument reserved for the typing.Callable . Legacy OliveClient Note There is also a legacy olivepy.api.oliveclient.OliveClient that only supports synchronous submissions, but it will be deprecated in the future and all integrators are encouraged to switch over to the AsyncOliveClient . Frame Score Request The snippet below submits a Frame Score Request to the connected server. analyze_request = client . analyze_frames ( plugin_id , plugin_domain , packaged_audio , None ) analyze_response = analyze_request . get_response () for i , score in enumerate ( analyze_response . result [ 0 ] . score ): print ( f \"frame[ { i } ]= { score } \" ) Note Only a plugin that support the FrameScorer trait can handle this request. See the Frame Scorer Messages section of the Enterprise API Message Reference for details about the FrameScorerRequest and FrameScorerResult objects. Global Score Request The snippet below submits a Global Score Request to the connected server. analyze_request = client . analyze_global ( plugin_id , plugin_domain , packaged_audio , None ) analyze_response = analyze_request . get_response () for score in analyze_response . score : print ( f \" { score . class_id } = { score . score } \" ) Note The code required to submit a GlobalScorerRequest message doesn't care what type of plugin is going to be doing the scoring, as long as the plugin implements the GlobalScorer Trait. This means that the exact same code can be used for submitting audio to global scoring SID plugins, LID plugins, or any other global scoring plugin. See the Global Scorer Messages section of the Enterprise API Message Reference for details about the GlobalScorerRequest and GlobalScorerResult objects. Region Score Request The snippet below submits a Region Score Request to the connected server. analyze_request = client . analyze_regions ( plugin_id , plugin_domain , packaged_audio , None ) analyze_response = analyze_request . get_response () for region in analyze_response . region : print ( f \" { region . start_t : .2f } - { region . end_t : .2f } secs { region . class_id } \" ) Note Only a plugin that support the RegionScorer trait can handle this request. See the Region Scorer Messages section of the Enterprise API Message Reference for details about the RegionScorerRequest and RegionScorerResult objects. Enrollments See the appropriate section of the OLIVE Plugin Overview documentation for a description of Enrollments . Enroll The snippet below enrolls a sample audio file for speaker EDMUND_YEO : # submit enrollment (i.e. Class Modification Request) for Speaker ID plugin using a serialized audio file class_modification_request = client . enroll ( plugin_id , plugin_domain , \"EDMUND_YEO\" , \"Edmund_Yeo_voice_ch.wav\" , None , mode = InputTransferType . SERIALIZED ) class_modification_response = class_modification_request . get_response () for addition_result in class_modification_response . addition_result : status_label = \"succeeded\" if addition_result . successful else \"failed\" additional_details = f \"Additional details for request: { addition_result . message } \" if addition_result . message else \"\" print ( f \"Update to { plugin_id } / { plugin_domain } { status_label } ! { additional_details } \" ) Not all plugins support enrollments. Refer to the individual plugin documentation to confirm if it supports class enrollment before proceeding. Unenroll The snippet below unenrolls speaker EDMUND_YEO : # submit unenrollment (i.e. Class Modification Request) for Speaker ID plugin for speaker class_modification_request = client . unenroll ( plugin_id , plugin_domain , \"EDMUND_YEO\" , None ) if ( class_modification_request . is_successful ()): print ( \"Unenrollment was successful!\" ) else : print ( f \"Unenrollment failed with error: { class_modification_request . get_error () } \" ) Vectorization and Stateless Score Request See the appropriate section of the OLIVE Plugin Overview documentation for a description of Vectorization and Stateless Scoring. Vectorization The snippet below creates audio vectors that can be used in future stateless scoring requests: packaged_audios = [] for audio_file_path in audio_file_paths : packaged_audios . append ( package_audio ( Audio (), serialize_file ( audio_file_path ), mode = InputTransferType . SERIALIZED )) response = client . vectorize_audio ( plugin_id , plugin_domain , packaged_audios , mode = InputTransferType . SERIALIZED , callback = None ) # generate audio vector(s) that can be used in future stateless scoring requests audio_vectors = [] for vector_result in response . get_response () . vector_result : if not vector_result . successful : print ( f \"Could not vectorize audio: { vector_result . message } \" ) else : audio_vectors . append ( vector_result . audio_vector ) Serialization and Deserialization The snippet below provides an example of how to serialize and deserialize an audio vector: with tempfile . TemporaryDirectory ( f \"_ { class_id_label } \" ) as temp_dir : for audio_vector in audio_vectors : # serialize and save audio vector(s) to disk (for demo purposes) with open ( os . path . join ( temp_dir , str ( uuid . uuid4 ())), 'wb' ) as f : f . write ( serialize_message ( audio_vector )) # reload audio vector(s) from disk (for demo purposes) reloaded_audio_vectors = [] for audio_vector_path in glob . glob ( os . path . join ( temp_dir , \"*\" )): with open ( audio_vector_path , 'rb' ) as f : reloaded_audio_vectors . append ( deserialize_message ( f . read (), AudioVector )) Warning Saving to disk is FOR DEMO PURPOSES ONLY . A production system should consider using SQLite or similar technology which allows associating a speaker label String to binary data blobs rather than storing on disk if they require long-term persistent storage. The examples are to illustrate serialization and deserialization of audio vectors. Stateless Score Request The snippet below shows a Stateless Score Request that uses the in-memory audio vectors: # associate audio vectors with a specific class (such as a Speaker) class_vectors = { class_id_label : [ reloaded_audio_vector for reloaded_audio_vector in reloaded_audio_vectors ] } # score using associated audio vectors for packaged_audio in packaged_audios : analyze_request = client . analyze_global_stateless ( plugin_id , plugin_domain , packaged_audio , class_vectors , None ) analyze_response = analyze_request . get_response () print ( analyze_response ) Workflow Integration In addition to the basic API integration mechanisms described above, olivepy also supports OLIVE Workflows. See Workflows for a primer on OLIVE Workflows and then continue reading below for olivepy specific details. Quickstart Here's a complete workflow example for those in a hurry: import os from olivepy.api.olive_async_client import AsyncOliveClient from olivepy.api.workflow import OliveWorkflowDefinition from olivepy.messaging.msgutil import InputTransferType # connect client = AsyncOliveClient ( \"example client\" ) client . connect () # load the workflow_definition into the AsyncOliveClient to get a workflow helper object workflow_definition = OliveWorkflowDefinition ( \"sad_lid_sid.workflow.json\" ) sad_lid_sid_workflow = workflow_definition . create_workflow ( client ) # package audio as a serialized buffer serialized_audio = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' )) # submit workflow request on serialized audio file response = sad_lid_sid_workflow . analyze ([ serialized_audio ]) print ( response . to_json ( indent = 2 )) # disconnect client . disconnect () Initializing a Workflow As described in Workflows , workflow logic is encapsulated in a Workflow Definition file distributed as either binary (i.e. *.workflow - deprecated) or JSON (i.e. *.workflow.json ). Workflows are preconfigured to perform tasks such as Speech Activity Detection (SAD), Language Identification (LID), Speaker Identification (SID), etc. with a single call to the OLIVE server. These Workflow Definition files must be initialized (aka 'created') with the olivepy client before the workflow can be used. The snippet below initializes a workflow with the client : workflow_definition = OliveWorkflowDefinition ( \"sad_lid_sid.workflow.json\" ) sad_lid_sid_workflow = workflow_definition . create_workflow ( client ) A workflow 'helper' object ( OliveWorkflow ) is returned and is used to submit audio files directly to that workflow for analysis, enrollment, or unenrollment. In the snippet above, sad_lid_sid_workflow is the 'helper'. Packaging Audio The same Audio Submission Guidelines discussed earlier in this guide apply to workflows and each of the following are supported for workflows: InputTransferType.PATH InputTransferType.SERIALIZED InputTransferType.DECODED The difference with workflows is that the workflow 'helper' is used to package audios rather than olivepy client directly; the audio is wrapped in a WorkflowDataRequest that is submitted to OLIVE for processing: # package audio as a serialized buffer using the workflow 'helper' packaged_audio = workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' )) Multi-channel Audio The default workflow behavior is to merge multi-channel audio into a single channel, which is known as MONO mode. To perform analysis on each channel individually instead of a merged channel, the Workflow Definition must be authored with a mode of SPLIT . When using the split mode, each channel in a multi-channel audio input is \"split\" into a job. Here is a mode within a workflow definition file that merges multi-channel audio into a single channel audio input: \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, ... and one that handles each channel individually: \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"SPLIT\" }, Audio Annotations The audio submitted for analysis (or enrollment) can be annotated with start/end regions when packaging audio using the package_audio() function. The snippet below specifies two regions within a file: # Provide annotations for two regions: 0.3 to 1.7 seconds, and 2.4 to 3.3 seconds in audio regions = [( 0.3 , 1.7 ), ( 2.4 , 3.3 )] packaged_audio = workflow . package_audio ( 'Edmund_Yeo_voice_ch.wav' , InputTransferType . AUDIO_SERIALIZED , annotations = regions ) Submitting Audio Submitting audio for analysis, enrollment, and unenrollment using workflows is supported. Analysis The snippet below packages and submits an audio file to the workflow 'helper' for analysis: # package audio as a serialized buffer serialized_audio = workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' )) # submit workflow request on serialized audio file response = workflow . analyze ([ serialized_audio ]) Batch request The analyze function accepts a list of audio files to so multiple files can be analyzed as a complete 'batch' request: # package audio files for analysis serialized_audio_1 = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' )) serialized_audio_2 = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_en.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_en.wav' )) # submit workflow request on multiple serialized audio files response = sad_lid_sid_workflow . analyze ([ serialized_audio_1 , serialized_audio_2 ]) Enrollments Enroll Some workflows support enrollment for one or more jobs. To list the jobs in a workflow that support enrollment, use the get_enrollment_job_names() function: print ( f \"Enrollment Jobs: { sid_enrollments_workflow . get_enrollment_job_names () } \" ) # Enrollment jobs '['SID Enrollment']' To enroll a speaker via this workflow, use the workflow 'helper's enroll function: # find a Speaker Identification (SID) enrollment job name for enrollment_job_name in sid_enrollments_workflow . get_enrollment_job_names (): if enrollment_job_name . startswith ( \"SID\" ): sid_enrollment_job_name = enrollment_job_name break # submit enrollment (i.e. Class Modification Request) for Speaker ID plugin using a serialized audio file speaker_label = \"EDMUND_YEO\" packaged_audio = sid_enrollments_workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( \"Edmund_Yeo_voice_ch.wav\" )) class_modification_response = sid_enrollments_workflow . enroll ([ packaged_audio ], speaker_label , [ sid_enrollment_job_name ]) Since there was only one enrollment job in the workflow definition, the enrollment request could have been made without specifying the job name. However, it is a best practice to explicitly request the enrollment job by name. Note also that not all workflows support enrollment. Please check that the workflow being used supports this before submitting an enrollment request. To confirm the new speaker name was added: sid_class_ids = sid_enrollments_workflow . get_analysis_class_ids () print ( sid_class_ids . to_json ( indent = 1 )) Which produces the following output: { \"job_class\" : [ { \"job_name\" : \"SID analysis\" , \"task\" : [ { \"task_name\" : \"SID\" , \"class_id\" : [ \"EDMUND_YEO\" ] } ] } ] } 'Class ID' is a general term used to describe 'labels' that apply to the specific plugin. Here, class_id represents all the speaker labels that are enrolled. Unenroll Workflow definitions that support enrollment often also support unenrollment. Similar to enrollment, use the get_unenrollment_job_names() to get a list of jobs that support unenrollment, send unenrollment requests to unenroll , and use get_analysis_class_ids() to list. Here is an example snippet of all three: # find Speaker Identification (SID) unenrollment job name for unenrollment_job_name in sid_enrollments_workflow . get_unenrollment_job_names (): if unenrollment_job_name . startswith ( \"SID\" ): sid_unenrollment_job_name = unenrollment_job_name break # submit unenrollment (i.e. Class Modification Request) for Speaker ID plugin speaker_label = \"EDMUND_YEO\" class_modification_response = sid_enrollments_workflow . unenroll ( speaker_label , [ sid_unenrollment_job_name ]) sid_class_ids = sid_enrollments_workflow . get_analysis_class_ids () print ( sid_class_ids . to_json ( indent = 1 )) Which produces the following output: { \"job_class\" : [ { \"job_name\" : \"SID analysis\" , \"task\" : [ { \"task_name\" : \"SID\" } ] } ] } There is no class_id attribute because the speaker was successfully unenrolled and there aren't any others enrolled in the system. Parsing Workflow Responses A successful workflow request produces a response that includes information about the results. Information is grouped into one or more 'jobs', where a job is includes the name, tasks that were performed as part of the job, the results of each task, and potentially information about the input audio. For example the following code snippet: # package audio files for analysis serialized_audio_1 = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' )) serialized_audio_2 = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_en.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_en.wav' )) # submit workflow request on multiple serialized audio files response = sad_lid_sid_workflow . analyze ([ serialized_audio_1 , serialized_audio_2 ]) print ( response . to_json ( indent = 2 )) would produce the following output: Example Workflow Output (click to expand) [ { \"job_name\" : \"SAD, LID, SID analysis\" , \"data\" : [ { \"data_id\" : \"Edmund_Yeo_voice_ch.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 29.2825 , \"number_channels\" : 1 , \"label\" : \"Edmund_Yeo_voice_ch.wav\" , \"id\" : \"cbc95af3f693de48654a72ec288adb8ad182a8f86993a3d0d42e3e2a5b4d5548\" } ], \"tasks\" : { \"SAD\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v8.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 28.95 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"LID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"lid-embedplda-v4.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"Mandarin\" , \"score\" : 6.0677257 }, { \"class_id\" : \"Korean\" , \"score\" : -1.3737706 }, { \"class_id\" : \"English\" , \"score\" : -4.7524 }, { \"class_id\" : \"Vietnamese\" , \"score\" : -4.7861123 }, { \"class_id\" : \"Japanese\" , \"score\" : -6.1542406 }, { \"class_id\" : \"Iraqi Arabic\" , \"score\" : -8.356203 }, { \"class_id\" : \"Levantine Arabic\" , \"score\" : -8.655978 }, { \"class_id\" : \"Tagalog\" , \"score\" : -8.790578 }, { \"class_id\" : \"French\" , \"score\" : -9.969812 }, { \"class_id\" : \"Modern Standard Arabic\" , \"score\" : -10.236586 }, { \"class_id\" : \"Iranian Persian\" , \"score\" : -11.295098 }, { \"class_id\" : \"Amharic\" , \"score\" : -12.617832 }, { \"class_id\" : \"Spanish\" , \"score\" : -14.459879 }, { \"class_id\" : \"Portuguese\" , \"score\" : -14.844175 }, { \"class_id\" : \"Russian\" , \"score\" : -15.082703 }, { \"class_id\" : \"Pashto\" , \"score\" : -15.151446 } ] } } ], \"SID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"SID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"sid-dplda-v3.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"EDMUND_YEO\" , \"score\" : 10.516918 } ] } } ] } }, { \"job_name\" : \"SAD, LID, SID analysis\" , \"data\" : [ { \"data_id\" : \"Edmund_Yeo_voice_en.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 37.0825 , \"number_channels\" : 1 , \"label\" : \"Edmund_Yeo_voice_en.wav\" , \"id\" : \"65a53db9b3ac1d1571082512cba37665634712738fed530ca00b3d5922e0d129\" } ], \"tasks\" : { \"SAD\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v8.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 36.78 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"LID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"lid-embedplda-v4.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"English\" , \"score\" : 5.056248 }, { \"class_id\" : \"Levantine Arabic\" , \"score\" : -1.0983323 }, { \"class_id\" : \"Tagalog\" , \"score\" : -1.4544584 }, { \"class_id\" : \"Iraqi Arabic\" , \"score\" : -1.7203265 }, { \"class_id\" : \"Vietnamese\" , \"score\" : -1.7773396 }, { \"class_id\" : \"French\" , \"score\" : -2.4661431 }, { \"class_id\" : \"Korean\" , \"score\" : -3.766336 }, { \"class_id\" : \"Mandarin\" , \"score\" : -3.8002808 }, { \"class_id\" : \"Japanese\" , \"score\" : -4.7714725 }, { \"class_id\" : \"Modern Standard Arabic\" , \"score\" : -5.692904 }, { \"class_id\" : \"Spanish\" , \"score\" : -6.5970984 }, { \"class_id\" : \"Iranian Persian\" , \"score\" : -6.6558266 }, { \"class_id\" : \"Portuguese\" , \"score\" : -8.045044 }, { \"class_id\" : \"Amharic\" , \"score\" : -8.093353 }, { \"class_id\" : \"Pashto\" , \"score\" : -9.445313 }, { \"class_id\" : \"Russian\" , \"score\" : -11.597008 } ] } } ], \"SID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"SID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"sid-dplda-v3.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"EDMUND_YEO\" , \"score\" : 7.9177084 } ] } } ] } } ] In the above output, note that the root array contains two objects because two audio files were submitted to the workflow. Each object contains information about job including the job_name that identifies which job was run, the data attribute that contains information about the input audio, and the tasks attribute has holds results of each task performed. Each task object with tasks has a message_type attribute identifies the type of output produced by the task which; it can be used to determine which fields will be available in the analysis attribute. See the Enterprise API Message Reference for a reference on what data will be available for each message type (ex. GlobalScorerResult and RegionScorerResult ) The results can be converted to a Python dict and iterated through programmatically: # convert to Python dict and iterate for result in json . loads ( response . to_json ()): label = result [ 'data' ][ 0 ][ 'label' ] for task_type , task_result in result [ 'tasks' ] . items (): plugin = task_result [ 0 ][ 'plugin' ] domain = task_result [ 0 ][ 'domain' ] analysis = task_result [ 0 ][ 'analysis' ] print ( f \" { task_type } on { label } using { plugin } / { domain } : { analysis } \" ) Which produces the following output: SAD on Edmund_Yeo_voice_ch . wav using sad - dnn - v8 .0.0 / multi - v1 : { 'region' : [{ 'start_t' : 0.0 , 'end_t' : 28.95 , 'class_id' : 'speech' , 'score' : 0.0 }]} LID on Edmund_Yeo_voice_ch . wav using lid - embedplda - v4 .0.0 / multi - v1 : { 'score' : [{ 'class_id' : 'Mandarin' , 'score' : 6.0677257 }, { 'class_id' : 'Korean' , 'score' : - 1.3737706 }, { 'class_id' : 'English' , 'score' : - 4.7524 }, { 'class_id' : 'Vietnamese' , 'score' : - 4.7861123 }, { 'class_id' : 'Japanese' , 'score' : - 6.1542406 }, { 'class_id' : 'Iraqi Arabic' , 'score' : - 8.356203 }, { 'class_id' : 'Levantine Arabic' , 'score' : - 8.655978 }, { 'class_id' : 'Tagalog' , 'score' : - 8.790578 }, { 'class_id' : 'French' , 'score' : - 9.969812 }, { 'class_id' : 'Modern Standard Arabic' , 'score' : - 10.236586 }, { 'class_id' : 'Iranian Persian' , 'score' : - 11.295098 }, { 'class_id' : 'Amharic' , 'score' : - 12.617832 }, { 'class_id' : 'Spanish' , 'score' : - 14.459879 }, { 'class_id' : 'Portuguese' , 'score' : - 14.844175 }, { 'class_id' : 'Russian' , 'score' : - 15.082703 }, { 'class_id' : 'Pashto' , 'score' : - 15.151446 }]} SID on Edmund_Yeo_voice_ch . wav using sid - dplda - v3 .0.0 / multi - v1 : { 'score' : [{ 'class_id' : 'EDMUND_YEO' , 'score' : 10.516918 }]} SAD on Edmund_Yeo_voice_en . wav using sad - dnn - v8 .0.0 / multi - v1 : { 'region' : [{ 'start_t' : 0.0 , 'end_t' : 36.78 , 'class_id' : 'speech' , 'score' : 0.0 }]} LID on Edmund_Yeo_voice_en . wav using lid - embedplda - v4 .0.0 / multi - v1 : { 'score' : [{ 'class_id' : 'English' , 'score' : 5.056248 }, { 'class_id' : 'Levantine Arabic' , 'score' : - 1.0983323 }, { 'class_id' : 'Tagalog' , 'score' : - 1.4544584 }, { 'class_id' : 'Iraqi Arabic' , 'score' : - 1.7203265 }, { 'class_id' : 'Vietnamese' , 'score' : - 1.7773396 }, { 'class_id' : 'French' , 'score' : - 2.4661431 }, { 'class_id' : 'Korean' , 'score' : - 3.766336 }, { 'class_id' : 'Mandarin' , 'score' : - 3.8002808 }, { 'class_id' : 'Japanese' , 'score' : - 4.7714725 }, { 'class_id' : 'Modern Standard Arabic' , 'score' : - 5.692904 }, { 'class_id' : 'Spanish' , 'score' : - 6.5970984 }, { 'class_id' : 'Iranian Persian' , 'score' : - 6.6558266 }, { 'class_id' : 'Portuguese' , 'score' : - 8.045044 }, { 'class_id' : 'Amharic' , 'score' : - 8.093353 }, { 'class_id' : 'Pashto' , 'score' : - 9.445313 }, { 'class_id' : 'Russian' , 'score' : - 11.597008 }]} SID on Edmund_Yeo_voice_en . wav using sid - dplda - v3 .0.0 / multi - v1 : { 'score' : [{ 'class_id' : 'EDMUND_YEO' , 'score' : 7.9177084 }]} Job Cancellation Request OLIVE 6.0 introduced the ability to cancel specific pending or in-progress jobs with an API message request. Among other use cases, this can help recover from large mistakenly-submitted batch jobs, or cancel submissions that may be overcome-by-events, or any other reason a cancellation may be necessary, without requiring a server restart. Cancelling enrollment requests is not supported. The cancellation message depends on using a user generated message ID in the analyze call. from olivepy.api.olive_async_client import AsyncOliveClient client = AsyncOliveClient ( \"example client\" ) client . connect () msg_id = str ( uuid . uuid4 ()) client . analyze_regions ( \"asr-end2end\" , \"spanish-speed-v1\" , input_data , result_callback , msg_id = msg_id ) client . cancel_server_jobs ([ msg_id ]) while True : response = client . get_active ( callback = None ) active_response = response . get_response () print ( active_response ) if len ( active_response . active_jobs ) == 0 : print ( \"jobs cancelled\" ) break time . sleep ( 0.5 )","title":"olivepy Primer"},{"location":"olivepyPrimer.html#integrating-with-the-olive-python-api-olivepy","text":"olivepy is an OLIVE Python API that's built on top of the OLIVE Enterprise API. New Python integrators are advised to read through the Enterprise API Primer for an overview of key concepts that also apply to olivepy .","title":"Integrating with the OLIVE Python API (olivepy)"},{"location":"olivepyPrimer.html#installation","text":"olivepy is distributed as a Python wheel package for easy installation. To install, navigate into the api folder distributed with your OLIVE delivery, and install it using your native pip3 : $ pip3 install olivepy-*-py3-none-any.whl A source distribution is also available as olivepy-*.tar.gz , but it should only be needed for unique operating environments and most clients should use the wheel instead.","title":"Installation"},{"location":"olivepyPrimer.html#dependencies","text":"olivepy direct dependencies include: protobuf soundfile numpy requests zmq These will be downloaded by pip when the olivepy package is installed.","title":"Dependencies"},{"location":"olivepyPrimer.html#integration","text":"","title":"Integration"},{"location":"olivepyPrimer.html#quickstart","text":"Here's a complete example for those in a hurry: from olivepy.api.olive_async_client import AsyncOliveClient from olivepy.messaging.msgutil import package_audio , serialize_file , InputTransferType from olivepy.messaging.olive_pb2 import Audio # connect client = AsyncOliveClient ( \"example client\" ) client . connect () print ( \"Client connected...\" ) print ( \" \\n -- Connection Information --\" ) print ( f \"Client: { client . client_id } \" ) print ( f \"Server: { client . server_address } \" ) print ( f \"Port: { client . server_status_port } \" ) # find a Language ID (LID) plugin plugin_request = client . request_plugins () plugin_response = plugin_request . get_response () for plugin in plugin_response . plugins : if plugin . task == \"LID\" : plugin_id = plugin . id plugin_domain = plugin . domain [ 0 ] . id break # run Language ID (LID) on serialized audio file print ( \" \\n -- Language ID Results --\" ) packaged_audio = package_audio ( Audio (), serialize_file ( \"Edmund_Yeo_voice_ch.wav\" ), mode = InputTransferType . SERIALIZED ) analyze_request = client . analyze_global ( plugin_id , plugin_domain , packaged_audio , None ) analyze_response = analyze_request . get_response () top_scoring = analyze_response . score [ 0 ] for score in analyze_response . score : print ( f \" { score . class_id } = { score . score } \" ) if score . score > top_scoring . score : top_scoring = score print ( f \" \\n Top scoring: { top_scoring . class_id } ( { top_scoring . score } )\" ) # disconnect client . disconnect () print ( \" \\n Client disconnected!\" )","title":"Quickstart"},{"location":"olivepyPrimer.html#establish-server-connection","text":"Before making any task request, a client must establish a connection with the server. By default, the OLIVE server listens on ports 5588 (request port) and 5589 (status port) for client connection and status requests. These ports are configurable, but if the server has not been instructed to change its listening ports, the code below should establish a connection. from olivepy.api.olive_async_client import AsyncOliveClient client = AsyncOliveClient ( \"example client\" ) client . connect () ... client . disconnect () The request port (5588 by default) is used for call and response messages. Each request message sent to this port is guaranteed a response from the server (this is why the messages in the API Message Reference are often suffixed with 'Request' and 'Result'). There is no need to poll the server for information on a submitted request, as the result/response for the a request is returned to the client as soon as it is available. The status port (5589 by default) is used by the Server to publish health and status messages (Heartbeat) to client(s). Clients can not send requests on this port.","title":"Establish Server Connection"},{"location":"olivepyPrimer.html#tls-encrypted-connections","text":"As of 5.6.0, olivepy supports TLS encrypted communication with the OLIVE Server. Encrypted connections require the OLIVE Server to be configured and started with TLS. For example, OLIVE Martini must be started with the flag --tls_server_only or --tls_server_and_client . By default, OLIVE listens on port 5588 for encrypted connections. To set up a encrypted connection using olivepy, connect to the server with client.secure_connect() function and provide the required PEM ( *.crt and *.key ) certificate arguments: from olivepy.api.olive_async_client import AsyncOliveClient client = AsyncOliveClient ( \"example client\" ) client . secure_connect ( certfile = \"path/to/certificate.crt\" , keyfile = \"/path/to/keyfile.key\" , password = \"keyfilePassword\" , ca_bundle_path = \"path/to/cert/authority.crt\" ) Connecting to the OLIVE Server using TLS encryption takes slightly longer than a standard non-encrypted connection; however, after the initial connection is made, all subsequent requests will run over TLS with no time penalty.","title":"TLS Encrypted Connections"},{"location":"olivepyPrimer.html#request-available-plugins","text":"In order to submit most server requests, the client must specify the plugin and domain to handle the request. To obtain the handle of a targeted plugin, the client first requests a list of all currently available valid plugins from the server. From the returned plugins list, the client looks up the specific plugin handle by the plugin's unique name (id) and its associated trait for the task to be performed. This handle can then be used in a future request message sent to the server. from olivepy.api.olive_async_client import AsyncOliveClient client = AsyncOliveClient ( \"example client\" ) client . connect () plugin_request = client . request_plugins () plugin_response = plugin_request . get_response () for plugin in plugin_response . plugins : print ( f 'Plugin { plugin . id } ( { plugin . task } , { plugin . group } ) { plugin . version } has { len ( plugin . domain ) } domain(s):' ) for domain in plugin . domain : print ( f ' \\t Domain: { domain . id } , Description: { domain . desc } ' ) client . disconnect () The targeted plugin's id and domain can then be used with subsequent requests. Note Each plugin has additional useful information that can be retrieved as needed: task : The type of task (e.g. LID, SID, SAD, KWS, AED, etc.) trait : See Traits group : Allows additional grouping of plugins such as Keyword, Speaker, Language, etc","title":"Request Available Plugins"},{"location":"olivepyPrimer.html#audio-submission-guidelines","text":"One of the core client activities is submitting Audio with a request. olivepy provides three ways for a client to package audio data to send to the OLIVE server: file path buffer of raw audio sample data serialized file buffer object The enum InputTransferType is used to specify they type of audio transfer to use. When the client and the OLIVE server share the same file system, the easiest way for the client to send audio data to the server is by specifying the audio's file path on disk along with InputTransferType.PATH : packaged_audio = package_audio ( Audio (), \"/home/olive/samples/Edmund_Yeo_voice_ch.wav\" , mode = InputTransferType . PATH ) When the client and the server don't share the same file system, as in the case of a client making a remote connection to the OLIVE server running in docker (Martini), it is necessary to send the client's local audio files using InputTransferType.SERIALIZED or InputTransferType.DECODED : packaged_audio = package_audio ( Audio (), serialize_file ( \"Edmund_Yeo_voice_ch.wav\" ), mode = InputTransferType . SERIALIZED ) Serialized Buffer Note Sending audio data as a serialized file buffer ensures that all audio header information is provided intact to the server. This allows the server to properly decode and process the audio once its received, since it can directly access the bit depth, encoding type, sample rate and other necessary information from the header itself. The tradeoff with serialized files is that there may be additional overhead needed to process the audio into a consumable form. If the client and server reside on the same hardware and file system, it is advisable to simply pass filepaths when possible. This saves the memory overhead burden of both the client and server loading audio into memory. When using 16-bit PCM .wav files, or if first processing audio samples to be in one of the formats supported by the OLIVE server , it's also possible to pass a non-serialized buffer of raw samples along with InputTransferType.DECODED : packaged_audio = package_audio ( Audio (), audio_samples , mode = InputTransferType . DECODED , num_channels = 1 , sample_rate = 8000 , num_samples = 1080 ) Buffer of Raw Samples Caution When submitting audio to the server as a buffer of raw samples, it is important to include information characterizing the audio, such as the bit depth, audio encoding, sample rate, and number of channels, to ensure the server knows how to properly treat the buffer it receives.","title":"Audio Submission Guidelines"},{"location":"olivepyPrimer.html#synchronous-vs-asynchronous-message-submission","text":"olivepy allows the client to choose between processing a task request synchronously or asynchronously. Processing a task request synchronously means the client will block and wait for the task result to return before proceeding to other task requests. On the other hand, asynchronous processing means the client will not wait for the result to come back before moving on, allowing several jobs to be submitted in parallel. The olivepy.api.olive_async_client.AsyncOliveClient class supports both synchronous and synchronous submissions. Most requests sent to the server accept a typing.Callable that will be executed when the asynchronous request has been processed. When a synchronous request is desired (which is the case in most of the examples in this reference guide for conveience), then None needs to be explicitly passed in place of the argument reserved for the typing.Callable . Legacy OliveClient Note There is also a legacy olivepy.api.oliveclient.OliveClient that only supports synchronous submissions, but it will be deprecated in the future and all integrators are encouraged to switch over to the AsyncOliveClient .","title":"Synchronous vs. Asynchronous Message Submission"},{"location":"olivepyPrimer.html#frame-score-request","text":"The snippet below submits a Frame Score Request to the connected server. analyze_request = client . analyze_frames ( plugin_id , plugin_domain , packaged_audio , None ) analyze_response = analyze_request . get_response () for i , score in enumerate ( analyze_response . result [ 0 ] . score ): print ( f \"frame[ { i } ]= { score } \" ) Note Only a plugin that support the FrameScorer trait can handle this request. See the Frame Scorer Messages section of the Enterprise API Message Reference for details about the FrameScorerRequest and FrameScorerResult objects.","title":"Frame Score Request"},{"location":"olivepyPrimer.html#global-score-request","text":"The snippet below submits a Global Score Request to the connected server. analyze_request = client . analyze_global ( plugin_id , plugin_domain , packaged_audio , None ) analyze_response = analyze_request . get_response () for score in analyze_response . score : print ( f \" { score . class_id } = { score . score } \" ) Note The code required to submit a GlobalScorerRequest message doesn't care what type of plugin is going to be doing the scoring, as long as the plugin implements the GlobalScorer Trait. This means that the exact same code can be used for submitting audio to global scoring SID plugins, LID plugins, or any other global scoring plugin. See the Global Scorer Messages section of the Enterprise API Message Reference for details about the GlobalScorerRequest and GlobalScorerResult objects.","title":"Global Score Request"},{"location":"olivepyPrimer.html#region-score-request","text":"The snippet below submits a Region Score Request to the connected server. analyze_request = client . analyze_regions ( plugin_id , plugin_domain , packaged_audio , None ) analyze_response = analyze_request . get_response () for region in analyze_response . region : print ( f \" { region . start_t : .2f } - { region . end_t : .2f } secs { region . class_id } \" ) Note Only a plugin that support the RegionScorer trait can handle this request. See the Region Scorer Messages section of the Enterprise API Message Reference for details about the RegionScorerRequest and RegionScorerResult objects.","title":"Region Score Request"},{"location":"olivepyPrimer.html#enrollments","text":"See the appropriate section of the OLIVE Plugin Overview documentation for a description of Enrollments .","title":"Enrollments"},{"location":"olivepyPrimer.html#enroll","text":"The snippet below enrolls a sample audio file for speaker EDMUND_YEO : # submit enrollment (i.e. Class Modification Request) for Speaker ID plugin using a serialized audio file class_modification_request = client . enroll ( plugin_id , plugin_domain , \"EDMUND_YEO\" , \"Edmund_Yeo_voice_ch.wav\" , None , mode = InputTransferType . SERIALIZED ) class_modification_response = class_modification_request . get_response () for addition_result in class_modification_response . addition_result : status_label = \"succeeded\" if addition_result . successful else \"failed\" additional_details = f \"Additional details for request: { addition_result . message } \" if addition_result . message else \"\" print ( f \"Update to { plugin_id } / { plugin_domain } { status_label } ! { additional_details } \" ) Not all plugins support enrollments. Refer to the individual plugin documentation to confirm if it supports class enrollment before proceeding.","title":"Enroll"},{"location":"olivepyPrimer.html#unenroll","text":"The snippet below unenrolls speaker EDMUND_YEO : # submit unenrollment (i.e. Class Modification Request) for Speaker ID plugin for speaker class_modification_request = client . unenroll ( plugin_id , plugin_domain , \"EDMUND_YEO\" , None ) if ( class_modification_request . is_successful ()): print ( \"Unenrollment was successful!\" ) else : print ( f \"Unenrollment failed with error: { class_modification_request . get_error () } \" )","title":"Unenroll"},{"location":"olivepyPrimer.html#vectorization-and-stateless-score-request","text":"See the appropriate section of the OLIVE Plugin Overview documentation for a description of Vectorization and Stateless Scoring.","title":"Vectorization and Stateless Score Request"},{"location":"olivepyPrimer.html#vectorization","text":"The snippet below creates audio vectors that can be used in future stateless scoring requests: packaged_audios = [] for audio_file_path in audio_file_paths : packaged_audios . append ( package_audio ( Audio (), serialize_file ( audio_file_path ), mode = InputTransferType . SERIALIZED )) response = client . vectorize_audio ( plugin_id , plugin_domain , packaged_audios , mode = InputTransferType . SERIALIZED , callback = None ) # generate audio vector(s) that can be used in future stateless scoring requests audio_vectors = [] for vector_result in response . get_response () . vector_result : if not vector_result . successful : print ( f \"Could not vectorize audio: { vector_result . message } \" ) else : audio_vectors . append ( vector_result . audio_vector )","title":"Vectorization"},{"location":"olivepyPrimer.html#serialization-and-deserialization","text":"The snippet below provides an example of how to serialize and deserialize an audio vector: with tempfile . TemporaryDirectory ( f \"_ { class_id_label } \" ) as temp_dir : for audio_vector in audio_vectors : # serialize and save audio vector(s) to disk (for demo purposes) with open ( os . path . join ( temp_dir , str ( uuid . uuid4 ())), 'wb' ) as f : f . write ( serialize_message ( audio_vector )) # reload audio vector(s) from disk (for demo purposes) reloaded_audio_vectors = [] for audio_vector_path in glob . glob ( os . path . join ( temp_dir , \"*\" )): with open ( audio_vector_path , 'rb' ) as f : reloaded_audio_vectors . append ( deserialize_message ( f . read (), AudioVector )) Warning Saving to disk is FOR DEMO PURPOSES ONLY . A production system should consider using SQLite or similar technology which allows associating a speaker label String to binary data blobs rather than storing on disk if they require long-term persistent storage. The examples are to illustrate serialization and deserialization of audio vectors.","title":"Serialization and Deserialization"},{"location":"olivepyPrimer.html#stateless-score-request","text":"The snippet below shows a Stateless Score Request that uses the in-memory audio vectors: # associate audio vectors with a specific class (such as a Speaker) class_vectors = { class_id_label : [ reloaded_audio_vector for reloaded_audio_vector in reloaded_audio_vectors ] } # score using associated audio vectors for packaged_audio in packaged_audios : analyze_request = client . analyze_global_stateless ( plugin_id , plugin_domain , packaged_audio , class_vectors , None ) analyze_response = analyze_request . get_response () print ( analyze_response )","title":"Stateless Score Request"},{"location":"olivepyPrimer.html#workflow-integration","text":"In addition to the basic API integration mechanisms described above, olivepy also supports OLIVE Workflows. See Workflows for a primer on OLIVE Workflows and then continue reading below for olivepy specific details.","title":"Workflow Integration"},{"location":"olivepyPrimer.html#quickstart_1","text":"Here's a complete workflow example for those in a hurry: import os from olivepy.api.olive_async_client import AsyncOliveClient from olivepy.api.workflow import OliveWorkflowDefinition from olivepy.messaging.msgutil import InputTransferType # connect client = AsyncOliveClient ( \"example client\" ) client . connect () # load the workflow_definition into the AsyncOliveClient to get a workflow helper object workflow_definition = OliveWorkflowDefinition ( \"sad_lid_sid.workflow.json\" ) sad_lid_sid_workflow = workflow_definition . create_workflow ( client ) # package audio as a serialized buffer serialized_audio = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' )) # submit workflow request on serialized audio file response = sad_lid_sid_workflow . analyze ([ serialized_audio ]) print ( response . to_json ( indent = 2 )) # disconnect client . disconnect ()","title":"Quickstart"},{"location":"olivepyPrimer.html#initializing-a-workflow","text":"As described in Workflows , workflow logic is encapsulated in a Workflow Definition file distributed as either binary (i.e. *.workflow - deprecated) or JSON (i.e. *.workflow.json ). Workflows are preconfigured to perform tasks such as Speech Activity Detection (SAD), Language Identification (LID), Speaker Identification (SID), etc. with a single call to the OLIVE server. These Workflow Definition files must be initialized (aka 'created') with the olivepy client before the workflow can be used. The snippet below initializes a workflow with the client : workflow_definition = OliveWorkflowDefinition ( \"sad_lid_sid.workflow.json\" ) sad_lid_sid_workflow = workflow_definition . create_workflow ( client ) A workflow 'helper' object ( OliveWorkflow ) is returned and is used to submit audio files directly to that workflow for analysis, enrollment, or unenrollment. In the snippet above, sad_lid_sid_workflow is the 'helper'.","title":"Initializing a Workflow"},{"location":"olivepyPrimer.html#packaging-audio","text":"The same Audio Submission Guidelines discussed earlier in this guide apply to workflows and each of the following are supported for workflows: InputTransferType.PATH InputTransferType.SERIALIZED InputTransferType.DECODED The difference with workflows is that the workflow 'helper' is used to package audios rather than olivepy client directly; the audio is wrapped in a WorkflowDataRequest that is submitted to OLIVE for processing: # package audio as a serialized buffer using the workflow 'helper' packaged_audio = workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' ))","title":"Packaging Audio"},{"location":"olivepyPrimer.html#multi-channel-audio","text":"The default workflow behavior is to merge multi-channel audio into a single channel, which is known as MONO mode. To perform analysis on each channel individually instead of a merged channel, the Workflow Definition must be authored with a mode of SPLIT . When using the split mode, each channel in a multi-channel audio input is \"split\" into a job. Here is a mode within a workflow definition file that merges multi-channel audio into a single channel audio input: \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, ... and one that handles each channel individually: \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"SPLIT\" },","title":"Multi-channel Audio"},{"location":"olivepyPrimer.html#audio-annotations","text":"The audio submitted for analysis (or enrollment) can be annotated with start/end regions when packaging audio using the package_audio() function. The snippet below specifies two regions within a file: # Provide annotations for two regions: 0.3 to 1.7 seconds, and 2.4 to 3.3 seconds in audio regions = [( 0.3 , 1.7 ), ( 2.4 , 3.3 )] packaged_audio = workflow . package_audio ( 'Edmund_Yeo_voice_ch.wav' , InputTransferType . AUDIO_SERIALIZED , annotations = regions )","title":"Audio Annotations"},{"location":"olivepyPrimer.html#submitting-audio","text":"Submitting audio for analysis, enrollment, and unenrollment using workflows is supported.","title":"Submitting Audio"},{"location":"olivepyPrimer.html#analysis","text":"The snippet below packages and submits an audio file to the workflow 'helper' for analysis: # package audio as a serialized buffer serialized_audio = workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' )) # submit workflow request on serialized audio file response = workflow . analyze ([ serialized_audio ])","title":"Analysis"},{"location":"olivepyPrimer.html#batch-request","text":"The analyze function accepts a list of audio files to so multiple files can be analyzed as a complete 'batch' request: # package audio files for analysis serialized_audio_1 = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' )) serialized_audio_2 = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_en.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_en.wav' )) # submit workflow request on multiple serialized audio files response = sad_lid_sid_workflow . analyze ([ serialized_audio_1 , serialized_audio_2 ])","title":"Batch request"},{"location":"olivepyPrimer.html#enrollments_1","text":"","title":"Enrollments"},{"location":"olivepyPrimer.html#enroll_1","text":"Some workflows support enrollment for one or more jobs. To list the jobs in a workflow that support enrollment, use the get_enrollment_job_names() function: print ( f \"Enrollment Jobs: { sid_enrollments_workflow . get_enrollment_job_names () } \" ) # Enrollment jobs '['SID Enrollment']' To enroll a speaker via this workflow, use the workflow 'helper's enroll function: # find a Speaker Identification (SID) enrollment job name for enrollment_job_name in sid_enrollments_workflow . get_enrollment_job_names (): if enrollment_job_name . startswith ( \"SID\" ): sid_enrollment_job_name = enrollment_job_name break # submit enrollment (i.e. Class Modification Request) for Speaker ID plugin using a serialized audio file speaker_label = \"EDMUND_YEO\" packaged_audio = sid_enrollments_workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( \"Edmund_Yeo_voice_ch.wav\" )) class_modification_response = sid_enrollments_workflow . enroll ([ packaged_audio ], speaker_label , [ sid_enrollment_job_name ]) Since there was only one enrollment job in the workflow definition, the enrollment request could have been made without specifying the job name. However, it is a best practice to explicitly request the enrollment job by name. Note also that not all workflows support enrollment. Please check that the workflow being used supports this before submitting an enrollment request. To confirm the new speaker name was added: sid_class_ids = sid_enrollments_workflow . get_analysis_class_ids () print ( sid_class_ids . to_json ( indent = 1 )) Which produces the following output: { \"job_class\" : [ { \"job_name\" : \"SID analysis\" , \"task\" : [ { \"task_name\" : \"SID\" , \"class_id\" : [ \"EDMUND_YEO\" ] } ] } ] } 'Class ID' is a general term used to describe 'labels' that apply to the specific plugin. Here, class_id represents all the speaker labels that are enrolled.","title":"Enroll"},{"location":"olivepyPrimer.html#unenroll_1","text":"Workflow definitions that support enrollment often also support unenrollment. Similar to enrollment, use the get_unenrollment_job_names() to get a list of jobs that support unenrollment, send unenrollment requests to unenroll , and use get_analysis_class_ids() to list. Here is an example snippet of all three: # find Speaker Identification (SID) unenrollment job name for unenrollment_job_name in sid_enrollments_workflow . get_unenrollment_job_names (): if unenrollment_job_name . startswith ( \"SID\" ): sid_unenrollment_job_name = unenrollment_job_name break # submit unenrollment (i.e. Class Modification Request) for Speaker ID plugin speaker_label = \"EDMUND_YEO\" class_modification_response = sid_enrollments_workflow . unenroll ( speaker_label , [ sid_unenrollment_job_name ]) sid_class_ids = sid_enrollments_workflow . get_analysis_class_ids () print ( sid_class_ids . to_json ( indent = 1 )) Which produces the following output: { \"job_class\" : [ { \"job_name\" : \"SID analysis\" , \"task\" : [ { \"task_name\" : \"SID\" } ] } ] } There is no class_id attribute because the speaker was successfully unenrolled and there aren't any others enrolled in the system.","title":"Unenroll"},{"location":"olivepyPrimer.html#parsing-workflow-responses","text":"A successful workflow request produces a response that includes information about the results. Information is grouped into one or more 'jobs', where a job is includes the name, tasks that were performed as part of the job, the results of each task, and potentially information about the input audio. For example the following code snippet: # package audio files for analysis serialized_audio_1 = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_ch.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_ch.wav' )) serialized_audio_2 = sad_lid_sid_workflow . package_audio ( \"Edmund_Yeo_voice_en.wav\" , InputTransferType . SERIALIZED , label = os . path . basename ( 'Edmund_Yeo_voice_en.wav' )) # submit workflow request on multiple serialized audio files response = sad_lid_sid_workflow . analyze ([ serialized_audio_1 , serialized_audio_2 ]) print ( response . to_json ( indent = 2 )) would produce the following output: Example Workflow Output (click to expand) [ { \"job_name\" : \"SAD, LID, SID analysis\" , \"data\" : [ { \"data_id\" : \"Edmund_Yeo_voice_ch.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 29.2825 , \"number_channels\" : 1 , \"label\" : \"Edmund_Yeo_voice_ch.wav\" , \"id\" : \"cbc95af3f693de48654a72ec288adb8ad182a8f86993a3d0d42e3e2a5b4d5548\" } ], \"tasks\" : { \"SAD\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v8.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 28.95 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"LID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"lid-embedplda-v4.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"Mandarin\" , \"score\" : 6.0677257 }, { \"class_id\" : \"Korean\" , \"score\" : -1.3737706 }, { \"class_id\" : \"English\" , \"score\" : -4.7524 }, { \"class_id\" : \"Vietnamese\" , \"score\" : -4.7861123 }, { \"class_id\" : \"Japanese\" , \"score\" : -6.1542406 }, { \"class_id\" : \"Iraqi Arabic\" , \"score\" : -8.356203 }, { \"class_id\" : \"Levantine Arabic\" , \"score\" : -8.655978 }, { \"class_id\" : \"Tagalog\" , \"score\" : -8.790578 }, { \"class_id\" : \"French\" , \"score\" : -9.969812 }, { \"class_id\" : \"Modern Standard Arabic\" , \"score\" : -10.236586 }, { \"class_id\" : \"Iranian Persian\" , \"score\" : -11.295098 }, { \"class_id\" : \"Amharic\" , \"score\" : -12.617832 }, { \"class_id\" : \"Spanish\" , \"score\" : -14.459879 }, { \"class_id\" : \"Portuguese\" , \"score\" : -14.844175 }, { \"class_id\" : \"Russian\" , \"score\" : -15.082703 }, { \"class_id\" : \"Pashto\" , \"score\" : -15.151446 } ] } } ], \"SID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"SID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"sid-dplda-v3.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"EDMUND_YEO\" , \"score\" : 10.516918 } ] } } ] } }, { \"job_name\" : \"SAD, LID, SID analysis\" , \"data\" : [ { \"data_id\" : \"Edmund_Yeo_voice_en.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 37.0825 , \"number_channels\" : 1 , \"label\" : \"Edmund_Yeo_voice_en.wav\" , \"id\" : \"65a53db9b3ac1d1571082512cba37665634712738fed530ca00b3d5922e0d129\" } ], \"tasks\" : { \"SAD\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v8.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 36.78 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"LID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"lid-embedplda-v4.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"English\" , \"score\" : 5.056248 }, { \"class_id\" : \"Levantine Arabic\" , \"score\" : -1.0983323 }, { \"class_id\" : \"Tagalog\" , \"score\" : -1.4544584 }, { \"class_id\" : \"Iraqi Arabic\" , \"score\" : -1.7203265 }, { \"class_id\" : \"Vietnamese\" , \"score\" : -1.7773396 }, { \"class_id\" : \"French\" , \"score\" : -2.4661431 }, { \"class_id\" : \"Korean\" , \"score\" : -3.766336 }, { \"class_id\" : \"Mandarin\" , \"score\" : -3.8002808 }, { \"class_id\" : \"Japanese\" , \"score\" : -4.7714725 }, { \"class_id\" : \"Modern Standard Arabic\" , \"score\" : -5.692904 }, { \"class_id\" : \"Spanish\" , \"score\" : -6.5970984 }, { \"class_id\" : \"Iranian Persian\" , \"score\" : -6.6558266 }, { \"class_id\" : \"Portuguese\" , \"score\" : -8.045044 }, { \"class_id\" : \"Amharic\" , \"score\" : -8.093353 }, { \"class_id\" : \"Pashto\" , \"score\" : -9.445313 }, { \"class_id\" : \"Russian\" , \"score\" : -11.597008 } ] } } ], \"SID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"SID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"sid-dplda-v3.0.0\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"EDMUND_YEO\" , \"score\" : 7.9177084 } ] } } ] } } ] In the above output, note that the root array contains two objects because two audio files were submitted to the workflow. Each object contains information about job including the job_name that identifies which job was run, the data attribute that contains information about the input audio, and the tasks attribute has holds results of each task performed. Each task object with tasks has a message_type attribute identifies the type of output produced by the task which; it can be used to determine which fields will be available in the analysis attribute. See the Enterprise API Message Reference for a reference on what data will be available for each message type (ex. GlobalScorerResult and RegionScorerResult ) The results can be converted to a Python dict and iterated through programmatically: # convert to Python dict and iterate for result in json . loads ( response . to_json ()): label = result [ 'data' ][ 0 ][ 'label' ] for task_type , task_result in result [ 'tasks' ] . items (): plugin = task_result [ 0 ][ 'plugin' ] domain = task_result [ 0 ][ 'domain' ] analysis = task_result [ 0 ][ 'analysis' ] print ( f \" { task_type } on { label } using { plugin } / { domain } : { analysis } \" ) Which produces the following output: SAD on Edmund_Yeo_voice_ch . wav using sad - dnn - v8 .0.0 / multi - v1 : { 'region' : [{ 'start_t' : 0.0 , 'end_t' : 28.95 , 'class_id' : 'speech' , 'score' : 0.0 }]} LID on Edmund_Yeo_voice_ch . wav using lid - embedplda - v4 .0.0 / multi - v1 : { 'score' : [{ 'class_id' : 'Mandarin' , 'score' : 6.0677257 }, { 'class_id' : 'Korean' , 'score' : - 1.3737706 }, { 'class_id' : 'English' , 'score' : - 4.7524 }, { 'class_id' : 'Vietnamese' , 'score' : - 4.7861123 }, { 'class_id' : 'Japanese' , 'score' : - 6.1542406 }, { 'class_id' : 'Iraqi Arabic' , 'score' : - 8.356203 }, { 'class_id' : 'Levantine Arabic' , 'score' : - 8.655978 }, { 'class_id' : 'Tagalog' , 'score' : - 8.790578 }, { 'class_id' : 'French' , 'score' : - 9.969812 }, { 'class_id' : 'Modern Standard Arabic' , 'score' : - 10.236586 }, { 'class_id' : 'Iranian Persian' , 'score' : - 11.295098 }, { 'class_id' : 'Amharic' , 'score' : - 12.617832 }, { 'class_id' : 'Spanish' , 'score' : - 14.459879 }, { 'class_id' : 'Portuguese' , 'score' : - 14.844175 }, { 'class_id' : 'Russian' , 'score' : - 15.082703 }, { 'class_id' : 'Pashto' , 'score' : - 15.151446 }]} SID on Edmund_Yeo_voice_ch . wav using sid - dplda - v3 .0.0 / multi - v1 : { 'score' : [{ 'class_id' : 'EDMUND_YEO' , 'score' : 10.516918 }]} SAD on Edmund_Yeo_voice_en . wav using sad - dnn - v8 .0.0 / multi - v1 : { 'region' : [{ 'start_t' : 0.0 , 'end_t' : 36.78 , 'class_id' : 'speech' , 'score' : 0.0 }]} LID on Edmund_Yeo_voice_en . wav using lid - embedplda - v4 .0.0 / multi - v1 : { 'score' : [{ 'class_id' : 'English' , 'score' : 5.056248 }, { 'class_id' : 'Levantine Arabic' , 'score' : - 1.0983323 }, { 'class_id' : 'Tagalog' , 'score' : - 1.4544584 }, { 'class_id' : 'Iraqi Arabic' , 'score' : - 1.7203265 }, { 'class_id' : 'Vietnamese' , 'score' : - 1.7773396 }, { 'class_id' : 'French' , 'score' : - 2.4661431 }, { 'class_id' : 'Korean' , 'score' : - 3.766336 }, { 'class_id' : 'Mandarin' , 'score' : - 3.8002808 }, { 'class_id' : 'Japanese' , 'score' : - 4.7714725 }, { 'class_id' : 'Modern Standard Arabic' , 'score' : - 5.692904 }, { 'class_id' : 'Spanish' , 'score' : - 6.5970984 }, { 'class_id' : 'Iranian Persian' , 'score' : - 6.6558266 }, { 'class_id' : 'Portuguese' , 'score' : - 8.045044 }, { 'class_id' : 'Amharic' , 'score' : - 8.093353 }, { 'class_id' : 'Pashto' , 'score' : - 9.445313 }, { 'class_id' : 'Russian' , 'score' : - 11.597008 }]} SID on Edmund_Yeo_voice_en . wav using sid - dplda - v3 .0.0 / multi - v1 : { 'score' : [{ 'class_id' : 'EDMUND_YEO' , 'score' : 7.9177084 }]}","title":"Parsing Workflow Responses"},{"location":"olivepyPrimer.html#job-cancellation-request","text":"OLIVE 6.0 introduced the ability to cancel specific pending or in-progress jobs with an API message request. Among other use cases, this can help recover from large mistakenly-submitted batch jobs, or cancel submissions that may be overcome-by-events, or any other reason a cancellation may be necessary, without requiring a server restart. Cancelling enrollment requests is not supported. The cancellation message depends on using a user generated message ID in the analyze call. from olivepy.api.olive_async_client import AsyncOliveClient client = AsyncOliveClient ( \"example client\" ) client . connect () msg_id = str ( uuid . uuid4 ()) client . analyze_regions ( \"asr-end2end\" , \"spanish-speed-v1\" , input_data , result_callback , msg_id = msg_id ) client . cancel_server_jobs ([ msg_id ]) while True : response = client . get_active ( callback = None ) active_response = response . get_response () print ( active_response ) if len ( active_response . active_jobs ) == 0 : print ( \"jobs cancelled\" ) break time . sleep ( 0.5 )","title":"Job Cancellation Request"},{"location":"plugins.html","text":"Plugin Documentation This page contains high level information about OLIVE plugins and their associated concepts - for more detailed information about integrating with plugins, please refer to the more low-level focused Plugin API Integration Details pages. For more information about a specific plugin, please find its info page link in the list at the bottom of this page . Anatomy of a Plugin OLIVE Plugins encapsulate the actual audio processing technologies and capabilities of the OLIVE system into modular pieces that facilitate system upgrades, capability additions, incremental updates, and provide other benefits such as allowing for tuning models to improve processing in targeted audio conditions or allowing for multiple tools or options to choose from to accomplish a given task. Each plugin has a specific type, which defines the task that it is capable of performing (see Plugin Types below). They consist of two parts, the plugin proper, which contains the recipe or algorithm information on how to perform the task, and one or more domains, which contain the data models used to run the algorithm and perform the function. Since plugins are generally machine learning-based the same algorithm may have multiple strengths based on the specific data used to train it (e.g. telephone audio, push-to-talk audio, high effort vocalization, conversational speech). This separation between the algorithm and the data model allows us to deliver new functionality that is based on training data independently of the algorithm by delivering a new domain. Classes and enrollments (described below) are associated with a domain. Plugin Types Plugin function types refer to the core task the plugin performs and have standard abbreviations. Most plugins are designed to perform one specific function (for example, language identification, keyword spotting). We refer to plugins as working on audio segments, since OLIVE can process both audio files (on the file system) and audio buffers (sent as data through the API). For more information regarding each plugin type, but not a specific plugin or domain, including plugin type definitions, general output formats, and use cases, please click on the name of the plugin in the 'Function Type' column of the table below to visit that plugin type's information page: Table of Plugin Function Types Function type Abbreviation Scoring type Classes Description Speech Activity Detection SAD Frame / Region speech Identifies speech regions in an audio segment Speaker Identification SID Global Enrolled speakers Identifies whether a single-talker audio segment contains a target speaker Speaker Detection SDD Region Enrolled speakers Detects a target speaker in audio with multiple talkers Speaker Diarization (Deprecated) DIA Region Detect each unlabeled speaker Segments audio into clusters of unique speakers Language Identification LID Global Languages in training data and/or enrolled languages in some plugins Detect and label a single language per input audio segment or file Language Detection LDD Region Languages in training data and/or enrolled languages in some plugins Detect and label one or more language regions per input audio Automatic Speech Recognition ASR Region Creates a text transcription of the input audio Keyword Spotting (Deprecated) KWS Region Enrolled text keywords Language-specific approach to keyword detection using speech recognition and text Query by Example Keyword Spotting QBE Region Enrolled audio keywords Language independent approach to word spotting using one or more audio examples Gender Identification GID Global male, female Determines whether the audio segment was spoken by a male or female voice. Single output class for a given audio input. Gender Detection GDD Region male, female Detectrs and labels whether speech is likely spoken by a male or female voice. Capable of detecting multiple regions within the input audio. Topic Detection (Deprecated) TPD Region Enrolled topics Detects topic regions in an audio segment Speech Enhancement ENH Audio to Audio N/A Reduces noise in an audio segment Voice Type Discrimination VTD Frame / Region live-speech Detects presence of live-produced human speech, differentiating from silence, noise, speech coming from electronic device Text Machine Translation TMT TextTransformer Performs a translation of text from the input language specified to the output language specified. Does not currently output timing information. Does not operate on audio. Audio Redaction RED Audio to Audio N/A Replaces selecte audio regions with either 'bleeped' or transformed audio for privacy protection purposes Deep Fake Audio Detection DFA Global synthetic Identifies whether audio is likely to be synthetically generated by a deep fake algorithm, or naturally generated by a human talker Speaker Highlighting SHL Region Highlighted Speaker Detects additional regions in audio where the seeded speaker is found. Requires human intervention in the form of a selected region where representative speech is present from a speaker to locate additional regions of this speaker in the file/audio. Face Detection from Image FDI BoundingBoxScorer face(s) Detect one or more faces in an image. Detects faces in general, not necessarily specific faces. Face Detection from Video FDV BoundingBoxScorer face(s) Detect one or more faces in a video. Detects faces in general, not necessarily specific faces. Face Recognition from Image FRI BoundingBoxScorer enrolled face(s) Detect one or more specific, enrolled face(s) in an image. Outputs a bounding box where face is detected. Face Recognition from Video FRV BoundingBoxScorer enrolled face(s) Detect one or more specific, enrolled face(s) in a video. Outputs a bounding box where face is detected, with accompanying timestamp(s). Scoring Types Different function types score audio segments on different levels of granularity. Some plugin functionality differences are essentially differences is how an audio segment is treated -- as a single unit or potentially multiple units. For example, the main difference between speaker identification and speaker detection is how a segment is scored, in that speaker identification assumes that the audio segment sent to it for scoring is homogenous and comes from a single speaker, where speaker detection will instead allow for the possibility of the presence of multiple speakers in a given audio segment. There are three major scoring types: Frame - Assigns a score for each 10ms frame of the audio segment submitted for scoring. Region - Assigns and reports time boundaries defining region(s) within the audio segment, and for each region, an accompanying score for each detected class . Global - Assigns a single score for the entire audio segment for each of the plugin's classes . For more information on these scoring types, refer to the Plugin Traits page. Classes Certain plugin types have classes as an attribute. These can be common, cross-mission categories that are often pre-trained - like speech, languages, or dialects - or they can be ad-hoc mission-specific classes like speakers or topics. A plugin\u2019s classes may be completely fixed as in gender identification (male, female) or speech activity detection (speech) or an open set as in language identification (English, Spanish, Mandarin, etc.), topics, or speakers. Some plugins allow the user to add new classes or modify existing classes. Some class sets are inherently closed, like SAD and GID, where the plugin is complete and covers the world of possible classes. Others, like LID/SID/TPD plugin will probably never be complete in covering all classes and thus will always need to be able to treat a segment as though it may not be from among the classes the plugin recognizes (i.e. \u2018out of set\u2019). Enrollments Enrollments are a sub-set of classes that the user can create and/or modify. Both creation of a class and modification of an existing class are class modification requests, where the first class modification request for a given class also has the effect of creating the new class if it does not yet exist. Enrollments may be generated by end users with examples from their own data and can be learned from a single or small number of examples (SID, QBE) to a relatively large number of examples (LID, TID). Speakers are typically enrollments, as are query-based keywords and topics. Languages can also be enrolled and augmented with certain plugins. Since enrollments are dynamic, they may be incrementally updated \u201con the fly\u201d with new examples. For integration details regarding enrollments, refer to the Enrollments section of the API Integration page. To determine if a plugin supports enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page from the Specific Plugins list below. Stateless Enrollment / Vectorization As of OLIVE 6.0.0, some plugins support what we are calling Stateless Enrollment . The basic concept is to optionally disconnect enrollments, like speakers or keywords of interest, from the 'state' of the OLIVE server, and to instead have clients manage their own \"enrollments\" by submitting the enrollment vectors for the classes they wish to score at score time. The process changes from traditional enrollment: Submit example audio for the Class of interest - whether this is a target speaker, a keyword, a language, etc. Each exemplar must have a ClassID. The OLIVE server and plugin will create a model from those examples for each class, and store knowledge of it within the server. These are persistent with the server, and unless instructed otherwise, all future scoring request will use these persistent enrollments to score against. The user submits one or more audio samples to score, which are each scored against the set of enrollments stored and managed by the server. To \"stateless enrollment\": User submits example audio for one or more classes of interest. The OLIVE server and plugin creates a vector or model representation of each of these inputs. Rather than storing them and having the server itself manage these vectors, they are passed back to the requesting client. The client can/should manage these however they deep appropriate - whether that is in a database, or a directory structure on disk, or some other method. When the user wishes to submit audio for scoring, the vectors of the classes of interest that were passed back from the previous step must also be provided. Only the classes the user wishes to store against should be provided. The OLIVE server / plugin will provide results back after scoring against the submitted vector enrollments, the same as if those enrollments were stored in the server as in traditional enrollment. No state is retained between scoring requests - each request is starting from a blank slate Most plugins that currently support stateless enrollment also suppor traditional enrollment - so it is up to the user or integrator to choose which operation is used. The provided example UIs ( Nightingale and Raven ) have not been updated to operate with stateless enrollment at this time. For integrators interested in adding this support to in-house clients or applications, refer to the appropriate section of the olivepy API Primer for example python client code for this enrollment and scoring methodology. Benefits of Stateless Enrollment The ability to decouple management of enrollments for classes of interest and allowing the OLIVE server to be fully stateless opens the doors in a couple of key areas. Some of those include: Better scaling of OLIVE server across hardware resources. Without a server-managed enrollments/ directory, scaling solutions like Kubernetes for managing many OLIVE server instances become possible, if very high data throughput is required, and available hardware resources allow. By distributing enrollment management to external clients, this allows integrators to introduce the concept of users to OLIVE, and have a client maintain separate target classes for each user on their end, rather than a single central \"enrollment storage\" that is shared among all clients. This means user 'A' can maintain their own target sets without worrying about accidentally overwriting or deleting another user's targets, or having to wade through distracting score returns from another user's target sets. Client-side management of sensitive enrollment models. If the server isn't retaining these models, it may better meet certain end user data requirements. Potential Downsides of Stateless Enrollment There are two main drawbacks to this approach at this time. The first is added client-side complexity. With traditional enrollments, managing theses classes of interest is \"free\" since it's a built-in operation performed by the OLIVE server already. Client-side management of these enrollments adds some burden to the integrator / user. The other is related to network traffic and bandwidth, and will really only be a factor if there are many, many target enrollments that a user wants to score at a given time. For most current OLIVE plugins, the actual enrollment model / vector is represented by a very small array of floats. This means for most use cases, passing these in for scoring is trivial. If this number becomes very, very large - most likely approaching levels outside of the expected dependable operating range of the plugins, or in other situations like a very slow network connection, this could cause unwanted system latency. Online Updates Considerable improvements to system accuracy and calibration can be found by updating a plugin post-deployment to better align with the conditions observed in recent history. Several plugins are able to perform unsupervised updates to certain submodules of the plugin. The updates do not require labels or human input and are based on automatically collected information during normal system use. In most cases, a system update must be invoked by the user via the API, and an option to determine if an update is ready to be applied is also provided in the API. For integration details regarding the update functionality, refer to the Update section of the API Integration page. To check if a plugin supports online updates, refer to its detailed information page from the Specific Plugins list below. Adaptation Similarly to online updates, it can be possible to achieve even larger boosts in performance by updating a plugin by exposing it to the mission's audio conditions, or similarly representative audio conditions. Adaptation, however, requires human input, and in some cases, data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate. Naming Conventions Plugin Names Each plugin proper is given a three part name in the following form: function-attribute-version: function - three-letter abbreviation from column two of the Plugin Types Table above attribute - string that identifies the key attribute of the plugin, generally the algorithm name version - tracks the iteration of the plug-in in development in the form of v<digit> For example, sid-embed-v2 is a speaker identification plugin using speaker embed dings algorithm and is the second release or update of the approach. An additonal alphanumeric character may be appended to the version number if a plugin is re-released with bug fixes, but the performance is expected to be the same. For example, sad-dnn-v6a is a modified version of sad-dnn-v6, but the changes were meant to address errors or shortcomings in the plugin, not to change the algorithm or data used. Domain Names Domain names typically have two or three parts: condition-version or language-condition-version for plugins that have language-dependent domain components. Keyword spotting domains also contain the language for which the domain was trained. language - the language for which the domain was trained if language-dependent, or a representation of the set of languages contained within a LID plugin's domain condition - the specific audio environment for which the domain was trained, or \u201cmulti\u201d if the domain was developed to be condition independent version - tracks the iteration of the plug-in in development in the form of v<digit> Specific Plugins For additional information about specific plugins, their options, implementation details and other, please refer to the specific plugin pages, accessible from each of the individual Plugin Task pages.","title":"Overview"},{"location":"plugins.html#plugin-documentation","text":"This page contains high level information about OLIVE plugins and their associated concepts - for more detailed information about integrating with plugins, please refer to the more low-level focused Plugin API Integration Details pages. For more information about a specific plugin, please find its info page link in the list at the bottom of this page .","title":"Plugin Documentation"},{"location":"plugins.html#anatomy-of-a-plugin","text":"OLIVE Plugins encapsulate the actual audio processing technologies and capabilities of the OLIVE system into modular pieces that facilitate system upgrades, capability additions, incremental updates, and provide other benefits such as allowing for tuning models to improve processing in targeted audio conditions or allowing for multiple tools or options to choose from to accomplish a given task. Each plugin has a specific type, which defines the task that it is capable of performing (see Plugin Types below). They consist of two parts, the plugin proper, which contains the recipe or algorithm information on how to perform the task, and one or more domains, which contain the data models used to run the algorithm and perform the function. Since plugins are generally machine learning-based the same algorithm may have multiple strengths based on the specific data used to train it (e.g. telephone audio, push-to-talk audio, high effort vocalization, conversational speech). This separation between the algorithm and the data model allows us to deliver new functionality that is based on training data independently of the algorithm by delivering a new domain. Classes and enrollments (described below) are associated with a domain.","title":"Anatomy of a Plugin"},{"location":"plugins.html#plugin-types","text":"Plugin function types refer to the core task the plugin performs and have standard abbreviations. Most plugins are designed to perform one specific function (for example, language identification, keyword spotting). We refer to plugins as working on audio segments, since OLIVE can process both audio files (on the file system) and audio buffers (sent as data through the API). For more information regarding each plugin type, but not a specific plugin or domain, including plugin type definitions, general output formats, and use cases, please click on the name of the plugin in the 'Function Type' column of the table below to visit that plugin type's information page:","title":"Plugin Types"},{"location":"plugins.html#table-of-plugin-function-types","text":"Function type Abbreviation Scoring type Classes Description Speech Activity Detection SAD Frame / Region speech Identifies speech regions in an audio segment Speaker Identification SID Global Enrolled speakers Identifies whether a single-talker audio segment contains a target speaker Speaker Detection SDD Region Enrolled speakers Detects a target speaker in audio with multiple talkers Speaker Diarization (Deprecated) DIA Region Detect each unlabeled speaker Segments audio into clusters of unique speakers Language Identification LID Global Languages in training data and/or enrolled languages in some plugins Detect and label a single language per input audio segment or file Language Detection LDD Region Languages in training data and/or enrolled languages in some plugins Detect and label one or more language regions per input audio Automatic Speech Recognition ASR Region Creates a text transcription of the input audio Keyword Spotting (Deprecated) KWS Region Enrolled text keywords Language-specific approach to keyword detection using speech recognition and text Query by Example Keyword Spotting QBE Region Enrolled audio keywords Language independent approach to word spotting using one or more audio examples Gender Identification GID Global male, female Determines whether the audio segment was spoken by a male or female voice. Single output class for a given audio input. Gender Detection GDD Region male, female Detectrs and labels whether speech is likely spoken by a male or female voice. Capable of detecting multiple regions within the input audio. Topic Detection (Deprecated) TPD Region Enrolled topics Detects topic regions in an audio segment Speech Enhancement ENH Audio to Audio N/A Reduces noise in an audio segment Voice Type Discrimination VTD Frame / Region live-speech Detects presence of live-produced human speech, differentiating from silence, noise, speech coming from electronic device Text Machine Translation TMT TextTransformer Performs a translation of text from the input language specified to the output language specified. Does not currently output timing information. Does not operate on audio. Audio Redaction RED Audio to Audio N/A Replaces selecte audio regions with either 'bleeped' or transformed audio for privacy protection purposes Deep Fake Audio Detection DFA Global synthetic Identifies whether audio is likely to be synthetically generated by a deep fake algorithm, or naturally generated by a human talker Speaker Highlighting SHL Region Highlighted Speaker Detects additional regions in audio where the seeded speaker is found. Requires human intervention in the form of a selected region where representative speech is present from a speaker to locate additional regions of this speaker in the file/audio. Face Detection from Image FDI BoundingBoxScorer face(s) Detect one or more faces in an image. Detects faces in general, not necessarily specific faces. Face Detection from Video FDV BoundingBoxScorer face(s) Detect one or more faces in a video. Detects faces in general, not necessarily specific faces. Face Recognition from Image FRI BoundingBoxScorer enrolled face(s) Detect one or more specific, enrolled face(s) in an image. Outputs a bounding box where face is detected. Face Recognition from Video FRV BoundingBoxScorer enrolled face(s) Detect one or more specific, enrolled face(s) in a video. Outputs a bounding box where face is detected, with accompanying timestamp(s).","title":"Table of Plugin Function Types"},{"location":"plugins.html#scoring-types","text":"Different function types score audio segments on different levels of granularity. Some plugin functionality differences are essentially differences is how an audio segment is treated -- as a single unit or potentially multiple units. For example, the main difference between speaker identification and speaker detection is how a segment is scored, in that speaker identification assumes that the audio segment sent to it for scoring is homogenous and comes from a single speaker, where speaker detection will instead allow for the possibility of the presence of multiple speakers in a given audio segment. There are three major scoring types: Frame - Assigns a score for each 10ms frame of the audio segment submitted for scoring. Region - Assigns and reports time boundaries defining region(s) within the audio segment, and for each region, an accompanying score for each detected class . Global - Assigns a single score for the entire audio segment for each of the plugin's classes . For more information on these scoring types, refer to the Plugin Traits page.","title":"Scoring Types"},{"location":"plugins.html#classes","text":"Certain plugin types have classes as an attribute. These can be common, cross-mission categories that are often pre-trained - like speech, languages, or dialects - or they can be ad-hoc mission-specific classes like speakers or topics. A plugin\u2019s classes may be completely fixed as in gender identification (male, female) or speech activity detection (speech) or an open set as in language identification (English, Spanish, Mandarin, etc.), topics, or speakers. Some plugins allow the user to add new classes or modify existing classes. Some class sets are inherently closed, like SAD and GID, where the plugin is complete and covers the world of possible classes. Others, like LID/SID/TPD plugin will probably never be complete in covering all classes and thus will always need to be able to treat a segment as though it may not be from among the classes the plugin recognizes (i.e. \u2018out of set\u2019).","title":"Classes"},{"location":"plugins.html#enrollments","text":"Enrollments are a sub-set of classes that the user can create and/or modify. Both creation of a class and modification of an existing class are class modification requests, where the first class modification request for a given class also has the effect of creating the new class if it does not yet exist. Enrollments may be generated by end users with examples from their own data and can be learned from a single or small number of examples (SID, QBE) to a relatively large number of examples (LID, TID). Speakers are typically enrollments, as are query-based keywords and topics. Languages can also be enrolled and augmented with certain plugins. Since enrollments are dynamic, they may be incrementally updated \u201con the fly\u201d with new examples. For integration details regarding enrollments, refer to the Enrollments section of the API Integration page. To determine if a plugin supports enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page from the Specific Plugins list below.","title":"Enrollments"},{"location":"plugins.html#stateless-enrollment-vectorization","text":"As of OLIVE 6.0.0, some plugins support what we are calling Stateless Enrollment . The basic concept is to optionally disconnect enrollments, like speakers or keywords of interest, from the 'state' of the OLIVE server, and to instead have clients manage their own \"enrollments\" by submitting the enrollment vectors for the classes they wish to score at score time. The process changes from traditional enrollment: Submit example audio for the Class of interest - whether this is a target speaker, a keyword, a language, etc. Each exemplar must have a ClassID. The OLIVE server and plugin will create a model from those examples for each class, and store knowledge of it within the server. These are persistent with the server, and unless instructed otherwise, all future scoring request will use these persistent enrollments to score against. The user submits one or more audio samples to score, which are each scored against the set of enrollments stored and managed by the server. To \"stateless enrollment\": User submits example audio for one or more classes of interest. The OLIVE server and plugin creates a vector or model representation of each of these inputs. Rather than storing them and having the server itself manage these vectors, they are passed back to the requesting client. The client can/should manage these however they deep appropriate - whether that is in a database, or a directory structure on disk, or some other method. When the user wishes to submit audio for scoring, the vectors of the classes of interest that were passed back from the previous step must also be provided. Only the classes the user wishes to store against should be provided. The OLIVE server / plugin will provide results back after scoring against the submitted vector enrollments, the same as if those enrollments were stored in the server as in traditional enrollment. No state is retained between scoring requests - each request is starting from a blank slate Most plugins that currently support stateless enrollment also suppor traditional enrollment - so it is up to the user or integrator to choose which operation is used. The provided example UIs ( Nightingale and Raven ) have not been updated to operate with stateless enrollment at this time. For integrators interested in adding this support to in-house clients or applications, refer to the appropriate section of the olivepy API Primer for example python client code for this enrollment and scoring methodology.","title":"Stateless Enrollment / Vectorization"},{"location":"plugins.html#benefits-of-stateless-enrollment","text":"The ability to decouple management of enrollments for classes of interest and allowing the OLIVE server to be fully stateless opens the doors in a couple of key areas. Some of those include: Better scaling of OLIVE server across hardware resources. Without a server-managed enrollments/ directory, scaling solutions like Kubernetes for managing many OLIVE server instances become possible, if very high data throughput is required, and available hardware resources allow. By distributing enrollment management to external clients, this allows integrators to introduce the concept of users to OLIVE, and have a client maintain separate target classes for each user on their end, rather than a single central \"enrollment storage\" that is shared among all clients. This means user 'A' can maintain their own target sets without worrying about accidentally overwriting or deleting another user's targets, or having to wade through distracting score returns from another user's target sets. Client-side management of sensitive enrollment models. If the server isn't retaining these models, it may better meet certain end user data requirements.","title":"Benefits of Stateless Enrollment"},{"location":"plugins.html#potential-downsides-of-stateless-enrollment","text":"There are two main drawbacks to this approach at this time. The first is added client-side complexity. With traditional enrollments, managing theses classes of interest is \"free\" since it's a built-in operation performed by the OLIVE server already. Client-side management of these enrollments adds some burden to the integrator / user. The other is related to network traffic and bandwidth, and will really only be a factor if there are many, many target enrollments that a user wants to score at a given time. For most current OLIVE plugins, the actual enrollment model / vector is represented by a very small array of floats. This means for most use cases, passing these in for scoring is trivial. If this number becomes very, very large - most likely approaching levels outside of the expected dependable operating range of the plugins, or in other situations like a very slow network connection, this could cause unwanted system latency.","title":"Potential Downsides of Stateless Enrollment"},{"location":"plugins.html#online-updates","text":"Considerable improvements to system accuracy and calibration can be found by updating a plugin post-deployment to better align with the conditions observed in recent history. Several plugins are able to perform unsupervised updates to certain submodules of the plugin. The updates do not require labels or human input and are based on automatically collected information during normal system use. In most cases, a system update must be invoked by the user via the API, and an option to determine if an update is ready to be applied is also provided in the API. For integration details regarding the update functionality, refer to the Update section of the API Integration page. To check if a plugin supports online updates, refer to its detailed information page from the Specific Plugins list below.","title":"Online Updates"},{"location":"plugins.html#adaptation","text":"Similarly to online updates, it can be possible to achieve even larger boosts in performance by updating a plugin by exposing it to the mission's audio conditions, or similarly representative audio conditions. Adaptation, however, requires human input, and in some cases, data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate.","title":"Adaptation"},{"location":"plugins.html#naming-conventions","text":"","title":"Naming Conventions"},{"location":"plugins.html#plugin-names","text":"Each plugin proper is given a three part name in the following form: function-attribute-version: function - three-letter abbreviation from column two of the Plugin Types Table above attribute - string that identifies the key attribute of the plugin, generally the algorithm name version - tracks the iteration of the plug-in in development in the form of v<digit> For example, sid-embed-v2 is a speaker identification plugin using speaker embed dings algorithm and is the second release or update of the approach. An additonal alphanumeric character may be appended to the version number if a plugin is re-released with bug fixes, but the performance is expected to be the same. For example, sad-dnn-v6a is a modified version of sad-dnn-v6, but the changes were meant to address errors or shortcomings in the plugin, not to change the algorithm or data used.","title":"Plugin Names"},{"location":"plugins.html#domain-names","text":"Domain names typically have two or three parts: condition-version or language-condition-version for plugins that have language-dependent domain components. Keyword spotting domains also contain the language for which the domain was trained. language - the language for which the domain was trained if language-dependent, or a representation of the set of languages contained within a LID plugin's domain condition - the specific audio environment for which the domain was trained, or \u201cmulti\u201d if the domain was developed to be condition independent version - tracks the iteration of the plug-in in development in the form of v<digit>","title":"Domain Names"},{"location":"plugins.html#specific-plugins","text":"For additional information about specific plugins, their options, implementation details and other, please refer to the specific plugin pages, accessible from each of the individual Plugin Task pages.","title":"Specific Plugins"},{"location":"raven.html","text":"Raven Web Graphical User Interface Raven Web GUI Overview The Raven Web GUI is a User Interface designed to primarily facilitate \"batch runs\" consisting of a large number of files through \"workflows\" consisting of multiple technologies. The Raven GUI can be used to run either local audio or serverside/shared audio through one of several preconfigured HLT workflows. Below is a basic example of the Raven Web GUI landing page: OLIVE TLS/Secure Mode Disclaimer If you have launched OLIVE / Martini with TLS enabled for secure communication, you will need to install a valid client certificate (with respect to the certificates Martini is using) into your browser before you will be able to access Raven. Please work with your IT department to obtain valid certificates. Using the Raven Web GUI for Analysis In order to submit audio to a workflow on the WebGUI you must: Add Media Select an available OLIVE Workflow from the Workflows List Submit the analysis request 1. Add Media There are two ways to do this. Local Media and Remote Media . Local Media The primary method is by using the \"Local Media\" tab, and adding media to the job by dragging them into the Drop your media / text files here or browse box, or by clicking within this box to open a file browser dialog to select files. Note! If dragging files in, make sure to 'drop' them on the box. If the box is missed, and the files are dropped elsewhere on the Raven page, the browser itself will attempt to open the media files, instead of adding them to the submission. \" As of the OLIVE 5.6.0 release, files can now be added incrementally, i.e. dragged in one at a time or in multiple batches. However, due to browser limitations, duplicate filenames are not allowed within the same request, even if they are unique files from a different location on disk. If duplicate files are detected, the user must resolve the conflict with one of the following actions: Rename the new file Overwrite the old file with the new file in the submission Delete the new file from the submission Once media is added and chosen, proceed to step #2 . Remote Media If many files or large files are intended to be processed, or the same files will be processed many times - and if the OLIVE server shares a file system with the audio file location, the \"Remote Media\" feature is provided as a convenience to save the overhead of uploading files through the browser with Raven for each analysis submission. To use this feature, first the audio must be copied, moved, or linked to a location that is mounted by the OLIVE Martini container. By default, Raven will look in this location for files to populate the \"Remote Media\" tab: olive6.0.0/oliveAppData/olive-data/media/shared/ For convenience, once the OLIVE Martini container is started for the first time, this location is created if it doesn't exist already. Any files here should be automatically added to the \"Remote Media\" tab in Raven and be allowable choices as Raven input. Advanced users may be able to mount additional volumes and configure this to a different location. Please contact SRI if this is a requirement. Once the desired audio is selected, job submission is exactly as previously described . 2. Select an available OLIVE Workflow from the Workflows List Below the Media panel, a list of available, validated workflows is displayed. When first connecting to the Raven UI, it has to query the server to retrieve a list of valid, available workflows - this can take a few moments, and the UI will prompt with a \"Please wait while workflows are loading ...\" message until this handshake is completed. Once the list is provided, select the desired workflow and move to the next step. Each workflow displays the workflow name, and information about which Tasks it performs (i.e. plugins it runs, or results it provides). Note! If any workflows fail to validate, due to missing plugins or for any other reason, they can be revealed by clicking the 3-vertical-dot icon next to the Workflows title. There will be a message offering more details on why a given workflow is unavailable when selected. This message should only be encountered by advanced users. 3. Submit Analysis Job Once the job is configured, it can be submitted for analysis clicking the Analyze button beneath the desired workflow. Analyzing Results After running a set of data through a workflow you'll be directed to the results page which displays the HLT results for all plugins within the workflow that was run for each file that was submitted. The results page has a menu bar with several different ways to organize the files and scores on the page. Stereo Media with Raven UI OLIVE 5.7.1 introduced the ability to change how Raven will treat stereo audio through a settings panel available at the top of the UI. The default behavior prior to this release was for all files to be merged to mono and then processed as single channel audio files. Now, users can specify to use this legacy behavior, specify one or the other channel to process, or split the audio and process both channels independently. Stereo Handling Configuration To access the Raven configuration options, select Settings from the top right of the Raven landing page. Then select the \"Stereo Media\" dropdown: And choose from the following options: Merge to mono - This is the default/legacy behavior of the Raven UI, and the standard behavior of the Nightingale UI. Both channels of the stereo audio are merged into one and processed as one job. Left channel only - This method will isolate the left channel of the input audio and report analysis results on just this channel. Nothing from the right channel will be scored or reported on, it is all discarded. Right channel only - Identical to the previous option, but focusing on the right channel. Both channels independently - This method retains the separation of both channels of the input audio and submits each side as a separate job. Results are reported on the analysis page for both channels. The next section outlines how to identify stereo results in the Raven Analysis display, and how to tell which results belong to which channel. Stereo Results Display There is now a small addition to the Filename heading for each media analysis result after running a workflow with Raven that allows users to distinguish between mono and stereo media files, and also to determine which channel the displayed results belong to. This addition is a small speaker iconograph, located to the right of the filename: There are three possible icons for this location. Mono Media Results A single, wider speaker indicates that the media was either mono audio to begin with, or was processed with the original \"merge to mono\" method. There will be only one analysis result entry displayed for this file. Here is an example of this: Left Channel Media Results A pair of speakers indicates that the media was originally stereo, and that one of the stereo-preserving processing options was chosen. If the left speaker is darker and bolder, with the right grayed out, that indicates that the results in this entry pertain to the left channel of the input media. If \"Left channel only\" processing was selected for the stereo handling configuration , there will be only one results entry for this file. If \"Both channels independently\" was selected, this represents the results for the left channel, and there will be another results entry with the same File name containing the results from the right. Right Channel Media Results If the right speaker is darker and bolder, with the left grayed out, that indicates that the results in this entry pertain to the right channel of the input media. If \"Right channel only\" processing was selected for the stereo handling configuration , there will be only one results entry for this file. If \"Both channels independently\" was selected, this represents the results for the right channel, and there will be another results entry with the same File name containing the results from the left. Results from Both Channels This shows an example where \" Both channels independently \" was selected, and the results for both channels of the input file can be clearly seen in the Raven analysis display: Saving/Exporting Results Raven now offers the capability of exporting batch results to an archive that can be later re-ingested by Raven to recall or revisit the results of an OLIVE Workflow analysis request without repeating the processing. To export results, once you the batch submission has completed, select the \"Save Results\" option adjacent to the Workflow name at the top of the results page: This will save a .olive results bundle to your browser's default \"Downloads\" location. The name will include the date and time the submission was performed, and will resemble: olive_result_2023-08-14T21_57_12.olive . You are free to rename as desired after the download completes. Due to web browser file permission access limitations, for audio to be reviewable in the context of the results at a later time, all submitted audio must be saved with these results. For batches of many files, or file batches containing very large files, this bundle can be quite large. Importing/Recalling Past Results To revisit saved results, select the Open menu item from the top right of the Raven landing page: And choose to import the file saved in the saving/exporting step above. The bundle should be unpacked and populate the Analysis page just as it appeared directly after processing the files initially. Enrollment and Unenrollment Raven now supports enrollment directly into qualifying plugins, without creating a workflow beforehand. Enrollment and unenrollment is performed on a plugin and domain level; not on the workflow level. This means that if you enroll a model using a workflow, it's actually enrolling for the specific plugin/domain that that workflow is using, and will be accessible if running that plugin directly, or if running other workflows that call on this plugin. For example, if you enroll a new speaker using a workflow enrollment, it is enrolled into the same location that it would be if enrolling with that plugin directly, and scores for the new model will be reported if scoring with this plugin directly. Workflow enrollment/unenrollment is provided as a convenience. Enrollment Enrolling through Raven is a similar process to scoring. Begin by adding media to the job as described above , and then selecting the Plugins tab in the Workflows / Plugins pane, and then choose Enroll from the Plugin you're interested in enrolling to: This will present the Enrollment Configuration dialog, that allows enrollments. Enrollment Type First, the \"Enrollment Type\" must be chosen. This determines what the enrollments for this job will represent, and each enrollment job can have only one type. Most workflows will also only have one \"Type\" available, so this step should be simple. The most common \"Enrollment Type\" is going to be \"Speaker\", for enrolling candidates into Speaker Identification or Speaker Detection plugins. You may encounter \"Keyword\" enrollment types if the workflow is configured to use Query-by-Example Keyword Spotting, for example. Class Name Mapping The next section configures how to select and assign class names to the enrolled models. There are a few options provided for convenience. After configuring this section, select \"Submit\" to continue. Class Name If all data is one \"Speaker\" (or whatever the corresponding Enrollment Type class is), this method can be used. The user inputs a single class name that is used for all of the enrollment input. If there are multiple classes that are going to be enrolled, each class requires a separate enrollment request when using this method. You can choose to augment an existing class enrollment, by choosing it from the drop-down. Or select \"Add a new class\" to enroll a new class completely. Delimiter If the input data filenames are already named in such a way that the start of the filename corresponds to the desired name of the enrollment class, the 'Delimiter' mapping mode may be used. This mode will use a character input by the user to split a filename and automatically extract a class name to use for enrollment. This allows many files from different classes all to be enrolled at once. As an example, if the chosen delimiter was - , and the input filenames: philip-radio_21Aug2023.wav philip-studioMic_12Nov2021.wav george-unknown.wav suzie-greetingCard_storytime.wav This would allow philip , george , and suzie to all be enrolled with a single Raven submission. The beginning part of the media file name until the first occurrence of the delimiter character will be used as the class name. That is, if the delimiter is \"-\", and the file names are \"jane-california-1.wav\" and \"john-oregon-1.wav\", \"jane-california-1.wav\" is enrolled with the class, \"jane\", and \"john-oregon-1.wav\" is enrolled with the class, \"john\". Class Mapping File The most flexible method of assigning class names is using a Class Mapping File. This allows the user to provide a text file listing all of the input files, and manually assigning an enrollment class name for each. If the input data isn't named in a manner that suits the delimiter method, and several different classes need to be enrolled, this is the best option. The Class Mapping File formatting is a tab-separated, two column list, where the columns are: <input filename 1> <className A> <input filename 2> <className B> ... <input filename N> <className Z> It is possible to mix and match multiple files for a given classname, and as many distinct classes as desired as long as the mapping file follows the format above. An example: 20131212T064501UTC_10831_A.wav philip 20131212T084501UTC_10856_A.wav philip 20131212T101501UTC_10870_A.wav suzie 20131212T104501UTC_10876_A.wav chuck 20131212T111501UTC_10878_A.wav suzie 20131212T114501UTC_10880_A.wav aaron 20131212T134501UTC_10884_A.wav philip No part of the filename is used to assign or determine the enrollment class name. Note that every input filename must be accounted for in the Class Mapping File. Unenrollment Unenrollment is used to remove a previously enrolled class from the system. It is the only Raven job submission feature that is accessible without adding media. To reach it first select the Plugins tab in the Workflows / Plugins pane, and then choose Unenroll from the Plugin you're interested in unenrolling from. This will present the Unenrollment Configuration dialog. To proceed, choose the desired class type (again, likely \"Speaker\") from the dropdown list that corresponds to the unenrollment action to perform. Next, select the class Only single-class unenrollment is currently supported through Raven. If multiple unenrollments are to be performed, they must be submitted one at a time. Currently, a class must be enrolled in its entirety, even if several audio files contributed to its enrollment - support is not in place to selectively remove audio from an enrollment.","title":"Raven Batch Web GUI"},{"location":"raven.html#raven-web-graphical-user-interface","text":"","title":"Raven Web Graphical User Interface"},{"location":"raven.html#raven-web-gui-overview","text":"The Raven Web GUI is a User Interface designed to primarily facilitate \"batch runs\" consisting of a large number of files through \"workflows\" consisting of multiple technologies. The Raven GUI can be used to run either local audio or serverside/shared audio through one of several preconfigured HLT workflows. Below is a basic example of the Raven Web GUI landing page:","title":"Raven Web GUI Overview"},{"location":"raven.html#olive-tlssecure-mode-disclaimer","text":"If you have launched OLIVE / Martini with TLS enabled for secure communication, you will need to install a valid client certificate (with respect to the certificates Martini is using) into your browser before you will be able to access Raven. Please work with your IT department to obtain valid certificates.","title":"OLIVE TLS/Secure Mode Disclaimer"},{"location":"raven.html#using-the-raven-web-gui-for-analysis","text":"In order to submit audio to a workflow on the WebGUI you must: Add Media Select an available OLIVE Workflow from the Workflows List Submit the analysis request","title":"Using the Raven Web GUI for Analysis"},{"location":"raven.html#1-add-media","text":"There are two ways to do this. Local Media and Remote Media .","title":"1. Add Media"},{"location":"raven.html#local-media","text":"The primary method is by using the \"Local Media\" tab, and adding media to the job by dragging them into the Drop your media / text files here or browse box, or by clicking within this box to open a file browser dialog to select files. Note! If dragging files in, make sure to 'drop' them on the box. If the box is missed, and the files are dropped elsewhere on the Raven page, the browser itself will attempt to open the media files, instead of adding them to the submission. \" As of the OLIVE 5.6.0 release, files can now be added incrementally, i.e. dragged in one at a time or in multiple batches. However, due to browser limitations, duplicate filenames are not allowed within the same request, even if they are unique files from a different location on disk. If duplicate files are detected, the user must resolve the conflict with one of the following actions: Rename the new file Overwrite the old file with the new file in the submission Delete the new file from the submission Once media is added and chosen, proceed to step #2 .","title":"Local Media"},{"location":"raven.html#remote-media","text":"If many files or large files are intended to be processed, or the same files will be processed many times - and if the OLIVE server shares a file system with the audio file location, the \"Remote Media\" feature is provided as a convenience to save the overhead of uploading files through the browser with Raven for each analysis submission. To use this feature, first the audio must be copied, moved, or linked to a location that is mounted by the OLIVE Martini container. By default, Raven will look in this location for files to populate the \"Remote Media\" tab: olive6.0.0/oliveAppData/olive-data/media/shared/ For convenience, once the OLIVE Martini container is started for the first time, this location is created if it doesn't exist already. Any files here should be automatically added to the \"Remote Media\" tab in Raven and be allowable choices as Raven input. Advanced users may be able to mount additional volumes and configure this to a different location. Please contact SRI if this is a requirement. Once the desired audio is selected, job submission is exactly as previously described .","title":"Remote Media"},{"location":"raven.html#2-select-an-available-olive-workflow-from-the-workflows-list","text":"Below the Media panel, a list of available, validated workflows is displayed. When first connecting to the Raven UI, it has to query the server to retrieve a list of valid, available workflows - this can take a few moments, and the UI will prompt with a \"Please wait while workflows are loading ...\" message until this handshake is completed. Once the list is provided, select the desired workflow and move to the next step. Each workflow displays the workflow name, and information about which Tasks it performs (i.e. plugins it runs, or results it provides). Note! If any workflows fail to validate, due to missing plugins or for any other reason, they can be revealed by clicking the 3-vertical-dot icon next to the Workflows title. There will be a message offering more details on why a given workflow is unavailable when selected. This message should only be encountered by advanced users.","title":"2. Select an available OLIVE Workflow from the Workflows List"},{"location":"raven.html#3-submit-analysis-job","text":"Once the job is configured, it can be submitted for analysis clicking the Analyze button beneath the desired workflow.","title":"3. Submit Analysis Job"},{"location":"raven.html#analyzing-results","text":"After running a set of data through a workflow you'll be directed to the results page which displays the HLT results for all plugins within the workflow that was run for each file that was submitted. The results page has a menu bar with several different ways to organize the files and scores on the page.","title":"Analyzing Results"},{"location":"raven.html#stereo-media-with-raven-ui","text":"OLIVE 5.7.1 introduced the ability to change how Raven will treat stereo audio through a settings panel available at the top of the UI. The default behavior prior to this release was for all files to be merged to mono and then processed as single channel audio files. Now, users can specify to use this legacy behavior, specify one or the other channel to process, or split the audio and process both channels independently.","title":"Stereo Media with Raven UI"},{"location":"raven.html#stereo-handling-configuration","text":"To access the Raven configuration options, select Settings from the top right of the Raven landing page. Then select the \"Stereo Media\" dropdown: And choose from the following options: Merge to mono - This is the default/legacy behavior of the Raven UI, and the standard behavior of the Nightingale UI. Both channels of the stereo audio are merged into one and processed as one job. Left channel only - This method will isolate the left channel of the input audio and report analysis results on just this channel. Nothing from the right channel will be scored or reported on, it is all discarded. Right channel only - Identical to the previous option, but focusing on the right channel. Both channels independently - This method retains the separation of both channels of the input audio and submits each side as a separate job. Results are reported on the analysis page for both channels. The next section outlines how to identify stereo results in the Raven Analysis display, and how to tell which results belong to which channel.","title":"Stereo Handling Configuration"},{"location":"raven.html#stereo-results-display","text":"There is now a small addition to the Filename heading for each media analysis result after running a workflow with Raven that allows users to distinguish between mono and stereo media files, and also to determine which channel the displayed results belong to. This addition is a small speaker iconograph, located to the right of the filename: There are three possible icons for this location.","title":"Stereo Results Display"},{"location":"raven.html#mono-media-results","text":"A single, wider speaker indicates that the media was either mono audio to begin with, or was processed with the original \"merge to mono\" method. There will be only one analysis result entry displayed for this file. Here is an example of this:","title":"Mono Media Results"},{"location":"raven.html#left-channel-media-results","text":"A pair of speakers indicates that the media was originally stereo, and that one of the stereo-preserving processing options was chosen. If the left speaker is darker and bolder, with the right grayed out, that indicates that the results in this entry pertain to the left channel of the input media. If \"Left channel only\" processing was selected for the stereo handling configuration , there will be only one results entry for this file. If \"Both channels independently\" was selected, this represents the results for the left channel, and there will be another results entry with the same File name containing the results from the right.","title":"Left Channel Media Results"},{"location":"raven.html#right-channel-media-results","text":"If the right speaker is darker and bolder, with the left grayed out, that indicates that the results in this entry pertain to the right channel of the input media. If \"Right channel only\" processing was selected for the stereo handling configuration , there will be only one results entry for this file. If \"Both channels independently\" was selected, this represents the results for the right channel, and there will be another results entry with the same File name containing the results from the left.","title":"Right Channel Media Results"},{"location":"raven.html#results-from-both-channels","text":"This shows an example where \" Both channels independently \" was selected, and the results for both channels of the input file can be clearly seen in the Raven analysis display:","title":"Results from Both Channels"},{"location":"raven.html#savingexporting-results","text":"Raven now offers the capability of exporting batch results to an archive that can be later re-ingested by Raven to recall or revisit the results of an OLIVE Workflow analysis request without repeating the processing. To export results, once you the batch submission has completed, select the \"Save Results\" option adjacent to the Workflow name at the top of the results page: This will save a .olive results bundle to your browser's default \"Downloads\" location. The name will include the date and time the submission was performed, and will resemble: olive_result_2023-08-14T21_57_12.olive . You are free to rename as desired after the download completes. Due to web browser file permission access limitations, for audio to be reviewable in the context of the results at a later time, all submitted audio must be saved with these results. For batches of many files, or file batches containing very large files, this bundle can be quite large.","title":"Saving/Exporting Results"},{"location":"raven.html#importingrecalling-past-results","text":"To revisit saved results, select the Open menu item from the top right of the Raven landing page: And choose to import the file saved in the saving/exporting step above. The bundle should be unpacked and populate the Analysis page just as it appeared directly after processing the files initially.","title":"Importing/Recalling Past Results"},{"location":"raven.html#enrollment-and-unenrollment","text":"Raven now supports enrollment directly into qualifying plugins, without creating a workflow beforehand. Enrollment and unenrollment is performed on a plugin and domain level; not on the workflow level. This means that if you enroll a model using a workflow, it's actually enrolling for the specific plugin/domain that that workflow is using, and will be accessible if running that plugin directly, or if running other workflows that call on this plugin. For example, if you enroll a new speaker using a workflow enrollment, it is enrolled into the same location that it would be if enrolling with that plugin directly, and scores for the new model will be reported if scoring with this plugin directly. Workflow enrollment/unenrollment is provided as a convenience.","title":"Enrollment and Unenrollment"},{"location":"raven.html#enrollment","text":"Enrolling through Raven is a similar process to scoring. Begin by adding media to the job as described above , and then selecting the Plugins tab in the Workflows / Plugins pane, and then choose Enroll from the Plugin you're interested in enrolling to: This will present the Enrollment Configuration dialog, that allows enrollments.","title":"Enrollment"},{"location":"raven.html#enrollment-type","text":"First, the \"Enrollment Type\" must be chosen. This determines what the enrollments for this job will represent, and each enrollment job can have only one type. Most workflows will also only have one \"Type\" available, so this step should be simple. The most common \"Enrollment Type\" is going to be \"Speaker\", for enrolling candidates into Speaker Identification or Speaker Detection plugins. You may encounter \"Keyword\" enrollment types if the workflow is configured to use Query-by-Example Keyword Spotting, for example.","title":"Enrollment Type"},{"location":"raven.html#class-name-mapping","text":"The next section configures how to select and assign class names to the enrolled models. There are a few options provided for convenience. After configuring this section, select \"Submit\" to continue.","title":"Class Name Mapping"},{"location":"raven.html#class-name","text":"If all data is one \"Speaker\" (or whatever the corresponding Enrollment Type class is), this method can be used. The user inputs a single class name that is used for all of the enrollment input. If there are multiple classes that are going to be enrolled, each class requires a separate enrollment request when using this method. You can choose to augment an existing class enrollment, by choosing it from the drop-down. Or select \"Add a new class\" to enroll a new class completely.","title":"Class Name"},{"location":"raven.html#delimiter","text":"If the input data filenames are already named in such a way that the start of the filename corresponds to the desired name of the enrollment class, the 'Delimiter' mapping mode may be used. This mode will use a character input by the user to split a filename and automatically extract a class name to use for enrollment. This allows many files from different classes all to be enrolled at once. As an example, if the chosen delimiter was - , and the input filenames: philip-radio_21Aug2023.wav philip-studioMic_12Nov2021.wav george-unknown.wav suzie-greetingCard_storytime.wav This would allow philip , george , and suzie to all be enrolled with a single Raven submission. The beginning part of the media file name until the first occurrence of the delimiter character will be used as the class name. That is, if the delimiter is \"-\", and the file names are \"jane-california-1.wav\" and \"john-oregon-1.wav\", \"jane-california-1.wav\" is enrolled with the class, \"jane\", and \"john-oregon-1.wav\" is enrolled with the class, \"john\".","title":"Delimiter"},{"location":"raven.html#class-mapping-file","text":"The most flexible method of assigning class names is using a Class Mapping File. This allows the user to provide a text file listing all of the input files, and manually assigning an enrollment class name for each. If the input data isn't named in a manner that suits the delimiter method, and several different classes need to be enrolled, this is the best option. The Class Mapping File formatting is a tab-separated, two column list, where the columns are: <input filename 1> <className A> <input filename 2> <className B> ... <input filename N> <className Z> It is possible to mix and match multiple files for a given classname, and as many distinct classes as desired as long as the mapping file follows the format above. An example: 20131212T064501UTC_10831_A.wav philip 20131212T084501UTC_10856_A.wav philip 20131212T101501UTC_10870_A.wav suzie 20131212T104501UTC_10876_A.wav chuck 20131212T111501UTC_10878_A.wav suzie 20131212T114501UTC_10880_A.wav aaron 20131212T134501UTC_10884_A.wav philip No part of the filename is used to assign or determine the enrollment class name. Note that every input filename must be accounted for in the Class Mapping File.","title":"Class Mapping File"},{"location":"raven.html#unenrollment","text":"Unenrollment is used to remove a previously enrolled class from the system. It is the only Raven job submission feature that is accessible without adding media. To reach it first select the Plugins tab in the Workflows / Plugins pane, and then choose Unenroll from the Plugin you're interested in unenrolling from. This will present the Unenrollment Configuration dialog. To proceed, choose the desired class type (again, likely \"Speaker\") from the dropdown list that corresponds to the unenrollment action to perform. Next, select the class Only single-class unenrollment is currently supported through Raven. If multiple unenrollments are to be performed, they must be submitted one at a time. Currently, a class must be enrolled in its entirety, even if several audio files contributed to its enrollment - support is not in place to selectively remove audio from an enrollment.","title":"Unenrollment"},{"location":"redaction.html","text":"Speaker Redaction Task This document covers how to use the features added to the OLIVE Nightingale GUI to support the task of \"Speaker Redaction\". This allows for the assisted labeling and discovery of all sections of speech within an audio recording that belong to a single speaker, whose audio is wished to be removed from the audio in order to protect his or her identity. Currently, the designated audio is replaced with a tone, and all content there is destroyed. In the future, different approaches may be available that disguise the identity of the voice while leaving the content of the speech audible. Speaker Redaction Task Overview Generally, the process of stepping through the Redaction task follows these summarized steps: Load the audio to be processed Find and select an example of the speaker whose voice is to be redacted Submit this audio to the system, so it can find and suggest additional regions where this speaker occurs Add additional selections from these candidates to the labeled regions Optionally iterate through steps 3 and 4 until all audio from the speaker to be redacted is appropriately labeled Submit this audio to the redaction system to replace the labeled regions with a tone Review the resulting audio to make sure all of the speaker's audio was removed or disguised Add regions and resubmit to redaction system if necessary This process is covered in more detail below. Speaker Redaction Walkthrough Getting Started To get started with this task, first open the OLIVE Nightingale GUI and load an audio file in by dragging and dropping a file into one of the GUI waveform slots, or by pressing ctrl+o or cmd+o while the GUI has focus to enter an 'Open File' dialog. Once the audio is loaded, select the \u20181. Set up tiers\u2019 button in the \u2018Redaction Task\u2019 panel. This will create two new tiers below the audio file: Suggested This tier is where the results from the speaker search algorithm will be displayed, providing suggested regions that the system believes belong to the speaker whose speech has been provided in the BLEEP regions of the tier below. BLEEP or keep This tier is where you provide 'seed' regions, as BLEEP labels to inform the speaker-search system what the speaker to redact sounds like. You can also use 'keep' labels to inform the system of regions that do not contain the speaker you're attempting to redact, so that those regions won't be suggested again in the future. All regions labeled as BLEEP in this tier are what will be removed by the redaction plugin during the final step of this process. All keep regions will be left alone, as will regions that have no labels. Only BLEEP regions will be affected. This initial setup step will also enable the \u20182. Find speaker\u2019 and \u20183. Create redacted audio\u2019 buttons on the \u2018Redaction task\u2019 panel. An in-app help pop-up with reminder instructions can be accessed by clicking the '2. Find speaker' button when no BLEEP regions have been added. Finding/Labeling Audio to Redact Start by selecting a portion of audio that contains only the speaker you wish to redact by clicking and dragging in the waveform portion of the audio after locating an appropriate speech segment. It is possible to perform this step with as little as 3-5 seconds of audio from the appropriate speaker, but providing more labeled audio at the start will make the system's suggestions for additional regions more accurate. Add this selection as a region to be 'bleeped' by clicking the '+' icon directly below your selection in the \" BLEEP or keep \" tier, and selecting BLEEP . Repeat this until you have selected at least 3-5 seconds of the speaker you wish to redact (more is better). Once you've selected some audio to get started, tell the system to find more speech from this speaker by clicking '2. Find speaker' in the 'Redaction Task' GUI panel. In the \u2018Suggested\u2019 tier you will see \u2018processing\u2019 followed by yellow boxes where the speaker of interest is likely to be speaking, this can be useful in finding additional areas with the speaker. Use the \u2018Suggested\u2019 tier to help review the rest of the file and add all other regions that you want to redact as BLEEP regions. You can perform the 'Find Speaker' step as many times as is desired to continue refining the provided suggestions. It will always use each BLEEP region together as a representation of the speaker you're attempting to find. Note that if the system suggests regions of speech that do not belong to the speaker to be redacted, you can stop future suggestions of this region by selecting the suggestion and labeling it as keep . The keep regions are not essential for this task, and can be used or not depending on user preference. Note that if you make a mistake and mark a region you wish to redact as keep , or a region you wish to be left alone as BLEEP , you can remove this selection by hovering over or clicking the respective label, and selecting the 'x' that appears in the top left corner. You will then be free to re-add this label as a different type, or adjust the boundaries and then do so. Redacting Selected Audio Once you have found and labeled all regions containing speech belonging to this speaker, you can proceed on to the final step in the process, of actually redacting the selected audio. This is done by selecting the '3. Create redacted audio' button from the 'Redactino Task' GUI panel. Once this is done, the GUI will prompt you with a file-save dialog to select where to save out the new audio file. Finally, a new slot is added below the BLEEP or keep tier that will display the redacted audio once it is created, and allow review of the redacted audio. Select which channel will be played back using the speaker icons to the left of each waveform slot, and review the final redacted audio. If you find while reviewing this audio that any audio from the speaker was missed and is still present in the final file, it is still possible to step through each part of this process again as necessary. A new BLEEP region (or regions) can be added at this point to catch the missed speech, and the audio can be resubmitted through '3. Create redacted audio'. At this point, the user can choose to save a new file, or overwrite the original redacted file. Cautions and Limitations Note that there is currently no mechanism for saving progress or state of the Redaction Task or BLEEP or keep tier. The final created audio file is saved in the location provided by the user, but it is not currently possible to save the GUI state while in the middle of labeling BLEEP regions and resume or complete the redaction task at a later time.","title":"Speaker Redaction"},{"location":"redaction.html#speaker-redaction-task","text":"This document covers how to use the features added to the OLIVE Nightingale GUI to support the task of \"Speaker Redaction\". This allows for the assisted labeling and discovery of all sections of speech within an audio recording that belong to a single speaker, whose audio is wished to be removed from the audio in order to protect his or her identity. Currently, the designated audio is replaced with a tone, and all content there is destroyed. In the future, different approaches may be available that disguise the identity of the voice while leaving the content of the speech audible.","title":"Speaker Redaction Task"},{"location":"redaction.html#speaker-redaction-task-overview","text":"Generally, the process of stepping through the Redaction task follows these summarized steps: Load the audio to be processed Find and select an example of the speaker whose voice is to be redacted Submit this audio to the system, so it can find and suggest additional regions where this speaker occurs Add additional selections from these candidates to the labeled regions Optionally iterate through steps 3 and 4 until all audio from the speaker to be redacted is appropriately labeled Submit this audio to the redaction system to replace the labeled regions with a tone Review the resulting audio to make sure all of the speaker's audio was removed or disguised Add regions and resubmit to redaction system if necessary This process is covered in more detail below.","title":"Speaker Redaction Task Overview"},{"location":"redaction.html#speaker-redaction-walkthrough","text":"","title":"Speaker Redaction Walkthrough"},{"location":"redaction.html#getting-started","text":"To get started with this task, first open the OLIVE Nightingale GUI and load an audio file in by dragging and dropping a file into one of the GUI waveform slots, or by pressing ctrl+o or cmd+o while the GUI has focus to enter an 'Open File' dialog. Once the audio is loaded, select the \u20181. Set up tiers\u2019 button in the \u2018Redaction Task\u2019 panel. This will create two new tiers below the audio file: Suggested This tier is where the results from the speaker search algorithm will be displayed, providing suggested regions that the system believes belong to the speaker whose speech has been provided in the BLEEP regions of the tier below. BLEEP or keep This tier is where you provide 'seed' regions, as BLEEP labels to inform the speaker-search system what the speaker to redact sounds like. You can also use 'keep' labels to inform the system of regions that do not contain the speaker you're attempting to redact, so that those regions won't be suggested again in the future. All regions labeled as BLEEP in this tier are what will be removed by the redaction plugin during the final step of this process. All keep regions will be left alone, as will regions that have no labels. Only BLEEP regions will be affected. This initial setup step will also enable the \u20182. Find speaker\u2019 and \u20183. Create redacted audio\u2019 buttons on the \u2018Redaction task\u2019 panel. An in-app help pop-up with reminder instructions can be accessed by clicking the '2. Find speaker' button when no BLEEP regions have been added.","title":"Getting Started"},{"location":"redaction.html#findinglabeling-audio-to-redact","text":"Start by selecting a portion of audio that contains only the speaker you wish to redact by clicking and dragging in the waveform portion of the audio after locating an appropriate speech segment. It is possible to perform this step with as little as 3-5 seconds of audio from the appropriate speaker, but providing more labeled audio at the start will make the system's suggestions for additional regions more accurate. Add this selection as a region to be 'bleeped' by clicking the '+' icon directly below your selection in the \" BLEEP or keep \" tier, and selecting BLEEP . Repeat this until you have selected at least 3-5 seconds of the speaker you wish to redact (more is better). Once you've selected some audio to get started, tell the system to find more speech from this speaker by clicking '2. Find speaker' in the 'Redaction Task' GUI panel. In the \u2018Suggested\u2019 tier you will see \u2018processing\u2019 followed by yellow boxes where the speaker of interest is likely to be speaking, this can be useful in finding additional areas with the speaker. Use the \u2018Suggested\u2019 tier to help review the rest of the file and add all other regions that you want to redact as BLEEP regions. You can perform the 'Find Speaker' step as many times as is desired to continue refining the provided suggestions. It will always use each BLEEP region together as a representation of the speaker you're attempting to find. Note that if the system suggests regions of speech that do not belong to the speaker to be redacted, you can stop future suggestions of this region by selecting the suggestion and labeling it as keep . The keep regions are not essential for this task, and can be used or not depending on user preference. Note that if you make a mistake and mark a region you wish to redact as keep , or a region you wish to be left alone as BLEEP , you can remove this selection by hovering over or clicking the respective label, and selecting the 'x' that appears in the top left corner. You will then be free to re-add this label as a different type, or adjust the boundaries and then do so.","title":"Finding/Labeling Audio to Redact"},{"location":"redaction.html#redacting-selected-audio","text":"Once you have found and labeled all regions containing speech belonging to this speaker, you can proceed on to the final step in the process, of actually redacting the selected audio. This is done by selecting the '3. Create redacted audio' button from the 'Redactino Task' GUI panel. Once this is done, the GUI will prompt you with a file-save dialog to select where to save out the new audio file. Finally, a new slot is added below the BLEEP or keep tier that will display the redacted audio once it is created, and allow review of the redacted audio. Select which channel will be played back using the speaker icons to the left of each waveform slot, and review the final redacted audio. If you find while reviewing this audio that any audio from the speaker was missed and is still present in the final file, it is still possible to step through each part of this process again as necessary. A new BLEEP region (or regions) can be added at this point to catch the missed speech, and the audio can be resubmitted through '3. Create redacted audio'. At this point, the user can choose to save a new file, or overwrite the original redacted file.","title":"Redacting Selected Audio"},{"location":"redaction.html#cautions-and-limitations","text":"Note that there is currently no mechanism for saving progress or state of the Redaction Task or BLEEP or keep tier. The final created audio file is saved in the location provided by the user, but it is not currently possible to save the GUI state while in the middle of labeling BLEEP regions and resume or complete the redaction task at a later time.","title":"Cautions and Limitations"},{"location":"releasePlugins.html","text":"OLIVE 6.0.0 Release Plugins The following plugins have been tested and certified for compatibility and release with the OLIVE 6.0.0 software package. Speech Speech Activity Detection (SAD) sad-dnn-v8.3.0 (GPU capable) Deep Fake Audio Detection (DFA) dfa-end2end-v1.0.0 Speaker Speaker Identification (SID) sid-dplda-v3.3.0 (GPU capable) Speaker Detection (SDD) sdd-embed-v2.0.0 (GPU capable) Speaker Highlighting (SHL) shl-sbcEmbed-v1.0.2 Language Language Identification (LID) lid-hdplda-v2.0.2 (GPU capable) Language Detection (LDD) ldd-embed-v2.0.0 Gender Gender Identification (GID) gid-embedplda-v1.0.2 Gender Detection (GDD) gdd-embedplda-v1.0.1 Keyword Query By Example (QBE) qbe-ftdnn-v2.0.0 (lower resource) Transcription Automatic Speech Recognition (ASR) asr-end2end-v4.0.0 (GPU capable) asr-dynapy-v4.2.0 Translation Text Machine Translation (TMT) tmt-ctranslate-v1.3.2 (GPU capable) Manipulation Audio Redaction (RED) red-transform-v1.0.0 Imagery Face Detection Image (FDI) fdi-pyEmbed-v1.2.0 Face Detection Video (FDV) fdv-pyEmbed-v1.2.0 Face Recognition Image (FRI) fri-pyEmbed-v1.2.0 Face Recognition Video (FRV) frv-pyEmbed-v1.2.0 Topic Topic Detection ASR-based (TPD) tpd-dynapy-v5.1.0 Topic Detection ASR & Acoustic Fusion (TPD) tpd-fusion-v1.1.0 For further information on each individual plugin and their capabilities, please refer to the pages linked above. For more information about how to integrate with OLIVE and get started using these plugins, refer to the appropriate documentation linked from the landing page .","title":"Release Plugins"},{"location":"releasePlugins.html#olive-600-release-plugins","text":"The following plugins have been tested and certified for compatibility and release with the OLIVE 6.0.0 software package.","title":"OLIVE 6.0.0 Release Plugins"},{"location":"releasePlugins.html#speech","text":"Speech Activity Detection (SAD) sad-dnn-v8.3.0 (GPU capable) Deep Fake Audio Detection (DFA) dfa-end2end-v1.0.0","title":"Speech"},{"location":"releasePlugins.html#speaker","text":"Speaker Identification (SID) sid-dplda-v3.3.0 (GPU capable) Speaker Detection (SDD) sdd-embed-v2.0.0 (GPU capable) Speaker Highlighting (SHL) shl-sbcEmbed-v1.0.2","title":"Speaker"},{"location":"releasePlugins.html#language","text":"Language Identification (LID) lid-hdplda-v2.0.2 (GPU capable) Language Detection (LDD) ldd-embed-v2.0.0","title":"Language"},{"location":"releasePlugins.html#gender","text":"Gender Identification (GID) gid-embedplda-v1.0.2 Gender Detection (GDD) gdd-embedplda-v1.0.1","title":"Gender"},{"location":"releasePlugins.html#keyword","text":"Query By Example (QBE) qbe-ftdnn-v2.0.0 (lower resource)","title":"Keyword"},{"location":"releasePlugins.html#transcription","text":"Automatic Speech Recognition (ASR) asr-end2end-v4.0.0 (GPU capable) asr-dynapy-v4.2.0","title":"Transcription"},{"location":"releasePlugins.html#translation","text":"Text Machine Translation (TMT) tmt-ctranslate-v1.3.2 (GPU capable)","title":"Translation"},{"location":"releasePlugins.html#manipulation","text":"Audio Redaction (RED) red-transform-v1.0.0","title":"Manipulation"},{"location":"releasePlugins.html#imagery","text":"Face Detection Image (FDI) fdi-pyEmbed-v1.2.0 Face Detection Video (FDV) fdv-pyEmbed-v1.2.0 Face Recognition Image (FRI) fri-pyEmbed-v1.2.0 Face Recognition Video (FRV) frv-pyEmbed-v1.2.0","title":"Imagery"},{"location":"releasePlugins.html#topic","text":"Topic Detection ASR-based (TPD) tpd-dynapy-v5.1.0 Topic Detection ASR & Acoustic Fusion (TPD) tpd-fusion-v1.1.0 For further information on each individual plugin and their capabilities, please refer to the pages linked above. For more information about how to integrate with OLIVE and get started using these plugins, refer to the appropriate documentation linked from the landing page .","title":"Topic"},{"location":"restApi.html","text":"OLIVE 6.0.0 REST API Swagger Documentation Below is the Swagger for the OLIVE REST API; Note that this appears to be interactive, but messages submitted from this page won't properly reach the OLIVE Martini. However, if you currently have an OLIVE Martini container running, you can check out a live/interactive version of this Swagger documentation here: http://localhost:5004/swagger Or if your Martini instance has TLS enabled: https://localhost:5004/swagger And if the OLIVE Martini is running remotely: <hostname or ip>:5004/swagger const ui = SwaggerUIBundle({ url: 'olive-message-broker-api.yaml', dom_id: '#swagger-ui', })","title":"OLIVE REST API"},{"location":"restApi.html#olive-600-rest-api-swagger-documentation","text":"Below is the Swagger for the OLIVE REST API; Note that this appears to be interactive, but messages submitted from this page won't properly reach the OLIVE Martini. However, if you currently have an OLIVE Martini container running, you can check out a live/interactive version of this Swagger documentation here: http://localhost:5004/swagger Or if your Martini instance has TLS enabled: https://localhost:5004/swagger And if the OLIVE Martini is running remotely: <hostname or ip>:5004/swagger const ui = SwaggerUIBundle({ url: 'olive-message-broker-api.yaml', dom_id: '#swagger-ui', })","title":"OLIVE 6.0.0 REST API Swagger Documentation"},{"location":"server.html","text":"OLIVE Server Overview In the OLIVE architecture, if the individual plugins are the muscles of the system, the Server serves as the brains. It provides coordination and tasking, and is responsible for properly receiving and interpreting messages from client applications, kicking off the appropriate plugin jobs that these messages may request, as well as routing the proper response or results from these jobs back to the requesting client. The OLIVE Enterprise API is client/server based. Therefore, you must run the OLIVE server and manage its lifecycle as part of your integration effort. The server is included as part of the primary system installation. The OLIVE server communicates with clients over two ports, 5588 and 5589 by default (configurable), using ZeroMQ and Google Protocol Buffers. It relies on other components to perform its duties; namely an assembly of third party and other libraries that are delivered with OLIVE as the OLIVE Runtime , two SRI-built libraries; Idento and dnn, that empower the final puzzle piece, the plugins themselves, to complete their assigned tasks. OLIVE is usually delivered with a simple startup script for ease of use and to facilitate a rapid deployment, so the actual details of the relationship between these pieces does not need to be considered too closely if sticking, but the next section outlines how to properly establish the environmental setup that the Server needs to properly function, and how to manually start and configure the server. Installation, Environment Setup and Startup This section outlines the steps needed to prepare the OLIVE environment for proper operation, describes the contents of the OLIVE runtime library, and covers starting the actual server. The final section of this page also outlines some common pitfalls encountered with running the OLIVE server, and offers troubleshooting advice. Installation If you haven't already installed the OLIVE software package, please jump over and refer to our OLIVE Installation Guide for information about the layout of the software and a quick-start setup guide before continuing on. Environment Variables As discussed in the Installation Guide , we recommend using the provided olive_env.sh scripts to do the heavy lifting and set up the OLIVE environment, but it may be helpful to know what some of the important environment variables being set are referring to. Below is a selection of some of the major variables, with a brief description of their function. OLIVE The OLIVE environment variable points to the actuall OLIVE installation location - If the software is installed as shown in the install page , this is typically something like $HOME/olive5.0.0/. From this, the OLIVE software is able to properly add the relevant bin/ and lib directories to the $PATH and $LD_LIBRARY_PATH environment variables, respectively, and find/set other important variables. OLIVE_RUNTIME OLIVE_RUNTIME points to the location of the OLIVE Runtime library outlined below. It's very important that this is set, as OLIVE will not function properly without access to the libraries and dependencies in the runtime. In the two examples in the install page , the OLIVE_RUNTIME variable would be set to: $HOME/olive5.0.0/runtime-5.0.0-centos-7.3.1611-x86_64/ OLIVE_APP_DATA This variable is very important if running the OLIVE Server, but not necessary when using the OLIVE command line tools. OLIVE_APP_DATA tells the Server where to find the OLIVE plugins, and also determines where it will store things like class enrollments (i.e. speakers for SID plugins, keyword examples for QBE plugins), as well as server logs and other files that must be written out by the system. OLIVE_APP_DATA must contain a directory called plugins that contains valid OLIVE plugins. In the two examples in the install page , the OLIVE_APP_DATA variable would be set to: $HOME/oliveAppData/ Starting the Server Once the OLIVE environment has been properly established, launching the server is as simple as entering this into the appropriate terminal: oliveserver Additional runtime/configuration options are available for advanced operation, and their details can be found in the oliveserver usage statement: usage : oliveserver [ - h ] [ -- version ] [ -- verbose ] [ -- interface INTERFACE ] [ -- port REQUEST_PORT ] [ -- upload_port UPLOAD_PORT ] [ -- enable_oob_upload ] [ -- workers WORKERS ] [ -- work_dir WORK_DIR ] [ -- enroll_dir ENROLL_DIR ] [ -- debug ] [ -- monitor ] [ -- quiet ] [ -- timeout TIMEOUT ] [ -- options OPTIONS_PATH ] optional arguments : - h , -- help show this help message and exit -- version show program 's version number and exit -- verbose increase server logging output -- interface INTERFACE server binds to this address ; default * ( all ) -- port REQUEST_PORT the first of three sequentially numbered ports used by the server . The request port is the first port in this range , followed by the status port , and then an interal port only used by the server ; default value is 5588. -- upload_port UPLOAD_PORT port for out of band uploading of files -- enable_oob_upload enable out of band file uploads -- workers WORKERS , - j WORKERS specify number of parallel WORKERS to run ; default is the number of local processors -- work_dir WORK_DIR path to work dir to create . default value of environment variable $ OLIVE_APP_DATA -- enroll_dir ENROLL_DIR path for storage of enrollment data . default value of environment variable $ OLIVE_APP_DATA -- debug debug mode prevents deletion of logs and intermediate files on success -- monitor special debug option to monitor memory usage . Must be used with the debug flag -- quiet , - q suppress sending log information to the console -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have more or more configuration sections . Currently only the server 'server' section is supported An example for setting up the OLIVE runtime environment using the first directory setup shown on the install page , and launching the OLIVE server: $ cd $ HOME / olive5 . 0.0 / runtime - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ cd $ HOME / olive5 . 0.0 / olive - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / $ oliveserver -- port 6678 -- work_dir / home / sysadmin / OLIVE In this example, the ports that the server will listen on and broadcast its status heartbeat messages on have been update, in addition to setting a custom WORK directory. For more information on the available server options, please refer to the OLIVE Server Options section below. Note that upon launching the server, if the operation was successful, the user will be greeted with a display of the plugins and domains that the server has access to, as well as a \u201cServer ready\u201d message. If a list of plugins does not appear, but the \u201cServer ready\u201d message does, the OLIVE_APP_DATA environment variable should be checked to ensure that it contains a valid plugins directory. An example output of a successful server startup: [ oliveuser@localhost olive5.7.0 ] $ oliveserver TASK PLUGIN VERSION DOMAINS ------ ----------------------------------- --------- ------------------------------------------- ASR asr - end2end - v1 .3.1 1.3.1 english - augmented - v1 ( gpu0 ) english - spanish - v1 ( cpu ) farsi - v1 ( cpu ) french - v1 ( cpu ) iraqiArabic - v1 ( cpu ) japanese - augmented - v1 ( cpu ) khmer - augmented - v2 ( gpu0 ) korean - augmented - v1 ( cpu ) levantineArabic - v1 ( cpu ) mandarin - augmented - v2 ( gpu0 ) pashto - v1 ( cpu ) russian - augmented - v1 ( cpu ) spanish - augmented - v1 ( cpu ) ukrainian - augmented - v1 ( cpu ) LID lid - hdplda - v2 .0.0 2.0.0 multi - araDialects - v1 ( cpu ) multi - v1 ( cpu ) SAD sad - dnn - v8 .2.0 8.2.0 fast - multi - v1 ( cpu ) multi - v1 ( cpu ) vtd - v1 ( cpu ) SDD sdd - diarizeEmbedSmolive - v1 .0.4 1.0.4 telClosetalk - smart - v1 SHL shl - sbcEmbed - v1 .0.2 1.0.2 micFarfield - v1 telClosetalk - v1 SID sid - dplda - v3 .1.0 3.1.0 multi - v1 ( cpu ) TMT tmt - ctranslate - v1 .3.0 1.3.0 english - mandarin - v2 ( cpu ) english - russian - v2 ( cpu ) english - spanish - v2 ( cpu ) english - ukrainian - v2 ( cpu ) iraqiArabic - english - v1 ( cpu ) mandarin - english - text - v2 ( cpu ) mandarin - english - v2 ( cpu ) russian - english - v2 ( cpu ) spanish - english - v2 ( cpu ) ukrainian - english - v2 ( cpu ) --------- Olive Server (v5.7.0) ready, Thu Jun 13 20:54:38 2024 --------- The olive_env.sh file is included with OLIVE deliveries and does the bulk of the environment setup required for running the server. Important variables that are handled with this script and that are required for running the OLIVE Server include: OLIVE_RUNTIME Must be set to the path of the OLIVE runtime directory. Allows OLIVE to find the libraries it requires. Ex: /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/ OLIVE Contains the path to the OLIVE installation directory. Allows OLIVE to find the binaries and utilities it relies on to function. Ex: /home/user1/olive5.0.0-installation/olive-5.0.0-centos-7.3.1611-x86_64/ or /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/olive-5.0.0-centos-7.3.1611-x86_64/ OLIVE_APP_DATA This is the path used by the server to store class enrollments and log files. This is also where the plugins that the server will have access to must be stored, in a directory simply named plugins. If OLIVE_APP_DATA is set to a directory that does not contain a plugins folder, the server will launch with no plugins. Ex: /home/user1/oliveAppData/ Where the contents of oliveAppData might be: plugins/ sad-dnn-v4a sid-embed-v2 lid-embed-v2 server/ (generated by the server) enrollments/ processing/ If your server is protected by a firewall you must ensure that any ports you attempt to utilize for the server are able to send and receive network traffic. The path you provide to --work_dir will ideally point to a large and performant hard drive. The location will house the enrollments, log files and temporary workspaces utilized by the server. Server Options interface Use this option to specify a single interface for the server to use for communication, otherwise the ports used by the server (5588, 5589, 5590 by default ) are available to all interfaces on the host. For example, --interface 192.168.10.20 would restrict the server to using only that interface, and only available to clients on that interface port Use this option to change the set of ports used by the server for external and internal communication. For example --port 6000 instructs the server to use ports 6000 (requests), 6001 (heartbeats), 60002 (internal communication) upload_port Specify the port used for experimental \"out of band\" upload feature. For advanced users only. enable_oob_upload Enable \"out of band\" file uploads. This is an experimental feature for very specific scenarios that should not be used without proper guidance. workers Use this option to specify the number of current jobs the server can process, this value should be set between 1 and the number of processors on the host work_dir Use this option to specify an alternate location of the directory where the server stores log files, enrollments, and temporary data. Setting this does NOT change the location of the plugins, which should still be located at $OLIVE_APP_DATA/plugins enroll_dir Use this option to specify an alternate location of the enrollments directory, when specified the work directory and plugins location is not changed. debug Use this option to preserver all logs files and provide extra output from OLIVE and plugins in some circumstances monitor Used in conjunction with the debug flag, enable a special memory-usage monitoring mode for internal troubleshooting. verbose Use this option with the --debug option to produce to maximize debug output quiet Use this option to start the server without non-error output messages timeout Use this option adjust the timeout (in seconds) used for jobs, unless instructed by OLIVE technical support this value should not specified options Used to pass special options to the server, this option should only be used with guidance from OLIVE technical support Remote Access The OLIVE server uses three sequential ports for communication; by default these are ports are 5588, 5589, and 5590. An alternate set of ports can be specified using the --port argument, which instructs the server to use 3 ports starting at the given value, so if --port 6000 is specified then ports 6000 (request port), 6001 (status port), and 6002 (internal port) are used. Clients send and receive responses on the request port (5588 by default), while the server publishes health and status information on the status port (5589 by default). The internal IPC port (5590 by default) is only used for internal server communication. You can restrict these ports to a specific interface by using the --interface option, so that the server is only available on that network (by default the server should be available on all interfaces, which may not be desirable on a multi-homed server). For example, specifying an interface of --interface 192.168.10.99 would restrict the server to communication only on that port. You could also use the interface option to restrict the server to local processing, not networked, by setting interface as: --interface 127.0.0.1 (on some OSs you may be able to specify --interface localhost instead of the loopback address, 127.0.0.1). If using the localhost interface, the server is not accessible to remote clients. Important Information Audio Formats The OLIVE server can handle a wide range of input audio file formats, but does have some limitations. For more information on the details of these limitations, please refer to the OLIVE Audio Formats information page. Common Pitfalls One common mistake that's encountered is when informing the OLIVE Server where to find plugins. There is a directory within the OLIVE distribution, olive/plugins that is often mistaken for the directory where OLIVE plugins should oro do reside. This directory has a different purpose, however, and should not be used as such. Guidelines are provided above for general recommendations for where to place your OLIVE plugins.","title":"Server Guide"},{"location":"server.html#olive-server","text":"","title":"OLIVE Server"},{"location":"server.html#overview","text":"In the OLIVE architecture, if the individual plugins are the muscles of the system, the Server serves as the brains. It provides coordination and tasking, and is responsible for properly receiving and interpreting messages from client applications, kicking off the appropriate plugin jobs that these messages may request, as well as routing the proper response or results from these jobs back to the requesting client. The OLIVE Enterprise API is client/server based. Therefore, you must run the OLIVE server and manage its lifecycle as part of your integration effort. The server is included as part of the primary system installation. The OLIVE server communicates with clients over two ports, 5588 and 5589 by default (configurable), using ZeroMQ and Google Protocol Buffers. It relies on other components to perform its duties; namely an assembly of third party and other libraries that are delivered with OLIVE as the OLIVE Runtime , two SRI-built libraries; Idento and dnn, that empower the final puzzle piece, the plugins themselves, to complete their assigned tasks. OLIVE is usually delivered with a simple startup script for ease of use and to facilitate a rapid deployment, so the actual details of the relationship between these pieces does not need to be considered too closely if sticking, but the next section outlines how to properly establish the environmental setup that the Server needs to properly function, and how to manually start and configure the server.","title":"Overview"},{"location":"server.html#installation-environment-setup-and-startup","text":"This section outlines the steps needed to prepare the OLIVE environment for proper operation, describes the contents of the OLIVE runtime library, and covers starting the actual server. The final section of this page also outlines some common pitfalls encountered with running the OLIVE server, and offers troubleshooting advice.","title":"Installation, Environment Setup and Startup"},{"location":"server.html#installation","text":"If you haven't already installed the OLIVE software package, please jump over and refer to our OLIVE Installation Guide for information about the layout of the software and a quick-start setup guide before continuing on.","title":"Installation"},{"location":"server.html#environment-variables","text":"As discussed in the Installation Guide , we recommend using the provided olive_env.sh scripts to do the heavy lifting and set up the OLIVE environment, but it may be helpful to know what some of the important environment variables being set are referring to. Below is a selection of some of the major variables, with a brief description of their function.","title":"Environment Variables"},{"location":"server.html#olive","text":"The OLIVE environment variable points to the actuall OLIVE installation location - If the software is installed as shown in the install page , this is typically something like $HOME/olive5.0.0/. From this, the OLIVE software is able to properly add the relevant bin/ and lib directories to the $PATH and $LD_LIBRARY_PATH environment variables, respectively, and find/set other important variables.","title":"OLIVE"},{"location":"server.html#olive_runtime","text":"OLIVE_RUNTIME points to the location of the OLIVE Runtime library outlined below. It's very important that this is set, as OLIVE will not function properly without access to the libraries and dependencies in the runtime. In the two examples in the install page , the OLIVE_RUNTIME variable would be set to: $HOME/olive5.0.0/runtime-5.0.0-centos-7.3.1611-x86_64/","title":"OLIVE_RUNTIME"},{"location":"server.html#olive_app_data","text":"This variable is very important if running the OLIVE Server, but not necessary when using the OLIVE command line tools. OLIVE_APP_DATA tells the Server where to find the OLIVE plugins, and also determines where it will store things like class enrollments (i.e. speakers for SID plugins, keyword examples for QBE plugins), as well as server logs and other files that must be written out by the system. OLIVE_APP_DATA must contain a directory called plugins that contains valid OLIVE plugins. In the two examples in the install page , the OLIVE_APP_DATA variable would be set to: $HOME/oliveAppData/","title":"OLIVE_APP_DATA"},{"location":"server.html#starting-the-server","text":"Once the OLIVE environment has been properly established, launching the server is as simple as entering this into the appropriate terminal: oliveserver Additional runtime/configuration options are available for advanced operation, and their details can be found in the oliveserver usage statement: usage : oliveserver [ - h ] [ -- version ] [ -- verbose ] [ -- interface INTERFACE ] [ -- port REQUEST_PORT ] [ -- upload_port UPLOAD_PORT ] [ -- enable_oob_upload ] [ -- workers WORKERS ] [ -- work_dir WORK_DIR ] [ -- enroll_dir ENROLL_DIR ] [ -- debug ] [ -- monitor ] [ -- quiet ] [ -- timeout TIMEOUT ] [ -- options OPTIONS_PATH ] optional arguments : - h , -- help show this help message and exit -- version show program 's version number and exit -- verbose increase server logging output -- interface INTERFACE server binds to this address ; default * ( all ) -- port REQUEST_PORT the first of three sequentially numbered ports used by the server . The request port is the first port in this range , followed by the status port , and then an interal port only used by the server ; default value is 5588. -- upload_port UPLOAD_PORT port for out of band uploading of files -- enable_oob_upload enable out of band file uploads -- workers WORKERS , - j WORKERS specify number of parallel WORKERS to run ; default is the number of local processors -- work_dir WORK_DIR path to work dir to create . default value of environment variable $ OLIVE_APP_DATA -- enroll_dir ENROLL_DIR path for storage of enrollment data . default value of environment variable $ OLIVE_APP_DATA -- debug debug mode prevents deletion of logs and intermediate files on success -- monitor special debug option to monitor memory usage . Must be used with the debug flag -- quiet , - q suppress sending log information to the console -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have more or more configuration sections . Currently only the server 'server' section is supported An example for setting up the OLIVE runtime environment using the first directory setup shown on the install page , and launching the OLIVE server: $ cd $ HOME / olive5 . 0.0 / runtime - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ cd $ HOME / olive5 . 0.0 / olive - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / $ oliveserver -- port 6678 -- work_dir / home / sysadmin / OLIVE In this example, the ports that the server will listen on and broadcast its status heartbeat messages on have been update, in addition to setting a custom WORK directory. For more information on the available server options, please refer to the OLIVE Server Options section below. Note that upon launching the server, if the operation was successful, the user will be greeted with a display of the plugins and domains that the server has access to, as well as a \u201cServer ready\u201d message. If a list of plugins does not appear, but the \u201cServer ready\u201d message does, the OLIVE_APP_DATA environment variable should be checked to ensure that it contains a valid plugins directory. An example output of a successful server startup: [ oliveuser@localhost olive5.7.0 ] $ oliveserver TASK PLUGIN VERSION DOMAINS ------ ----------------------------------- --------- ------------------------------------------- ASR asr - end2end - v1 .3.1 1.3.1 english - augmented - v1 ( gpu0 ) english - spanish - v1 ( cpu ) farsi - v1 ( cpu ) french - v1 ( cpu ) iraqiArabic - v1 ( cpu ) japanese - augmented - v1 ( cpu ) khmer - augmented - v2 ( gpu0 ) korean - augmented - v1 ( cpu ) levantineArabic - v1 ( cpu ) mandarin - augmented - v2 ( gpu0 ) pashto - v1 ( cpu ) russian - augmented - v1 ( cpu ) spanish - augmented - v1 ( cpu ) ukrainian - augmented - v1 ( cpu ) LID lid - hdplda - v2 .0.0 2.0.0 multi - araDialects - v1 ( cpu ) multi - v1 ( cpu ) SAD sad - dnn - v8 .2.0 8.2.0 fast - multi - v1 ( cpu ) multi - v1 ( cpu ) vtd - v1 ( cpu ) SDD sdd - diarizeEmbedSmolive - v1 .0.4 1.0.4 telClosetalk - smart - v1 SHL shl - sbcEmbed - v1 .0.2 1.0.2 micFarfield - v1 telClosetalk - v1 SID sid - dplda - v3 .1.0 3.1.0 multi - v1 ( cpu ) TMT tmt - ctranslate - v1 .3.0 1.3.0 english - mandarin - v2 ( cpu ) english - russian - v2 ( cpu ) english - spanish - v2 ( cpu ) english - ukrainian - v2 ( cpu ) iraqiArabic - english - v1 ( cpu ) mandarin - english - text - v2 ( cpu ) mandarin - english - v2 ( cpu ) russian - english - v2 ( cpu ) spanish - english - v2 ( cpu ) ukrainian - english - v2 ( cpu ) --------- Olive Server (v5.7.0) ready, Thu Jun 13 20:54:38 2024 --------- The olive_env.sh file is included with OLIVE deliveries and does the bulk of the environment setup required for running the server. Important variables that are handled with this script and that are required for running the OLIVE Server include: OLIVE_RUNTIME Must be set to the path of the OLIVE runtime directory. Allows OLIVE to find the libraries it requires. Ex: /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/ OLIVE Contains the path to the OLIVE installation directory. Allows OLIVE to find the binaries and utilities it relies on to function. Ex: /home/user1/olive5.0.0-installation/olive-5.0.0-centos-7.3.1611-x86_64/ or /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/olive-5.0.0-centos-7.3.1611-x86_64/ OLIVE_APP_DATA This is the path used by the server to store class enrollments and log files. This is also where the plugins that the server will have access to must be stored, in a directory simply named plugins. If OLIVE_APP_DATA is set to a directory that does not contain a plugins folder, the server will launch with no plugins. Ex: /home/user1/oliveAppData/ Where the contents of oliveAppData might be: plugins/ sad-dnn-v4a sid-embed-v2 lid-embed-v2 server/ (generated by the server) enrollments/ processing/ If your server is protected by a firewall you must ensure that any ports you attempt to utilize for the server are able to send and receive network traffic. The path you provide to --work_dir will ideally point to a large and performant hard drive. The location will house the enrollments, log files and temporary workspaces utilized by the server.","title":"Starting the Server"},{"location":"server.html#server-options","text":"","title":"Server Options"},{"location":"server.html#interface","text":"Use this option to specify a single interface for the server to use for communication, otherwise the ports used by the server (5588, 5589, 5590 by default ) are available to all interfaces on the host. For example, --interface 192.168.10.20 would restrict the server to using only that interface, and only available to clients on that interface","title":"interface"},{"location":"server.html#port","text":"Use this option to change the set of ports used by the server for external and internal communication. For example --port 6000 instructs the server to use ports 6000 (requests), 6001 (heartbeats), 60002 (internal communication)","title":"port"},{"location":"server.html#upload_port","text":"Specify the port used for experimental \"out of band\" upload feature. For advanced users only.","title":"upload_port"},{"location":"server.html#enable_oob_upload","text":"Enable \"out of band\" file uploads. This is an experimental feature for very specific scenarios that should not be used without proper guidance.","title":"enable_oob_upload"},{"location":"server.html#workers","text":"Use this option to specify the number of current jobs the server can process, this value should be set between 1 and the number of processors on the host","title":"workers"},{"location":"server.html#work_dir","text":"Use this option to specify an alternate location of the directory where the server stores log files, enrollments, and temporary data. Setting this does NOT change the location of the plugins, which should still be located at $OLIVE_APP_DATA/plugins","title":"work_dir"},{"location":"server.html#enroll_dir","text":"Use this option to specify an alternate location of the enrollments directory, when specified the work directory and plugins location is not changed.","title":"enroll_dir"},{"location":"server.html#debug","text":"Use this option to preserver all logs files and provide extra output from OLIVE and plugins in some circumstances","title":"debug"},{"location":"server.html#monitor","text":"Used in conjunction with the debug flag, enable a special memory-usage monitoring mode for internal troubleshooting.","title":"monitor"},{"location":"server.html#verbose","text":"Use this option with the --debug option to produce to maximize debug output","title":"verbose"},{"location":"server.html#quiet","text":"Use this option to start the server without non-error output messages","title":"quiet"},{"location":"server.html#timeout","text":"Use this option adjust the timeout (in seconds) used for jobs, unless instructed by OLIVE technical support this value should not specified","title":"timeout"},{"location":"server.html#options","text":"Used to pass special options to the server, this option should only be used with guidance from OLIVE technical support","title":"options"},{"location":"server.html#remote-access","text":"The OLIVE server uses three sequential ports for communication; by default these are ports are 5588, 5589, and 5590. An alternate set of ports can be specified using the --port argument, which instructs the server to use 3 ports starting at the given value, so if --port 6000 is specified then ports 6000 (request port), 6001 (status port), and 6002 (internal port) are used. Clients send and receive responses on the request port (5588 by default), while the server publishes health and status information on the status port (5589 by default). The internal IPC port (5590 by default) is only used for internal server communication. You can restrict these ports to a specific interface by using the --interface option, so that the server is only available on that network (by default the server should be available on all interfaces, which may not be desirable on a multi-homed server). For example, specifying an interface of --interface 192.168.10.99 would restrict the server to communication only on that port. You could also use the interface option to restrict the server to local processing, not networked, by setting interface as: --interface 127.0.0.1 (on some OSs you may be able to specify --interface localhost instead of the loopback address, 127.0.0.1). If using the localhost interface, the server is not accessible to remote clients.","title":"Remote Access"},{"location":"server.html#important-information","text":"","title":"Important Information"},{"location":"server.html#audio-formats","text":"The OLIVE server can handle a wide range of input audio file formats, but does have some limitations. For more information on the details of these limitations, please refer to the OLIVE Audio Formats information page.","title":"Audio Formats"},{"location":"server.html#common-pitfalls","text":"One common mistake that's encountered is when informing the OLIVE Server where to find plugins. There is a directory within the OLIVE distribution, olive/plugins that is often mistaken for the directory where OLIVE plugins should oro do reside. This directory has a different purpose, however, and should not be used as such. Guidelines are provided above for general recommendations for where to place your OLIVE plugins.","title":"Common Pitfalls"},{"location":"traits.html","text":"OLIVE Plugin Traits Traits Overview The functionality of each OLIVE plugin is defined by the API Traits that it implements. Each Trait defines a message or set of messages that the plugin must implement to perform an associated task. The available Traits are listed below, along with their associated implemented API messages, in the format: Trait Implemented Message 1 ... Implemented Message N Traits and their Corresponding Messages The Trait and Message List follows. Each Trait name links to the relevant section on this page . Each API message entry links to the relevant portion of the Protocol Buffers Message Definition Reference Page. GlobalScorer - ( Java Tutorial -- Python Tutorial ) GlobalScorerRequest GlobalScorerStereoRequest RegionScorer - ( Java Tutorial -- Python Tutorial ) RegionScorerRequest RegionScorerStereoRequest FrameScorer - ( Java Tutorial -- Python Tutorial ) FrameScorerRequest FrameScorerStereoRequest ClassModifier - ( Java Tutorial -- Python Tutorial ) ClassModificationRequest ClassRemovalRequest ClassExporter ClassExportRequest ClassImportRequest AudioConverter AudioModificationRequest AudioVectorizer PluginAudioVectorRequest LearningTrait (SupervisedAdapter, SupervisedTrainer, UnsupervisedAdapter) - ( Java Tutorial ) PreprocessAudioTrainRequest PreprocessAudioAdaptRequest SupervisedTrainingRequest UnsupervisedAdaptationRequest UpdateTrait GetUpdateStatusRequest ApplyUpdateRequest GlobalComparer GlobalComparerRequest TextTransformer TextTransformationRequest AudioAlignmentScorer AudioAlignmentScoreRequest In addition to the messages above, the following messages exist for interacting with the server itself and are independent of individual plugins or plugin Traits: GetStatusRequest PluginDirectoryRequest LoadPluginDomainRequest RemovePluginDomainRequest Full message definition details for all of the messages mentioned here Request/Result Message Pairs In general, each Request message is paired with a Result message, that defines the structure and contents of the server's reply to a given client Request message. The table below defines the pairs of the Result messages for the Request messages mentioned above; and the full message definition details can be found in the OLIVE API Message Protocol Documentation , which each entry below is linked to. Request Message Result (Response) Message GlobalScorerRequest GlobalScorerResult RegionScorerRequest RegionScorerResult FrameScorerRequest FrameScorerResult ClassModificationRequest ClassModificationResult ClassRemovalRequest ClassRemovalResult ClassExportRequest ClassExportResult ClassImportRequest ClassImportResult AudioModificationRequest AudioModificationResult PluginAudioVectorRequest PluginAudioVectorResult PreprocessAudioTrainRequest PreprocessAudioTrainResult PreprocessAudioAdaptRequest PreprocessAudioAdaptResult SupervisedTrainingRequest SupervisedTrainingResult UnsupervisedAdaptationRequest UnsupervisedAdaptationResult GetUpdateStatusRequest GetUpdateStatusResult ApplyUpdateRequest ApplyUpdateResult GlobalComparerRequest GlobalComparerResult TextTransformationRequest TextTransformationResult AudioAlignmentScoreRequest AudioAlignmentScoreResult or Task Request Message Result (Response) Message Global Score GlobalScorerRequest GlobalScorerResult Region Score RegionScorerRequest RegionScorerResult Frame Score FrameScorerRequest FrameScorerResult Create or modify a class enrollment ClassModificationRequest ClassModificationResult Remove an enrolled class ClassRemovalRequest ClassRemovalResult Export a class for later use or for model sharing ClassExportRequest ClassExportResult Import a previously exported class ClassImportRequest ClassImportResult Submit audio for modification (enhancement) AudioModificationRequest AudioModificationResult Request a vectorized representation of audio PluginAudioVectorRequest PluginAudioVectorResult Prepare audio for submission to training a new domain from scratch PreprocessAudioTrainRequest PreprocessAudioTrainResult Prepare audio for submission to adapt an existing domain PreprocessAudioAdaptRequest PreprocessAudioAdaptResult Query a plugin to see if it is ready to apply an \"Update\" GetUpdateStatusRequest GetUpdateStatusResult Instruct a plugin to perform an \"Update\" ApplyUpdateRequest ApplyUpdateResult Submit two audio files for forensic comparison GlobalComparerRequest GlobalComparerResult Submit text (string) for translation TextTransformationRequest TextTransformationResult Submit two or more audio files for alignment shift scores AudioAlignmentScoreRequest AudioAlignmentScoreResult Traits Deep Dive This section provides more details on what the \"real world\" usage and definition of each of the above Traits actually entails. If anything is unclear or could use more expansion, please contact us and we will provide updates and/or clarification. GlobalScorer A GlobalScorer plugin has the output of reporting a single score for each relevant plugin class representing the likelihood that the entire audio file or clip contains this class. For example, when audio is sent to a Speaker Identification (SID) plugin with the GlobalScorer Trait, if there are 3 enrolled speakers at that time, the plugin will return a single score for each enrolled speaker representing the likelihood that the audio is comprised of speech from each speaker, respectively. The output of such a request will contain information that looks something like this: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 Keep in mind that speakers and speaker identification are just examples in this case, and that this generalizes to any global scoring plugin and whatever classes it is meant to distinguish between. This same example could be represented more generically as: /data/sid/audio/file1.wav class1 -0.5348 /data/sid/audio/file1.wav class2 3.2122 /data/sid/audio/file1.wav class3 -5.5340 Due to the nature of global scoring plugins, they are best used when the attribute that's being estimated is known to be, or very likely to be static. For example, using global scoring SID on one side of a 4-wire telephone conversation, where the speaker is likely to remain the same throughout, or using global scoring LID on a TV broadcast that is expected to only contain a single language. These plugins can miss the nuances of or be confused by things like code-switching within recordings, or unexpected speaker changes. The benefits of global scoring plugins are that they are often very fast, since they treat the entirety of the submitted audio as a single unit, and do not worry about trying to subdivide or chunk it in any way. For a first-pass or quick triage approach, or when the data is known or suspected to be homogenous in the ways discussed above, these plugins are very effective. When finer grained results are necessary, though, a RegionScorer or FrameScorer may be more appropriate. RegionScorer For each audio file or recording submitted to a RegionScorer plugin, results are returned consisting of 0 or more regions with an associated score and plugin class. Typically, regions are returned in the case of a 'detection' of an instance of the respective class. A 'region' is a pair of timestamps, referring to the start time and end time of the detected class presence, and includes the name of the respective class, and an associated score. An example of this might be a keyword spotting plugin returning the keyword class name, as well as the location and likelihood of that keyword's presence. This might look something like: /data/test/testFile1.wav 0.630 1.170 Airplane 4.3725 /data/test/testFile1.wav 1.520 2.010 Watermelon -1.1978 Another example of output you may see from a region scoring plugin follows, showing what a region scoring speaker detection plugin might output. In this example, an enrolled speaker, speaker2 was detected in testFile1.wav from 0.630 s to 1.170 s with a likelihood score of 4.3725. Likewise for 1.520 s to 2.010 s in the same file for the enrolled speaker called speaker1 , this time with a likelihood score of -1.1978. /data/test/testFile1.wav 0.630 1.170 speaker2 4.3725 /data/test/testFile1.wav 1.520 2.010 speaker1 -1.1978 An even simpler output of this type may just label the regions within a file that the plugin determines contain speech. Again, these are just arbitrary examples using a specific plugin type to more easily describe the scoring type; a more generic output example could be: /data/test/testFile1.wav 0.630 1.170 class1 4.3725 /data/test/testFile1.wav 1.520 2.010 class2 -1.1978 RegionScorer plugins allow a finer resolution with respect to results granularity, and allow plugins to be more flexible and deal with transitions between languages or speakers or other classes of interest within a given audio file or recording. Sometimes this comes at a cost of increased processing complexity and/or slower runtime. FrameScorer A plugin with the FrameScorer Trait that is queried with a FrameScorerRequest will provide score results for each frame of whatever audio has been submitted. Unless otherwise noted, an audio frame is 10 milliseconds. The most common OLIVE plugin that has the FrameScorer Trait are speech activity detection (SAD) plugins, where the score for each frame represents the likelihood of speech being present in that 10 ms audio frame. The output in this case is simply a sequential list of numbers, corresponding to the output score for each frame, in order: 1.9047 1.8088 1.2382 - 0.8862 - 2.5509 In this example, these frames can then be processed to turn them into region scores, labeling the locations where speech has been detected as present within the file. Returning raw frame scores as a result allows more down-stream flexibility, allowing thresholds to be adjusted and regions re-labeled if desired, for example to allow for tuning for more difficult or unexpected audio conditions. ClassModifier Any plugin capable of adding or removing classes to or from its set of enrollments or target classes carries the ClassModifier Trait. This trait means the set of classes the plugin is interested in is mutable, and can be altered. Typically this is done by providing labeled data that belongs to the new class of interest to the server, which then enrolls a new model representing what it has learned about distinguishing this class from others. Existing class models can also be augmented by providing the system with additional data with this class label. In addition to adding new classes, and improving/augmenting existing ones, it is also possible to remove enrolled classes from domains of plugins carrying this trait, using the ClassRemovalRequest message. ClassExporter A plugin that implements the ClassExporter Trait is capable of exporting enrolled class models in a form that can be either imported back into the same system, as a way to save the model for preservation, or it can be imported into a different system, so long as that system has the same plugin and domain that was used to initially create the class model. This allows enrollments to be shared between systems. In general, exported models are specific to the plugin and domain that created them, so care must be taken to ensure models are not mixed in to other plugins. It is up to the client program or end user to keep tabs on where the exported models came from and what class they represent, and to manage these models once they've been exported. AudioConverter An AudioConverter plugin has audio as both its input and as its output. This Trait allows the system to take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. The only current plugin to implement the AudioConverter Trait is a speech enhancement plugin for enhancing the speech intelligibility of the submitted audio. AudioVectorizer This is a prototype feature that allows the system to preprocess an audio file or model to perform the compute-heavy steps of feature extraction and others, so that at a later time a user can either enroll a model or score a file very quickly, since the most time-consuming steps have already been performed. Like the ClassExporter Trait and exported classes/models, the vectorized audio representations are plugin/domain specific and cannot be used with plugins other than the one that created them. This is helpful if enrollments will be frequently rolled in or out of the system, or if the same audio files will be frequently re-tested, to avoid wasted repeat compute cycles. LearningTrait There are three total LearningTrait s: SupervisedAdapter SupervisedTrainer UnsupervisedAdapter Only SupervisedAdapter is currently supported by plugins in the OLIVE ecosystem; the others are deprecated or prototype features and should be ignored. SupervisedAdapter Plugins with the SupervisedAdapter Trait enable the ability to perform Supervised Adaptation of a domain. This is human assisted improvement of a plugin's domain, generally with feedback to the system in the form of annotations of target phenomena, or in some cases, error corrections. For more information on Adaptation, refer to this section . UpdateTrait Certain systems, such as recent speaker and language recognition plugins, sid-embed-v5 and lid-embedplda-v1], have the capability to adapt themselves based purely on raw data provided by the user in the normal use of the system (enrollment and test data). These systems collect statistics over from the data feed through the system that can be used to update system parameters in unsupervised (autonomous) adaptation, thereby improving the performance of the plugin in the conditions in which is has been deployed. Since the use of this data in adaptation changes the behavior of the plugin the system does not automatically update itself, but rather requires the end user to \"trigger\" the update and use the data the system has collected to adapt. Implementing and invoking the associated Update Request Messages to start the Update process will use the accrued data from the test and enroll conditions to pdate system parameters and apply the update. One can always revert to the plugin's original state by clearing out the data and statistics collected the learn directory from the server's storage. GlobalComparer A plugin that supports the GlobalComparer Trait has the capability of accepting two waveforms as input, performing some sort of analysis or comparison of the two, and returning a PDF report summarizing the analysis. TextTransformer A plugin with the TextTransformer Trait is used to translate a text input when queried with a TextTransformationRequest, providing translation results for the submitted string (it does not take an audio input, unlike other scoring traits). The output in this case is simply a string, which is the translation result AudioAlignmentScorer A plugin with the AudioAlignmentScorer Trait can be used to provide alignment shift scores for two or more audio inputs using an AudioAlignmentScoreRequest. The output is a set of shift scores betwen each combination of two audio inputs in the AudioAlignmentScoreRequest","title":"OLIVE Plugin Traits"},{"location":"traits.html#olive-plugin-traits","text":"","title":"OLIVE Plugin Traits"},{"location":"traits.html#traits-overview","text":"The functionality of each OLIVE plugin is defined by the API Traits that it implements. Each Trait defines a message or set of messages that the plugin must implement to perform an associated task. The available Traits are listed below, along with their associated implemented API messages, in the format: Trait Implemented Message 1 ... Implemented Message N","title":"Traits Overview"},{"location":"traits.html#traits-and-their-corresponding-messages","text":"The Trait and Message List follows. Each Trait name links to the relevant section on this page . Each API message entry links to the relevant portion of the Protocol Buffers Message Definition Reference Page. GlobalScorer - ( Java Tutorial -- Python Tutorial ) GlobalScorerRequest GlobalScorerStereoRequest RegionScorer - ( Java Tutorial -- Python Tutorial ) RegionScorerRequest RegionScorerStereoRequest FrameScorer - ( Java Tutorial -- Python Tutorial ) FrameScorerRequest FrameScorerStereoRequest ClassModifier - ( Java Tutorial -- Python Tutorial ) ClassModificationRequest ClassRemovalRequest ClassExporter ClassExportRequest ClassImportRequest AudioConverter AudioModificationRequest AudioVectorizer PluginAudioVectorRequest LearningTrait (SupervisedAdapter, SupervisedTrainer, UnsupervisedAdapter) - ( Java Tutorial ) PreprocessAudioTrainRequest PreprocessAudioAdaptRequest SupervisedTrainingRequest UnsupervisedAdaptationRequest UpdateTrait GetUpdateStatusRequest ApplyUpdateRequest GlobalComparer GlobalComparerRequest TextTransformer TextTransformationRequest AudioAlignmentScorer AudioAlignmentScoreRequest In addition to the messages above, the following messages exist for interacting with the server itself and are independent of individual plugins or plugin Traits: GetStatusRequest PluginDirectoryRequest LoadPluginDomainRequest RemovePluginDomainRequest Full message definition details for all of the messages mentioned here","title":"Traits and their Corresponding Messages"},{"location":"traits.html#requestresult-message-pairs","text":"In general, each Request message is paired with a Result message, that defines the structure and contents of the server's reply to a given client Request message. The table below defines the pairs of the Result messages for the Request messages mentioned above; and the full message definition details can be found in the OLIVE API Message Protocol Documentation , which each entry below is linked to. Request Message Result (Response) Message GlobalScorerRequest GlobalScorerResult RegionScorerRequest RegionScorerResult FrameScorerRequest FrameScorerResult ClassModificationRequest ClassModificationResult ClassRemovalRequest ClassRemovalResult ClassExportRequest ClassExportResult ClassImportRequest ClassImportResult AudioModificationRequest AudioModificationResult PluginAudioVectorRequest PluginAudioVectorResult PreprocessAudioTrainRequest PreprocessAudioTrainResult PreprocessAudioAdaptRequest PreprocessAudioAdaptResult SupervisedTrainingRequest SupervisedTrainingResult UnsupervisedAdaptationRequest UnsupervisedAdaptationResult GetUpdateStatusRequest GetUpdateStatusResult ApplyUpdateRequest ApplyUpdateResult GlobalComparerRequest GlobalComparerResult TextTransformationRequest TextTransformationResult AudioAlignmentScoreRequest AudioAlignmentScoreResult or Task Request Message Result (Response) Message Global Score GlobalScorerRequest GlobalScorerResult Region Score RegionScorerRequest RegionScorerResult Frame Score FrameScorerRequest FrameScorerResult Create or modify a class enrollment ClassModificationRequest ClassModificationResult Remove an enrolled class ClassRemovalRequest ClassRemovalResult Export a class for later use or for model sharing ClassExportRequest ClassExportResult Import a previously exported class ClassImportRequest ClassImportResult Submit audio for modification (enhancement) AudioModificationRequest AudioModificationResult Request a vectorized representation of audio PluginAudioVectorRequest PluginAudioVectorResult Prepare audio for submission to training a new domain from scratch PreprocessAudioTrainRequest PreprocessAudioTrainResult Prepare audio for submission to adapt an existing domain PreprocessAudioAdaptRequest PreprocessAudioAdaptResult Query a plugin to see if it is ready to apply an \"Update\" GetUpdateStatusRequest GetUpdateStatusResult Instruct a plugin to perform an \"Update\" ApplyUpdateRequest ApplyUpdateResult Submit two audio files for forensic comparison GlobalComparerRequest GlobalComparerResult Submit text (string) for translation TextTransformationRequest TextTransformationResult Submit two or more audio files for alignment shift scores AudioAlignmentScoreRequest AudioAlignmentScoreResult","title":"Request/Result Message Pairs"},{"location":"traits.html#traits-deep-dive","text":"This section provides more details on what the \"real world\" usage and definition of each of the above Traits actually entails. If anything is unclear or could use more expansion, please contact us and we will provide updates and/or clarification.","title":"Traits Deep Dive"},{"location":"traits.html#globalscorer","text":"A GlobalScorer plugin has the output of reporting a single score for each relevant plugin class representing the likelihood that the entire audio file or clip contains this class. For example, when audio is sent to a Speaker Identification (SID) plugin with the GlobalScorer Trait, if there are 3 enrolled speakers at that time, the plugin will return a single score for each enrolled speaker representing the likelihood that the audio is comprised of speech from each speaker, respectively. The output of such a request will contain information that looks something like this: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 Keep in mind that speakers and speaker identification are just examples in this case, and that this generalizes to any global scoring plugin and whatever classes it is meant to distinguish between. This same example could be represented more generically as: /data/sid/audio/file1.wav class1 -0.5348 /data/sid/audio/file1.wav class2 3.2122 /data/sid/audio/file1.wav class3 -5.5340 Due to the nature of global scoring plugins, they are best used when the attribute that's being estimated is known to be, or very likely to be static. For example, using global scoring SID on one side of a 4-wire telephone conversation, where the speaker is likely to remain the same throughout, or using global scoring LID on a TV broadcast that is expected to only contain a single language. These plugins can miss the nuances of or be confused by things like code-switching within recordings, or unexpected speaker changes. The benefits of global scoring plugins are that they are often very fast, since they treat the entirety of the submitted audio as a single unit, and do not worry about trying to subdivide or chunk it in any way. For a first-pass or quick triage approach, or when the data is known or suspected to be homogenous in the ways discussed above, these plugins are very effective. When finer grained results are necessary, though, a RegionScorer or FrameScorer may be more appropriate.","title":"GlobalScorer"},{"location":"traits.html#regionscorer","text":"For each audio file or recording submitted to a RegionScorer plugin, results are returned consisting of 0 or more regions with an associated score and plugin class. Typically, regions are returned in the case of a 'detection' of an instance of the respective class. A 'region' is a pair of timestamps, referring to the start time and end time of the detected class presence, and includes the name of the respective class, and an associated score. An example of this might be a keyword spotting plugin returning the keyword class name, as well as the location and likelihood of that keyword's presence. This might look something like: /data/test/testFile1.wav 0.630 1.170 Airplane 4.3725 /data/test/testFile1.wav 1.520 2.010 Watermelon -1.1978 Another example of output you may see from a region scoring plugin follows, showing what a region scoring speaker detection plugin might output. In this example, an enrolled speaker, speaker2 was detected in testFile1.wav from 0.630 s to 1.170 s with a likelihood score of 4.3725. Likewise for 1.520 s to 2.010 s in the same file for the enrolled speaker called speaker1 , this time with a likelihood score of -1.1978. /data/test/testFile1.wav 0.630 1.170 speaker2 4.3725 /data/test/testFile1.wav 1.520 2.010 speaker1 -1.1978 An even simpler output of this type may just label the regions within a file that the plugin determines contain speech. Again, these are just arbitrary examples using a specific plugin type to more easily describe the scoring type; a more generic output example could be: /data/test/testFile1.wav 0.630 1.170 class1 4.3725 /data/test/testFile1.wav 1.520 2.010 class2 -1.1978 RegionScorer plugins allow a finer resolution with respect to results granularity, and allow plugins to be more flexible and deal with transitions between languages or speakers or other classes of interest within a given audio file or recording. Sometimes this comes at a cost of increased processing complexity and/or slower runtime.","title":"RegionScorer"},{"location":"traits.html#framescorer","text":"A plugin with the FrameScorer Trait that is queried with a FrameScorerRequest will provide score results for each frame of whatever audio has been submitted. Unless otherwise noted, an audio frame is 10 milliseconds. The most common OLIVE plugin that has the FrameScorer Trait are speech activity detection (SAD) plugins, where the score for each frame represents the likelihood of speech being present in that 10 ms audio frame. The output in this case is simply a sequential list of numbers, corresponding to the output score for each frame, in order: 1.9047 1.8088 1.2382 - 0.8862 - 2.5509 In this example, these frames can then be processed to turn them into region scores, labeling the locations where speech has been detected as present within the file. Returning raw frame scores as a result allows more down-stream flexibility, allowing thresholds to be adjusted and regions re-labeled if desired, for example to allow for tuning for more difficult or unexpected audio conditions.","title":"FrameScorer"},{"location":"traits.html#classmodifier","text":"Any plugin capable of adding or removing classes to or from its set of enrollments or target classes carries the ClassModifier Trait. This trait means the set of classes the plugin is interested in is mutable, and can be altered. Typically this is done by providing labeled data that belongs to the new class of interest to the server, which then enrolls a new model representing what it has learned about distinguishing this class from others. Existing class models can also be augmented by providing the system with additional data with this class label. In addition to adding new classes, and improving/augmenting existing ones, it is also possible to remove enrolled classes from domains of plugins carrying this trait, using the ClassRemovalRequest message.","title":"ClassModifier"},{"location":"traits.html#classexporter","text":"A plugin that implements the ClassExporter Trait is capable of exporting enrolled class models in a form that can be either imported back into the same system, as a way to save the model for preservation, or it can be imported into a different system, so long as that system has the same plugin and domain that was used to initially create the class model. This allows enrollments to be shared between systems. In general, exported models are specific to the plugin and domain that created them, so care must be taken to ensure models are not mixed in to other plugins. It is up to the client program or end user to keep tabs on where the exported models came from and what class they represent, and to manage these models once they've been exported.","title":"ClassExporter"},{"location":"traits.html#audioconverter","text":"An AudioConverter plugin has audio as both its input and as its output. This Trait allows the system to take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. The only current plugin to implement the AudioConverter Trait is a speech enhancement plugin for enhancing the speech intelligibility of the submitted audio.","title":"AudioConverter"},{"location":"traits.html#audiovectorizer","text":"This is a prototype feature that allows the system to preprocess an audio file or model to perform the compute-heavy steps of feature extraction and others, so that at a later time a user can either enroll a model or score a file very quickly, since the most time-consuming steps have already been performed. Like the ClassExporter Trait and exported classes/models, the vectorized audio representations are plugin/domain specific and cannot be used with plugins other than the one that created them. This is helpful if enrollments will be frequently rolled in or out of the system, or if the same audio files will be frequently re-tested, to avoid wasted repeat compute cycles.","title":"AudioVectorizer"},{"location":"traits.html#learningtrait","text":"There are three total LearningTrait s: SupervisedAdapter SupervisedTrainer UnsupervisedAdapter Only SupervisedAdapter is currently supported by plugins in the OLIVE ecosystem; the others are deprecated or prototype features and should be ignored.","title":"LearningTrait"},{"location":"traits.html#supervisedadapter","text":"Plugins with the SupervisedAdapter Trait enable the ability to perform Supervised Adaptation of a domain. This is human assisted improvement of a plugin's domain, generally with feedback to the system in the form of annotations of target phenomena, or in some cases, error corrections. For more information on Adaptation, refer to this section .","title":"SupervisedAdapter"},{"location":"traits.html#updatetrait","text":"Certain systems, such as recent speaker and language recognition plugins, sid-embed-v5 and lid-embedplda-v1], have the capability to adapt themselves based purely on raw data provided by the user in the normal use of the system (enrollment and test data). These systems collect statistics over from the data feed through the system that can be used to update system parameters in unsupervised (autonomous) adaptation, thereby improving the performance of the plugin in the conditions in which is has been deployed. Since the use of this data in adaptation changes the behavior of the plugin the system does not automatically update itself, but rather requires the end user to \"trigger\" the update and use the data the system has collected to adapt. Implementing and invoking the associated Update Request Messages to start the Update process will use the accrued data from the test and enroll conditions to pdate system parameters and apply the update. One can always revert to the plugin's original state by clearing out the data and statistics collected the learn directory from the server's storage.","title":"UpdateTrait"},{"location":"traits.html#globalcomparer","text":"A plugin that supports the GlobalComparer Trait has the capability of accepting two waveforms as input, performing some sort of analysis or comparison of the two, and returning a PDF report summarizing the analysis.","title":"GlobalComparer"},{"location":"traits.html#texttransformer","text":"A plugin with the TextTransformer Trait is used to translate a text input when queried with a TextTransformationRequest, providing translation results for the submitted string (it does not take an audio input, unlike other scoring traits). The output in this case is simply a string, which is the translation result","title":"TextTransformer"},{"location":"traits.html#audioalignmentscorer","text":"A plugin with the AudioAlignmentScorer Trait can be used to provide alignment shift scores for two or more audio inputs using an AudioAlignmentScoreRequest. The output is a set of shift scores betwen each combination of two audio inputs in the AudioAlignmentScoreRequest","title":"AudioAlignmentScorer"},{"location":"windows.html","text":"OLIVE on Windows Setup and Usage Introduction With the release of version 5.7.1, OLIVE now supports natively running on the Windows operating system. The Windows native OLIVE release simplifies the installation, management, and launching of OLIVE and OLIVE components by providing an OS-integrated installer tool, and a new GUI-based launcher utility. The streamlined procedure for using these new features is outlined below. Requirements and Assumptions This release was built with specific hardware and OS targets in mind. Compatibility assumes a 64-bit Intel CPU architecture, and an available Nvidia GPU meeting the requirements below. The software can be run without a GPU, but may require reconfiguration, or disabling the GPU configuration through the OLIVE Windows Launcher, and the speed performance of certain plugins (especially ASR) will be impacted. Windows OS Requirements OLIVE on Windows has targeted Windows 11 version 23H2 for compatibility. All testing was performed on this version. GPU Requirements In order for a GPU to be used by OLIVE, it must be an Nvidia GPU with the appropriate drivers installed. We have tested to verifty compatibility with: NVIDIA GPU Driver version R535 or greater (primarily tested using 546.01, 556.12, 566) NVIDIA CUDA 12.2 or higher (primarily tested using CUDA 12.3, 12.5) Due to the volume of GPUs available on the market, we cannot guarantee that all models are supported, or that all will perform acceptably speed-wise. Most of our testing was performed on high-end laptop class GPUs like the RTX 3080-Ti, with 16GB of available VRAM. Lower amounts of available GPU memory may limit how many GPU-utilizing plugins can be loaded into memory at one time. Installation The distribution format for the OLIVE Windows installer is a single zip archive. This archive contains the installation script, and several binary files used by it during installation. This can be seen once the deliverable is unzipped by selecting the archive and either right clicking then selecting \"Extract all\" from the context menu, or selecting \"Extract all\" from the top function bar in Explorer: To begin the install process, doubleclick the \"OLIVE-6.0.0-Installer.exe\" application in the resulting folder. Windows Defender Note (Click to expand) Because SRI is not submitting OLIVE to Microsoft for malware analysis, the Windows Defender SmartScreen may flag it as an \"Unrecognized App\" and provide a warning pop-up and prevent the OLIVE install from beginning. To proceed past this, choose \"More info\" and then \"Run anyway.\" Windows protected your PC Run anyway Once the installer starts up, the user will be prompted to provide an installation location for OLIVE. By default, this will be the current user's Local AppData directory: C:\\Users\\<username>\\AppData\\Local\\Programs\\OLIVE But any location with the appropriate permissions for the current user should be valid. Next, the user will be prompted to choose whether or not they would like a desktop icon created (recommended), and then installation progresses as expected for a standard Windows installation package. Choose Installation Location Choose Additonal Options (Desktop Shortcut) Ready to Install Preparing to Install Installing Setup has finished Now installation is complete, and OLIVE should be ready for execution. The contents of the installation should resemble this screenshot: But no manual intervention or configuration in this location should be needed, unless performing a specialty GPU configuration , installing new separately-delivered plugins, or trying to retrieve logs for troubleshooting. The screenshot also shows the unins000.exe uninstaller executable that can be run manually as an alternative to the Windows uninstall procedure outlined below. After installation, if the option for creating it was selected, there will be an OLIVE on Windows shortcut/icon on the Desktop that can be used to access the OLIVE Launcher to get started using OLIVE. Launcher The OLIVE launcher can be accessed in several different ways: Double-clicking the OLIVE Desktop Icon Finding OLIVE in the Windows App List Accessing OLIVE in the Windows Start Menu Desktop Icon App List Start Menu Launcher Anatomy No matter which method you use to access the OLIVE Launcher, you're greeted with the same screen: There are controls that provide access to: Starting (and Stopping) the OLIVE server Launching Nightingale UI Launching / Accessing the Raven Batch UI Disabling GPU configuration Accessing GPU hardware information for troubleshooting OLIVE log display window for status and troubleshooting The next section covers using these functions. Server Management Starting and stopping the OLIVE server is done with the Start and Stop buttons in the lower right. Clicking Start will trigger launching the OLIVE server, which will be up and waiting for a connection from one of the OLIVE UIs, or any other properly configured external client. Likewise, clicking Stop will shut down this server: Note Stop will NOT shut down Nightingale, but it will sever Nightingale's connection to the OLIVE server, so no new OLIVE queries will be available until OLIVE server is restarted. Note Likewise, Stop will NOT close down any browser tabs that have already loaded Raven UI, but it will be shutting down the service that is hosting the UI, so any operation or page refresh in Raven that occurs after OLIVE server shutdown will fail. GPU Configuration and Info By default, OLIVE 6.0.0 Windows distribution packages are configured assuming that a single Nvidia GPU is available on the hardware OLIVE will be installed on, and will be leveraging this for key plugins where a GPU makes a significant difference in the execution speed of the provided plugins. Namely, the end-to-end ASR plugin. If a GPU is not available, OLIVE can still be run, but at the cost of speed for these plugins. A Use GPU toggle has been provided to quickly disable this GPU configuration in such instances ( shown here as #4 ). Un-selecting this toggle will bypass any OLIVE configuration setup, and re-set all plugins to use CPU only. The launcher also provides a GPU Info button ( shown here as #5 ). This button opens a pop-up info display: This display can be useful when troubleshooting if OLIVE is having trouble accessing a GPU, perhaps for missing, incompatible, or improper GPU driver installation, or help diagnose if the GPU is running out of available memory; whether from OLIVE tasks or some other application using it. If for some reason there is one or more compatible GPUs available, but the provided GPU configuration isn't appropriate, the configuration assigning GPU device resources to different plugin domains can be modified. Please follow the standard GPU Configuration instructions , with the important caveat that the location of the olive.conf file is different for Windows. By default, the file will be located in: C:\\Users\\<username>\\AppData\\Local\\Programs\\OLIVE\\oliveAppData\\olive.conf If a custom installation location was chosen at installation time, the file will instead be located at: <user-selected-install-location>\\OLIVE\\oliveAppData\\olive.conf OLIVE UIs Nightingale Choose the Launch Nightingale button to launch Nightingale. Nightingale will start up and automatically attempt to establish a connection to the OLIVE server running on the machine. Refer to the primary Nightingale documentation for usage instructions. Raven UI Choose the Launch Raven UI button to launch a brower tab directed to the port where OLIVE Web Broker is serving the Raven UI. The default system browser will be used. Refer to the primary Raven UI documentation for Raven UI usage instructions. Uninstallation Uninstallation is now performed through the native Windows \"Add or remove programs\" Control Panel feature. To uninstall, open the Windows system Settings, and navigate to the \"Apps\" pane, and \"Installed Apps\" list. Alternately, use the Windows search feature to look for \"Add or remove programs\" to navigate directly here. Enter \"OLIVE\" in the search field: Then select the 3-dot menu and choose \"Uninstall.\" Confirm that you're sure you want to remove: Wait for the uninstaller to complete: Where you're given the Uninstall Success message. OLIVE is now uninstalled. Enrollments and Server Logs Note that by design, the OLIVE uninstall process currently leaves enrollments (such as speakers or keywords of interest) behind - this is in case a user would like to consult the logs for troubleshooting information, or in case they would like to carry enrollments forward into a new OLIVE version or transfer to another machine. These files can be manually removed by navigating to the install location (default is C:\\Users\\ \\AppData\\Local\\Programs\\OLIVE) and removing it.","title":"Windows Setup"},{"location":"windows.html#olive-on-windows-setup-and-usage","text":"","title":"OLIVE on Windows Setup and Usage"},{"location":"windows.html#introduction","text":"With the release of version 5.7.1, OLIVE now supports natively running on the Windows operating system. The Windows native OLIVE release simplifies the installation, management, and launching of OLIVE and OLIVE components by providing an OS-integrated installer tool, and a new GUI-based launcher utility. The streamlined procedure for using these new features is outlined below.","title":"Introduction"},{"location":"windows.html#requirements-and-assumptions","text":"This release was built with specific hardware and OS targets in mind. Compatibility assumes a 64-bit Intel CPU architecture, and an available Nvidia GPU meeting the requirements below. The software can be run without a GPU, but may require reconfiguration, or disabling the GPU configuration through the OLIVE Windows Launcher, and the speed performance of certain plugins (especially ASR) will be impacted.","title":"Requirements and Assumptions"},{"location":"windows.html#windows-os-requirements","text":"OLIVE on Windows has targeted Windows 11 version 23H2 for compatibility. All testing was performed on this version.","title":"Windows OS Requirements"},{"location":"windows.html#gpu-requirements","text":"In order for a GPU to be used by OLIVE, it must be an Nvidia GPU with the appropriate drivers installed. We have tested to verifty compatibility with: NVIDIA GPU Driver version R535 or greater (primarily tested using 546.01, 556.12, 566) NVIDIA CUDA 12.2 or higher (primarily tested using CUDA 12.3, 12.5) Due to the volume of GPUs available on the market, we cannot guarantee that all models are supported, or that all will perform acceptably speed-wise. Most of our testing was performed on high-end laptop class GPUs like the RTX 3080-Ti, with 16GB of available VRAM. Lower amounts of available GPU memory may limit how many GPU-utilizing plugins can be loaded into memory at one time.","title":"GPU Requirements"},{"location":"windows.html#installation","text":"The distribution format for the OLIVE Windows installer is a single zip archive. This archive contains the installation script, and several binary files used by it during installation. This can be seen once the deliverable is unzipped by selecting the archive and either right clicking then selecting \"Extract all\" from the context menu, or selecting \"Extract all\" from the top function bar in Explorer: To begin the install process, doubleclick the \"OLIVE-6.0.0-Installer.exe\" application in the resulting folder. Windows Defender Note (Click to expand) Because SRI is not submitting OLIVE to Microsoft for malware analysis, the Windows Defender SmartScreen may flag it as an \"Unrecognized App\" and provide a warning pop-up and prevent the OLIVE install from beginning. To proceed past this, choose \"More info\" and then \"Run anyway.\" Windows protected your PC Run anyway Once the installer starts up, the user will be prompted to provide an installation location for OLIVE. By default, this will be the current user's Local AppData directory: C:\\Users\\<username>\\AppData\\Local\\Programs\\OLIVE But any location with the appropriate permissions for the current user should be valid. Next, the user will be prompted to choose whether or not they would like a desktop icon created (recommended), and then installation progresses as expected for a standard Windows installation package. Choose Installation Location Choose Additonal Options (Desktop Shortcut) Ready to Install Preparing to Install Installing Setup has finished Now installation is complete, and OLIVE should be ready for execution. The contents of the installation should resemble this screenshot: But no manual intervention or configuration in this location should be needed, unless performing a specialty GPU configuration , installing new separately-delivered plugins, or trying to retrieve logs for troubleshooting. The screenshot also shows the unins000.exe uninstaller executable that can be run manually as an alternative to the Windows uninstall procedure outlined below. After installation, if the option for creating it was selected, there will be an OLIVE on Windows shortcut/icon on the Desktop that can be used to access the OLIVE Launcher to get started using OLIVE.","title":"Installation"},{"location":"windows.html#launcher","text":"The OLIVE launcher can be accessed in several different ways: Double-clicking the OLIVE Desktop Icon Finding OLIVE in the Windows App List Accessing OLIVE in the Windows Start Menu Desktop Icon App List Start Menu","title":"Launcher"},{"location":"windows.html#launcher-anatomy","text":"No matter which method you use to access the OLIVE Launcher, you're greeted with the same screen: There are controls that provide access to: Starting (and Stopping) the OLIVE server Launching Nightingale UI Launching / Accessing the Raven Batch UI Disabling GPU configuration Accessing GPU hardware information for troubleshooting OLIVE log display window for status and troubleshooting The next section covers using these functions.","title":"Launcher Anatomy"},{"location":"windows.html#server-management","text":"Starting and stopping the OLIVE server is done with the Start and Stop buttons in the lower right. Clicking Start will trigger launching the OLIVE server, which will be up and waiting for a connection from one of the OLIVE UIs, or any other properly configured external client. Likewise, clicking Stop will shut down this server: Note Stop will NOT shut down Nightingale, but it will sever Nightingale's connection to the OLIVE server, so no new OLIVE queries will be available until OLIVE server is restarted. Note Likewise, Stop will NOT close down any browser tabs that have already loaded Raven UI, but it will be shutting down the service that is hosting the UI, so any operation or page refresh in Raven that occurs after OLIVE server shutdown will fail.","title":"Server Management"},{"location":"windows.html#gpu-configuration-and-info","text":"By default, OLIVE 6.0.0 Windows distribution packages are configured assuming that a single Nvidia GPU is available on the hardware OLIVE will be installed on, and will be leveraging this for key plugins where a GPU makes a significant difference in the execution speed of the provided plugins. Namely, the end-to-end ASR plugin. If a GPU is not available, OLIVE can still be run, but at the cost of speed for these plugins. A Use GPU toggle has been provided to quickly disable this GPU configuration in such instances ( shown here as #4 ). Un-selecting this toggle will bypass any OLIVE configuration setup, and re-set all plugins to use CPU only. The launcher also provides a GPU Info button ( shown here as #5 ). This button opens a pop-up info display: This display can be useful when troubleshooting if OLIVE is having trouble accessing a GPU, perhaps for missing, incompatible, or improper GPU driver installation, or help diagnose if the GPU is running out of available memory; whether from OLIVE tasks or some other application using it. If for some reason there is one or more compatible GPUs available, but the provided GPU configuration isn't appropriate, the configuration assigning GPU device resources to different plugin domains can be modified. Please follow the standard GPU Configuration instructions , with the important caveat that the location of the olive.conf file is different for Windows. By default, the file will be located in: C:\\Users\\<username>\\AppData\\Local\\Programs\\OLIVE\\oliveAppData\\olive.conf If a custom installation location was chosen at installation time, the file will instead be located at: <user-selected-install-location>\\OLIVE\\oliveAppData\\olive.conf","title":"GPU Configuration and Info"},{"location":"windows.html#olive-uis","text":"","title":"OLIVE UIs"},{"location":"windows.html#nightingale","text":"Choose the Launch Nightingale button to launch Nightingale. Nightingale will start up and automatically attempt to establish a connection to the OLIVE server running on the machine. Refer to the primary Nightingale documentation for usage instructions.","title":"Nightingale"},{"location":"windows.html#raven-ui","text":"Choose the Launch Raven UI button to launch a brower tab directed to the port where OLIVE Web Broker is serving the Raven UI. The default system browser will be used. Refer to the primary Raven UI documentation for Raven UI usage instructions.","title":"Raven UI"},{"location":"windows.html#uninstallation","text":"Uninstallation is now performed through the native Windows \"Add or remove programs\" Control Panel feature. To uninstall, open the Windows system Settings, and navigate to the \"Apps\" pane, and \"Installed Apps\" list. Alternately, use the Windows search feature to look for \"Add or remove programs\" to navigate directly here. Enter \"OLIVE\" in the search field: Then select the 3-dot menu and choose \"Uninstall.\" Confirm that you're sure you want to remove: Wait for the uninstaller to complete: Where you're given the Uninstall Success message. OLIVE is now uninstalled. Enrollments and Server Logs Note that by design, the OLIVE uninstall process currently leaves enrollments (such as speakers or keywords of interest) behind - this is in case a user would like to consult the logs for troubleshooting information, or in case they would like to carry enrollments forward into a new OLIVE version or transfer to another machine. These files can be manually removed by navigating to the install location (default is C:\\Users\\ \\AppData\\Local\\Programs\\OLIVE) and removing it.","title":"Uninstallation"},{"location":"workflows.html","text":"OLIVE Workflow API Introduction The OLIVE Workflow API extends the OLIVE Enterprise API to simplify working with an OLIVE server, allowing clients to request multiple OLIVE tasks with one API call. This eliminates the more verbose and complex calls necessary when using the Enterprise API. In particular, this framework will encapsulate 'feeding' functionality, to link together tasks, like speech activity detection and a plugin that uses speech regions or frames in its processing, or to 'bundle' multiple requests in a single call, rather than having every API call to a plugin be a separate action. The Workflow API is based around binary or text \"Workflow Definition\" files that SRI distributes to clients. These files contain a 'recipe' to be executed on an OLIVE server to perform analysis, enrollment, and eventually adaption using one or more audio files/inputs. This places the work of specifying how to execute a complex task to execute within the OLIVE server, instead of the client. To use this API, a client submits the SRI provided Workflow Definition file to an OLIVE server. The server verifies it can run the Workflow Definition through a process called \"actualization\". If successful, an activated Workflow is returned to the client. This activated Workflow is then ready for one or more analysis (or enrollment) requests. These requests can be made numerous times with one or more audio submissions. OLIVE still supports the original OLIVE Enterprise API that was based on creating and sending Google Protocol Buffers (Protobuf) message for each request, so if desired, clients can combine classic OLIVE API calls with the Workflow API to implement advanced functionality. For advanced users, see Creating a Workflow Definition for information on composing your own Workflow Definition files. For more information about working with OLIVE, please see: The OLIVE Server The OLIVE Plugins The OLIVE Enterprise API Useful Concepts to Know Workflow Definition - distributed as a file (text or binary) with these characteristics: Similar to Plugins, Workflows are independent of the OLIVE software and can be updated and/or distributed outside of an OLIVE software release. Can be submitted to multiple OLIVE servers for actualization in parallel, where actualization is the process of verifying that the server can perform the activities defined in a Workflow Definition. Each actualized Workflow is considered unique to the server where it was actualized, as it is possible the plugin/domain names could vary by each server that actualizes the same WorkflowDefinition Most Workflow Definitions are implemented with the specific names of plugins/domains delivered with the OLIVE system. Changing the name of a plugin or domain will cause workflows that use them to cease to function. Order: a set one or more jobs. There are 3 supported Workflow orders: analysis, enrollment, and unenrollment. Analysis orders require at least one data (audio) input, enrollment orders require one or more data (audio) inputs, plus a class ID, unenrollment inputs do not accept data, only a class ID. Job: A set of tasks, plus depending on the order (analysis, enrollment, unenrollment) may include audio (data) and/or as class ID. An analysis order typically only includes one job, with that job accepting one audio input. An enrollment order may have multiple jobs, where each jobs has it's own set of tasks, one or more data/audio(s), and a class id (i.e. speaker name). Only tasks that support the Class Enroller trait such as speaker enrollment for SID and language enrollment for LID can be part of an enrollment job. Each enrollment job should support enrollment for a single class enroller. This allows enrollments (such as for LID and SID) to be handled separately, since it is very unlikely the same audio and class id (speaker name/language name) would be used for both tasks. Unenrollment jobs are similar to enrollment jobs, except that do NOT consume audio/data. They only accept a class ID Tasks: at the lowest level, a Workflow is composed of one or more tasks, such as SAD, LID, SID, QBE, ASR, etc. A task typically maps to a traditional plugin/domain, but as we expand the capabilities of the Workflow API, 'tasks' are likely to include functionality that is not implemented by a traditional OLIVE plugin, but by private, helper, components that assist other tasks in the workflow but do not return values to the user. Limitations The Workflow API is a new OLIVE extension whose interfaces are subject to change. This release covers two types of behavior: bundling of disparate tasks together into a single call and restricted feeding processes, wherein the output of one plugin may be fed into one or more other \"downstream\" plugins that will use this information in their processing. The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the plugin must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the downstream plugin(s) must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). Further, the current implementation of workflows does not support tasks that create entirely new domains, like adaptation. This is much easier to accomplish with a direct call to the plugin, as bundling and feeding behavior is not relevant to this activity. Creating a Workflow Definition Internally, the OLIVE design uses a simple kitchen metaphor, which is expressed in the elements that compose an OLIVE workflow. If one thinks of OLIVE as a kitchen, orders are submitted to OLIVE, which are then extracted into jobs that are cooked/executed (as tasks). In this analogy, when authoring OLIVE workflows you are authoring both a menu and a recipe. Clients use the Workflow to request an order (analysis, enrollment, unenrollment), while the Workflow also includes the recipe to cook/execute those orders. The \"job\" portion of a Workflow is used to build a simple directed acyclic graph (DAG) within OLIVE. This DAG, which OLIVE refers to as a \"Job Graph\", allows data to be shared by multiple tasks, while also allowing connections between these tasks. When connected, the outputs of upstream tasks (tasks that have been executed) and provided to downstream tasks (tasks that have not yet been executed). For example the output of a SAD task can be supplied to one or more downstream tasks such as LID or SID, so those tasks do not have to internally run SAD. To enable this complexity, the workflow recipe defines a set of elements/objects: WorkflowDefinition - The outer container for all orders and their jobs (Job Graphs) WorkflowOrderDefinition - Used to group jobs into processing pipelines for analysis, enrollment, or unenrollment activities JobDefinition - Defines the set of tasks that are executed in a DAG and how task output is shared/returned. To simplify authoring, tasks in a job are executed sequentially. WorkflowTask - The task that is executed, which is usually implemented by a plugin A quick note on the structure of elements: The WorkflowDefinition, WorkflowOrderDefinition, JobDefinition, and WorkflowTask are all based on a Protobuf message of the same name in the Enterprise API. Should you have questions about these elements it maybe be helpful to refer to these protobuf messages; however, the relationship between these messages/elements is not clear when looking at the Enterprise API messages, so the tables below include a \"cardinality\" column to make it clearer about the relationship between these elements. WorkflowDefinition At the top-level, the Workflow Definition element includes these required and optional fields: Attribute Type Cardinality Description order WorkflowOrderDefinition 1 to 3 See WorkflowOrderDefinition below. There must be one or more of these elements actualized boolean 1 Always set to 'false' when creating a new Workflow. This is an internal field set by the OLIVE server when actualized version string 0 or 1 An optional user defined version number for this workflow description string 0 or 1 An optional description of this workflow created DateTime 0 or 1 An optional date this workflow was created updated DateTime 0 or 1 An optional date this workflow was updated An example Workflow Definition element (the WorkflowOrderDefinition element(s) within order defined later to avoid confusion ) { \"order\" : [], \"actualized\" : false , \"version\" : \"1.0\" , \"description\" : \"Example of a Workflow Definition Object\" , \"created\" : { \"year\" : 2021 , \"month\" : 6 , \"day\" : 29 , \"hour\" : 8 , \"min\" : 0 , \"sec\" : 0 } } WorkflowOrderDefinition Within a WorkflowDefinition element there can be 1 to 3 WorkflowOrderDefinition elements defined (one for analysis, one for enrollment, and one for unenrollment). The WorkflowOrderDefinition includes these required and optional fields: Attribute Type Cardinality Description workflow_type WorkflowType 1 One of: 'WORKFLOW_ANALYSIS_TYPE', 'WORKFLOW_ENROLLMENT_TYPE', or 'WORKFLOW_UNENROLLMENT_TYPE' job_definition JobDefinition 1 or more See JobDefinition element below. An order must have at least one job order_name JobDefinition 1 or more See JobDefinition element below. An order must have at least one job Below is an example of a WorkflowOrderDefinition (JobDefinition element(s) defined later). 1 to 3 of these elements can be added to a WorkflowDefintion: { \"workflow_type\" : \"WORKFLOW_ANALYSIS_TYPE\" , \"job_definition\" : [], \"order_name\" : \"Analysis Order\" } JobDefinition One or more JobDefinition elements must be added to WorkflowOrderDefinition element (see above). This element includes these required and optional fields: Attribute Type Cardinality Description job_name string 1 A unique name for this job tasks WorkflowTask 1 or more See the WorkflowTask below. There must be at least one task defined for a Job. Assume tasks are executed in the order they are defined in this list. data_properties DataHandlerProperty 1 See DataHandlerProperty below. This defines the data (normally audio) properties for all tasks in this job description string 0 or 1 An optional description processing_type WorkflowJobType 1 Optional, do not specify unless creating a conditional workflow. The default value is \"MERGE\", with allowable values: \"MERGE\", \"PARALLEL\", or \"SEQUENTIAL\". See section on conditional workflows conditional_job_output boolean 0 or 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows dynamic_job_name string 0 or 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows resolved bool 0 or 1 DO NOT SET. This value is assgiend by the server transfer_result_labels string 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows Here is an example of JobDefinition element (the WorkflowTask and DataHandlerProperty elements are defined below). One or more of these elements must be added to a WorkflowOrderDefinition. [ { \"job_name\" : \"SAD, LID, SID analysis with LID and SID enrollment\" , \"tasks\" : [], \"data_properties\" : {} } ] WorkflowTask One or more WorkflowTask elements must be defined for a JobDefinition element (see above). This element includes these required and optional fields: Attribute Type Cardinality Description message_type MessageType 1 The type of the task. One of: \"REGION_SCORER_REQUEST\", \"GLOBAL_SCORER_REQUEST\", \"FRAME_SCORER_REQUEST\", \"AUDIO_ALIGN_REQUEST\", \"CLASS_MODIFICATION_REQUEST\", \"CLASS_REMOVAL_REQUEST\", or \"PLUGIN_2_PLUGIN_REQUEST\" message_data varies 1 This element contains values based on the message type. For non-enrollment requests this is a dictionary that contains the keys 'plugin', and 'domain' For enrollment tasks, the dictionary also includes \"class id\": \"None\" trait_output TraitType 1 Classifies the type of output produced by this task. Must be one of: \"GLOBAL_SCORER\", 'REGION_SCORER', 'FRAME_SCORER', \"CLASS_MODIFIER\", \"PLUGIN_2_PLUGIN\", \"AUDIO_ALIGNMENT_SCORER\" task string 1 A label used to define the task type. By convention of one: \"SAD\", \"LID\", \"SID\", \"LDD\", \"SDD\", \"QBE\", \"ASR\", etc; however, one can define your own type name. For example if you prefere \"STT\" to \"ASR\" consumer_data_label string 1 This task consumes data data input having this name, which is 'audio' for almost all tasks. If using a non 'audio' lable, then this value must match a 'consumer_data_label' used in the workflow's DataHandlerProperty. consumer_result_label string 1 The unique name assigned to this task. Each task in a job must specify a unique name, which is often the same as task. One can also consider this 'task_id' return_result bool 0 or 1 If true, then output produced by this task is returned to the client. option_mappings OptionMap 0 or more This is used to connect the outputs from one or more upstream tasks to this (plugin) task. If one or more values are defined in this mapping, then any upstream (completed) tasks that produced output (defined by the tasks 'consumer_result_label') matching the value of 'workflow_keyword_name' are added to the option dictionary as 'plugin_keyword_name'. allow_failure bool zero or 1 If true, then this task can fail without stopping the execution of workflow. If false then a failure of this task will prevent downstream tasks/jobs from being ran supported_options OptionDefinition 0 or more DO NOT SET. This is set by the server when the workflow is actualized, letting clients know the options supported by the plugin/task available on the server class_id string 0 or 1 NOT YET SUPPORTED - class IDs can be added to the message_data section (if supported by the message type) description string 0 or 1 An optional description of this task Example of defining a SAD task: { \"message_type\" : \"REGION_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"sad-dnn-v7.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"REGION_SCORER\" , \"task\" : \"SAD\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SAD\" , \"return_result\" : true } Example of defining an enrollment task for a LID plugin (note the inclusion of class_id in the message_data element): { \"message_type\" : \"CLASS_MODIFICATION_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"LID_Enroll\" , \"return_result\" : true , \"allow_failure\" : false } DataHandlerProperty The JobDefinition element requires a DataHandlerProperty element that defines the type of data (currently only supporting 'audio') data handling properties used by tasks in the job. This element includes these required and optional fields: Attribute Type Cardinality Description min_number_inputs int 1 The minimum number of data inputs required for a job. This value can be 0, but almost all tasks require 1 audio input. An audio comparison task is one of the few tasks that will require 2 inputs max_number_inputs int 0 or 1 Optional value, for furture use of batch processing of tasks that consume more than one input. This specifies the max number of data inputs consumed by task(s) in the job when doing batch processing, but is not currently used by any tasks type InputDataType 1 For now use \"AUDIO\", but can be one of \"AUDIO\", \"VIDEO\", \"TEXT\", or \"IMAGE\" preprocessing_required boolean 1 Set to 'true'. Not configurable at this time resample_rate int 0 or 1 Do not specify. Currently a value of 8000 is used mode MultiChannelMode 0 or 1 One of \"MONO\", \"SPLIT\", or \"SELECTED\". This determines how multi channel audio is handled in a workflow, with \"MONO\" being the default. In \"MONO\" mode, any multi channel data/audio is converted to mono when processed by task(s) in this job. For \"SPLIT\" each channel is handled by the task(s) in this job, so there is a set of results for each channel. For \"SELECTED\" a channel number must be provided when packaing the audio for the workflow and that channel is used for the job task(s) consumer_data_label string 0 or 1 Data supplied for Analysis, Enrollment, or Unenrollment is labeled by this name when passed to tasks within the job. Bu default use a value of 'audio' { \"data_properties\" : { \"min_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"mode\" : \"MONO\" } }","title":"Workflows"},{"location":"workflows.html#olive-workflow-api","text":"","title":"OLIVE Workflow API"},{"location":"workflows.html#introduction","text":"The OLIVE Workflow API extends the OLIVE Enterprise API to simplify working with an OLIVE server, allowing clients to request multiple OLIVE tasks with one API call. This eliminates the more verbose and complex calls necessary when using the Enterprise API. In particular, this framework will encapsulate 'feeding' functionality, to link together tasks, like speech activity detection and a plugin that uses speech regions or frames in its processing, or to 'bundle' multiple requests in a single call, rather than having every API call to a plugin be a separate action. The Workflow API is based around binary or text \"Workflow Definition\" files that SRI distributes to clients. These files contain a 'recipe' to be executed on an OLIVE server to perform analysis, enrollment, and eventually adaption using one or more audio files/inputs. This places the work of specifying how to execute a complex task to execute within the OLIVE server, instead of the client. To use this API, a client submits the SRI provided Workflow Definition file to an OLIVE server. The server verifies it can run the Workflow Definition through a process called \"actualization\". If successful, an activated Workflow is returned to the client. This activated Workflow is then ready for one or more analysis (or enrollment) requests. These requests can be made numerous times with one or more audio submissions. OLIVE still supports the original OLIVE Enterprise API that was based on creating and sending Google Protocol Buffers (Protobuf) message for each request, so if desired, clients can combine classic OLIVE API calls with the Workflow API to implement advanced functionality. For advanced users, see Creating a Workflow Definition for information on composing your own Workflow Definition files. For more information about working with OLIVE, please see: The OLIVE Server The OLIVE Plugins The OLIVE Enterprise API","title":"Introduction"},{"location":"workflows.html#useful-concepts-to-know","text":"Workflow Definition - distributed as a file (text or binary) with these characteristics: Similar to Plugins, Workflows are independent of the OLIVE software and can be updated and/or distributed outside of an OLIVE software release. Can be submitted to multiple OLIVE servers for actualization in parallel, where actualization is the process of verifying that the server can perform the activities defined in a Workflow Definition. Each actualized Workflow is considered unique to the server where it was actualized, as it is possible the plugin/domain names could vary by each server that actualizes the same WorkflowDefinition Most Workflow Definitions are implemented with the specific names of plugins/domains delivered with the OLIVE system. Changing the name of a plugin or domain will cause workflows that use them to cease to function. Order: a set one or more jobs. There are 3 supported Workflow orders: analysis, enrollment, and unenrollment. Analysis orders require at least one data (audio) input, enrollment orders require one or more data (audio) inputs, plus a class ID, unenrollment inputs do not accept data, only a class ID. Job: A set of tasks, plus depending on the order (analysis, enrollment, unenrollment) may include audio (data) and/or as class ID. An analysis order typically only includes one job, with that job accepting one audio input. An enrollment order may have multiple jobs, where each jobs has it's own set of tasks, one or more data/audio(s), and a class id (i.e. speaker name). Only tasks that support the Class Enroller trait such as speaker enrollment for SID and language enrollment for LID can be part of an enrollment job. Each enrollment job should support enrollment for a single class enroller. This allows enrollments (such as for LID and SID) to be handled separately, since it is very unlikely the same audio and class id (speaker name/language name) would be used for both tasks. Unenrollment jobs are similar to enrollment jobs, except that do NOT consume audio/data. They only accept a class ID Tasks: at the lowest level, a Workflow is composed of one or more tasks, such as SAD, LID, SID, QBE, ASR, etc. A task typically maps to a traditional plugin/domain, but as we expand the capabilities of the Workflow API, 'tasks' are likely to include functionality that is not implemented by a traditional OLIVE plugin, but by private, helper, components that assist other tasks in the workflow but do not return values to the user.","title":"Useful Concepts to Know"},{"location":"workflows.html#limitations","text":"The Workflow API is a new OLIVE extension whose interfaces are subject to change. This release covers two types of behavior: bundling of disparate tasks together into a single call and restricted feeding processes, wherein the output of one plugin may be fed into one or more other \"downstream\" plugins that will use this information in their processing. The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the plugin must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the downstream plugin(s) must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). Further, the current implementation of workflows does not support tasks that create entirely new domains, like adaptation. This is much easier to accomplish with a direct call to the plugin, as bundling and feeding behavior is not relevant to this activity.","title":"Limitations"},{"location":"workflows.html#creating-a-workflow-definition","text":"Internally, the OLIVE design uses a simple kitchen metaphor, which is expressed in the elements that compose an OLIVE workflow. If one thinks of OLIVE as a kitchen, orders are submitted to OLIVE, which are then extracted into jobs that are cooked/executed (as tasks). In this analogy, when authoring OLIVE workflows you are authoring both a menu and a recipe. Clients use the Workflow to request an order (analysis, enrollment, unenrollment), while the Workflow also includes the recipe to cook/execute those orders. The \"job\" portion of a Workflow is used to build a simple directed acyclic graph (DAG) within OLIVE. This DAG, which OLIVE refers to as a \"Job Graph\", allows data to be shared by multiple tasks, while also allowing connections between these tasks. When connected, the outputs of upstream tasks (tasks that have been executed) and provided to downstream tasks (tasks that have not yet been executed). For example the output of a SAD task can be supplied to one or more downstream tasks such as LID or SID, so those tasks do not have to internally run SAD. To enable this complexity, the workflow recipe defines a set of elements/objects: WorkflowDefinition - The outer container for all orders and their jobs (Job Graphs) WorkflowOrderDefinition - Used to group jobs into processing pipelines for analysis, enrollment, or unenrollment activities JobDefinition - Defines the set of tasks that are executed in a DAG and how task output is shared/returned. To simplify authoring, tasks in a job are executed sequentially. WorkflowTask - The task that is executed, which is usually implemented by a plugin A quick note on the structure of elements: The WorkflowDefinition, WorkflowOrderDefinition, JobDefinition, and WorkflowTask are all based on a Protobuf message of the same name in the Enterprise API. Should you have questions about these elements it maybe be helpful to refer to these protobuf messages; however, the relationship between these messages/elements is not clear when looking at the Enterprise API messages, so the tables below include a \"cardinality\" column to make it clearer about the relationship between these elements.","title":"Creating a Workflow Definition"},{"location":"workflows.html#workflowdefinition","text":"At the top-level, the Workflow Definition element includes these required and optional fields: Attribute Type Cardinality Description order WorkflowOrderDefinition 1 to 3 See WorkflowOrderDefinition below. There must be one or more of these elements actualized boolean 1 Always set to 'false' when creating a new Workflow. This is an internal field set by the OLIVE server when actualized version string 0 or 1 An optional user defined version number for this workflow description string 0 or 1 An optional description of this workflow created DateTime 0 or 1 An optional date this workflow was created updated DateTime 0 or 1 An optional date this workflow was updated An example Workflow Definition element (the WorkflowOrderDefinition element(s) within order defined later to avoid confusion ) { \"order\" : [], \"actualized\" : false , \"version\" : \"1.0\" , \"description\" : \"Example of a Workflow Definition Object\" , \"created\" : { \"year\" : 2021 , \"month\" : 6 , \"day\" : 29 , \"hour\" : 8 , \"min\" : 0 , \"sec\" : 0 } }","title":"WorkflowDefinition"},{"location":"workflows.html#workfloworderdefinition","text":"Within a WorkflowDefinition element there can be 1 to 3 WorkflowOrderDefinition elements defined (one for analysis, one for enrollment, and one for unenrollment). The WorkflowOrderDefinition includes these required and optional fields: Attribute Type Cardinality Description workflow_type WorkflowType 1 One of: 'WORKFLOW_ANALYSIS_TYPE', 'WORKFLOW_ENROLLMENT_TYPE', or 'WORKFLOW_UNENROLLMENT_TYPE' job_definition JobDefinition 1 or more See JobDefinition element below. An order must have at least one job order_name JobDefinition 1 or more See JobDefinition element below. An order must have at least one job Below is an example of a WorkflowOrderDefinition (JobDefinition element(s) defined later). 1 to 3 of these elements can be added to a WorkflowDefintion: { \"workflow_type\" : \"WORKFLOW_ANALYSIS_TYPE\" , \"job_definition\" : [], \"order_name\" : \"Analysis Order\" }","title":"WorkflowOrderDefinition"},{"location":"workflows.html#jobdefinition","text":"One or more JobDefinition elements must be added to WorkflowOrderDefinition element (see above). This element includes these required and optional fields: Attribute Type Cardinality Description job_name string 1 A unique name for this job tasks WorkflowTask 1 or more See the WorkflowTask below. There must be at least one task defined for a Job. Assume tasks are executed in the order they are defined in this list. data_properties DataHandlerProperty 1 See DataHandlerProperty below. This defines the data (normally audio) properties for all tasks in this job description string 0 or 1 An optional description processing_type WorkflowJobType 1 Optional, do not specify unless creating a conditional workflow. The default value is \"MERGE\", with allowable values: \"MERGE\", \"PARALLEL\", or \"SEQUENTIAL\". See section on conditional workflows conditional_job_output boolean 0 or 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows dynamic_job_name string 0 or 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows resolved bool 0 or 1 DO NOT SET. This value is assgiend by the server transfer_result_labels string 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows Here is an example of JobDefinition element (the WorkflowTask and DataHandlerProperty elements are defined below). One or more of these elements must be added to a WorkflowOrderDefinition. [ { \"job_name\" : \"SAD, LID, SID analysis with LID and SID enrollment\" , \"tasks\" : [], \"data_properties\" : {} } ]","title":"JobDefinition"},{"location":"workflows.html#workflowtask","text":"One or more WorkflowTask elements must be defined for a JobDefinition element (see above). This element includes these required and optional fields: Attribute Type Cardinality Description message_type MessageType 1 The type of the task. One of: \"REGION_SCORER_REQUEST\", \"GLOBAL_SCORER_REQUEST\", \"FRAME_SCORER_REQUEST\", \"AUDIO_ALIGN_REQUEST\", \"CLASS_MODIFICATION_REQUEST\", \"CLASS_REMOVAL_REQUEST\", or \"PLUGIN_2_PLUGIN_REQUEST\" message_data varies 1 This element contains values based on the message type. For non-enrollment requests this is a dictionary that contains the keys 'plugin', and 'domain' For enrollment tasks, the dictionary also includes \"class id\": \"None\" trait_output TraitType 1 Classifies the type of output produced by this task. Must be one of: \"GLOBAL_SCORER\", 'REGION_SCORER', 'FRAME_SCORER', \"CLASS_MODIFIER\", \"PLUGIN_2_PLUGIN\", \"AUDIO_ALIGNMENT_SCORER\" task string 1 A label used to define the task type. By convention of one: \"SAD\", \"LID\", \"SID\", \"LDD\", \"SDD\", \"QBE\", \"ASR\", etc; however, one can define your own type name. For example if you prefere \"STT\" to \"ASR\" consumer_data_label string 1 This task consumes data data input having this name, which is 'audio' for almost all tasks. If using a non 'audio' lable, then this value must match a 'consumer_data_label' used in the workflow's DataHandlerProperty. consumer_result_label string 1 The unique name assigned to this task. Each task in a job must specify a unique name, which is often the same as task. One can also consider this 'task_id' return_result bool 0 or 1 If true, then output produced by this task is returned to the client. option_mappings OptionMap 0 or more This is used to connect the outputs from one or more upstream tasks to this (plugin) task. If one or more values are defined in this mapping, then any upstream (completed) tasks that produced output (defined by the tasks 'consumer_result_label') matching the value of 'workflow_keyword_name' are added to the option dictionary as 'plugin_keyword_name'. allow_failure bool zero or 1 If true, then this task can fail without stopping the execution of workflow. If false then a failure of this task will prevent downstream tasks/jobs from being ran supported_options OptionDefinition 0 or more DO NOT SET. This is set by the server when the workflow is actualized, letting clients know the options supported by the plugin/task available on the server class_id string 0 or 1 NOT YET SUPPORTED - class IDs can be added to the message_data section (if supported by the message type) description string 0 or 1 An optional description of this task Example of defining a SAD task: { \"message_type\" : \"REGION_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"sad-dnn-v7.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"REGION_SCORER\" , \"task\" : \"SAD\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SAD\" , \"return_result\" : true } Example of defining an enrollment task for a LID plugin (note the inclusion of class_id in the message_data element): { \"message_type\" : \"CLASS_MODIFICATION_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"LID_Enroll\" , \"return_result\" : true , \"allow_failure\" : false }","title":"WorkflowTask"},{"location":"workflows.html#datahandlerproperty","text":"The JobDefinition element requires a DataHandlerProperty element that defines the type of data (currently only supporting 'audio') data handling properties used by tasks in the job. This element includes these required and optional fields: Attribute Type Cardinality Description min_number_inputs int 1 The minimum number of data inputs required for a job. This value can be 0, but almost all tasks require 1 audio input. An audio comparison task is one of the few tasks that will require 2 inputs max_number_inputs int 0 or 1 Optional value, for furture use of batch processing of tasks that consume more than one input. This specifies the max number of data inputs consumed by task(s) in the job when doing batch processing, but is not currently used by any tasks type InputDataType 1 For now use \"AUDIO\", but can be one of \"AUDIO\", \"VIDEO\", \"TEXT\", or \"IMAGE\" preprocessing_required boolean 1 Set to 'true'. Not configurable at this time resample_rate int 0 or 1 Do not specify. Currently a value of 8000 is used mode MultiChannelMode 0 or 1 One of \"MONO\", \"SPLIT\", or \"SELECTED\". This determines how multi channel audio is handled in a workflow, with \"MONO\" being the default. In \"MONO\" mode, any multi channel data/audio is converted to mono when processed by task(s) in this job. For \"SPLIT\" each channel is handled by the task(s) in this job, so there is a set of results for each channel. For \"SELECTED\" a channel number must be provided when packaing the audio for the workflow and that channel is used for the job task(s) consumer_data_label string 0 or 1 Data supplied for Analysis, Enrollment, or Unenrollment is labeled by this name when passed to tasks within the job. Bu default use a value of 'audio' { \"data_properties\" : { \"min_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"mode\" : \"MONO\" } }","title":"DataHandlerProperty"},{"location":"olivepy-docs/api.html","text":"olivepy api module olivepy.api.oliveclient ClientBrokerWorker ( Thread ) Performs async interactions with Olive run ( self ) Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/oliveclient.py def run ( self ): logging . debug ( \"Starting Olive Status Monitor Worker for id: {} \" . format ( self . client_id )) self . working = True self . status_socket . subscribe ( \"\" ) poller = zmq . Poller () poller . register ( self . status_socket , zmq . POLLIN ) while self . working : # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . status_socket in socks : logging . debug ( \"Received status message from OLIVE...\" ) heatbeat_data = self . status_socket . recv () heatbeat = Heartbeat () heatbeat . ParseFromString ( heatbeat_data ) # do something with heartbeat... if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats logging . info ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) logging . info ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) logging . info ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) logging . info ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) logging . info ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) logging . info ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) logging . debug ( \"Number active jobs: \" + str ( stats . pool_busy )) logging . debug ( \"Number pending jobs: \" + str ( stats . pool_pending )) logging . debug ( \"Number finished jobs: \" + str ( stats . pool_finished )) logging . debug ( \"Max number jobs: \" + str ( stats . max_num_jobs )) logging . debug ( \"Server version: \" + str ( stats . server_version )) self . status_socket . close () OliveClient This is a simplified version of network library used to contact the Olive server via python code. All OLIVE calls below are synchronous, and block and until a response is received from the OLIVE server. These example API calls are intended to make working with the OLIVE API clearer since all calls are blocking. To make asynchronous requests to the OLIVE server use olivepy.api.olive_async_client.AsyncOliveClient for your enterprise application. __init__ ( self , client_id , address = 'localhost' , request_port = 5588 , timeout_second = 10 ) special Parameters: Name Type Description Default client_id The unique name of this client. Due to a ZMQ bug on some platforms this ID can not end in '1' required address the address of the olive server, such as localhost 'localhost' request_port default olive port is 5588 5588 timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/oliveclient.py def __init__ ( self , client_id , address = 'localhost' , request_port = 5588 , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug on some platforms this ID can not end in '1' :param address: the address of the olive server, such as localhost :param request_port: default olive port is 5588 :param timeout_second: time in seconds, to wait for a response from the server \"\"\" self . client_id = client_id # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . server_request_port = request_port self . server_status_port = request_port + 1 self . timeout_seconds = timeout_second self . olive_connected = False self . info = self . fullobj = None OliveClient . setup_multithreading () adapt_supervised ( self , plugin , domain , annotations_file_name , new_domain_name ) Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required adapt_workspace a unique label for this client's adaptation required annotations_file_name the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. required Returns: Type Description the full path name of the new domain. Source code in olivepy/api/oliveclient.py def adapt_supervised ( self , plugin , domain , annotations_file_name , new_domain_name ): \"\"\" :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param adapt_workspace: a unique label for this client's adaptation :param annotations_file_name: the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. :return: the full path name of the new domain. \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] file_annotations = self . parse_annotation_file ( annotations_file_name ) for filename , regions in file_annotations . items (): audio_id = self . preprocess_supervised_audio ( plugin , domain , filename , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace ) adapt_supervised_old ( self , plugin , domain , file_annotations , new_domain_name ) Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required adapt_workspace a unique label for this client's adaptation required file_annotations a dictionary of files to preprocess, each file has one or more annotated regions for processing {filename: [(start_ms, end_ms, class)]}, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} required Returns: Type Description the full path name of the new domain. Source code in olivepy/api/oliveclient.py def adapt_supervised_old ( self , plugin , domain , file_annotations , new_domain_name ): \"\"\" :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param adapt_workspace: a unique label for this client's adaptation :param file_annotations: a dictionary of files to preprocess, each file has one or more annotated regions for processing {filename: [(start_ms, end_ms, class)]}, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} :return: the full path name of the new domain. \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] for filename , regions in file_annotations . items (): audio_id = self . preprocess_supervised_audio ( plugin , domain , filename , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace ) analyze_bounding_box ( self , plugin , domain , filename , data_msg = None , mode =< InputTransferType . PATH : 1 > , opts = None , classes = None ) Request a analysis of 'filename' returning bounding box scores Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server <InputTransferType.PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. Source code in olivepy/api/oliveclient.py def analyze_bounding_box ( self , plugin , domain , filename , data_msg = None , mode = InputTransferType . PATH , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' returning bounding box scores :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. \"\"\" self . info = self . fullobj = None region_score_result = self . _request_bounding_box_scores ( plugin , domain , filename , data_msg = data_msg , mode = mode , opts = opts , classes = classes ) self . fullobj = region_score_result return region_score_result analyze_frames ( self , plugin , domain , filename , data_msg = None , opts = None , classes = None , mode =< InputTransferType . PATH : 1 > ) Request a analysis of 'filename' returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score. if None, then provide (audio) input as a required data_msg Optionally specify the data input as a fully formed Audio or BinaryMedia message instead of creating from filename None opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis as a list of (frame) scores Source code in olivepy/api/oliveclient.py def analyze_frames ( self , plugin , domain , filename , data_msg = None , opts = None , classes = None , mode = InputTransferType . PATH ): \"\"\" Request a analysis of 'filename' returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score. if None, then provide (audio) input as a :param data_msg: Optionally specify the data input as a fully formed Audio or BinaryMedia message instead of creating from filename :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis as a list of (frame) scores \"\"\" self . info = self . fullobj = None # todo actual def: def _request_frame_scores(self, plugin, domain, filename, data_msg=None, mode=AudioTransferType.AUDIO_PATH, opts=None, classes=None): frame_score_result = self . _request_frame_scores ( plugin , domain , filename , data_msg = data_msg , opts = opts , classes = classes , mode = mode ) if frame_score_result is not None : return frame_score_result . score return [] analyze_global ( self , plugin , domain , filename , data_msg = None , mode =< InputTransferType . PATH : 1 > , opts = None , classes = None ) Request a LID analysis of 'filename' Parameters: Name Type Description Default plugin the name of the LID plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the audio transfer mode <InputTransferType.PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis result as a list of (global) scores Source code in olivepy/api/oliveclient.py def analyze_global ( self , plugin , domain , filename , data_msg = None , mode = InputTransferType . PATH , opts = None , classes = None ): \"\"\" Request a LID analysis of 'filename' :param plugin: the name of the LID plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the audio transfer mode :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis result as a list of (global) scores \"\"\" self . info = self . fullobj = None request = GlobalScorerRequest () request . plugin = plugin request . domain = domain if data_msg : request . audio . CopyFrom ( data_msg ) else : audio = request . audio package_audio ( audio , filename , mode = mode ) self . _add_options ( request , opts ) self . _add_classes ( request , classes ) # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) _ , env = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a global score request message\" ) result = self . _sync_request ( env ) return result . score analyze_regions ( self , plugin , domain , filename , data_msg = None , mode =< InputTransferType . PATH : 1 > , opts = None , classes = None ) Request a analysis of 'filename' returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server <InputTransferType.PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. Source code in olivepy/api/oliveclient.py def analyze_regions ( self , plugin , domain , filename , data_msg = None , mode = InputTransferType . PATH , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. \"\"\" self . info = self . fullobj = None region_score_result = self . _request_region_scores ( plugin , domain , filename , data_msg = data_msg , mode = mode , opts = opts , classes = classes ) self . fullobj = region_score_result return region_score_result analyze_text_transformation ( self , plugin , domain , text_input , opts = None , classes = None ) Request a analysis of 'filename' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server required opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis as a list of (region) scores Source code in olivepy/api/oliveclient.py def analyze_text_transformation ( self , plugin , domain , text_input , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis as a list of (region) scores \"\"\" request = TextTransformationRequest () request . plugin = plugin request . domain = domain request . text = text_input self . _add_options ( request , opts ) self . _add_classes ( request , classes ) # Wrap message in an Envelope _ , env = _wrap_message ( self . client_id , request ) # Now send the envelope logging . debug ( \"Sending a (text transform request) message\" ) result = self . _sync_request ( env ) results = [] for txtTransform in result . transformation : results . append ( txtTransform . transformed_text ) return results apply_threshold ( self , scores , threshold , rate ) Very simple method to convert frame scores to regions. If speech regions are desired we can provide a SAD plugin that returns regions instead of frame scores Parameters: Name Type Description Default scores required threshold required rate required Returns: Type Description frame scores a regions Source code in olivepy/api/oliveclient.py def apply_threshold ( self , scores , threshold , rate ): \"\"\" Very simple method to convert frame scores to regions. If speech regions are desired we can provide a SAD plugin that returns regions instead of frame scores :param scores: :param threshold: :param rate: :return: frame scores a regions \"\"\" inSegment = False start = 0 segments = [] for i in range ( len ( scores )): if not inSegment and scores [ i ] >= threshold : inSegment = True start = i elif inSegment and ( scores [ i ] < threshold or i == len ( scores ) - 1 ): inSegment = False startT = (( 1.0 * start / rate )) endT = ( 1.0 * i / rate ) segments . append (( startT , endT )) return segments audio_modification ( self , plugin , domain , filename , data_msg = None , mode =< InputTransferType . PATH : 1 > ) Do an audio modification (such as an enhansement). This function only accepts one audio and returns on modified audio. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required Returns: Type Description the analysis as a list of (frame) scores Source code in olivepy/api/oliveclient.py def audio_modification ( self , plugin , domain , filename , data_msg = None , mode = InputTransferType . PATH ): \"\"\" Do an audio modification (such as an enhansement). This function only accepts one audio and returns on modified audio. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :return: the analysis as a list of (frame) scores \"\"\" if mode != InputTransferType . PATH : raise Exception ( 'oliveclient.audio_modification requires an filename path and will not work with binary audio data.' ) request = AudioModificationRequest () request . plugin = plugin request . domain = domain request . requested_channels = 1 request . requested_rate = 8000 if data_msg : request . modifications . append ( data_msg ) else : audio = Audio () package_audio ( audio , filename , mode = mode ) # audio = Audio() # audio.path = filename request . modifications . append ( audio ) _ , env = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a audio modification/enhancement request message\" ) result = self . _sync_request ( env ) return result . successful , result . modification_result [ 0 ] connect ( self , monitor_server = False ) Connect this client to the server Parameters: Name Type Description Default monitor_server if true, start a thread to monitor the server connection (helpful if debugging connection issues) False Source code in olivepy/api/oliveclient.py def connect ( self , monitor_server = False ): \"\"\" Connect this client to the server :param monitor_server: if true, start a thread to monitor the server connection (helpful if debugging connection issues) \"\"\" # init the request and status socket request_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_request_port ) status_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_status_port ) context = zmq . Context () self . request_socket = context . socket ( zmq . DEALER ) self . status_socket = context . socket ( zmq . SUB ) self . request_socket . connect ( request_addr ) self . status_socket . connect ( status_addr ) # logging.debug(\"Starting Olive status monitor...\") # Run this to get status about the server (helpful to confirm the server is connected and up) if ( monitor_server ): self . worker = ClientBrokerWorker ( self . status_socket , self . client_id ) self . worker . start () else : self . worker = None self . olive_connected = True logging . debug ( \"Olive client ready\" ) convert_preprocessed_annotations ( self , processed_audio_list ) Convert the file annotations (a dictionary grouped by file ID, where annotations are grouped by file ID, which has one or more regions/classes) into class annotations (where annotations are grouped by class ID, with each class having one or more files, then each file having one or more regions). Parameters: Name Type Description Default processed_audio_list the list of files (indexed by an OLIVE generated ID) and the regions/classes annotated in that file required Returns: Type Description a dictionary of ClassAnnotation objects, indexed by class ID Source code in olivepy/api/oliveclient.py def convert_preprocessed_annotations ( self , processed_audio_list ): \"\"\" Convert the file annotations (a dictionary grouped by file ID, where annotations are grouped by file ID, which has one or more regions/classes) into class annotations (where annotations are grouped by class ID, with each class having one or more files, then each file having one or more regions). :param processed_audio_list: the list of files (indexed by an OLIVE generated ID) and the regions/classes annotated in that file :return: a dictionary of ClassAnnotation objects, indexed by class ID \"\"\" # Now convert the annotations that are grouped by file into a list of annotations grouped by class ID # (speech, non-speech). This is done in two passes, the first passes builds then new mapping of # class_id -->* audio_id -->* region, # then we convert this new data structure into ClassAnnotation (Protobuf) message(s) class_annots = {} for audio_id , regions in processed_audio_list : for region in regions : start = region [ 0 ] end = region [ 1 ] class_id = region [ 2 ] if class_id not in class_annots : class_annots [ class_id ] = {} if audio_id not in class_annots [ class_id ]: class_annots [ class_id ][ audio_id ] = [] class_annots [ class_id ][ audio_id ] . append (( start , end )) # now that the annotations have been grouped by class id, create the annotation protobuf(s) protobuf_class_annots = {} for class_id in class_annots . keys (): protobuf_class_annots [ class_id ] = ClassAnnotation () protobuf_class_annots [ class_id ] . class_id = class_id # Add AudioAnnotation(s) for audio_id in class_annots [ class_id ]: aa = AudioAnnotation () # aa = protobuf_class_annots[class_id].annotations.add() in python2.7? aa . audio_id = audio_id for region in class_annots [ class_id ][ audio_id ]: # times are in milliseconds ar = AnnotationRegion () # might need to do ar = aa.regions.add() for Python2.7 ar . start_t = region [ 0 ] ar . end_t = region [ 1 ] aa . regions . append ( ar ) protobuf_class_annots [ class_id ] . annotations . append ( aa ) return protobuf_class_annots enroll ( self , plugin , domain , class_id , filename , data_msg = None ) Request a enrollment of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required filename the filename to add as an audio only enrollment addition required data_msg an BinaryMedia message to add as an enrollment addition None Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def enroll ( self , plugin , domain , class_id , filename , data_msg = None ): \"\"\" Request a enrollment of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :param filename: the filename to add as an audio only enrollment addition :param data_msg: an BinaryMedia message to add as an enrollment addition :return: True if enrollment successful \"\"\" self . info = self . fullobj = None enrollment = ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True if data_msg : if isinstance ( data_msg , Audio ): enrollment . addition . append ( data_msg ) else : enrollment . addition_media . append ( data_msg ) else : audio = Audio () package_audio ( audio , filename ) enrollment . addition . append ( audio ) # Wrap message in an Envelope _ , env = _wrap_message ( self . client_id , enrollment ) # Now send the envelope logging . debug ( \"Sending an enrollment message\" ) result = self . _sync_request ( env ) return result # ClassModificationResult # Wrap message in an Envelope # request = self._wrap_message(enrollment) # # Now send the message # logging.debug(\"Sending a class modification request (enrollment) message\") # self.request_socket.send(request.SerializeToString()) # logging.debug(\"Sending a class modification request (enrollment) message\") # # TODO THIS IS A SYNC REQUST, CAN BE DONE ASYN WITH A CALLBACK... # # Wait for the response from the server # # logging.info(\"checking for response\") # protobuf_data = self.request_socket.recv() # logging.info(\"Received message from server...\") # envelope = Envelope() # envelope.ParseFromString(protobuf_data) # # # for this use case the server will only have one response in the evevelope: # for i in range(len(envelope.message)): # olive_msg = envelope.message[i] # # if olive_msg.HasField(\"info\"): # self.info = olive_msg.info # if olive_msg.HasField(\"error\"): # raise ExceptionFromServer('Got an error from the server: ' + olive_msg.error) # else: # enrollment_msg = ClassModificationResult() # enrollment_msg.ParseFromString(olive_msg.message_data[0]) # # # Assume there is only one result set (for 'speech'): frame_score_msg.result[0] # # TODO - clean up return. Maybe do something with message. # self.fullobj = enrollment_msg # self.info = enrollment_msg.addition_result[0].message # CLG this would only be set if there was an issue with the enrollment # return enrollment_msg.addition_result[0].successful # # return False # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) enroll_batch ( self , plugin , domain , class_id , data_msgs ) Request a batch enrollment of 'audios' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required data_msgs list of BinaryMedia message to add as an enrollment addition required Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def enroll_batch ( self , plugin , domain , class_id , data_msgs ): \"\"\" Request a batch enrollment of 'audios' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :param data_msgs: list of BinaryMedia message to add as an enrollment addition :return: True if enrollment successful \"\"\" self . info = self . fullobj = None enrollment = ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True if data_msgs and len ( data_msgs ) > 0 : for data_msg in data_msgs : if isinstance ( data_msg , Audio ): enrollment . addition . append ( data_msg ) else : enrollment . addition_media . append ( data_msg ) _ , env = _wrap_message ( self . client_id , enrollment ) logging . debug ( \"Sending a batch enrollment message\" ) result = self . _sync_request ( env ) return result # ClassModificationResult finalize_supervised_adaptation ( self , plugin , domain , new_domain_name , class_annotations , adapt_workspace ) Complete the adaptation Parameters: Name Type Description Default plugin the name of the plugin to adapt required domain the name of the plugin domain to adapt required new_domain_name the name of the new domain that is created within the plugin required class_annotations the audio annotations, grouped by class ID required Returns: Type Description the name of the new domain Source code in olivepy/api/oliveclient.py def finalize_supervised_adaptation ( self , plugin , domain , new_domain_name , class_annotations , adapt_workspace ): \"\"\" Complete the adaptation :param plugin: the name of the plugin to adapt :param domain: the name of the plugin domain to adapt :param new_domain_name: the name of the new domain that is created within the plugin :param class_annotations: the audio annotations, grouped by class ID :return: the name of the new domain \"\"\" self . info = self . fullobj = None request = SupervisedAdaptationRequest () request . plugin = plugin request . domain = domain request . adapt_space = adapt_workspace request . new_domain = new_domain_name # Add the class annotations for class_id in class_annotations : request . class_annotations . append ( class_annotations [ class_id ]) # request.class_annotations.extend([class_annotations[class_id]]) for Python2.7? # package the request _ , request = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a finalize adatation message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message - boiler plate code, this can be simplified envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): olive_msg = envelope . message [ i ] if olive_msg . HasField ( \"info\" ): self . info = olive_msg . info if olive_msg . HasField ( \"error\" ): raise ExceptionFromServer ( 'Got an error from the server: ' + olive_msg . error ) else : result_msg = SupervisedAdaptationResult () result_msg . ParseFromString ( olive_msg . message_data [ 0 ]) # get the new domain #if hasattr(result_msg, 'new_domain') and result_msg.new_domain is not None: # print(\"Adaptation successfully created new domain: '{}'\".format(result_msg.new_domain)) self . fullobj = result_msg return result_msg . new_domain # adapt failed... TODO: thrown exception instead? return None get_fullobj ( self ) This object should be used for debugging only. Example use::success = client.enroll('sid-embed-v5-py3', 'multilang-v1', 'joshua', 'file') if troubleshooting: fullobj = client.get_fullobj() print('Whole object returned from server: '+str(fullobj)) Returns: Type Description the full object returned from the last call to the server. Source code in olivepy/api/oliveclient.py def get_fullobj ( self ): \"\"\" This object should be used for debugging only. Example use::success = client.enroll('sid-embed-v5-py3', 'multilang-v1', 'joshua', 'file') \\ if troubleshooting: fullobj = client.get_fullobj() print('Whole object returned from server: '+str(fullobj)) :return: the full object returned from the last call to the server. \"\"\" return self . fullobj get_info ( self ) Returns: Type Description the info data from the last call to the server. Will return None if the last call did not return any info. Source code in olivepy/api/oliveclient.py def get_info ( self ): \"\"\" :return: the info data from the last call to the server. Will return None if the last call did not return any info. \"\"\" return self . info parse_annotation_file ( self , filename ) Parse a file for the names of files of audio files and their regions to use for adaptation. Parameters: Name Type Description Default filename the path and name of the file that contains the input. This file must have one or more lines having 4 columns: # filename, class, start_region_ms, end_region_ms required Returns: Type Description the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} Source code in olivepy/api/oliveclient.py def parse_annotation_file ( self , filename ): \"\"\" Parse a file for the names of files of audio files and their regions to use for adaptation. :param filename: the path and name of the file that contains the input. This file must have one or more lines having 4 columns: # filename, class, start_region_ms, end_region_ms :return: the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} \"\"\" data_lines = [] file_annotations = {} if not os . path . exists ( filename ): raise Exception ( \"The annotation file ' {} ' does not exist\" . format ( filename )) with open ( filename ) as f : data_lines . extend ([ line . strip () for line in f . readlines ()]) # process the file for line in data_lines : pieces = line . split () if len ( pieces ) != 4 : raise Exception ( \"The annotation file does not contain data in the correct format, found line ' {} '\" . format ( line )) adapt_audio_path = pieces [ 0 ] # assume a relative file is used, so the full path must be specified since being sent to server # This is being sent to server. If full path is given, do nothing. Otherwise make absolute. # TODO: this will not work from UNIX to Windows or other way around. # TODO: should use Python's abspath here, don't you think? if adapt_audio_path [ 0 ] != '/' and adapt_audio_path [ 1 ] != ':' : adapt_audio_path = os . path . join ( os . getcwd (), adapt_audio_path ) # todo validate file is valid... if adapt_audio_path not in file_annotations : file_annotations [ adapt_audio_path ] = [] class_id = pieces [ 1 ] start = float ( pieces [ 2 ]) end = float ( pieces [ 3 ]) file_annotations [ adapt_audio_path ] . append (( start , end , class_id )) return file_annotations preprocess_supervised_audio ( self , plugin , domain , filename , adapt_workspace ) Submit audio for pre-processing phase of adaptation. Parameters: Name Type Description Default plugin the name of the plugin to adapt required domain the name of the plugin domain to adapt required filename the name of the audio file to submit to the server/plugin/domain for preprocessing required Returns: Type Description the unique id generated by the server for the preprocess audio, which must be used Source code in olivepy/api/oliveclient.py def preprocess_supervised_audio ( self , plugin , domain , filename , adapt_workspace ): \"\"\" Submit audio for pre-processing phase of adaptation. :param plugin: the name of the plugin to adapt :param domain: the name of the plugin domain to adapt :param filename: the name of the audio file to submit to the server/plugin/domain for preprocessing :return: the unique id generated by the server for the preprocess audio, which must be used \"\"\" # [(2.618, 6.2, 'S'), (7.2, 9.5, 'NS')] self . info = self . fullobj = None request = PreprocessAudioAdaptRequest () request . plugin = plugin request . domain = domain request . adapt_space = adapt_workspace request . class_id = \"supervised\" # HACK: for supervised validation in the backend - we will fix this in a future release so not needed # we currently don't need to set annotations (start_t, end_t) when doing pre-processing # finally, set the audio: audio = request . audio # send the name of the file to the server: audio . path = filename # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) # TODO SERIALIZE EXAMPLE... # package the request _ , request = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a preprocess audio (for adaptation) message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): olive_msg = envelope . message [ i ] if olive_msg . HasField ( \"info\" ): self . info = olive_msg . info if olive_msg . HasField ( \"error\" ): raise ExceptionFromServer ( 'Got an error from the server: ' + olive_msg . error ) else : result_msg = PreprocessAudioAdaptResult () result_msg . ParseFromString ( olive_msg . message_data [ 0 ]) # get audio id from results, use for final annotations... # print(\"Preprocess audio ID {} having duration {}\".format(result_msg.audio_id, result_msg.duration)) self . fullobj = result_msg return result_msg . audio_id # preprocessing failed... TODO: thrown exception instead? return None requst_sad_adaptation ( self ) Example of performing SAD adaptation Returns: Type Description Source code in olivepy/api/oliveclient.py def requst_sad_adaptation ( self ): \"\"\" Example of performing SAD adaptation :return: \"\"\" # todo move to client example (i.e. olivelearn) # using Julie's sadRegression dataset... # Assume the working directory is root directory for the SAD regression tests # Setup processing variables (get this config or via command line optons plugin = \"sad-dnn-v6a\" domain = \"multi-v1\" new_domain_name = \"python_adapted_multi-v2\" # Build the list of files plus the regions in the those files to adaptn by parsing the input file: file_annotations = self . parse_annotation_file ( \"lists/adapt_ms.lst\" ) return self . adapt_supervised_old ( plugin , domain , file_annotations , new_domain_name ) setup_multithreading () classmethod This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/oliveclient.py @classmethod def setup_multithreading ( cls ): \"\"\"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. \"\"\" # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL ) unenroll ( self , plugin , domain , class_id ) Unenrollment the class_id Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def unenroll ( self , plugin , domain , class_id ): \"\"\" Unenrollment the class_id :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :return: True if enrollment successful \"\"\" self . info = self . fullobj = None removal = ClassRemovalRequest () removal . plugin = plugin removal . domain = domain removal . class_id = class_id # Wrap message in an Envelope _ , request = _wrap_message ( self . client_id , removal ) logging . debug ( \"Sending a class modification request (removal) message\" ) result = self . _sync_request ( request ) # do something? return True get_bit_depth ( audio ) Not using since not assuming numpy is available... Source code in olivepy/api/oliveclient.py def get_bit_depth ( audio ): \"\"\"Not using since not assuming numpy is available...\"\"\" # Numpy is needed to support this... dt = audio . dtype if dt == np . int8 : return BIT_DEPTH_8 elif dt == np . int16 : return BIT_DEPTH_16 elif dt == np . int32 : return BIT_DEPTH_24 else : return BIT_DEPTH_32 package_buffer_audio ( audio , data , num_samples , sample_rate = 8000 , num_channels = 1 ) Helper function to wrap audio data (decoded samples) into a AudioBuffer message that can submitted to the server instead of a file name. Parameters: Name Type Description Default data the data as a numpy ndarray required num_samples the number of samples required sample_rate the audio sample rate 8000 num_channels the number of channels in the audio 1 Returns: Type Description Source code in olivepy/api/oliveclient.py def package_buffer_audio ( audio , data , num_samples , sample_rate = 8000 , num_channels = 1 ): \"\"\" Helper function to wrap audio data (decoded samples) into a AudioBuffer message that can submitted to the server instead of a file name. :param data: the data as a numpy ndarray :param num_samples: the number of samples :param sample_rate: the audio sample rate :param num_channels: the number of channels in the audio :return: \"\"\" # from scipy.io import wavfile # sample_rate, data = wavfile.read('somefilename.wav') buffer = audio . audioSamples buffer . channels = num_channels buffer . samples = num_samples #data.shape[0] buffer . rate = sample_rate buffer . bit_depth = get_bit_depth ( data ) buffer . data = data . tostring () return audio olivepy.api.olive_async_client AsyncOliveClient ( Thread ) This class is used to make asynchronous requests to the OLIVE server __init__ ( self , client_id , address = 'localhost' , request_port = 5588 , timeout_second = 10 ) special Parameters: Name Type Description Default client_id str The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems required address the address of the olive server, such as localhost 'localhost' request_port default olive port is 5588 5588 timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/olive_async_client.py def __init__ ( self , client_id : str , address = 'localhost' , request_port = 5588 , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems :param address: the address of the olive server, such as localhost :param request_port: default olive port is 5588 :param timeout_second: time in seconds, to wait for a response from the server \"\"\" threading . Thread . __init__ ( self ) self . client_id = client_id self . secure = False # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . server_request_port = request_port self . server_status_port = request_port + 1 self . timeout_seconds = timeout_second self . request_queue = queue . Queue () # special queue used to emulate blocking requests # self.completed_sync_request_queue = queue.Queue() self . sync_message = {} self . response_queue = {} self . working = False self . request_socket = None self . status_socket = None # thread to monitor OLIVE server heartbeats self . worker = None self . heartbeat_callback = None self . active_ids = {} # self.status_socket = context.socket(zmq.SUB) self . olive_connected = False self . monitor_status = False self . daemon = True self . last_status = None oc . OliveClient . setup_multithreading () add_heartbeat_listener ( self , heartbeat_callback ) Register a callback function to be notified when a heartbeat is received from the OLIVE server Parameters: Name Type Description Default heartbeat_callback Callable[[olive_pb2.Heartbeat], NoneType] The callback method that is notified each time a heartbeat message is received from the OLIVE server required Source code in olivepy/api/olive_async_client.py def add_heartbeat_listener ( self , heartbeat_callback : Callable [[ olive_pb2 . Heartbeat ], None ]): \"\"\" Register a callback function to be notified when a heartbeat is received from the OLIVE server :param heartbeat_callback: The callback method that is notified each time a heartbeat message is received \\ from the OLIVE server \"\"\" self . heartbeat_callback = heartbeat_callback if self . worker : self . worker . add_event_callback ( heartbeat_callback ) self . heartbeat_callback = heartbeat_callback analyze_bounding_box ( self , plugin , domain , data_msg , callback , opts = None , classes = None , msg_id = None ) Request a analysis of 'filename', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified Source code in olivepy/api/olive_async_client.py def analyze_bounding_box ( self , plugin , domain , data_msg , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified \"\"\" request = olive_pb2 . BoundingBoxScorerRequest () request . plugin = plugin request . domain = domain request . data . CopyFrom ( data_msg ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) analyze_composite ( self , plugin , domain , inputs , callback , opts = None , classes = None , msg_id = None ) Request a analysis of 'filename', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required inputs the data input required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified Source code in olivepy/api/olive_async_client.py def analyze_composite ( self , plugin , domain , inputs , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param inputs: the data input :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified \"\"\" request = olive_pb2 . CompositeScorerRequest () request . plugin = plugin request . domain = domain label = 0 for input in inputs : scorer_input = olive_pb2 . CompositeScorerInput () scorer_input . data_id = str ( label ) scorer_input . data . CopyFrom ( input ) request . input . append ( scorer_input ) label = label + 1 if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) analyze_frames ( self , plugin , domain , audio_input , callback , opts = None , classes = None , msg_id = None ) Request a analysis of 'filename', returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the Audio message to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode required opts dict a dictionary of name/value pair options for this plugin request None Returns: Type Description a OliveServerResponse containing the status of the request (FrameScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_frames ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the Audio message to score :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :param opts: a dictionary of name/value pair options for this plugin request :return: a OliveServerResponse containing the status of the request (FrameScorerResult) \"\"\" request = olive_pb2 . FrameScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio_input ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) analyze_global ( self , plugin , domain , audio , callback , opts = None , classes = None , msg_id = None ) Request a global score analysis of 'filename' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio the Audio message to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (GlobalScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_global ( self , plugin , domain , audio , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a global score analysis of 'filename' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio: the Audio message to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (GlobalScorerResult) \"\"\" self . info = self . fullobj = None request = olive_pb2 . GlobalScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) analyze_global_stateless ( self , plugin , domain , audio , class_vectors , callback , opts = None , msg_id = None ) Request a stateless global score analysis of audio Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio the Audio message to score required class_vectors dictionary of class_id to vector required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (GlobalScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_global_stateless ( self , plugin , domain , audio , class_vectors , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , msg_id = None ): \"\"\" Request a stateless global score analysis of audio :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio: the Audio message to score :param class_vectors: dictionary of class_id to vector :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (GlobalScorerResult) \"\"\" self . info = self . fullobj = None request = olive_pb2 . StatelessGlobalScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) for class_id in class_vectors : sv = olive_pb2 . ClassVector () sv . class_id = class_id for av in class_vectors [ class_id ]: # MessageToDict from google.protobuf.json_format import ParseDict # from google.protobuf.json_format import DictToMessage audio_vector = olive_pb2 . AudioVector () ParseDict ( av [ 'audioVector' ], audio_vector , ignore_unknown_fields = True ) #olive_pb2.AudioVector() # audio_vector.plugin_id = av.plugin_id # audio_vector.domain = av.domain # print(audio_vector) sv . vector . append ( audio_vector ) request . class_vector . append ( sv ) # print(request) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) analyze_regions ( self , plugin , domain , audio , callback , opts = None , classes = None , msg_id = None ) Request a analysis of 'filename', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified Source code in olivepy/api/olive_async_client.py def analyze_regions ( self , plugin , domain , audio , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified \"\"\" request = olive_pb2 . RegionScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) analyze_regions_stateless ( self , plugin , domain , audio , class_vectors , callback , opts = None , msg_id = None ) Request a analysis of 'audio', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio the name of the audio file to score required class_vectors dictionary of class_id to vector required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified Source code in olivepy/api/olive_async_client.py def analyze_regions_stateless ( self , plugin , domain , audio , class_vectors , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , msg_id = None ): \"\"\" Request a analysis of 'audio', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio: the name of the audio file to score :param class_vectors: dictionary of class_id to vector :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified \"\"\" request = olive_pb2 . StatelessRegionScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) for class_id in class_vectors : sv = olive_pb2 . ClassVector () sv . class_id = class_id for av in class_vectors [ class_id ]: # MessageToDict from google.protobuf.json_format import ParseDict # from google.protobuf.json_format import DictToMessage audio_vector = olive_pb2 . AudioVector () ParseDict ( av [ 'audioVector' ], audio_vector , ignore_unknown_fields = True ) #olive_pb2.AudioVector() # audio_vector.plugin_id = av.plugin_id # audio_vector.domain = av.domain # print(audio_vector) sv . vector . append ( audio_vector ) request . class_vector . append ( sv ) # print(request) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) analyze_text ( self , plugin , domain , text_input , callback , opts = None , classes = None , msg_id = None ) Request a analysis of 'filename', returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required text_input the text to transfrom required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required opts a dictionary of name/value pair options for this plugin request None Returns: Type Description a OliveServerResponse containing the status of the request (FrameScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_text ( self , plugin , domain , text_input , callback : Callable [[ response . OliveServerResponse ], None ], opts = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param text_input: the text to transfrom :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :param opts: a dictionary of name/value pair options for this plugin request :return: a OliveServerResponse containing the status of the request (FrameScorerResult) \"\"\" request = olive_pb2 . TextTransformationRequest () request . plugin = plugin request . domain = domain request . text = text_input if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) audio_modification ( self , plugin , domain , audio , callback , opts = None , requested_channel = 1 , requested_sample_rate = 8000 , msg_id = None ) Used to make a AudioModificationRequest (enhancement). Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the audio path or buffer to submit for modification required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode required Returns: Type Description a OliveServerResponse containing the status of the request (AudioModificationResult) Source code in olivepy/api/olive_async_client.py def audio_modification ( self , plugin , domain , audio , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , requested_channel = 1 , requested_sample_rate = 8000 , msg_id = None ): \"\"\" Used to make a AudioModificationRequest (enhancement). :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the audio path or buffer to submit for modification :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (AudioModificationResult) \"\"\" # if mode != olivepy.messaging.msgutil.AudioTransferType.AUDIO_PATH: # raise Exception('oliveclient.audio_modification requires an filename path and will not work with binary audio data.') request = olive_pb2 . AudioModificationRequest () request . plugin = plugin request . domain = domain request . requested_channels = requested_channel request . requested_rate = requested_sample_rate request . modifications . append ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) # def request_stream(self, client_id, workflow_definition, sample_rate): # \"\"\" # Used to make a AudioModificationRequest (enhancement). This call is blocking, waits for a server response # then returning the StartStreamingResult message (fixme return port number or throw exception if bad request) # # :param client_id: the unique name of this client # :param workflow_definition: the streaming workflow definition # :param sample_rate: the sample rate of the audio to be streamed # # :return: a OliveServerResponse containing the status of the request (AudioModificationResult) # \"\"\" # # request = stream_pb2.StartStreamingRequest() # request.client_stream_id = client_id # request.sampleRate = sample_rate # request.workflow_definition.CopyFrom(workflow_definition) # # #todo respose is a # return self.sync_request(request) # # # if callback: # self.enqueue_request(request, callback) # else: # return self.sync_request(request) clear_heartbeat_listeners ( self ) Remove all heartbeat listeners Source code in olivepy/api/olive_async_client.py def clear_heartbeat_listeners ( self ): \"\"\" Remove all heartbeat listeners \"\"\" if self . worker : self . worker . clear_callback () connect ( self , monitor_status = True ) Connect this client to the server Parameters: Name Type Description Default monitor_server if true, starts a thread to monitor the server status connection for heartbeat messages required Source code in olivepy/api/olive_async_client.py def connect ( self , monitor_status = True ): \"\"\" Connect this client to the server :param monitor_server: if true, starts a thread to monitor the server status connection for heartbeat messages \"\"\" # logging.debug(\"Starting Olive async monitor...\") self . monitor_status = monitor_status self . connection_done = threading . Event () self . start () # block until connected self . olive_connected = True self . connection_done . wait () self . last_status = time . time () logging . debug ( \"Olive async client ready\" ) disconnect ( self ) Closes the connection to the OLIVE server Source code in olivepy/api/olive_async_client.py def disconnect ( self ): \"\"\" Closes the connection to the OLIVE server \"\"\" if self . worker : self . worker . stopWorker () self . working = False self . olive_connected = False self . join () if self . request_socket : self . request_socket . close () enqueue_request ( self , message , callback , wrapper = None , msg_id = None ) Add a message request to the outbound queue Parameters: Name Type Description Default message the request message to send required callback this is called when response message is received from the server required wrapper the message wrapper None Source code in olivepy/api/olive_async_client.py def enqueue_request ( self , message , callback , wrapper = None , msg_id = None ): \"\"\" Add a message request to the outbound queue :param message: the request message to send :param callback: this is called when response message is received from the server :param wrapper: the message wrapper \"\"\" if wrapper is None : wrapper = response . OliveServerResponse () self . request_queue . put (( message , callback , wrapper , msg_id )) enroll ( self , plugin , domain , class_id , audio_input , callback , mode =< AudioTransferType . AUDIO_SERIALIZED : 3 > , msg_id = None ) Request a enrollment of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to enroll required audio_input the Audio message (or list of messages) to add as an enrollment addition required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (ClassModificationResult) Source code in olivepy/api/olive_async_client.py def enroll ( self , plugin , domain , class_id , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED , msg_id = None ): \"\"\" Request a enrollment of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to enroll :param audio_input: the Audio message (or list of messages) to add as an enrollment addition :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (ClassModificationResult) \"\"\" enrollment = olive_pb2 . ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True if not isinstance ( audio_input , list ): audio_input = [ audio_input ] for audio_msg in audio_input : if isinstance ( audio_msg , olive_pb2 . BinaryMedia ): enrollment . addition_media . append ( audio_msg ) elif not isinstance ( audio_msg , olive_pb2 . Audio ): audio = olive_pb2 . Audio () olivepy . messaging . msgutil . package_audio ( audio , audio_msg , mode = mode ) enrollment . addition . append ( audio ) else : audio = audio_msg enrollment . addition . append ( audio ) if callback : self . enqueue_request ( enrollment , callback , msg_id = msg_id ) else : return self . sync_request ( enrollment , msg_id = msg_id ) get_active ( self , callback = None ) Used to make a GetActiveRequest Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing the status of the request (GetActiveResult) Source code in olivepy/api/olive_async_client.py def get_active ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetActiveRequest :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (GetActiveResult) \"\"\" request = olive_pb2 . GetActiveRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) get_status ( self , callback = None ) Used to make a GetStatusRequest and receive a GetStatusResult Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse that contains the most recent server status (GetStatusResult) Source code in olivepy/api/olive_async_client.py def get_status ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetStatusRequest and receive a GetStatusResult :param callback: optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse that contains the most recent server status (GetStatusResult) \"\"\" request = olive_pb2 . GetStatusRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) get_update_status ( self , plugin , domain , callback = None ) Used to make a GetUpdateStatusRequest Parameters: Name Type Description Default plugin the name of the plugin to query required domain the name of the domain to query required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult Source code in olivepy/api/olive_async_client.py def get_update_status ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetUpdateStatusRequest :param plugin: the name of the plugin to query :param domain: the name of the domain to query :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult \"\"\" request = olive_pb2 . GetUpdateStatusRequest () request . plugin = plugin request . domain = domain if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) is_connected ( self ) Status of the connection to the OLIVE server Returns: Type Description True if connected Source code in olivepy/api/olive_async_client.py def is_connected ( self ): \"\"\" Status of the connection to the OLIVE server :return: True if connected \"\"\" return self . olive_connected load_plugin_domain ( self , plugin , domain , callback ) Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest message) Parameters: Name Type Description Default plugin the name of the plugin to pre-load required domain the name of hte domain to pre-load required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the update status of the request (LoadPluginDomainResult) Source code in olivepy/api/olive_async_client.py def load_plugin_domain ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest message) :param plugin: the name of the plugin to pre-load :param domain: the name of hte domain to pre-load :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the update status of the request (LoadPluginDomainResult) \"\"\" request = olive_pb2 . LoadPluginDomainRequest () request . plugin = plugin request . domain = domain if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) request_drain_stream ( self , session_id ) Used to send a drain request to the specified streaming session Parameters: Name Type Description Default session_id the ID of the session to flush required Returns: Type Description True if the session was flushed Source code in olivepy/api/olive_async_client.py def request_drain_stream ( self , session_id ): \"\"\" Used to send a drain request to the specified streaming session :param session_id: the ID of the session to flush :return: True if the session was flushed \"\"\" request = stream_pb2 . DrainStreamingRequest () request . session_id = session_id response = self . sync_request ( request ) return response . get_response () . successful request_flush_stream ( self , session_id ) Used to send a flush request to the specified streaming session Parameters: Name Type Description Default session_id the ID of the session to flush required Returns: Type Description True if the session was flushed Source code in olivepy/api/olive_async_client.py def request_flush_stream ( self , session_id ): \"\"\" Used to send a flush request to the specified streaming session :param session_id: the ID of the session to flush :return: True if the session was flushed \"\"\" request = stream_pb2 . FlushStreamingRequest () request . session_id = session_id response = self . sync_request ( request ) return response . get_response () . successful request_plugins ( self , callback = None , include_protected = False ) Used to make a PluginDirectoryRequest Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None include_protected (Optional) Will include \"protected\" plugins in the response. Defaults to false. False Returns: Type Description a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult) Source code in olivepy/api/olive_async_client.py def request_plugins ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None , include_protected = False ): \"\"\" Used to make a PluginDirectoryRequest :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :param include_protected: (Optional) Will include \"protected\" plugins in the response. Defaults to false. :return: a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult) \"\"\" request = olive_pb2 . PluginDirectoryRequest () if include_protected : request . include_protected = True if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) request_stop_stream ( self , session_id ) Stop a streaming session. Parameters: Name Type Description Default session_id The streaming session ID to stop. If a value of None is passed, then request that all active streaming sessions be stopped required Returns: Type Description True if the request was received by the server Source code in olivepy/api/olive_async_client.py def request_stop_stream ( self , session_id ): \"\"\" Stop a streaming session. :param session_id: The streaming session ID to stop. If a value of None is passed, then request that all active streaming sessions be stopped :return: True if the request was received by the server \"\"\" request = stream_pb2 . StopStreamingRequest () if session_id : request . session_id = session_id #todo respose is a? self . sync_request ( request ) return True run ( self ) Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/olive_async_client.py def run ( self ): self . run_zmq () if not self . secure else self . run_wss () run_zmq ( self ) Starts the thread to handle async messages Source code in olivepy/api/olive_async_client.py def run_zmq ( self ): \"\"\" Starts the thread to handle async messages \"\"\" try : logging . debug ( \"Starting OLIVE Async Message Worker for id: {} \" . format ( self . client_id )) context = zmq . Context () self . request_socket = context . socket ( zmq . DEALER ) # init the request and status socket request_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_request_port ) status_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_status_port ) self . request_socket . connect ( request_addr ) if self . monitor_status : self . status_socket = context . socket ( zmq . SUB ) self . status_socket . connect ( status_addr ) self . worker = ClientMonitorThread ( self . status_socket , self . client_id , self . monitor_status ) self . worker . start () self . working = True poller = zmq . Poller () poller . register ( self . request_socket , zmq . POLLIN ) except Exception as e : logging . error ( \"Error connecting to the OLIVE server: {} \" . format ( e )) self . olive_connected = False finally : self . connection_done . set () last_active_request_query = 0 while self . working : # First, send any client requests while not self . request_queue . empty (): request_msg , cb , wrapper , msg_id = self . request_queue . get () msg_id , env = msgutil . _wrap_message ( self . client_id , request_msg , msg_id = msg_id ) # Add to our callback queue self . response_queue [ msg_id ] = ( request_msg , cb , wrapper ) # Now send the message logging . debug ( \"Sending client request msg type: {} \" . format ( env . message [ 0 ] . message_type )) self . request_socket . send ( env . SerializeToString ()) # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . request_socket in socks : # logging.info(\"Received message from OLIVE...\") protobuf_data = self . request_socket . recv () envelope = olive_pb2 . Envelope () envelope . ParseFromString ( protobuf_data ) for i in range ( len ( envelope . message )): self . _process_response ( envelope . message [ i ]) if time . time () - last_active_request_query > ACTIVE_REQUEST_SECONDS : logging . debug ( \"Updating status...\" ) # self._issue_active_status() last_active_request_query = time . time () poller . unregister ( self . request_socket ) self . request_socket . close () setup_multithreading () classmethod This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/olive_async_client.py @classmethod def setup_multithreading ( cls ): '''This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. ''' # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL ) sync_adapt_supervised ( self , plugin , domain , annotations_file_name , new_domain_name , mode =< InputTransferType . SERIALIZED : 3 > ) Supervised adaptation. Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required annotations_file_name the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. required new_domain_name the name of the new domain required mode (Optional) the audio transfer mode. Defaults to InputTransferType.SERIALIZED <InputTransferType.SERIALIZED: 3> Returns: Type Description the full path name of the new domain Source code in olivepy/api/olive_async_client.py def sync_adapt_supervised ( self , plugin , domain , annotations_file_name , new_domain_name , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED ): \"\"\" Supervised adaptation. :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param annotations_file_name: the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. :param new_domain_name: the name of the new domain :param mode: (Optional) the audio transfer mode. Defaults to InputTransferType.SERIALIZED :return: the full path name of the new domain \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] file_annotations = utils . parse_annotation_file ( annotations_file_name ) for filename , regions in file_annotations . items (): audio = msgutil . package_audio ( olive_pb2 . Audio (), filename , mode = mode ) audio_id = self . _sync_preprocess_supervised_audio ( plugin , domain , audio , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . _sync_convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . _sync_finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace ) sync_request ( self , message , wrapper = None , msg_id = None ) Send a request to the OLIVE server, but wait for a response from the server Parameters: Name Type Description Default message the request message to send to the OLIVE server required Returns: Type Description the response from the server Source code in olivepy/api/olive_async_client.py def sync_request ( self , message , wrapper = None , msg_id = None ): \"\"\" Send a request to the OLIVE server, but wait for a response from the server :param message: the request message to send to the OLIVE server :return: the response from the server \"\"\" if wrapper is None : wrapper = response . OliveServerResponse () # create an ID for this sync_request sync_id = msgutil . get_uuid () result_available = threading . Event () cb = lambda response : self . _sync_callback ( response , sync_id , result_available ) self . enqueue_request ( message , cb , wrapper , msg_id ) result_available . wait () # get the result if sync_id in self . sync_message : return self . sync_message . pop ( sync_id ) else : # unexpected.... callback event completed with no result raise Exception ( \"Error waiting for a response from the server\" ) unenroll ( self , plugin , domain , class_id , callback = None , msg_id = None ) Unenroll class_id Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to remove required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing the status of the request (ClassRemovalResult) Source code in olivepy/api/olive_async_client.py def unenroll ( self , plugin , domain , class_id , callback : Callable [[ response . OliveServerResponse ], None ] = None , msg_id = None ): \"\"\" Unenroll class_id :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to remove :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (ClassRemovalResult) \"\"\" removal = olive_pb2 . ClassRemovalRequest () removal . plugin = plugin removal . domain = domain removal . class_id = class_id if callback : self . enqueue_request ( removal , callback , msg_id = msg_id ) else : return self . sync_request ( removal , msg_id = msg_id ) unload_plugin_domain ( self , plugin , domain , callback ) Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded plugin from server memory) Parameters: Name Type Description Default plugin the name of the plugin to unload required domain the name of hte domain to unload required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RemovePluginDomainResult) Source code in olivepy/api/olive_async_client.py def unload_plugin_domain ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded \\ plugin from server memory) :param plugin: the name of the plugin to unload :param domain: the name of hte domain to unload :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RemovePluginDomainResult) \"\"\" request = olive_pb2 . RemovePluginDomainRequest () request . plugin = plugin . strip () request . domain = domain . strip () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) update_plugin_domain ( self , plugin , domain , metadata , callback ) Used to make a ApplyUpdateRequest Parameters: Name Type Description Default plugin the name of the plugin to update required domain the name of hte domain to update required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (ApplyUpdateResult) Source code in olivepy/api/olive_async_client.py def update_plugin_domain ( self , plugin , domain , metadata , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a ApplyUpdateRequest :param plugin: the name of the plugin to update :param domain: the name of hte domain to update :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (ApplyUpdateResult) \"\"\" request = olive_pb2 . ApplyUpdateRequest () request . plugin = plugin request . domain = domain mds = request . params for key , item in metadata : md = olive_pb2 . Metadata () md . name = key if isinstance ( item , str ): md . type = 1 elif isinstance ( item , int ): md . type = 2 elif isinstance ( item , float ): md . type = 3 elif isinstance ( item , bool ): md . type = 4 elif isinstance ( item , list ): md . type = 5 else : raise Exception ( 'Metadata {} had a {} type that was not str, int, float, bool, or list.' . format ( key , str ( type ( item )))) md . value = item mds . append ( md ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) vectorize_audio ( self , plugin , domain , audio_input , callback , mode =< AudioTransferType . AUDIO_SERIALIZED : 3 > , msg_id = None ) Request a vectorized version of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to enroll required audio_input the Audio message (or list of messages) to add as an enrollment addition required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (ClassModificationResult) Source code in olivepy/api/olive_async_client.py def vectorize_audio ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED , msg_id = None ): \"\"\" Request a vectorized version of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to enroll :param audio_input: the Audio message (or list of messages) to add as an enrollment addition :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (ClassModificationResult) \"\"\" vectorization = olive_pb2 . PluginAudioVectorRequest () vectorization . plugin = plugin vectorization . domain = domain if not isinstance ( audio_input , list ): audio_input = [ audio_input ] for audio_msg in audio_input : if not isinstance ( audio_msg , olive_pb2 . Audio ): audio = olive_pb2 . Audio () olivepy . messaging . msgutil . package_audio ( audio , audio_msg , mode = mode ) vectorization . addition . append ( audio ) else : audio = audio_msg vectorization . addition . append ( audio ) if callback : self . enqueue_request ( vectorization , callback , msg_id = msg_id ) else : return self . sync_request ( vectorization , msg_id = msg_id ) ClientMonitorThread ( Thread ) Helper used to monitor the status of the Oliveserver add_event_callback ( self , callback ) Callback function that is notified of a heartbeat Parameters: Name Type Description Default callback Callable[[olive_pb2.Heartbeat], NoneType] the function that is called with a Heartbeat object required Source code in olivepy/api/olive_async_client.py def add_event_callback ( self , callback : Callable [[ olive_pb2 . Heartbeat ], None ]): \"\"\" Callback function that is notified of a heartbeat :param callback: the function that is called with a Heartbeat object \"\"\" self . event_callback . append ( callback ) run ( self ) Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/olive_async_client.py def run ( self ): # print(\"Starting Olive Status Monitor for id: {}\".format(self.client_id)) self . working = True self . status_socket . subscribe ( \"\" ) poller = zmq . Poller () poller . register ( self . status_socket , zmq . POLLIN ) last_heartbeat = time . time () heartbeat_data = None notified_conn_fail = False while self . working : socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . status_socket in socks : last_heartbeat = time . time () heartbeat_data = self . status_socket . recv () heatbeat = olive_pb2 . Heartbeat () heatbeat . ParseFromString ( heartbeat_data ) if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats self . olive_status = OliveStatusRecord ( stats . pool_pending , stats . max_num_jobs , stats . cpu_percent , stats . mem_percent ) if self . log_status : for cb in self . event_callback : cb ( heatbeat ) else : if not notified_conn_fail and not heartbeat_data and time . time () - last_heartbeat > HEARTBEAT_TIMEOUT_SECONDS : print ( \"Unable to connect to server\" ) notified_conn_fail = True # Consider using the same timeout for messages? elif heartbeat_data and time . time () - last_heartbeat > HEARTBEAT_TIMEOUT_SECONDS : print ( \"heartbeat timeout\" ) # it has been too long since a heatbeat message was received from the server... assume there server is down if self . log_status : for cb in self . event_callback : cb ( None ) self . status_socket . close () OliveStatusRecord Tracks status of an olive server StreamOliveClient ( Thread ) This class is used to make streaming requests to an OLIVE server. Each streaming 'session' has its own StreamOliveClient. Any results are asynchronous, and unlike the AsyncOliveClient there is no request/response message expectation. One might submit multiple audio/data inputs before getting a response from the server (in the form of a WorkflowAnalysisResult) __init__ ( self , client_id , data_port , address = 'localhost' , timeout_second = 10 ) special Parameters: Name Type Description Default client_id str The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems required data_port the streaming port number required address the address of the olive server, such as localhost 'localhost' timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/olive_async_client.py def __init__ ( self , client_id : str , data_port , address = 'localhost' , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems :param data_port: the streaming port number :param address: the address of the olive server, such as localhost :param timeout_second: time in seconds, to wait for a response from the server \"\"\" threading . Thread . __init__ ( self ) self . client_id = client_id # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . stream_data_port = data_port self . timeout_seconds = timeout_second self . request_queue = queue . Queue () self . working = False self . request_socket = None self . status_socket = None # thread to monitor OLIVE server heartbeats self . worker = None # self.status_socket = context.socket(zmq.SUB) self . stream_connected = False self . monitor_status = False oc . OliveClient . setup_multithreading () self . streaming_callbacks = dict () connect ( self ) Connect this client to the server Parameters: Name Type Description Default monitor_server if true, starts a thread to monitor the server status connection for heartbeat messages required Source code in olivepy/api/olive_async_client.py def connect ( self ): \"\"\" Connect this client to the server :param monitor_server: if true, starts a thread to monitor the server status connection for heartbeat messages \"\"\" # logging.debug(\"Starting Olive async monitor...\") self . connection_done = threading . Event () self . start () # block until connected self . stream_connected = True self . connection_done . wait () logging . debug ( \"Olive async client ready\" ) disconnect ( self ) Closes the connection to the OLIVE server Source code in olivepy/api/olive_async_client.py def disconnect ( self ): \"\"\" Closes the connection to the OLIVE server \"\"\" if self . worker : self . worker . stopWorker () self . working = False self . stream_connected = False self . join () self . request_socket . close () enqueue_data ( self , data_message ) Send a data (audio) a message to the streaming session. Only data can be sent, non-data messages are not supported. All server requests (even to stop this streaming session) must be sent on the standard OLIVE request socket using the AsyncOliveClient) Parameters: Name Type Description Default data_message the data (audio) to send, currently limited to an Audio message, although that may expand over time required Source code in olivepy/api/olive_async_client.py def enqueue_data ( self , data_message ): \"\"\" Send a data (audio) a message to the streaming session. Only data can be sent, non-data messages are not \\ supported. All server requests (even to stop this streaming session) must be sent on the standard OLIVE \\ request socket using the AsyncOliveClient) :param data_message: the data (audio) to send, currently limited to an Audio message, although that may expand over time \"\"\" self . request_queue . put ( data_message ) is_connected ( self ) Status of the connection to the OLIVE server Returns: Type Description True if connected Source code in olivepy/api/olive_async_client.py def is_connected ( self ): \"\"\" Status of the connection to the OLIVE server :return: True if connected \"\"\" return self . stream_connected run ( self ) Starts the thread to handle async messages Source code in olivepy/api/olive_async_client.py def run ( self ): \"\"\" Starts the thread to handle async messages \"\"\" try : logging . debug ( \"Starting OLIVE Streaming Worker for id: {} \" . format ( self . client_id )) context = zmq . Context () self . request_socket = context . socket ( zmq . PAIR ) # init the request and status socket data_addr = \"tcp://\" + self . server_address + \":\" + str ( self . stream_data_port ) self . request_socket . connect ( data_addr ) # todo if/when we provide heatbeats from the streaming exec... # if self.monitor_status: # logging.debug(\"connecting to status socket...\") # self.status_socket = context.socket(zmq.SUB) # self.status_socket.connect(status_addr) # self.worker = ClientMonitorThread(self.status_socket, self.client_id) # self.worker.start() # else: # self.worker = None poller = zmq . Poller () poller . register ( self . request_socket , zmq . POLLIN ) self . working = True except Exception as e : logging . error ( \"Error connecting to the OLIVE streaming server: {} \" . format ( e )) self . stream_connected = False self . working = False finally : self . connection_done . set () while self . working : # First, send any client requests while not self . request_queue . empty (): request_msg = self . request_queue . get () logging . debug ( \"Sending client data\" ) self . request_socket . send ( request_msg . SerializeToString ()) # FIXME - check if a result was lost # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . request_socket in socks : # logging.info(\"Received streaming message from OLIVE...\") protobuf_data = self . request_socket . recv () envelope = olive_pb2 . Envelope () envelope . ParseFromString ( protobuf_data ) print ( 'Handle {} stream messages' . format ( len ( envelope . message ))) for i in range ( len ( envelope . message )): self . _process_response ( envelope . message [ i ]) poller . unregister ( self . request_socket ) self . request_socket . close () setup_multithreading () classmethod This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/olive_async_client.py @classmethod def setup_multithreading ( cls ): '''This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. ''' # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL ) olivepy.api.workflow OliveWorkflow An OliveWorkflow instance represents a Workflow Definition actualized by an OLIVE server. Once actualized, an OliveWorkflow instance is used to make analysis, or enrollment/unenrollment requests. An OliveWorkflow should be created using an OliveWorkflowDefinition's create_workflow() method. All calls to the server include an optional callback. When the callback is provided, the call does not block and the callback method is invoked when a response is received from the server. A callback method has 3 arguments: the original request, the response, and an error message if the request failed. Exceptions: Type Description WorkflowException If the workflow was not actualized __init__ ( self , olive_async_client , actualized_workflow ) special Parameters: Name Type Description Default olive_async_client AsyncOliveClient the client connection to the OLIVE server required actualized_workflow OliveWorkflowActualizedResponse An OliveWorkflowDefinition actualized by the server required Source code in olivepy/api/workflow.py def __init__ ( self , olive_async_client : AsyncOliveClient , actualized_workflow : response . OliveWorkflowActualizedResponse ): \"\"\" :param olive_async_client: the client connection to the OLIVE server :param actualized_workflow: An OliveWorkflowDefinition actualized by the server \"\"\" self . client = olive_async_client self . workflow_response = actualized_workflow actualized_workflow_definition = actualized_workflow . get_workflow () # make sure an OLIvE server has actualized this workflow if not actualized_workflow_definition . actualized : raise WorkflowException ( \"Error: Can not create an OliveWorkflow using a Workflow Definition that has not \" \"been actualized by an OLIVE server\" ) self . workflow_def = actualized_workflow_definition # note: enrollment and adapt should only have one task/job # but there could be multiple plugins/task that could support enrollment or adaptation.. so we focus on # analysis adapt ( self , data_input , callback , options = None , finalize = True ) NOT YET SUPPORTED -- and not sure it will ever be supported via workflow Parameters: Name Type Description Default data_input required callback required options None finalize True Returns: Type Description not supported Source code in olivepy/api/workflow.py def adapt ( self , data_input , callback , options = None , finalize = True ): \"\"\" NOT YET SUPPORTED -- and not sure it will ever be supported via workflow :param data_input: :param callback: :param options: :param finalize: :return: not supported \"\"\" raise Exception ( \"Workflow adaption not supported\" ) analyze ( self , data_inputs , callback = None , options = None , class_ids = None ) Perform a workflow analysis Parameters: Name Type Description Default data_inputs List[workflow_pb2.WorkflowDataRequest] a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. required callback an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. None options str a JSON string of name/value options to include with the analysis request. None class_ids str a JSON string of name/value options to include with the analysis request. None Returns: Type Description OliveWorkflowAnalysisResponse an OliveWorkflowAnalysisResponse (if no callback provided) Source code in olivepy/api/workflow.py def analyze ( self , data_inputs : List [ WorkflowDataRequest ], callback = None , options : str = None , class_ids : str = None ) -> response . OliveWorkflowAnalysisResponse : \"\"\" Perform a workflow analysis :param data_inputs: a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. :param callback: an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. :param options: a JSON string of name/value options to include with the analysis request. :param class_ids: a JSON string of name/value options to include with the analysis request. :return: an OliveWorkflowAnalysisResponse (if no callback provided) \"\"\" # make call blocking if no callback or always assume it is async? analysis_request = WorkflowAnalysisRequest () for di in data_inputs : analysis_request . workflow_data_input . append ( di ) analysis_request . workflow_definition . CopyFrom ( self . workflow_def ) # Parse options (if any) if options : jopts = utils . parse_json_options ( options ) analysis_request . option . extend ( jopts ) if class_ids : json_class_ids = utils . parse_json_options ( class_ids ) analysis_request . class_id . extend ( json_class_ids ) if callback : self . client . enqueue_request ( analysis_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( analysis_request , response . OliveWorkflowAnalysisResponse ()) enroll ( self , data_inputs , class_id , job_names , callback = None , options = None ) Submit data for enrollment. Parameters: Name Type Description Default data_inputs List[workflow_pb2.WorkflowDataRequest] a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. required class_id str the name of the enrollment required job_names List[str] a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. required callback an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. None options a dictionary of name/value option pairs to include with the enrollment request None Returns: Type Description server enrollment response if no callback provided Source code in olivepy/api/workflow.py def enroll ( self , data_inputs : List [ WorkflowDataRequest ], class_id : str , job_names : List [ str ], callback = None , options = None ): \"\"\" Submit data for enrollment. :param data_inputs: a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. :param class_id: the name of the enrollment :param job_names: a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. :param callback: an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. :param options: a dictionary of name/value option pairs to include with the enrollment request :return: server enrollment response if no callback provided \"\"\" # # first, get the enrollment order # for order in self.workflow_def.order: # if order.workflow_type == WORKFLOW_ENROLLMENT_TYPE: # workflow_enrollment_order_msg = order # break # # if workflow_enrollment_order_msg is None: # raise Exception(\"This workflow does not contain any \") # # # for name in task_names: # make call blocking if no callback or always assume it is async? enroll_request = WorkflowEnrollRequest () for di in data_inputs : enroll_request . workflow_data_input . append ( di ) enroll_request . workflow_definition . CopyFrom ( self . workflow_def ) enroll_request . class_id = class_id for job_task in job_names : enroll_request . job_names . append ( job_task ) if options : jopts = utils . parse_json_options ( options ) enroll_request . option . extend ( jopts ) if callback : # self.client.enqueue_request(enroll_request, callback, response.OliveWorkflowEnrollmentResponse()) self . client . enqueue_request ( enroll_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( enroll_request , response . OliveWorkflowAnalysisResponse ()) get_analysis_class_ids ( self , type = 1 , callback = None ) Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. Parameters: Name Type Description Default callback an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) None Returns: Type Description OliveClassStatusResponse an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server Source code in olivepy/api/workflow.py def get_analysis_class_ids ( self , type = WORKFLOW_ANALYSIS_TYPE , callback = None ) -> response . OliveClassStatusResponse : \"\"\" Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. :param type the WorkflowOrder type (WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, or WORKFLOW_UNENROLLMENT_TYPE) :param callback: an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) :return: an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server \"\"\" class_request = WorkflowClassStatusRequest () class_request . workflow_definition . CopyFrom ( self . workflow_def ) if type : class_request . type = type if callback : self . client . enqueue_request ( class_request , callback , response . OliveClassStatusResponse ()) else : return self . client . sync_request ( class_request , response . OliveClassStatusResponse ()) get_analysis_job_names ( self ) The names of analysis jobs in this workflow (usually only one analysis job) Returns: Type Description List[str] A list of analysis job names in this workflow Source code in olivepy/api/workflow.py def get_analysis_job_names ( self ) -> List [ str ]: \"\"\" The names of analysis jobs in this workflow (usually only one analysis job) :return: A list of analysis job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_ANALYSIS_TYPE ) get_analysis_task_info ( self ) A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is not known until runtime) Returns: Type Description List[Dict[str, Dict]] JSON structured detailed information of analysis tasks used in this workflow Source code in olivepy/api/workflow.py def get_analysis_task_info ( self ) -> List [ Dict [ str , Dict ]]: \"\"\" A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report \\ includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is \\ not known until runtime) :return: JSON structured detailed information of analysis tasks used in this workflow \"\"\" # return [task.consumer_result_label for task in analysis_jobs[job_name]] return self . workflow_response . to_json ( indent = 1 ) get_analysis_tasks ( self , job_name = None ) Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) Parameters: Name Type Description Default job_name str filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_analysis_tasks ( self , job_name : str = None ) -> List [ str ]: \"\"\" Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) :param job_name: filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. :return: a list of task names \"\"\" analysis_jobs = response . get_workflow_jobs ( self . workflow_def , WORKFLOW_ANALYSIS_TYPE ) # better to exception or empty dict???? if len ( analysis_jobs ) == 0 : return None if job_name is not None : if job_name not in analysis_jobs : return None else : # get the default job name job_name = list ( analysis_jobs . keys ())[ 0 ] return [ task . consumer_result_label for task in analysis_jobs [ job_name ]] get_enrollment_job_names ( self ) The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment Returns: Type Description List[str] A list of enrollment job names in this workflow Source code in olivepy/api/workflow.py def get_enrollment_job_names ( self ) -> List [ str ]: \"\"\" The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment :return: A list of enrollment job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_ENROLLMENT_TYPE ) get_enrollment_tasks ( self , job_name = None , type = 2 ) Return a list of tasks that support enrollment in this workflow. Parameters: Name Type Description Default job_name str optionally the name of the enrollment job. Optional since most workflows only support one job None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_enrollment_tasks ( self , job_name : str = None , type = WORKFLOW_ENROLLMENT_TYPE ) -> List [ str ]: \"\"\" Return a list of tasks that support enrollment in this workflow. :param job_name: optionally the name of the enrollment job. Optional since most workflows only support one job :return: a list of task names \"\"\" enrollment_jobs = response . get_workflow_jobs ( self . workflow_def , type ) if len ( enrollment_jobs ) == 0 : return None if job_name is not None : if job_name not in enrollment_jobs : return None # normally (and currently the only supported option) should be just one enrollment_job... return list ( response . get_workflow_job_tasks ( enrollment_jobs , job_name ) . keys ()) get_unenrollment_job_names ( self ) The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment Returns: Type Description List[str] A list of un-enrollment job names in this workflow Source code in olivepy/api/workflow.py def get_unenrollment_job_names ( self ) -> List [ str ]: \"\"\" The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment :return: A list of un-enrollment job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_UNENROLLMENT_TYPE ) get_unenrollment_tasks ( self , job_name = None ) Return a list of tasks that support UNenrollment in this workflow. Parameters: Name Type Description Default job_name str optionally the name of the enrollment job. Optional since most workflows only support one job None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_unenrollment_tasks ( self , job_name : str = None ) -> List [ str ]: \"\"\" Return a list of tasks that support UNenrollment in this workflow. :param job_name: optionally the name of the enrollment job. Optional since most workflows only support one job :return: a list of task names \"\"\" return self . get_enrollment_tasks ( job_name , type = WORKFLOW_UNENROLLMENT_TYPE ) package_audio ( self , audio_data , mode =< InputTransferType . SERIALIZED : 3 > , annotations = None , task_annotations = None , selected_channel = None , num_channels = None , sample_rate = None , num_samples = None , validate_local_path = True , label = None ) Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. Parameters: Name Type Description Default audio_data ~AnyStr the input data is a string (file path) if mode is 'AUDIO_PATH', otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples required mode specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. <InputTransferType.SERIALIZED: 3> annotations List[Tuple[float, float]] optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) None task_annotations Dict[str, Dict[str, List[Tuple[float, float]]]] optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {'SHL': {'speaker'':[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the 'SHL' task, which are labeled as class 'speaker' having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . None selected_channel int optional - the channel to process if using multi-channel audio None num_channels int The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None sample_rate int The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None num_samples int The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None validate_local_path bool If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired True label an optional name to use with the audio None Returns: Type Description WorkflowDataRequest A populated WorkflowDataRequest to use in a workflow activity Source code in olivepy/api/workflow.py def package_audio ( self , audio_data : AnyStr , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , annotations : List [ Tuple [ float , float ]] = None , task_annotations : Dict [ str , Dict [ str , List [ Tuple [ float , float ]]]] = None , selected_channel : int = None , num_channels : int = None , sample_rate : int = None , num_samples : int = None , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. :param audio_data: the input data is a string (file path) if mode is 'AUDIO_PATH', otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples :param mode: specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. :param annotations: optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) :param task_annotations: optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {'SHL': {'speaker'':[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the 'SHL' task, which are labeled as class 'speaker' having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . :param selected_channel: optional - the channel to process if using multi-channel audio :param num_channels: The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param sample_rate: The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param num_samples: The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param validate_local_path: If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired :param label: an optional name to use with the audio :return: A populated WorkflowDataRequest to use in a workflow activity \"\"\" audio = Audio () msgutil . package_audio ( audio , audio_data , annotations , selected_channel , mode , num_channels , sample_rate , num_samples , validate_local_path ) # Add any task specific regions: if task_annotations : for task_label in task_annotations . keys (): ta = audio . task_annotations . add () ta . task_label = task_label # we only expect to have one set of annotations, so just one region_label for region_label in task_annotations [ task_label ]: ta . region_label = region_label for annots in task_annotations [ task_label ][ region_label ]: region = ta . regions . add () region . start_t = np . float ( annots [ 0 ]) region . end_t = np . float ( annots [ 1 ]) wkf_data_request = WorkflowDataRequest () #fixme: this should be set based on the audio.label (filename) or given a unique name here... wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = AUDIO wkf_data_request . workflow_data = audio . SerializeToString () # consumer_data_label doesn't need to be set... use default # set job name? Currently we assume one job per workflow so punting on this for now return wkf_data_request package_binary ( self , binary_input , mode =< InputTransferType . SERIALIZED : 3 > , annotations = None , validate_local_path = True , label = None ) Parameters: Name Type Description Default video_input a video input required Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_binary ( self , binary_input , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , annotations : List [ Tuple [ float , float ]] = None , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" :param video_input: a video input :return: TBD \"\"\" media = BinaryMedia () msgutil . package_binary_media ( media , binary_input , mode = mode , validate_local_path = validate_local_path ) if label : media . label = label wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = VIDEO wkf_data_request . workflow_data = media . SerializeToString () return wkf_data_request package_image ( self , image_input , mode =< InputTransferType . SERIALIZED : 3 > , validate_local_path = True , label = None ) Not yet supported Parameters: Name Type Description Default image_input An image input required Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_image ( self , image_input , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" Not yet supported :param image_input: An image input :return: TBD \"\"\" media = BinaryMedia () msgutil . package_binary_media ( media , image_input , mode = mode , validate_local_path = validate_local_path ) if label : media . label = label media . motion = False # todo if annotations... wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = IMAGE wkf_data_request . workflow_data = media . SerializeToString () return wkf_data_request package_text ( self , text_input , optional_label = None , text_workflow_key = None ) Used to package data for a workflow that accepts string (text) input Parameters: Name Type Description Default text_input str a text input required optional_label str an optional label, namoe or comment associated with this input None text_workflow_key str the keyword used to identify this data in the workflow. By default a value of 'text' is assumed and recommend None Returns: Type Description WorkflowDataRequest a WorkflowDataRequest populated with the text input Source code in olivepy/api/workflow.py def package_text ( self , text_input : str , optional_label : str = None , text_workflow_key : str = None ) -> WorkflowDataRequest : \"\"\" Used to package data for a workflow that accepts string (text) input :param text_input: a text input :param optional_label: an optional label, namoe or comment associated with this input :param text_workflow_key: the keyword used to identify this data in the workflow. By default a value of 'text' is assumed and recommend :return: a WorkflowDataRequest populated with the text input \"\"\" text_msg = Text () # not (yet?) supported multiple text inputs in a request text_msg . text . append ( text_input ) if optional_label : text_msg . label = optional_label wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = text_workflow_key if text_workflow_key else 'text' wkf_data_request . data_type = TEXT wkf_data_request . workflow_data = text_msg . SerializeToString () return wkf_data_request package_workflow_input ( self , input_msg , expected_data_type =< OliveInputDataType . AUDIO_DATA_TYPE : 2 > ) Parameters: Name Type Description Default input_msg the OLIVE data message to package required expected_data_type the data type of the message (Binary <OliveInputDataType.AUDIO_DATA_TYPE: 2> Returns: Type Description WorkflowDataRequest a WorkflowDataRequest Source code in olivepy/api/workflow.py def package_workflow_input ( self , input_msg , expected_data_type = msgutil . OliveInputDataType . AUDIO_DATA_TYPE ) -> WorkflowDataRequest : \"\"\" :param input_msg: the OLIVE data message to package :param expected_data_type: the data type of the message (Binary :return: a WorkflowDataRequest \"\"\" wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = input_msg . label if input_msg . label else msgutil . get_uuid () wkf_data_request . data_type = msgutil . data_type_class_map [ expected_data_type ] wkf_data_request . workflow_data = input_msg . SerializeToString () return wkf_data_request serialize_audio ( self , filename ) Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters: Name Type Description Default filename str the local path to the file to serialize required Returns: Type Description ~AnyStr the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples Source code in olivepy/api/workflow.py def serialize_audio ( self , filename : str ) -> AnyStr : \"\"\" Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() \\ when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized :param filename: the local path to the file to serialize :return: the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples \"\"\" with open ( os . path . expanduser ( filename ), 'rb' ) as f : serialized_buffer = f . read () # return the buffer return serialized_buffer stream ( self , client_id , sample_rate , options = None ) Request a new streaming session. This call is blocking, as it waits for a response from the client that acknowledges or denies the streaming request Parameters: Name Type Description Default client_id str the unique name of this streaming client required sample_rate int the sample rate of the audio to be streamed required options str a JSON string of name/value options to include with the analysis request such as '{\"filter_length\":99, \"interpolate\":1.0, \"test_name\":\"midge\"}' None Returns: Type Description [<class 'str'>, <class 'int'>] a tuple of the new session ID and the streaming port number. Source code in olivepy/api/workflow.py def stream ( self , client_id : str , sample_rate : int , options : str = None ) -> [ str , int ]: \"\"\" Request a new streaming session. This call is blocking, as it waits for a response from the client that acknowledges or denies the streaming request :param client_id: the unique name of this streaming client :param sample_rate: the sample rate of the audio to be streamed :param options: a JSON string of name/value options to include with the analysis request such as '{\"filter_length\":99, \"interpolate\":1.0, \"test_name\":\"midge\"}' :return: a tuple of the new session ID and the streaming port number. \"\"\" stream_request = StartStreamingRequest () stream_request . client_stream_id = client_id stream_request . sampleRate = sample_rate stream_request . workflow_definition . CopyFrom ( self . workflow_def ) # Parse options (if any) if options : jopts = utils . parse_json_options ( options ) stream_request . option . extend ( jopts ) stream_response = self . client . sync_request ( stream_request , response . OliveWorkflowAnalysisResponse ()) if stream_response . is_successful (): if stream_response . get_response () . successful : return stream_response . get_response () . session_id , stream_response . get_response () . data_port else : # failed to start the streaming session (likely too many sessions) raise msgutil . ExceptionFromServer ( stream_response . get_response () . info ) # could not start streaming... raise msgutil . ExceptionFromServer ( stream_response . get_error ()) to_json ( self , indent = None ) Generate the workflow as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/api/workflow.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" return self . workflow_response . to_json ( indent = indent ) unenroll ( self , class_id , job_names , callback = None , options = None ) Submit a class id (speaker name, language name, etc) for un-enrollment. Parameters: Name Type Description Default class_id str the name of the enrollment class to remove required job_names List[str] a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). required callback an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. None options a dictionary of name/value option pairs to include with the enrollment request None Returns: Type Description server unenrollment response if no callback provided Source code in olivepy/api/workflow.py def unenroll ( self , class_id : str , job_names : List [ str ], callback = None , options = None ): \"\"\" Submit a class id (speaker name, language name, etc) for un-enrollment. :param class_id: the name of the enrollment class to remove :param job_names: a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). :param callback: an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. :param options: a dictionary of name/value option pairs to include with the enrollment request :return: server unenrollment response if no callback provided \"\"\" # make call blocking if no callback or always assume it is async? unenroll_request = WorkflowUnenrollRequest () unenroll_request . workflow_definition . CopyFrom ( self . workflow_def ) unenroll_request . class_id = class_id for job_task in job_names : unenroll_request . job_names . append ( job_task ) if options : jopts = utils . parse_json_options ( options ) unenroll_request . option . extend ( jopts ) if callback : # self.client.enqueue_request(enroll_request, callback, response.OliveWorkflowEnrollmentResponse()) self . client . enqueue_request ( unenroll_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( unenroll_request , response . OliveWorkflowAnalysisResponse ()) OliveWorkflowDefinition Used to load a Workflow Definition from a file. __init__ ( self , filename ) special Create an OliveWorkflowDefinition to access a workflow definition file Parameters: Name Type Description Default filename str the path/filename of a workflow definition file to load required Source code in olivepy/api/workflow.py def __init__ ( self , filename : str ): \"\"\" Create an OliveWorkflowDefinition to access a workflow definition file :param filename: the path/filename of a workflow definition file to load \"\"\" # First, make sure the workflow definition (WD) file exists filename = os . path . expanduser ( filename ) if not os . path . exists ( filename ): raise IOError ( \"Workflow definition file ' {} ' does not exists\" . format ( filename )) # Load the WD, then submit to the server # Read the workflow - either a workflow or a text file try : with open ( filename , 'rb' ) as f : self . wd = WorkflowDefinition () self . wd . ParseFromString ( f . read ()) except IOError as e : raise IOError ( \"Workflow definition file ' {} ' does not exist\" . format ( filename )) except DecodeError as de : self . wd = WorkflowDefinition () # Try parsing as text file (will fail for a protobuf file) with open ( filename , 'r' ) as f : # First load as json json_input = json . loads ( f . read ()) # Next, we need to convert message data in task(s) to byte strings for element in json_input : if element == 'order' : for job in json_input [ element ]: # print(\"Job: {}\".format(job)) for job_def in job [ 'job_definition' ]: for task in job_def [ 'tasks' ]: task_type = task [ 'message_type' ] # Covert 'messageData' into a protobuf and save the byte string in the json # so it can be correctly deserialized tmp_json = task [ 'message_data' ] msg = msgutil . type_class_map [ MessageType . Value ( task_type )]() Parse ( json . dumps ( tmp_json ), msg ) # now serialized msg as messageData data = base64 . b64encode ( msg . SerializeToString ()) . decode ( 'utf-8' ) task [ 'message_data' ] = data # Now we should be able to create a WorkflowDefinition from the json data Parse ( json . dumps ( json_input ), self . wd ) # Create JSON formatted output from the the Workflow? create_workflow ( self , client ) Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, or enrollment requests Parameters: Name Type Description Default client AsyncOliveClient an open client connection to an OLIVE server required Returns: Type Description a new OliveWorkflow object, which has been actualized (activated) by the olive server Source code in olivepy/api/workflow.py def create_workflow ( self , client : olivepy . api . olive_async_client . AsyncOliveClient ): \"\"\" Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, or enrollment requests :param client: an open client connection to an OLIVE server :return: a new OliveWorkflow object, which has been actualized (activated) by the olive server \"\"\" if not client . is_connected (): raise IOError ( \"No connection to the Olive server\" ) # Create a workflow request request = WorkflowActualizeRequest () request . workflow_definition . CopyFrom ( self . wd ) workflow_result = client . sync_request ( request , response . OliveWorkflowActualizedResponse ()) if workflow_result . is_error (): raise msgutil . ExceptionFromServer ( workflow_result . get_error ()) # if msg: # raise msgutil.ExceptionFromServer(msg) return OliveWorkflow ( client , workflow_result ) # todo send WD to server, return an OliveWorklow to the user get_json ( self , indent = 1 ) Create a JSON structure of the Workflow Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document 1 Returns: Type Description A JSON (dictionary) representation of the Workflow Definition Source code in olivepy/api/workflow.py def get_json ( self , indent = 1 ): \"\"\" Create a JSON structure of the Workflow :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document :return: A JSON (dictionary) representation of the Workflow Definition \"\"\" analysis_task = [] job_names = set () workflow_analysis_order_msg = None for order in self . wd . order : if order . workflow_type == WORKFLOW_ANALYSIS_TYPE : workflow_analysis_order_msg = order break if workflow_analysis_order_msg is None : # no analysis results return analysis_task # for job in self._response.job_result: for job in workflow_analysis_order_msg . job_definition : # create a dictionary for each job result job_dict = dict () job_name = job . job_name job_names . add ( job_name ) # get data handling info for this job data_prop = job . data_properties job_dict [ 'Data Input' ] = json . loads ( MessageToJson ( data_prop , preserving_proto_field_name = True )) # if data_prop.mode == SPLIT: # # Hack to make split/mulit-channel mode more clear # job_dict['data']['mode'] = 'SPLIT: Process each channel as a job' # and a dictionary of tasks: # add to our results - in most cases we will have just one job analysis_task . append ( job_dict ) for task in job . tasks : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # Deserialize message_data, and replace it in the task_result_dict task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ 'job_name' ] = job_name task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) del task_result_dict [ 'message_data' ] job_dict [ task . consumer_result_label ] = task_result_dict return json . dumps ( analysis_task , indent = indent ) to_json ( self , indent = None ) Generate the workflow as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/api/workflow.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" json_str_output = MessageToJson ( self . wd , preserving_proto_field_name = True ) json_output = json . loads ( json_str_output ) for element in json_output : if element == 'order' : for job in json_output [ element ]: # print(\"Job: {}\".format(job)) for job_def in job [ 'job_definition' ]: for task in job_def [ 'tasks' ]: task_type = task [ 'message_type' ] data = base64 . b64decode ( task [ 'message_data' ]) msg = self . _extract_serialized_message ( MessageType . Value ( task_type ), data ) task [ 'message_data' ] = json . loads ( MessageToJson ( msg , preserving_proto_field_name = True )) # print(\"Task: {}\".format(task)) if indent and indent < 0 : return json_output return json . dumps ( json_output , indent = indent ) WorkflowException ( Exception ) This exception means that an error occurred handling a Workflow","title":"`olivepy` `api` module"},{"location":"olivepy-docs/api.html#olivepy-api-module","text":"","title":"olivepy api module"},{"location":"olivepy-docs/api.html#olivepyapioliveclient","text":"","title":"olivepy.api.oliveclient"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.ClientBrokerWorker","text":"Performs async interactions with Olive","title":"ClientBrokerWorker"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.ClientBrokerWorker.run","text":"Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/oliveclient.py def run ( self ): logging . debug ( \"Starting Olive Status Monitor Worker for id: {} \" . format ( self . client_id )) self . working = True self . status_socket . subscribe ( \"\" ) poller = zmq . Poller () poller . register ( self . status_socket , zmq . POLLIN ) while self . working : # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . status_socket in socks : logging . debug ( \"Received status message from OLIVE...\" ) heatbeat_data = self . status_socket . recv () heatbeat = Heartbeat () heatbeat . ParseFromString ( heatbeat_data ) # do something with heartbeat... if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats logging . info ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) logging . info ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) logging . info ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) logging . info ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) logging . info ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) logging . info ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) logging . debug ( \"Number active jobs: \" + str ( stats . pool_busy )) logging . debug ( \"Number pending jobs: \" + str ( stats . pool_pending )) logging . debug ( \"Number finished jobs: \" + str ( stats . pool_finished )) logging . debug ( \"Max number jobs: \" + str ( stats . max_num_jobs )) logging . debug ( \"Server version: \" + str ( stats . server_version )) self . status_socket . close ()","title":"run()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient","text":"This is a simplified version of network library used to contact the Olive server via python code. All OLIVE calls below are synchronous, and block and until a response is received from the OLIVE server. These example API calls are intended to make working with the OLIVE API clearer since all calls are blocking. To make asynchronous requests to the OLIVE server use olivepy.api.olive_async_client.AsyncOliveClient for your enterprise application.","title":"OliveClient"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.__init__","text":"Parameters: Name Type Description Default client_id The unique name of this client. Due to a ZMQ bug on some platforms this ID can not end in '1' required address the address of the olive server, such as localhost 'localhost' request_port default olive port is 5588 5588 timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/oliveclient.py def __init__ ( self , client_id , address = 'localhost' , request_port = 5588 , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug on some platforms this ID can not end in '1' :param address: the address of the olive server, such as localhost :param request_port: default olive port is 5588 :param timeout_second: time in seconds, to wait for a response from the server \"\"\" self . client_id = client_id # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . server_request_port = request_port self . server_status_port = request_port + 1 self . timeout_seconds = timeout_second self . olive_connected = False self . info = self . fullobj = None OliveClient . setup_multithreading ()","title":"__init__()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.adapt_supervised","text":"Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required adapt_workspace a unique label for this client's adaptation required annotations_file_name the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. required Returns: Type Description the full path name of the new domain. Source code in olivepy/api/oliveclient.py def adapt_supervised ( self , plugin , domain , annotations_file_name , new_domain_name ): \"\"\" :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param adapt_workspace: a unique label for this client's adaptation :param annotations_file_name: the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. :return: the full path name of the new domain. \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] file_annotations = self . parse_annotation_file ( annotations_file_name ) for filename , regions in file_annotations . items (): audio_id = self . preprocess_supervised_audio ( plugin , domain , filename , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace )","title":"adapt_supervised()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.adapt_supervised_old","text":"Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required adapt_workspace a unique label for this client's adaptation required file_annotations a dictionary of files to preprocess, each file has one or more annotated regions for processing {filename: [(start_ms, end_ms, class)]}, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} required Returns: Type Description the full path name of the new domain. Source code in olivepy/api/oliveclient.py def adapt_supervised_old ( self , plugin , domain , file_annotations , new_domain_name ): \"\"\" :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param adapt_workspace: a unique label for this client's adaptation :param file_annotations: a dictionary of files to preprocess, each file has one or more annotated regions for processing {filename: [(start_ms, end_ms, class)]}, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} :return: the full path name of the new domain. \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] for filename , regions in file_annotations . items (): audio_id = self . preprocess_supervised_audio ( plugin , domain , filename , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace )","title":"adapt_supervised_old()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.analyze_bounding_box","text":"Request a analysis of 'filename' returning bounding box scores Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server <InputTransferType.PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. Source code in olivepy/api/oliveclient.py def analyze_bounding_box ( self , plugin , domain , filename , data_msg = None , mode = InputTransferType . PATH , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' returning bounding box scores :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. \"\"\" self . info = self . fullobj = None region_score_result = self . _request_bounding_box_scores ( plugin , domain , filename , data_msg = data_msg , mode = mode , opts = opts , classes = classes ) self . fullobj = region_score_result return region_score_result","title":"analyze_bounding_box()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.analyze_frames","text":"Request a analysis of 'filename' returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score. if None, then provide (audio) input as a required data_msg Optionally specify the data input as a fully formed Audio or BinaryMedia message instead of creating from filename None opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis as a list of (frame) scores Source code in olivepy/api/oliveclient.py def analyze_frames ( self , plugin , domain , filename , data_msg = None , opts = None , classes = None , mode = InputTransferType . PATH ): \"\"\" Request a analysis of 'filename' returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score. if None, then provide (audio) input as a :param data_msg: Optionally specify the data input as a fully formed Audio or BinaryMedia message instead of creating from filename :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis as a list of (frame) scores \"\"\" self . info = self . fullobj = None # todo actual def: def _request_frame_scores(self, plugin, domain, filename, data_msg=None, mode=AudioTransferType.AUDIO_PATH, opts=None, classes=None): frame_score_result = self . _request_frame_scores ( plugin , domain , filename , data_msg = data_msg , opts = opts , classes = classes , mode = mode ) if frame_score_result is not None : return frame_score_result . score return []","title":"analyze_frames()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.analyze_global","text":"Request a LID analysis of 'filename' Parameters: Name Type Description Default plugin the name of the LID plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the audio transfer mode <InputTransferType.PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis result as a list of (global) scores Source code in olivepy/api/oliveclient.py def analyze_global ( self , plugin , domain , filename , data_msg = None , mode = InputTransferType . PATH , opts = None , classes = None ): \"\"\" Request a LID analysis of 'filename' :param plugin: the name of the LID plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the audio transfer mode :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis result as a list of (global) scores \"\"\" self . info = self . fullobj = None request = GlobalScorerRequest () request . plugin = plugin request . domain = domain if data_msg : request . audio . CopyFrom ( data_msg ) else : audio = request . audio package_audio ( audio , filename , mode = mode ) self . _add_options ( request , opts ) self . _add_classes ( request , classes ) # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) _ , env = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a global score request message\" ) result = self . _sync_request ( env ) return result . score","title":"analyze_global()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.analyze_regions","text":"Request a analysis of 'filename' returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server <InputTransferType.PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. Source code in olivepy/api/oliveclient.py def analyze_regions ( self , plugin , domain , filename , data_msg = None , mode = InputTransferType . PATH , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. \"\"\" self . info = self . fullobj = None region_score_result = self . _request_region_scores ( plugin , domain , filename , data_msg = data_msg , mode = mode , opts = opts , classes = classes ) self . fullobj = region_score_result return region_score_result","title":"analyze_regions()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.analyze_text_transformation","text":"Request a analysis of 'filename' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server required opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis as a list of (region) scores Source code in olivepy/api/oliveclient.py def analyze_text_transformation ( self , plugin , domain , text_input , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis as a list of (region) scores \"\"\" request = TextTransformationRequest () request . plugin = plugin request . domain = domain request . text = text_input self . _add_options ( request , opts ) self . _add_classes ( request , classes ) # Wrap message in an Envelope _ , env = _wrap_message ( self . client_id , request ) # Now send the envelope logging . debug ( \"Sending a (text transform request) message\" ) result = self . _sync_request ( env ) results = [] for txtTransform in result . transformation : results . append ( txtTransform . transformed_text ) return results","title":"analyze_text_transformation()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.apply_threshold","text":"Very simple method to convert frame scores to regions. If speech regions are desired we can provide a SAD plugin that returns regions instead of frame scores Parameters: Name Type Description Default scores required threshold required rate required Returns: Type Description frame scores a regions Source code in olivepy/api/oliveclient.py def apply_threshold ( self , scores , threshold , rate ): \"\"\" Very simple method to convert frame scores to regions. If speech regions are desired we can provide a SAD plugin that returns regions instead of frame scores :param scores: :param threshold: :param rate: :return: frame scores a regions \"\"\" inSegment = False start = 0 segments = [] for i in range ( len ( scores )): if not inSegment and scores [ i ] >= threshold : inSegment = True start = i elif inSegment and ( scores [ i ] < threshold or i == len ( scores ) - 1 ): inSegment = False startT = (( 1.0 * start / rate )) endT = ( 1.0 * i / rate ) segments . append (( startT , endT )) return segments","title":"apply_threshold()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.audio_modification","text":"Do an audio modification (such as an enhansement). This function only accepts one audio and returns on modified audio. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required Returns: Type Description the analysis as a list of (frame) scores Source code in olivepy/api/oliveclient.py def audio_modification ( self , plugin , domain , filename , data_msg = None , mode = InputTransferType . PATH ): \"\"\" Do an audio modification (such as an enhansement). This function only accepts one audio and returns on modified audio. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :return: the analysis as a list of (frame) scores \"\"\" if mode != InputTransferType . PATH : raise Exception ( 'oliveclient.audio_modification requires an filename path and will not work with binary audio data.' ) request = AudioModificationRequest () request . plugin = plugin request . domain = domain request . requested_channels = 1 request . requested_rate = 8000 if data_msg : request . modifications . append ( data_msg ) else : audio = Audio () package_audio ( audio , filename , mode = mode ) # audio = Audio() # audio.path = filename request . modifications . append ( audio ) _ , env = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a audio modification/enhancement request message\" ) result = self . _sync_request ( env ) return result . successful , result . modification_result [ 0 ]","title":"audio_modification()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.connect","text":"Connect this client to the server Parameters: Name Type Description Default monitor_server if true, start a thread to monitor the server connection (helpful if debugging connection issues) False Source code in olivepy/api/oliveclient.py def connect ( self , monitor_server = False ): \"\"\" Connect this client to the server :param monitor_server: if true, start a thread to monitor the server connection (helpful if debugging connection issues) \"\"\" # init the request and status socket request_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_request_port ) status_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_status_port ) context = zmq . Context () self . request_socket = context . socket ( zmq . DEALER ) self . status_socket = context . socket ( zmq . SUB ) self . request_socket . connect ( request_addr ) self . status_socket . connect ( status_addr ) # logging.debug(\"Starting Olive status monitor...\") # Run this to get status about the server (helpful to confirm the server is connected and up) if ( monitor_server ): self . worker = ClientBrokerWorker ( self . status_socket , self . client_id ) self . worker . start () else : self . worker = None self . olive_connected = True logging . debug ( \"Olive client ready\" )","title":"connect()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.convert_preprocessed_annotations","text":"Convert the file annotations (a dictionary grouped by file ID, where annotations are grouped by file ID, which has one or more regions/classes) into class annotations (where annotations are grouped by class ID, with each class having one or more files, then each file having one or more regions). Parameters: Name Type Description Default processed_audio_list the list of files (indexed by an OLIVE generated ID) and the regions/classes annotated in that file required Returns: Type Description a dictionary of ClassAnnotation objects, indexed by class ID Source code in olivepy/api/oliveclient.py def convert_preprocessed_annotations ( self , processed_audio_list ): \"\"\" Convert the file annotations (a dictionary grouped by file ID, where annotations are grouped by file ID, which has one or more regions/classes) into class annotations (where annotations are grouped by class ID, with each class having one or more files, then each file having one or more regions). :param processed_audio_list: the list of files (indexed by an OLIVE generated ID) and the regions/classes annotated in that file :return: a dictionary of ClassAnnotation objects, indexed by class ID \"\"\" # Now convert the annotations that are grouped by file into a list of annotations grouped by class ID # (speech, non-speech). This is done in two passes, the first passes builds then new mapping of # class_id -->* audio_id -->* region, # then we convert this new data structure into ClassAnnotation (Protobuf) message(s) class_annots = {} for audio_id , regions in processed_audio_list : for region in regions : start = region [ 0 ] end = region [ 1 ] class_id = region [ 2 ] if class_id not in class_annots : class_annots [ class_id ] = {} if audio_id not in class_annots [ class_id ]: class_annots [ class_id ][ audio_id ] = [] class_annots [ class_id ][ audio_id ] . append (( start , end )) # now that the annotations have been grouped by class id, create the annotation protobuf(s) protobuf_class_annots = {} for class_id in class_annots . keys (): protobuf_class_annots [ class_id ] = ClassAnnotation () protobuf_class_annots [ class_id ] . class_id = class_id # Add AudioAnnotation(s) for audio_id in class_annots [ class_id ]: aa = AudioAnnotation () # aa = protobuf_class_annots[class_id].annotations.add() in python2.7? aa . audio_id = audio_id for region in class_annots [ class_id ][ audio_id ]: # times are in milliseconds ar = AnnotationRegion () # might need to do ar = aa.regions.add() for Python2.7 ar . start_t = region [ 0 ] ar . end_t = region [ 1 ] aa . regions . append ( ar ) protobuf_class_annots [ class_id ] . annotations . append ( aa ) return protobuf_class_annots","title":"convert_preprocessed_annotations()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.enroll","text":"Request a enrollment of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required filename the filename to add as an audio only enrollment addition required data_msg an BinaryMedia message to add as an enrollment addition None Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def enroll ( self , plugin , domain , class_id , filename , data_msg = None ): \"\"\" Request a enrollment of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :param filename: the filename to add as an audio only enrollment addition :param data_msg: an BinaryMedia message to add as an enrollment addition :return: True if enrollment successful \"\"\" self . info = self . fullobj = None enrollment = ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True if data_msg : if isinstance ( data_msg , Audio ): enrollment . addition . append ( data_msg ) else : enrollment . addition_media . append ( data_msg ) else : audio = Audio () package_audio ( audio , filename ) enrollment . addition . append ( audio ) # Wrap message in an Envelope _ , env = _wrap_message ( self . client_id , enrollment ) # Now send the envelope logging . debug ( \"Sending an enrollment message\" ) result = self . _sync_request ( env ) return result # ClassModificationResult # Wrap message in an Envelope # request = self._wrap_message(enrollment) # # Now send the message # logging.debug(\"Sending a class modification request (enrollment) message\") # self.request_socket.send(request.SerializeToString()) # logging.debug(\"Sending a class modification request (enrollment) message\") # # TODO THIS IS A SYNC REQUST, CAN BE DONE ASYN WITH A CALLBACK... # # Wait for the response from the server # # logging.info(\"checking for response\") # protobuf_data = self.request_socket.recv() # logging.info(\"Received message from server...\") # envelope = Envelope() # envelope.ParseFromString(protobuf_data) # # # for this use case the server will only have one response in the evevelope: # for i in range(len(envelope.message)): # olive_msg = envelope.message[i] # # if olive_msg.HasField(\"info\"): # self.info = olive_msg.info # if olive_msg.HasField(\"error\"): # raise ExceptionFromServer('Got an error from the server: ' + olive_msg.error) # else: # enrollment_msg = ClassModificationResult() # enrollment_msg.ParseFromString(olive_msg.message_data[0]) # # # Assume there is only one result set (for 'speech'): frame_score_msg.result[0] # # TODO - clean up return. Maybe do something with message. # self.fullobj = enrollment_msg # self.info = enrollment_msg.addition_result[0].message # CLG this would only be set if there was an issue with the enrollment # return enrollment_msg.addition_result[0].successful # # return False # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate)","title":"enroll()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.enroll_batch","text":"Request a batch enrollment of 'audios' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required data_msgs list of BinaryMedia message to add as an enrollment addition required Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def enroll_batch ( self , plugin , domain , class_id , data_msgs ): \"\"\" Request a batch enrollment of 'audios' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :param data_msgs: list of BinaryMedia message to add as an enrollment addition :return: True if enrollment successful \"\"\" self . info = self . fullobj = None enrollment = ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True if data_msgs and len ( data_msgs ) > 0 : for data_msg in data_msgs : if isinstance ( data_msg , Audio ): enrollment . addition . append ( data_msg ) else : enrollment . addition_media . append ( data_msg ) _ , env = _wrap_message ( self . client_id , enrollment ) logging . debug ( \"Sending a batch enrollment message\" ) result = self . _sync_request ( env ) return result # ClassModificationResult","title":"enroll_batch()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.finalize_supervised_adaptation","text":"Complete the adaptation Parameters: Name Type Description Default plugin the name of the plugin to adapt required domain the name of the plugin domain to adapt required new_domain_name the name of the new domain that is created within the plugin required class_annotations the audio annotations, grouped by class ID required Returns: Type Description the name of the new domain Source code in olivepy/api/oliveclient.py def finalize_supervised_adaptation ( self , plugin , domain , new_domain_name , class_annotations , adapt_workspace ): \"\"\" Complete the adaptation :param plugin: the name of the plugin to adapt :param domain: the name of the plugin domain to adapt :param new_domain_name: the name of the new domain that is created within the plugin :param class_annotations: the audio annotations, grouped by class ID :return: the name of the new domain \"\"\" self . info = self . fullobj = None request = SupervisedAdaptationRequest () request . plugin = plugin request . domain = domain request . adapt_space = adapt_workspace request . new_domain = new_domain_name # Add the class annotations for class_id in class_annotations : request . class_annotations . append ( class_annotations [ class_id ]) # request.class_annotations.extend([class_annotations[class_id]]) for Python2.7? # package the request _ , request = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a finalize adatation message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message - boiler plate code, this can be simplified envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): olive_msg = envelope . message [ i ] if olive_msg . HasField ( \"info\" ): self . info = olive_msg . info if olive_msg . HasField ( \"error\" ): raise ExceptionFromServer ( 'Got an error from the server: ' + olive_msg . error ) else : result_msg = SupervisedAdaptationResult () result_msg . ParseFromString ( olive_msg . message_data [ 0 ]) # get the new domain #if hasattr(result_msg, 'new_domain') and result_msg.new_domain is not None: # print(\"Adaptation successfully created new domain: '{}'\".format(result_msg.new_domain)) self . fullobj = result_msg return result_msg . new_domain # adapt failed... TODO: thrown exception instead? return None","title":"finalize_supervised_adaptation()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.get_fullobj","text":"This object should be used for debugging only. Example use::success = client.enroll('sid-embed-v5-py3', 'multilang-v1', 'joshua', 'file') if troubleshooting: fullobj = client.get_fullobj() print('Whole object returned from server: '+str(fullobj)) Returns: Type Description the full object returned from the last call to the server. Source code in olivepy/api/oliveclient.py def get_fullobj ( self ): \"\"\" This object should be used for debugging only. Example use::success = client.enroll('sid-embed-v5-py3', 'multilang-v1', 'joshua', 'file') \\ if troubleshooting: fullobj = client.get_fullobj() print('Whole object returned from server: '+str(fullobj)) :return: the full object returned from the last call to the server. \"\"\" return self . fullobj","title":"get_fullobj()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.get_info","text":"Returns: Type Description the info data from the last call to the server. Will return None if the last call did not return any info. Source code in olivepy/api/oliveclient.py def get_info ( self ): \"\"\" :return: the info data from the last call to the server. Will return None if the last call did not return any info. \"\"\" return self . info","title":"get_info()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.parse_annotation_file","text":"Parse a file for the names of files of audio files and their regions to use for adaptation. Parameters: Name Type Description Default filename the path and name of the file that contains the input. This file must have one or more lines having 4 columns: # filename, class, start_region_ms, end_region_ms required Returns: Type Description the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} Source code in olivepy/api/oliveclient.py def parse_annotation_file ( self , filename ): \"\"\" Parse a file for the names of files of audio files and their regions to use for adaptation. :param filename: the path and name of the file that contains the input. This file must have one or more lines having 4 columns: # filename, class, start_region_ms, end_region_ms :return: the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} \"\"\" data_lines = [] file_annotations = {} if not os . path . exists ( filename ): raise Exception ( \"The annotation file ' {} ' does not exist\" . format ( filename )) with open ( filename ) as f : data_lines . extend ([ line . strip () for line in f . readlines ()]) # process the file for line in data_lines : pieces = line . split () if len ( pieces ) != 4 : raise Exception ( \"The annotation file does not contain data in the correct format, found line ' {} '\" . format ( line )) adapt_audio_path = pieces [ 0 ] # assume a relative file is used, so the full path must be specified since being sent to server # This is being sent to server. If full path is given, do nothing. Otherwise make absolute. # TODO: this will not work from UNIX to Windows or other way around. # TODO: should use Python's abspath here, don't you think? if adapt_audio_path [ 0 ] != '/' and adapt_audio_path [ 1 ] != ':' : adapt_audio_path = os . path . join ( os . getcwd (), adapt_audio_path ) # todo validate file is valid... if adapt_audio_path not in file_annotations : file_annotations [ adapt_audio_path ] = [] class_id = pieces [ 1 ] start = float ( pieces [ 2 ]) end = float ( pieces [ 3 ]) file_annotations [ adapt_audio_path ] . append (( start , end , class_id )) return file_annotations","title":"parse_annotation_file()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.preprocess_supervised_audio","text":"Submit audio for pre-processing phase of adaptation. Parameters: Name Type Description Default plugin the name of the plugin to adapt required domain the name of the plugin domain to adapt required filename the name of the audio file to submit to the server/plugin/domain for preprocessing required Returns: Type Description the unique id generated by the server for the preprocess audio, which must be used Source code in olivepy/api/oliveclient.py def preprocess_supervised_audio ( self , plugin , domain , filename , adapt_workspace ): \"\"\" Submit audio for pre-processing phase of adaptation. :param plugin: the name of the plugin to adapt :param domain: the name of the plugin domain to adapt :param filename: the name of the audio file to submit to the server/plugin/domain for preprocessing :return: the unique id generated by the server for the preprocess audio, which must be used \"\"\" # [(2.618, 6.2, 'S'), (7.2, 9.5, 'NS')] self . info = self . fullobj = None request = PreprocessAudioAdaptRequest () request . plugin = plugin request . domain = domain request . adapt_space = adapt_workspace request . class_id = \"supervised\" # HACK: for supervised validation in the backend - we will fix this in a future release so not needed # we currently don't need to set annotations (start_t, end_t) when doing pre-processing # finally, set the audio: audio = request . audio # send the name of the file to the server: audio . path = filename # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) # TODO SERIALIZE EXAMPLE... # package the request _ , request = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a preprocess audio (for adaptation) message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): olive_msg = envelope . message [ i ] if olive_msg . HasField ( \"info\" ): self . info = olive_msg . info if olive_msg . HasField ( \"error\" ): raise ExceptionFromServer ( 'Got an error from the server: ' + olive_msg . error ) else : result_msg = PreprocessAudioAdaptResult () result_msg . ParseFromString ( olive_msg . message_data [ 0 ]) # get audio id from results, use for final annotations... # print(\"Preprocess audio ID {} having duration {}\".format(result_msg.audio_id, result_msg.duration)) self . fullobj = result_msg return result_msg . audio_id # preprocessing failed... TODO: thrown exception instead? return None","title":"preprocess_supervised_audio()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.requst_sad_adaptation","text":"Example of performing SAD adaptation Returns: Type Description Source code in olivepy/api/oliveclient.py def requst_sad_adaptation ( self ): \"\"\" Example of performing SAD adaptation :return: \"\"\" # todo move to client example (i.e. olivelearn) # using Julie's sadRegression dataset... # Assume the working directory is root directory for the SAD regression tests # Setup processing variables (get this config or via command line optons plugin = \"sad-dnn-v6a\" domain = \"multi-v1\" new_domain_name = \"python_adapted_multi-v2\" # Build the list of files plus the regions in the those files to adaptn by parsing the input file: file_annotations = self . parse_annotation_file ( \"lists/adapt_ms.lst\" ) return self . adapt_supervised_old ( plugin , domain , file_annotations , new_domain_name )","title":"requst_sad_adaptation()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.setup_multithreading","text":"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/oliveclient.py @classmethod def setup_multithreading ( cls ): \"\"\"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. \"\"\" # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL )","title":"setup_multithreading()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.unenroll","text":"Unenrollment the class_id Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def unenroll ( self , plugin , domain , class_id ): \"\"\" Unenrollment the class_id :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :return: True if enrollment successful \"\"\" self . info = self . fullobj = None removal = ClassRemovalRequest () removal . plugin = plugin removal . domain = domain removal . class_id = class_id # Wrap message in an Envelope _ , request = _wrap_message ( self . client_id , removal ) logging . debug ( \"Sending a class modification request (removal) message\" ) result = self . _sync_request ( request ) # do something? return True","title":"unenroll()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.get_bit_depth","text":"Not using since not assuming numpy is available... Source code in olivepy/api/oliveclient.py def get_bit_depth ( audio ): \"\"\"Not using since not assuming numpy is available...\"\"\" # Numpy is needed to support this... dt = audio . dtype if dt == np . int8 : return BIT_DEPTH_8 elif dt == np . int16 : return BIT_DEPTH_16 elif dt == np . int32 : return BIT_DEPTH_24 else : return BIT_DEPTH_32","title":"get_bit_depth()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.package_buffer_audio","text":"Helper function to wrap audio data (decoded samples) into a AudioBuffer message that can submitted to the server instead of a file name. Parameters: Name Type Description Default data the data as a numpy ndarray required num_samples the number of samples required sample_rate the audio sample rate 8000 num_channels the number of channels in the audio 1 Returns: Type Description Source code in olivepy/api/oliveclient.py def package_buffer_audio ( audio , data , num_samples , sample_rate = 8000 , num_channels = 1 ): \"\"\" Helper function to wrap audio data (decoded samples) into a AudioBuffer message that can submitted to the server instead of a file name. :param data: the data as a numpy ndarray :param num_samples: the number of samples :param sample_rate: the audio sample rate :param num_channels: the number of channels in the audio :return: \"\"\" # from scipy.io import wavfile # sample_rate, data = wavfile.read('somefilename.wav') buffer = audio . audioSamples buffer . channels = num_channels buffer . samples = num_samples #data.shape[0] buffer . rate = sample_rate buffer . bit_depth = get_bit_depth ( data ) buffer . data = data . tostring () return audio","title":"package_buffer_audio()"},{"location":"olivepy-docs/api.html#olivepyapiolive_async_client","text":"","title":"olivepy.api.olive_async_client"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient","text":"This class is used to make asynchronous requests to the OLIVE server","title":"AsyncOliveClient"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.__init__","text":"Parameters: Name Type Description Default client_id str The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems required address the address of the olive server, such as localhost 'localhost' request_port default olive port is 5588 5588 timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/olive_async_client.py def __init__ ( self , client_id : str , address = 'localhost' , request_port = 5588 , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems :param address: the address of the olive server, such as localhost :param request_port: default olive port is 5588 :param timeout_second: time in seconds, to wait for a response from the server \"\"\" threading . Thread . __init__ ( self ) self . client_id = client_id self . secure = False # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . server_request_port = request_port self . server_status_port = request_port + 1 self . timeout_seconds = timeout_second self . request_queue = queue . Queue () # special queue used to emulate blocking requests # self.completed_sync_request_queue = queue.Queue() self . sync_message = {} self . response_queue = {} self . working = False self . request_socket = None self . status_socket = None # thread to monitor OLIVE server heartbeats self . worker = None self . heartbeat_callback = None self . active_ids = {} # self.status_socket = context.socket(zmq.SUB) self . olive_connected = False self . monitor_status = False self . daemon = True self . last_status = None oc . OliveClient . setup_multithreading ()","title":"__init__()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.add_heartbeat_listener","text":"Register a callback function to be notified when a heartbeat is received from the OLIVE server Parameters: Name Type Description Default heartbeat_callback Callable[[olive_pb2.Heartbeat], NoneType] The callback method that is notified each time a heartbeat message is received from the OLIVE server required Source code in olivepy/api/olive_async_client.py def add_heartbeat_listener ( self , heartbeat_callback : Callable [[ olive_pb2 . Heartbeat ], None ]): \"\"\" Register a callback function to be notified when a heartbeat is received from the OLIVE server :param heartbeat_callback: The callback method that is notified each time a heartbeat message is received \\ from the OLIVE server \"\"\" self . heartbeat_callback = heartbeat_callback if self . worker : self . worker . add_event_callback ( heartbeat_callback ) self . heartbeat_callback = heartbeat_callback","title":"add_heartbeat_listener()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_bounding_box","text":"Request a analysis of 'filename', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified Source code in olivepy/api/olive_async_client.py def analyze_bounding_box ( self , plugin , domain , data_msg , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified \"\"\" request = olive_pb2 . BoundingBoxScorerRequest () request . plugin = plugin request . domain = domain request . data . CopyFrom ( data_msg ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id )","title":"analyze_bounding_box()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_composite","text":"Request a analysis of 'filename', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required inputs the data input required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified Source code in olivepy/api/olive_async_client.py def analyze_composite ( self , plugin , domain , inputs , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param inputs: the data input :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified \"\"\" request = olive_pb2 . CompositeScorerRequest () request . plugin = plugin request . domain = domain label = 0 for input in inputs : scorer_input = olive_pb2 . CompositeScorerInput () scorer_input . data_id = str ( label ) scorer_input . data . CopyFrom ( input ) request . input . append ( scorer_input ) label = label + 1 if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id )","title":"analyze_composite()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_frames","text":"Request a analysis of 'filename', returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the Audio message to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode required opts dict a dictionary of name/value pair options for this plugin request None Returns: Type Description a OliveServerResponse containing the status of the request (FrameScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_frames ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the Audio message to score :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :param opts: a dictionary of name/value pair options for this plugin request :return: a OliveServerResponse containing the status of the request (FrameScorerResult) \"\"\" request = olive_pb2 . FrameScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio_input ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id )","title":"analyze_frames()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_global","text":"Request a global score analysis of 'filename' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio the Audio message to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (GlobalScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_global ( self , plugin , domain , audio , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a global score analysis of 'filename' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio: the Audio message to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (GlobalScorerResult) \"\"\" self . info = self . fullobj = None request = olive_pb2 . GlobalScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id )","title":"analyze_global()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_global_stateless","text":"Request a stateless global score analysis of audio Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio the Audio message to score required class_vectors dictionary of class_id to vector required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (GlobalScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_global_stateless ( self , plugin , domain , audio , class_vectors , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , msg_id = None ): \"\"\" Request a stateless global score analysis of audio :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio: the Audio message to score :param class_vectors: dictionary of class_id to vector :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (GlobalScorerResult) \"\"\" self . info = self . fullobj = None request = olive_pb2 . StatelessGlobalScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) for class_id in class_vectors : sv = olive_pb2 . ClassVector () sv . class_id = class_id for av in class_vectors [ class_id ]: # MessageToDict from google.protobuf.json_format import ParseDict # from google.protobuf.json_format import DictToMessage audio_vector = olive_pb2 . AudioVector () ParseDict ( av [ 'audioVector' ], audio_vector , ignore_unknown_fields = True ) #olive_pb2.AudioVector() # audio_vector.plugin_id = av.plugin_id # audio_vector.domain = av.domain # print(audio_vector) sv . vector . append ( audio_vector ) request . class_vector . append ( sv ) # print(request) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id )","title":"analyze_global_stateless()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_regions","text":"Request a analysis of 'filename', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified Source code in olivepy/api/olive_async_client.py def analyze_regions ( self , plugin , domain , audio , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified \"\"\" request = olive_pb2 . RegionScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id )","title":"analyze_regions()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_regions_stateless","text":"Request a analysis of 'audio', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio the name of the audio file to score required class_vectors dictionary of class_id to vector required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified Source code in olivepy/api/olive_async_client.py def analyze_regions_stateless ( self , plugin , domain , audio , class_vectors , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , msg_id = None ): \"\"\" Request a analysis of 'audio', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio: the name of the audio file to score :param class_vectors: dictionary of class_id to vector :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RegionScorerResult) if no callback is specified \"\"\" request = olive_pb2 . StatelessRegionScorerRequest () request . plugin = plugin request . domain = domain request . audio . CopyFrom ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) for class_id in class_vectors : sv = olive_pb2 . ClassVector () sv . class_id = class_id for av in class_vectors [ class_id ]: # MessageToDict from google.protobuf.json_format import ParseDict # from google.protobuf.json_format import DictToMessage audio_vector = olive_pb2 . AudioVector () ParseDict ( av [ 'audioVector' ], audio_vector , ignore_unknown_fields = True ) #olive_pb2.AudioVector() # audio_vector.plugin_id = av.plugin_id # audio_vector.domain = av.domain # print(audio_vector) sv . vector . append ( audio_vector ) request . class_vector . append ( sv ) # print(request) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id )","title":"analyze_regions_stateless()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_text","text":"Request a analysis of 'filename', returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required text_input the text to transfrom required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required opts a dictionary of name/value pair options for this plugin request None Returns: Type Description a OliveServerResponse containing the status of the request (FrameScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_text ( self , plugin , domain , text_input , callback : Callable [[ response . OliveServerResponse ], None ], opts = None , classes = None , msg_id = None ): \"\"\" Request a analysis of 'filename', returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param text_input: the text to transfrom :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :param opts: a dictionary of name/value pair options for this plugin request :return: a OliveServerResponse containing the status of the request (FrameScorerResult) \"\"\" request = olive_pb2 . TextTransformationRequest () request . plugin = plugin request . domain = domain request . text = text_input if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) self . _add_classes ( request , classes ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id )","title":"analyze_text()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.audio_modification","text":"Used to make a AudioModificationRequest (enhancement). Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the audio path or buffer to submit for modification required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode required Returns: Type Description a OliveServerResponse containing the status of the request (AudioModificationResult) Source code in olivepy/api/olive_async_client.py def audio_modification ( self , plugin , domain , audio , callback : Callable [[ response . OliveServerResponse ], None ], opts : dict = None , requested_channel = 1 , requested_sample_rate = 8000 , msg_id = None ): \"\"\" Used to make a AudioModificationRequest (enhancement). :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the audio path or buffer to submit for modification :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (AudioModificationResult) \"\"\" # if mode != olivepy.messaging.msgutil.AudioTransferType.AUDIO_PATH: # raise Exception('oliveclient.audio_modification requires an filename path and will not work with binary audio data.') request = olive_pb2 . AudioModificationRequest () request . plugin = plugin request . domain = domain request . requested_channels = requested_channel request . requested_rate = requested_sample_rate request . modifications . append ( audio ) if opts : # convert our option dict to an OptionValue list jopts = utils . parse_json_options ( opts ) request . option . extend ( jopts ) if callback : self . enqueue_request ( request , callback , msg_id = msg_id ) else : return self . sync_request ( request , msg_id = msg_id ) # def request_stream(self, client_id, workflow_definition, sample_rate): # \"\"\" # Used to make a AudioModificationRequest (enhancement). This call is blocking, waits for a server response # then returning the StartStreamingResult message (fixme return port number or throw exception if bad request) # # :param client_id: the unique name of this client # :param workflow_definition: the streaming workflow definition # :param sample_rate: the sample rate of the audio to be streamed # # :return: a OliveServerResponse containing the status of the request (AudioModificationResult) # \"\"\" # # request = stream_pb2.StartStreamingRequest() # request.client_stream_id = client_id # request.sampleRate = sample_rate # request.workflow_definition.CopyFrom(workflow_definition) # # #todo respose is a # return self.sync_request(request) # # # if callback: # self.enqueue_request(request, callback) # else: # return self.sync_request(request)","title":"audio_modification()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.clear_heartbeat_listeners","text":"Remove all heartbeat listeners Source code in olivepy/api/olive_async_client.py def clear_heartbeat_listeners ( self ): \"\"\" Remove all heartbeat listeners \"\"\" if self . worker : self . worker . clear_callback ()","title":"clear_heartbeat_listeners()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.connect","text":"Connect this client to the server Parameters: Name Type Description Default monitor_server if true, starts a thread to monitor the server status connection for heartbeat messages required Source code in olivepy/api/olive_async_client.py def connect ( self , monitor_status = True ): \"\"\" Connect this client to the server :param monitor_server: if true, starts a thread to monitor the server status connection for heartbeat messages \"\"\" # logging.debug(\"Starting Olive async monitor...\") self . monitor_status = monitor_status self . connection_done = threading . Event () self . start () # block until connected self . olive_connected = True self . connection_done . wait () self . last_status = time . time () logging . debug ( \"Olive async client ready\" )","title":"connect()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.disconnect","text":"Closes the connection to the OLIVE server Source code in olivepy/api/olive_async_client.py def disconnect ( self ): \"\"\" Closes the connection to the OLIVE server \"\"\" if self . worker : self . worker . stopWorker () self . working = False self . olive_connected = False self . join () if self . request_socket : self . request_socket . close ()","title":"disconnect()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.enqueue_request","text":"Add a message request to the outbound queue Parameters: Name Type Description Default message the request message to send required callback this is called when response message is received from the server required wrapper the message wrapper None Source code in olivepy/api/olive_async_client.py def enqueue_request ( self , message , callback , wrapper = None , msg_id = None ): \"\"\" Add a message request to the outbound queue :param message: the request message to send :param callback: this is called when response message is received from the server :param wrapper: the message wrapper \"\"\" if wrapper is None : wrapper = response . OliveServerResponse () self . request_queue . put (( message , callback , wrapper , msg_id ))","title":"enqueue_request()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.enroll","text":"Request a enrollment of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to enroll required audio_input the Audio message (or list of messages) to add as an enrollment addition required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (ClassModificationResult) Source code in olivepy/api/olive_async_client.py def enroll ( self , plugin , domain , class_id , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED , msg_id = None ): \"\"\" Request a enrollment of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to enroll :param audio_input: the Audio message (or list of messages) to add as an enrollment addition :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (ClassModificationResult) \"\"\" enrollment = olive_pb2 . ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True if not isinstance ( audio_input , list ): audio_input = [ audio_input ] for audio_msg in audio_input : if isinstance ( audio_msg , olive_pb2 . BinaryMedia ): enrollment . addition_media . append ( audio_msg ) elif not isinstance ( audio_msg , olive_pb2 . Audio ): audio = olive_pb2 . Audio () olivepy . messaging . msgutil . package_audio ( audio , audio_msg , mode = mode ) enrollment . addition . append ( audio ) else : audio = audio_msg enrollment . addition . append ( audio ) if callback : self . enqueue_request ( enrollment , callback , msg_id = msg_id ) else : return self . sync_request ( enrollment , msg_id = msg_id )","title":"enroll()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.get_active","text":"Used to make a GetActiveRequest Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing the status of the request (GetActiveResult) Source code in olivepy/api/olive_async_client.py def get_active ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetActiveRequest :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (GetActiveResult) \"\"\" request = olive_pb2 . GetActiveRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"get_active()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.get_status","text":"Used to make a GetStatusRequest and receive a GetStatusResult Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse that contains the most recent server status (GetStatusResult) Source code in olivepy/api/olive_async_client.py def get_status ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetStatusRequest and receive a GetStatusResult :param callback: optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse that contains the most recent server status (GetStatusResult) \"\"\" request = olive_pb2 . GetStatusRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"get_status()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.get_update_status","text":"Used to make a GetUpdateStatusRequest Parameters: Name Type Description Default plugin the name of the plugin to query required domain the name of the domain to query required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult Source code in olivepy/api/olive_async_client.py def get_update_status ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetUpdateStatusRequest :param plugin: the name of the plugin to query :param domain: the name of the domain to query :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult \"\"\" request = olive_pb2 . GetUpdateStatusRequest () request . plugin = plugin request . domain = domain if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"get_update_status()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.is_connected","text":"Status of the connection to the OLIVE server Returns: Type Description True if connected Source code in olivepy/api/olive_async_client.py def is_connected ( self ): \"\"\" Status of the connection to the OLIVE server :return: True if connected \"\"\" return self . olive_connected","title":"is_connected()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.load_plugin_domain","text":"Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest message) Parameters: Name Type Description Default plugin the name of the plugin to pre-load required domain the name of hte domain to pre-load required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the update status of the request (LoadPluginDomainResult) Source code in olivepy/api/olive_async_client.py def load_plugin_domain ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest message) :param plugin: the name of the plugin to pre-load :param domain: the name of hte domain to pre-load :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the update status of the request (LoadPluginDomainResult) \"\"\" request = olive_pb2 . LoadPluginDomainRequest () request . plugin = plugin request . domain = domain if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"load_plugin_domain()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.request_drain_stream","text":"Used to send a drain request to the specified streaming session Parameters: Name Type Description Default session_id the ID of the session to flush required Returns: Type Description True if the session was flushed Source code in olivepy/api/olive_async_client.py def request_drain_stream ( self , session_id ): \"\"\" Used to send a drain request to the specified streaming session :param session_id: the ID of the session to flush :return: True if the session was flushed \"\"\" request = stream_pb2 . DrainStreamingRequest () request . session_id = session_id response = self . sync_request ( request ) return response . get_response () . successful","title":"request_drain_stream()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.request_flush_stream","text":"Used to send a flush request to the specified streaming session Parameters: Name Type Description Default session_id the ID of the session to flush required Returns: Type Description True if the session was flushed Source code in olivepy/api/olive_async_client.py def request_flush_stream ( self , session_id ): \"\"\" Used to send a flush request to the specified streaming session :param session_id: the ID of the session to flush :return: True if the session was flushed \"\"\" request = stream_pb2 . FlushStreamingRequest () request . session_id = session_id response = self . sync_request ( request ) return response . get_response () . successful","title":"request_flush_stream()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.request_plugins","text":"Used to make a PluginDirectoryRequest Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None include_protected (Optional) Will include \"protected\" plugins in the response. Defaults to false. False Returns: Type Description a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult) Source code in olivepy/api/olive_async_client.py def request_plugins ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None , include_protected = False ): \"\"\" Used to make a PluginDirectoryRequest :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :param include_protected: (Optional) Will include \"protected\" plugins in the response. Defaults to false. :return: a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult) \"\"\" request = olive_pb2 . PluginDirectoryRequest () if include_protected : request . include_protected = True if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"request_plugins()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.request_stop_stream","text":"Stop a streaming session. Parameters: Name Type Description Default session_id The streaming session ID to stop. If a value of None is passed, then request that all active streaming sessions be stopped required Returns: Type Description True if the request was received by the server Source code in olivepy/api/olive_async_client.py def request_stop_stream ( self , session_id ): \"\"\" Stop a streaming session. :param session_id: The streaming session ID to stop. If a value of None is passed, then request that all active streaming sessions be stopped :return: True if the request was received by the server \"\"\" request = stream_pb2 . StopStreamingRequest () if session_id : request . session_id = session_id #todo respose is a? self . sync_request ( request ) return True","title":"request_stop_stream()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.run","text":"Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/olive_async_client.py def run ( self ): self . run_zmq () if not self . secure else self . run_wss ()","title":"run()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.run_zmq","text":"Starts the thread to handle async messages Source code in olivepy/api/olive_async_client.py def run_zmq ( self ): \"\"\" Starts the thread to handle async messages \"\"\" try : logging . debug ( \"Starting OLIVE Async Message Worker for id: {} \" . format ( self . client_id )) context = zmq . Context () self . request_socket = context . socket ( zmq . DEALER ) # init the request and status socket request_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_request_port ) status_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_status_port ) self . request_socket . connect ( request_addr ) if self . monitor_status : self . status_socket = context . socket ( zmq . SUB ) self . status_socket . connect ( status_addr ) self . worker = ClientMonitorThread ( self . status_socket , self . client_id , self . monitor_status ) self . worker . start () self . working = True poller = zmq . Poller () poller . register ( self . request_socket , zmq . POLLIN ) except Exception as e : logging . error ( \"Error connecting to the OLIVE server: {} \" . format ( e )) self . olive_connected = False finally : self . connection_done . set () last_active_request_query = 0 while self . working : # First, send any client requests while not self . request_queue . empty (): request_msg , cb , wrapper , msg_id = self . request_queue . get () msg_id , env = msgutil . _wrap_message ( self . client_id , request_msg , msg_id = msg_id ) # Add to our callback queue self . response_queue [ msg_id ] = ( request_msg , cb , wrapper ) # Now send the message logging . debug ( \"Sending client request msg type: {} \" . format ( env . message [ 0 ] . message_type )) self . request_socket . send ( env . SerializeToString ()) # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . request_socket in socks : # logging.info(\"Received message from OLIVE...\") protobuf_data = self . request_socket . recv () envelope = olive_pb2 . Envelope () envelope . ParseFromString ( protobuf_data ) for i in range ( len ( envelope . message )): self . _process_response ( envelope . message [ i ]) if time . time () - last_active_request_query > ACTIVE_REQUEST_SECONDS : logging . debug ( \"Updating status...\" ) # self._issue_active_status() last_active_request_query = time . time () poller . unregister ( self . request_socket ) self . request_socket . close ()","title":"run_zmq()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.setup_multithreading","text":"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/olive_async_client.py @classmethod def setup_multithreading ( cls ): '''This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. ''' # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL )","title":"setup_multithreading()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.sync_adapt_supervised","text":"Supervised adaptation. Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required annotations_file_name the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. required new_domain_name the name of the new domain required mode (Optional) the audio transfer mode. Defaults to InputTransferType.SERIALIZED <InputTransferType.SERIALIZED: 3> Returns: Type Description the full path name of the new domain Source code in olivepy/api/olive_async_client.py def sync_adapt_supervised ( self , plugin , domain , annotations_file_name , new_domain_name , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED ): \"\"\" Supervised adaptation. :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param annotations_file_name: the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. :param new_domain_name: the name of the new domain :param mode: (Optional) the audio transfer mode. Defaults to InputTransferType.SERIALIZED :return: the full path name of the new domain \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] file_annotations = utils . parse_annotation_file ( annotations_file_name ) for filename , regions in file_annotations . items (): audio = msgutil . package_audio ( olive_pb2 . Audio (), filename , mode = mode ) audio_id = self . _sync_preprocess_supervised_audio ( plugin , domain , audio , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . _sync_convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . _sync_finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace )","title":"sync_adapt_supervised()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.sync_request","text":"Send a request to the OLIVE server, but wait for a response from the server Parameters: Name Type Description Default message the request message to send to the OLIVE server required Returns: Type Description the response from the server Source code in olivepy/api/olive_async_client.py def sync_request ( self , message , wrapper = None , msg_id = None ): \"\"\" Send a request to the OLIVE server, but wait for a response from the server :param message: the request message to send to the OLIVE server :return: the response from the server \"\"\" if wrapper is None : wrapper = response . OliveServerResponse () # create an ID for this sync_request sync_id = msgutil . get_uuid () result_available = threading . Event () cb = lambda response : self . _sync_callback ( response , sync_id , result_available ) self . enqueue_request ( message , cb , wrapper , msg_id ) result_available . wait () # get the result if sync_id in self . sync_message : return self . sync_message . pop ( sync_id ) else : # unexpected.... callback event completed with no result raise Exception ( \"Error waiting for a response from the server\" )","title":"sync_request()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.unenroll","text":"Unenroll class_id Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to remove required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing the status of the request (ClassRemovalResult) Source code in olivepy/api/olive_async_client.py def unenroll ( self , plugin , domain , class_id , callback : Callable [[ response . OliveServerResponse ], None ] = None , msg_id = None ): \"\"\" Unenroll class_id :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to remove :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (ClassRemovalResult) \"\"\" removal = olive_pb2 . ClassRemovalRequest () removal . plugin = plugin removal . domain = domain removal . class_id = class_id if callback : self . enqueue_request ( removal , callback , msg_id = msg_id ) else : return self . sync_request ( removal , msg_id = msg_id )","title":"unenroll()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.unload_plugin_domain","text":"Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded plugin from server memory) Parameters: Name Type Description Default plugin the name of the plugin to unload required domain the name of hte domain to unload required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RemovePluginDomainResult) Source code in olivepy/api/olive_async_client.py def unload_plugin_domain ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded \\ plugin from server memory) :param plugin: the name of the plugin to unload :param domain: the name of hte domain to unload :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RemovePluginDomainResult) \"\"\" request = olive_pb2 . RemovePluginDomainRequest () request . plugin = plugin . strip () request . domain = domain . strip () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"unload_plugin_domain()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.update_plugin_domain","text":"Used to make a ApplyUpdateRequest Parameters: Name Type Description Default plugin the name of the plugin to update required domain the name of hte domain to update required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (ApplyUpdateResult) Source code in olivepy/api/olive_async_client.py def update_plugin_domain ( self , plugin , domain , metadata , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a ApplyUpdateRequest :param plugin: the name of the plugin to update :param domain: the name of hte domain to update :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (ApplyUpdateResult) \"\"\" request = olive_pb2 . ApplyUpdateRequest () request . plugin = plugin request . domain = domain mds = request . params for key , item in metadata : md = olive_pb2 . Metadata () md . name = key if isinstance ( item , str ): md . type = 1 elif isinstance ( item , int ): md . type = 2 elif isinstance ( item , float ): md . type = 3 elif isinstance ( item , bool ): md . type = 4 elif isinstance ( item , list ): md . type = 5 else : raise Exception ( 'Metadata {} had a {} type that was not str, int, float, bool, or list.' . format ( key , str ( type ( item )))) md . value = item mds . append ( md ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"update_plugin_domain()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.vectorize_audio","text":"Request a vectorized version of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to enroll required audio_input the Audio message (or list of messages) to add as an enrollment addition required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (ClassModificationResult) Source code in olivepy/api/olive_async_client.py def vectorize_audio ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED , msg_id = None ): \"\"\" Request a vectorized version of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to enroll :param audio_input: the Audio message (or list of messages) to add as an enrollment addition :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (ClassModificationResult) \"\"\" vectorization = olive_pb2 . PluginAudioVectorRequest () vectorization . plugin = plugin vectorization . domain = domain if not isinstance ( audio_input , list ): audio_input = [ audio_input ] for audio_msg in audio_input : if not isinstance ( audio_msg , olive_pb2 . Audio ): audio = olive_pb2 . Audio () olivepy . messaging . msgutil . package_audio ( audio , audio_msg , mode = mode ) vectorization . addition . append ( audio ) else : audio = audio_msg vectorization . addition . append ( audio ) if callback : self . enqueue_request ( vectorization , callback , msg_id = msg_id ) else : return self . sync_request ( vectorization , msg_id = msg_id )","title":"vectorize_audio()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.ClientMonitorThread","text":"Helper used to monitor the status of the Oliveserver","title":"ClientMonitorThread"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.ClientMonitorThread.add_event_callback","text":"Callback function that is notified of a heartbeat Parameters: Name Type Description Default callback Callable[[olive_pb2.Heartbeat], NoneType] the function that is called with a Heartbeat object required Source code in olivepy/api/olive_async_client.py def add_event_callback ( self , callback : Callable [[ olive_pb2 . Heartbeat ], None ]): \"\"\" Callback function that is notified of a heartbeat :param callback: the function that is called with a Heartbeat object \"\"\" self . event_callback . append ( callback )","title":"add_event_callback()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.ClientMonitorThread.run","text":"Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/olive_async_client.py def run ( self ): # print(\"Starting Olive Status Monitor for id: {}\".format(self.client_id)) self . working = True self . status_socket . subscribe ( \"\" ) poller = zmq . Poller () poller . register ( self . status_socket , zmq . POLLIN ) last_heartbeat = time . time () heartbeat_data = None notified_conn_fail = False while self . working : socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . status_socket in socks : last_heartbeat = time . time () heartbeat_data = self . status_socket . recv () heatbeat = olive_pb2 . Heartbeat () heatbeat . ParseFromString ( heartbeat_data ) if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats self . olive_status = OliveStatusRecord ( stats . pool_pending , stats . max_num_jobs , stats . cpu_percent , stats . mem_percent ) if self . log_status : for cb in self . event_callback : cb ( heatbeat ) else : if not notified_conn_fail and not heartbeat_data and time . time () - last_heartbeat > HEARTBEAT_TIMEOUT_SECONDS : print ( \"Unable to connect to server\" ) notified_conn_fail = True # Consider using the same timeout for messages? elif heartbeat_data and time . time () - last_heartbeat > HEARTBEAT_TIMEOUT_SECONDS : print ( \"heartbeat timeout\" ) # it has been too long since a heatbeat message was received from the server... assume there server is down if self . log_status : for cb in self . event_callback : cb ( None ) self . status_socket . close ()","title":"run()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.OliveStatusRecord","text":"Tracks status of an olive server","title":"OliveStatusRecord"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.StreamOliveClient","text":"This class is used to make streaming requests to an OLIVE server. Each streaming 'session' has its own StreamOliveClient. Any results are asynchronous, and unlike the AsyncOliveClient there is no request/response message expectation. One might submit multiple audio/data inputs before getting a response from the server (in the form of a WorkflowAnalysisResult)","title":"StreamOliveClient"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.StreamOliveClient.__init__","text":"Parameters: Name Type Description Default client_id str The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems required data_port the streaming port number required address the address of the olive server, such as localhost 'localhost' timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/olive_async_client.py def __init__ ( self , client_id : str , data_port , address = 'localhost' , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems :param data_port: the streaming port number :param address: the address of the olive server, such as localhost :param timeout_second: time in seconds, to wait for a response from the server \"\"\" threading . Thread . __init__ ( self ) self . client_id = client_id # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . stream_data_port = data_port self . timeout_seconds = timeout_second self . request_queue = queue . Queue () self . working = False self . request_socket = None self . status_socket = None # thread to monitor OLIVE server heartbeats self . worker = None # self.status_socket = context.socket(zmq.SUB) self . stream_connected = False self . monitor_status = False oc . OliveClient . setup_multithreading () self . streaming_callbacks = dict ()","title":"__init__()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.StreamOliveClient.connect","text":"Connect this client to the server Parameters: Name Type Description Default monitor_server if true, starts a thread to monitor the server status connection for heartbeat messages required Source code in olivepy/api/olive_async_client.py def connect ( self ): \"\"\" Connect this client to the server :param monitor_server: if true, starts a thread to monitor the server status connection for heartbeat messages \"\"\" # logging.debug(\"Starting Olive async monitor...\") self . connection_done = threading . Event () self . start () # block until connected self . stream_connected = True self . connection_done . wait () logging . debug ( \"Olive async client ready\" )","title":"connect()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.StreamOliveClient.disconnect","text":"Closes the connection to the OLIVE server Source code in olivepy/api/olive_async_client.py def disconnect ( self ): \"\"\" Closes the connection to the OLIVE server \"\"\" if self . worker : self . worker . stopWorker () self . working = False self . stream_connected = False self . join () self . request_socket . close ()","title":"disconnect()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.StreamOliveClient.enqueue_data","text":"Send a data (audio) a message to the streaming session. Only data can be sent, non-data messages are not supported. All server requests (even to stop this streaming session) must be sent on the standard OLIVE request socket using the AsyncOliveClient) Parameters: Name Type Description Default data_message the data (audio) to send, currently limited to an Audio message, although that may expand over time required Source code in olivepy/api/olive_async_client.py def enqueue_data ( self , data_message ): \"\"\" Send a data (audio) a message to the streaming session. Only data can be sent, non-data messages are not \\ supported. All server requests (even to stop this streaming session) must be sent on the standard OLIVE \\ request socket using the AsyncOliveClient) :param data_message: the data (audio) to send, currently limited to an Audio message, although that may expand over time \"\"\" self . request_queue . put ( data_message )","title":"enqueue_data()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.StreamOliveClient.is_connected","text":"Status of the connection to the OLIVE server Returns: Type Description True if connected Source code in olivepy/api/olive_async_client.py def is_connected ( self ): \"\"\" Status of the connection to the OLIVE server :return: True if connected \"\"\" return self . stream_connected","title":"is_connected()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.StreamOliveClient.run","text":"Starts the thread to handle async messages Source code in olivepy/api/olive_async_client.py def run ( self ): \"\"\" Starts the thread to handle async messages \"\"\" try : logging . debug ( \"Starting OLIVE Streaming Worker for id: {} \" . format ( self . client_id )) context = zmq . Context () self . request_socket = context . socket ( zmq . PAIR ) # init the request and status socket data_addr = \"tcp://\" + self . server_address + \":\" + str ( self . stream_data_port ) self . request_socket . connect ( data_addr ) # todo if/when we provide heatbeats from the streaming exec... # if self.monitor_status: # logging.debug(\"connecting to status socket...\") # self.status_socket = context.socket(zmq.SUB) # self.status_socket.connect(status_addr) # self.worker = ClientMonitorThread(self.status_socket, self.client_id) # self.worker.start() # else: # self.worker = None poller = zmq . Poller () poller . register ( self . request_socket , zmq . POLLIN ) self . working = True except Exception as e : logging . error ( \"Error connecting to the OLIVE streaming server: {} \" . format ( e )) self . stream_connected = False self . working = False finally : self . connection_done . set () while self . working : # First, send any client requests while not self . request_queue . empty (): request_msg = self . request_queue . get () logging . debug ( \"Sending client data\" ) self . request_socket . send ( request_msg . SerializeToString ()) # FIXME - check if a result was lost # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . request_socket in socks : # logging.info(\"Received streaming message from OLIVE...\") protobuf_data = self . request_socket . recv () envelope = olive_pb2 . Envelope () envelope . ParseFromString ( protobuf_data ) print ( 'Handle {} stream messages' . format ( len ( envelope . message ))) for i in range ( len ( envelope . message )): self . _process_response ( envelope . message [ i ]) poller . unregister ( self . request_socket ) self . request_socket . close ()","title":"run()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.StreamOliveClient.setup_multithreading","text":"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/olive_async_client.py @classmethod def setup_multithreading ( cls ): '''This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. ''' # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL )","title":"setup_multithreading()"},{"location":"olivepy-docs/api.html#olivepyapiworkflow","text":"","title":"olivepy.api.workflow"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow","text":"An OliveWorkflow instance represents a Workflow Definition actualized by an OLIVE server. Once actualized, an OliveWorkflow instance is used to make analysis, or enrollment/unenrollment requests. An OliveWorkflow should be created using an OliveWorkflowDefinition's create_workflow() method. All calls to the server include an optional callback. When the callback is provided, the call does not block and the callback method is invoked when a response is received from the server. A callback method has 3 arguments: the original request, the response, and an error message if the request failed. Exceptions: Type Description WorkflowException If the workflow was not actualized","title":"OliveWorkflow"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.__init__","text":"Parameters: Name Type Description Default olive_async_client AsyncOliveClient the client connection to the OLIVE server required actualized_workflow OliveWorkflowActualizedResponse An OliveWorkflowDefinition actualized by the server required Source code in olivepy/api/workflow.py def __init__ ( self , olive_async_client : AsyncOliveClient , actualized_workflow : response . OliveWorkflowActualizedResponse ): \"\"\" :param olive_async_client: the client connection to the OLIVE server :param actualized_workflow: An OliveWorkflowDefinition actualized by the server \"\"\" self . client = olive_async_client self . workflow_response = actualized_workflow actualized_workflow_definition = actualized_workflow . get_workflow () # make sure an OLIvE server has actualized this workflow if not actualized_workflow_definition . actualized : raise WorkflowException ( \"Error: Can not create an OliveWorkflow using a Workflow Definition that has not \" \"been actualized by an OLIVE server\" ) self . workflow_def = actualized_workflow_definition # note: enrollment and adapt should only have one task/job # but there could be multiple plugins/task that could support enrollment or adaptation.. so we focus on # analysis","title":"__init__()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.adapt","text":"NOT YET SUPPORTED -- and not sure it will ever be supported via workflow Parameters: Name Type Description Default data_input required callback required options None finalize True Returns: Type Description not supported Source code in olivepy/api/workflow.py def adapt ( self , data_input , callback , options = None , finalize = True ): \"\"\" NOT YET SUPPORTED -- and not sure it will ever be supported via workflow :param data_input: :param callback: :param options: :param finalize: :return: not supported \"\"\" raise Exception ( \"Workflow adaption not supported\" )","title":"adapt()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.analyze","text":"Perform a workflow analysis Parameters: Name Type Description Default data_inputs List[workflow_pb2.WorkflowDataRequest] a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. required callback an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. None options str a JSON string of name/value options to include with the analysis request. None class_ids str a JSON string of name/value options to include with the analysis request. None Returns: Type Description OliveWorkflowAnalysisResponse an OliveWorkflowAnalysisResponse (if no callback provided) Source code in olivepy/api/workflow.py def analyze ( self , data_inputs : List [ WorkflowDataRequest ], callback = None , options : str = None , class_ids : str = None ) -> response . OliveWorkflowAnalysisResponse : \"\"\" Perform a workflow analysis :param data_inputs: a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. :param callback: an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. :param options: a JSON string of name/value options to include with the analysis request. :param class_ids: a JSON string of name/value options to include with the analysis request. :return: an OliveWorkflowAnalysisResponse (if no callback provided) \"\"\" # make call blocking if no callback or always assume it is async? analysis_request = WorkflowAnalysisRequest () for di in data_inputs : analysis_request . workflow_data_input . append ( di ) analysis_request . workflow_definition . CopyFrom ( self . workflow_def ) # Parse options (if any) if options : jopts = utils . parse_json_options ( options ) analysis_request . option . extend ( jopts ) if class_ids : json_class_ids = utils . parse_json_options ( class_ids ) analysis_request . class_id . extend ( json_class_ids ) if callback : self . client . enqueue_request ( analysis_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( analysis_request , response . OliveWorkflowAnalysisResponse ())","title":"analyze()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.enroll","text":"Submit data for enrollment. Parameters: Name Type Description Default data_inputs List[workflow_pb2.WorkflowDataRequest] a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. required class_id str the name of the enrollment required job_names List[str] a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. required callback an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. None options a dictionary of name/value option pairs to include with the enrollment request None Returns: Type Description server enrollment response if no callback provided Source code in olivepy/api/workflow.py def enroll ( self , data_inputs : List [ WorkflowDataRequest ], class_id : str , job_names : List [ str ], callback = None , options = None ): \"\"\" Submit data for enrollment. :param data_inputs: a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. :param class_id: the name of the enrollment :param job_names: a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. :param callback: an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. :param options: a dictionary of name/value option pairs to include with the enrollment request :return: server enrollment response if no callback provided \"\"\" # # first, get the enrollment order # for order in self.workflow_def.order: # if order.workflow_type == WORKFLOW_ENROLLMENT_TYPE: # workflow_enrollment_order_msg = order # break # # if workflow_enrollment_order_msg is None: # raise Exception(\"This workflow does not contain any \") # # # for name in task_names: # make call blocking if no callback or always assume it is async? enroll_request = WorkflowEnrollRequest () for di in data_inputs : enroll_request . workflow_data_input . append ( di ) enroll_request . workflow_definition . CopyFrom ( self . workflow_def ) enroll_request . class_id = class_id for job_task in job_names : enroll_request . job_names . append ( job_task ) if options : jopts = utils . parse_json_options ( options ) enroll_request . option . extend ( jopts ) if callback : # self.client.enqueue_request(enroll_request, callback, response.OliveWorkflowEnrollmentResponse()) self . client . enqueue_request ( enroll_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( enroll_request , response . OliveWorkflowAnalysisResponse ())","title":"enroll()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_analysis_class_ids","text":"Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. Parameters: Name Type Description Default callback an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) None Returns: Type Description OliveClassStatusResponse an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server Source code in olivepy/api/workflow.py def get_analysis_class_ids ( self , type = WORKFLOW_ANALYSIS_TYPE , callback = None ) -> response . OliveClassStatusResponse : \"\"\" Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. :param type the WorkflowOrder type (WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, or WORKFLOW_UNENROLLMENT_TYPE) :param callback: an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) :return: an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server \"\"\" class_request = WorkflowClassStatusRequest () class_request . workflow_definition . CopyFrom ( self . workflow_def ) if type : class_request . type = type if callback : self . client . enqueue_request ( class_request , callback , response . OliveClassStatusResponse ()) else : return self . client . sync_request ( class_request , response . OliveClassStatusResponse ())","title":"get_analysis_class_ids()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_analysis_job_names","text":"The names of analysis jobs in this workflow (usually only one analysis job) Returns: Type Description List[str] A list of analysis job names in this workflow Source code in olivepy/api/workflow.py def get_analysis_job_names ( self ) -> List [ str ]: \"\"\" The names of analysis jobs in this workflow (usually only one analysis job) :return: A list of analysis job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_ANALYSIS_TYPE )","title":"get_analysis_job_names()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_analysis_task_info","text":"A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is not known until runtime) Returns: Type Description List[Dict[str, Dict]] JSON structured detailed information of analysis tasks used in this workflow Source code in olivepy/api/workflow.py def get_analysis_task_info ( self ) -> List [ Dict [ str , Dict ]]: \"\"\" A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report \\ includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is \\ not known until runtime) :return: JSON structured detailed information of analysis tasks used in this workflow \"\"\" # return [task.consumer_result_label for task in analysis_jobs[job_name]] return self . workflow_response . to_json ( indent = 1 )","title":"get_analysis_task_info()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_analysis_tasks","text":"Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) Parameters: Name Type Description Default job_name str filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_analysis_tasks ( self , job_name : str = None ) -> List [ str ]: \"\"\" Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) :param job_name: filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. :return: a list of task names \"\"\" analysis_jobs = response . get_workflow_jobs ( self . workflow_def , WORKFLOW_ANALYSIS_TYPE ) # better to exception or empty dict???? if len ( analysis_jobs ) == 0 : return None if job_name is not None : if job_name not in analysis_jobs : return None else : # get the default job name job_name = list ( analysis_jobs . keys ())[ 0 ] return [ task . consumer_result_label for task in analysis_jobs [ job_name ]]","title":"get_analysis_tasks()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_enrollment_job_names","text":"The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment Returns: Type Description List[str] A list of enrollment job names in this workflow Source code in olivepy/api/workflow.py def get_enrollment_job_names ( self ) -> List [ str ]: \"\"\" The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment :return: A list of enrollment job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_ENROLLMENT_TYPE )","title":"get_enrollment_job_names()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_enrollment_tasks","text":"Return a list of tasks that support enrollment in this workflow. Parameters: Name Type Description Default job_name str optionally the name of the enrollment job. Optional since most workflows only support one job None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_enrollment_tasks ( self , job_name : str = None , type = WORKFLOW_ENROLLMENT_TYPE ) -> List [ str ]: \"\"\" Return a list of tasks that support enrollment in this workflow. :param job_name: optionally the name of the enrollment job. Optional since most workflows only support one job :return: a list of task names \"\"\" enrollment_jobs = response . get_workflow_jobs ( self . workflow_def , type ) if len ( enrollment_jobs ) == 0 : return None if job_name is not None : if job_name not in enrollment_jobs : return None # normally (and currently the only supported option) should be just one enrollment_job... return list ( response . get_workflow_job_tasks ( enrollment_jobs , job_name ) . keys ())","title":"get_enrollment_tasks()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_unenrollment_job_names","text":"The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment Returns: Type Description List[str] A list of un-enrollment job names in this workflow Source code in olivepy/api/workflow.py def get_unenrollment_job_names ( self ) -> List [ str ]: \"\"\" The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment :return: A list of un-enrollment job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_UNENROLLMENT_TYPE )","title":"get_unenrollment_job_names()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_unenrollment_tasks","text":"Return a list of tasks that support UNenrollment in this workflow. Parameters: Name Type Description Default job_name str optionally the name of the enrollment job. Optional since most workflows only support one job None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_unenrollment_tasks ( self , job_name : str = None ) -> List [ str ]: \"\"\" Return a list of tasks that support UNenrollment in this workflow. :param job_name: optionally the name of the enrollment job. Optional since most workflows only support one job :return: a list of task names \"\"\" return self . get_enrollment_tasks ( job_name , type = WORKFLOW_UNENROLLMENT_TYPE )","title":"get_unenrollment_tasks()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_audio","text":"Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. Parameters: Name Type Description Default audio_data ~AnyStr the input data is a string (file path) if mode is 'AUDIO_PATH', otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples required mode specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. <InputTransferType.SERIALIZED: 3> annotations List[Tuple[float, float]] optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) None task_annotations Dict[str, Dict[str, List[Tuple[float, float]]]] optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {'SHL': {'speaker'':[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the 'SHL' task, which are labeled as class 'speaker' having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . None selected_channel int optional - the channel to process if using multi-channel audio None num_channels int The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None sample_rate int The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None num_samples int The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None validate_local_path bool If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired True label an optional name to use with the audio None Returns: Type Description WorkflowDataRequest A populated WorkflowDataRequest to use in a workflow activity Source code in olivepy/api/workflow.py def package_audio ( self , audio_data : AnyStr , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , annotations : List [ Tuple [ float , float ]] = None , task_annotations : Dict [ str , Dict [ str , List [ Tuple [ float , float ]]]] = None , selected_channel : int = None , num_channels : int = None , sample_rate : int = None , num_samples : int = None , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. :param audio_data: the input data is a string (file path) if mode is 'AUDIO_PATH', otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples :param mode: specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. :param annotations: optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) :param task_annotations: optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {'SHL': {'speaker'':[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the 'SHL' task, which are labeled as class 'speaker' having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . :param selected_channel: optional - the channel to process if using multi-channel audio :param num_channels: The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param sample_rate: The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param num_samples: The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param validate_local_path: If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired :param label: an optional name to use with the audio :return: A populated WorkflowDataRequest to use in a workflow activity \"\"\" audio = Audio () msgutil . package_audio ( audio , audio_data , annotations , selected_channel , mode , num_channels , sample_rate , num_samples , validate_local_path ) # Add any task specific regions: if task_annotations : for task_label in task_annotations . keys (): ta = audio . task_annotations . add () ta . task_label = task_label # we only expect to have one set of annotations, so just one region_label for region_label in task_annotations [ task_label ]: ta . region_label = region_label for annots in task_annotations [ task_label ][ region_label ]: region = ta . regions . add () region . start_t = np . float ( annots [ 0 ]) region . end_t = np . float ( annots [ 1 ]) wkf_data_request = WorkflowDataRequest () #fixme: this should be set based on the audio.label (filename) or given a unique name here... wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = AUDIO wkf_data_request . workflow_data = audio . SerializeToString () # consumer_data_label doesn't need to be set... use default # set job name? Currently we assume one job per workflow so punting on this for now return wkf_data_request","title":"package_audio()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_binary","text":"Parameters: Name Type Description Default video_input a video input required Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_binary ( self , binary_input , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , annotations : List [ Tuple [ float , float ]] = None , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" :param video_input: a video input :return: TBD \"\"\" media = BinaryMedia () msgutil . package_binary_media ( media , binary_input , mode = mode , validate_local_path = validate_local_path ) if label : media . label = label wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = VIDEO wkf_data_request . workflow_data = media . SerializeToString () return wkf_data_request","title":"package_binary()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_image","text":"Not yet supported Parameters: Name Type Description Default image_input An image input required Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_image ( self , image_input , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" Not yet supported :param image_input: An image input :return: TBD \"\"\" media = BinaryMedia () msgutil . package_binary_media ( media , image_input , mode = mode , validate_local_path = validate_local_path ) if label : media . label = label media . motion = False # todo if annotations... wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = IMAGE wkf_data_request . workflow_data = media . SerializeToString () return wkf_data_request","title":"package_image()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_text","text":"Used to package data for a workflow that accepts string (text) input Parameters: Name Type Description Default text_input str a text input required optional_label str an optional label, namoe or comment associated with this input None text_workflow_key str the keyword used to identify this data in the workflow. By default a value of 'text' is assumed and recommend None Returns: Type Description WorkflowDataRequest a WorkflowDataRequest populated with the text input Source code in olivepy/api/workflow.py def package_text ( self , text_input : str , optional_label : str = None , text_workflow_key : str = None ) -> WorkflowDataRequest : \"\"\" Used to package data for a workflow that accepts string (text) input :param text_input: a text input :param optional_label: an optional label, namoe or comment associated with this input :param text_workflow_key: the keyword used to identify this data in the workflow. By default a value of 'text' is assumed and recommend :return: a WorkflowDataRequest populated with the text input \"\"\" text_msg = Text () # not (yet?) supported multiple text inputs in a request text_msg . text . append ( text_input ) if optional_label : text_msg . label = optional_label wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = text_workflow_key if text_workflow_key else 'text' wkf_data_request . data_type = TEXT wkf_data_request . workflow_data = text_msg . SerializeToString () return wkf_data_request","title":"package_text()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_workflow_input","text":"Parameters: Name Type Description Default input_msg the OLIVE data message to package required expected_data_type the data type of the message (Binary <OliveInputDataType.AUDIO_DATA_TYPE: 2> Returns: Type Description WorkflowDataRequest a WorkflowDataRequest Source code in olivepy/api/workflow.py def package_workflow_input ( self , input_msg , expected_data_type = msgutil . OliveInputDataType . AUDIO_DATA_TYPE ) -> WorkflowDataRequest : \"\"\" :param input_msg: the OLIVE data message to package :param expected_data_type: the data type of the message (Binary :return: a WorkflowDataRequest \"\"\" wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = input_msg . label if input_msg . label else msgutil . get_uuid () wkf_data_request . data_type = msgutil . data_type_class_map [ expected_data_type ] wkf_data_request . workflow_data = input_msg . SerializeToString () return wkf_data_request","title":"package_workflow_input()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.serialize_audio","text":"Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters: Name Type Description Default filename str the local path to the file to serialize required Returns: Type Description ~AnyStr the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples Source code in olivepy/api/workflow.py def serialize_audio ( self , filename : str ) -> AnyStr : \"\"\" Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() \\ when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized :param filename: the local path to the file to serialize :return: the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples \"\"\" with open ( os . path . expanduser ( filename ), 'rb' ) as f : serialized_buffer = f . read () # return the buffer return serialized_buffer","title":"serialize_audio()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.stream","text":"Request a new streaming session. This call is blocking, as it waits for a response from the client that acknowledges or denies the streaming request Parameters: Name Type Description Default client_id str the unique name of this streaming client required sample_rate int the sample rate of the audio to be streamed required options str a JSON string of name/value options to include with the analysis request such as '{\"filter_length\":99, \"interpolate\":1.0, \"test_name\":\"midge\"}' None Returns: Type Description [<class 'str'>, <class 'int'>] a tuple of the new session ID and the streaming port number. Source code in olivepy/api/workflow.py def stream ( self , client_id : str , sample_rate : int , options : str = None ) -> [ str , int ]: \"\"\" Request a new streaming session. This call is blocking, as it waits for a response from the client that acknowledges or denies the streaming request :param client_id: the unique name of this streaming client :param sample_rate: the sample rate of the audio to be streamed :param options: a JSON string of name/value options to include with the analysis request such as '{\"filter_length\":99, \"interpolate\":1.0, \"test_name\":\"midge\"}' :return: a tuple of the new session ID and the streaming port number. \"\"\" stream_request = StartStreamingRequest () stream_request . client_stream_id = client_id stream_request . sampleRate = sample_rate stream_request . workflow_definition . CopyFrom ( self . workflow_def ) # Parse options (if any) if options : jopts = utils . parse_json_options ( options ) stream_request . option . extend ( jopts ) stream_response = self . client . sync_request ( stream_request , response . OliveWorkflowAnalysisResponse ()) if stream_response . is_successful (): if stream_response . get_response () . successful : return stream_response . get_response () . session_id , stream_response . get_response () . data_port else : # failed to start the streaming session (likely too many sessions) raise msgutil . ExceptionFromServer ( stream_response . get_response () . info ) # could not start streaming... raise msgutil . ExceptionFromServer ( stream_response . get_error ())","title":"stream()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.to_json","text":"Generate the workflow as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/api/workflow.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" return self . workflow_response . to_json ( indent = indent )","title":"to_json()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.unenroll","text":"Submit a class id (speaker name, language name, etc) for un-enrollment. Parameters: Name Type Description Default class_id str the name of the enrollment class to remove required job_names List[str] a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). required callback an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. None options a dictionary of name/value option pairs to include with the enrollment request None Returns: Type Description server unenrollment response if no callback provided Source code in olivepy/api/workflow.py def unenroll ( self , class_id : str , job_names : List [ str ], callback = None , options = None ): \"\"\" Submit a class id (speaker name, language name, etc) for un-enrollment. :param class_id: the name of the enrollment class to remove :param job_names: a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). :param callback: an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. :param options: a dictionary of name/value option pairs to include with the enrollment request :return: server unenrollment response if no callback provided \"\"\" # make call blocking if no callback or always assume it is async? unenroll_request = WorkflowUnenrollRequest () unenroll_request . workflow_definition . CopyFrom ( self . workflow_def ) unenroll_request . class_id = class_id for job_task in job_names : unenroll_request . job_names . append ( job_task ) if options : jopts = utils . parse_json_options ( options ) unenroll_request . option . extend ( jopts ) if callback : # self.client.enqueue_request(enroll_request, callback, response.OliveWorkflowEnrollmentResponse()) self . client . enqueue_request ( unenroll_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( unenroll_request , response . OliveWorkflowAnalysisResponse ())","title":"unenroll()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition","text":"Used to load a Workflow Definition from a file.","title":"OliveWorkflowDefinition"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition.__init__","text":"Create an OliveWorkflowDefinition to access a workflow definition file Parameters: Name Type Description Default filename str the path/filename of a workflow definition file to load required Source code in olivepy/api/workflow.py def __init__ ( self , filename : str ): \"\"\" Create an OliveWorkflowDefinition to access a workflow definition file :param filename: the path/filename of a workflow definition file to load \"\"\" # First, make sure the workflow definition (WD) file exists filename = os . path . expanduser ( filename ) if not os . path . exists ( filename ): raise IOError ( \"Workflow definition file ' {} ' does not exists\" . format ( filename )) # Load the WD, then submit to the server # Read the workflow - either a workflow or a text file try : with open ( filename , 'rb' ) as f : self . wd = WorkflowDefinition () self . wd . ParseFromString ( f . read ()) except IOError as e : raise IOError ( \"Workflow definition file ' {} ' does not exist\" . format ( filename )) except DecodeError as de : self . wd = WorkflowDefinition () # Try parsing as text file (will fail for a protobuf file) with open ( filename , 'r' ) as f : # First load as json json_input = json . loads ( f . read ()) # Next, we need to convert message data in task(s) to byte strings for element in json_input : if element == 'order' : for job in json_input [ element ]: # print(\"Job: {}\".format(job)) for job_def in job [ 'job_definition' ]: for task in job_def [ 'tasks' ]: task_type = task [ 'message_type' ] # Covert 'messageData' into a protobuf and save the byte string in the json # so it can be correctly deserialized tmp_json = task [ 'message_data' ] msg = msgutil . type_class_map [ MessageType . Value ( task_type )]() Parse ( json . dumps ( tmp_json ), msg ) # now serialized msg as messageData data = base64 . b64encode ( msg . SerializeToString ()) . decode ( 'utf-8' ) task [ 'message_data' ] = data # Now we should be able to create a WorkflowDefinition from the json data Parse ( json . dumps ( json_input ), self . wd ) # Create JSON formatted output from the the Workflow?","title":"__init__()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition.create_workflow","text":"Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, or enrollment requests Parameters: Name Type Description Default client AsyncOliveClient an open client connection to an OLIVE server required Returns: Type Description a new OliveWorkflow object, which has been actualized (activated) by the olive server Source code in olivepy/api/workflow.py def create_workflow ( self , client : olivepy . api . olive_async_client . AsyncOliveClient ): \"\"\" Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, or enrollment requests :param client: an open client connection to an OLIVE server :return: a new OliveWorkflow object, which has been actualized (activated) by the olive server \"\"\" if not client . is_connected (): raise IOError ( \"No connection to the Olive server\" ) # Create a workflow request request = WorkflowActualizeRequest () request . workflow_definition . CopyFrom ( self . wd ) workflow_result = client . sync_request ( request , response . OliveWorkflowActualizedResponse ()) if workflow_result . is_error (): raise msgutil . ExceptionFromServer ( workflow_result . get_error ()) # if msg: # raise msgutil.ExceptionFromServer(msg) return OliveWorkflow ( client , workflow_result ) # todo send WD to server, return an OliveWorklow to the user","title":"create_workflow()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition.get_json","text":"Create a JSON structure of the Workflow Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document 1 Returns: Type Description A JSON (dictionary) representation of the Workflow Definition Source code in olivepy/api/workflow.py def get_json ( self , indent = 1 ): \"\"\" Create a JSON structure of the Workflow :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document :return: A JSON (dictionary) representation of the Workflow Definition \"\"\" analysis_task = [] job_names = set () workflow_analysis_order_msg = None for order in self . wd . order : if order . workflow_type == WORKFLOW_ANALYSIS_TYPE : workflow_analysis_order_msg = order break if workflow_analysis_order_msg is None : # no analysis results return analysis_task # for job in self._response.job_result: for job in workflow_analysis_order_msg . job_definition : # create a dictionary for each job result job_dict = dict () job_name = job . job_name job_names . add ( job_name ) # get data handling info for this job data_prop = job . data_properties job_dict [ 'Data Input' ] = json . loads ( MessageToJson ( data_prop , preserving_proto_field_name = True )) # if data_prop.mode == SPLIT: # # Hack to make split/mulit-channel mode more clear # job_dict['data']['mode'] = 'SPLIT: Process each channel as a job' # and a dictionary of tasks: # add to our results - in most cases we will have just one job analysis_task . append ( job_dict ) for task in job . tasks : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # Deserialize message_data, and replace it in the task_result_dict task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ 'job_name' ] = job_name task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) del task_result_dict [ 'message_data' ] job_dict [ task . consumer_result_label ] = task_result_dict return json . dumps ( analysis_task , indent = indent )","title":"get_json()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition.to_json","text":"Generate the workflow as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/api/workflow.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" json_str_output = MessageToJson ( self . wd , preserving_proto_field_name = True ) json_output = json . loads ( json_str_output ) for element in json_output : if element == 'order' : for job in json_output [ element ]: # print(\"Job: {}\".format(job)) for job_def in job [ 'job_definition' ]: for task in job_def [ 'tasks' ]: task_type = task [ 'message_type' ] data = base64 . b64decode ( task [ 'message_data' ]) msg = self . _extract_serialized_message ( MessageType . Value ( task_type ), data ) task [ 'message_data' ] = json . loads ( MessageToJson ( msg , preserving_proto_field_name = True )) # print(\"Task: {}\".format(task)) if indent and indent < 0 : return json_output return json . dumps ( json_output , indent = indent )","title":"to_json()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.WorkflowException","text":"This exception means that an error occurred handling a Workflow","title":"WorkflowException"},{"location":"olivepy-docs/client.html","text":"olivepy client module olivepy.client.analyze_client olivepy.client.client_common extract_input_data ( args , expected_data_type =< OliveInputDataType . AUDIO_DATA_TYPE : 2 > , fail_if_no_data = True , has_class_ids = False ) Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server Parameters: Name Type Description Default args required fail_if_no_data True has_class_ids False Returns: Type Description Source code in olivepy/client/client_common.py def extract_input_data ( args , expected_data_type = OliveInputDataType . AUDIO_DATA_TYPE , fail_if_no_data = True , has_class_ids = False ): \"\"\" Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server :param args: :param fail_if_no_data: :param has_class_ids: :return data_input, transfer_mode, send_pathname: \"\"\" send_pathname = True if args . path else False send_serialized = not send_pathname data_input = [] if send_pathname : # fixme more generic for image and video? transfer_mode = InputTransferType . PATH else : transfer_mode = InputTransferType . SERIALIZED audio = text = image = video = False if hasattr ( args , 'upload_files' ) and args . upload_files and args . path : if args . input : uploaded_input = upload_files ( args . input , args . server , args . upload_port ) data_input = uploaded_input elif args . input_list : data_input = parse_data_list2 ( args . input_list , transfer_mode , expected_data_type , has_class_ids ) data_input = upload_files ( data_input , args . server , args . upload_port ) elif args . input_list : # parse input list to make sure we actually have one or more files... data_input = parse_data_list2 ( args . input_list , transfer_mode , expected_data_type , has_class_ids ) # data_intpu --> [{filename: DATA_MSG, {channel: [start, end, class]}] , filename: Audio, {-1: (None, None, None)} if len ( data_input ) == 0 : args_bad = True print ( \"Data input list ' {} ' contains no valid files\" . format ( args . input_list )) elif args . input : # Do we want to support sending a path that is not local? In case they want to specify a path that is a # available for the server if isinstance ( args . input , list ): for input in args . input : input_data = convert_filename_to_data ( input , transfer_mode , expected_data_type ) if input_data : if has_class_ids : # data_input.append( (convert_filename_to_data(args.input, transfer_mode, expected_data_type), args.enroll)) data_input = { args . enroll : [ input_data ]} else : data_input . append ( input_data ) else : input_data = convert_filename_to_data ( args . input , transfer_mode , expected_data_type ) if input_data : if has_class_ids : # data_input.append( (convert_filename_to_data(args.input, transfer_mode, expected_data_type), args.enroll)) data_input = { args . enroll : [ input_data ]} else : data_input . append ( input_data ) # do some basic validation # If mo data input supplied, make sure that is okay with the other options provided if not data_input and fail_if_no_data : print ( 'The command requires data input(s).' ) exit ( 1 ) return data_input , transfer_mode , send_pathname prepare_input_data ( enroll = None , input = None , input_list = None , path = False , server = 'localhost' , upload_files = False , upload_port = 5000 , expected_data_type =< OliveInputDataType . AUDIO_DATA_TYPE : 2 > , fail_if_no_data = True , has_class_ids = False ) Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server Parameters: Name Type Description Default enroll Whether this is an enrollment None input The singular input None input_list The list of inputs None path Send pathnames rather than raw data, use with upload_files to upload using http if the files don't share a filesystem False server The server host 'localhost' upload_files Requires path to be True, controls whether to upload files using http, this allows for processing more than 2 GB of data and will be more memory efficient False upload_port 5000 fail_if_no_data True has_class_ids False Returns: Type Description Source code in olivepy/client/client_common.py def prepare_input_data ( enroll = None , input = None , input_list = None , path = False , server = \"localhost\" , upload_files = False , upload_port = 5000 , expected_data_type = OliveInputDataType . AUDIO_DATA_TYPE , fail_if_no_data = True , has_class_ids = False ): \"\"\" Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server :param enroll: Whether this is an enrollment :param input: The singular input :param input_list: The list of inputs :param path: Send pathnames rather than raw data, use with upload_files to upload using http if the files don't share a filesystem :param server: The server host :param upload_files: Requires path to be True, controls whether to upload files using http, this allows for processing more than 2 GB of data and will be more memory efficient :param upload_port: :param fail_if_no_data: :param has_class_ids: :return data_input, transfer_mode, send_pathname: \"\"\" args = {} args . enroll = enroll args . path = path args . upload_files = True if upload_files and path else False args . input = input args . input_list = input_list if input is None else None args . server = server args . upload_port = upload_port return extract_input_data ( args , expected_data_type , fail_if_no_data , has_class_ids ) olivepy.client.enroll_client olivepy.client.learn_client olivepy.client.status_client heartbeat_notification ( heatbeat ) Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server Source code in olivepy/client/status_client.py def heartbeat_notification ( heatbeat ): \"\"\"Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server\"\"\" if heatbeat : if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats print ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) print ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) print ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) print ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) print ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) print ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) print ( \"Number active jobs: \" + str ( stats . pool_busy )) print ( \"Number pending jobs: \" + str ( stats . pool_pending )) print ( \"Number finished jobs: \" + str ( stats . pool_finished )) print ( \"Max number jobs: \" + str ( stats . max_num_jobs )) print ( \"Server version: \" + str ( stats . server_version )) print ( \" \\n \" ) else : print ( \"No OLIVE heartbeat received. Olive server or connection down\" ) olivepy.client.utils_client main () Returns: Type Description Source code in olivepy/client/utils_client.py def main (): \"\"\" :return: \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyutils' ) parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--save_as_text' , action = 'store' , help = 'Save the workflow to a JSON formatted text file having this name.' ) parser . add_argument ( '--save_as_binary' , action = 'store' , help = 'Save the workflow to a binary formatted workflow file.' ) parser . add_argument ( '--print_workflow' , action = 'store_true' , help = 'Print the workflow definition file info (before it is actualized/sent to server)' ) args_bad = False args = parser . parse_args () if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True if not ( args . save_as_text or args . print_workflow or args . save_as_binary ): args_bad = True print ( 'The command requires one or more tasks.' ) if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) try : # first, create the workflow definition from the workflow file: owd = ow . OliveWorkflowDefinition ( args . workflow ) if args . save_as_text : print ( \"Saving Workflow Definition ' {} ' as ' {} '\" . format ( args . workflow , args . save_as_text )) owd . _save_as_json ( args . save_as_text ) if args . save_as_binary : print ( \"Saving Workflow Definition ' {} ' as ' {} '\" . format ( args . workflow , args . save_as_binary )) owd . _save_as_binary ( args . save_as_binary ) if args . print_workflow : wdef_json = owd . to_json ( indent = 1 ) print ( \"Workflow Definition Task Info: {} \" . format ( wdef_json )) print ( \"\" ) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) olivepy.client.workflow_client heartbeat_notification ( heatbeat ) Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server Source code in olivepy/client/workflow_client.py def heartbeat_notification ( heatbeat ): \"\"\" Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server \"\"\" if heatbeat : if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats print ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) print ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) print ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) print ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) print ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) print ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) print ( \"Number active jobs: \" + str ( stats . pool_busy )) print ( \"Number pending jobs: \" + str ( stats . pool_pending )) print ( \"Number finished jobs: \" + str ( stats . pool_finished )) print ( \"Max number jobs: \" + str ( stats . max_num_jobs )) print ( \"Server version: \" + str ( stats . server_version )) print ( \" \\n \" ) else : print ( \"Too long since a heatbeat message was received. Olive server or connection down\" ) main () Client for interacting with the workflow API Source code in olivepy/client/workflow_client.py def main (): \"\"\" Client for interacting with the workflow API \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyworkflow' , description = \"Perform OLIVE analysis using a Workflow \" \"Definition file\" ) # Required positional option parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--tasks' , action = 'store_true' , help = 'Print the workflow analysis tasks.' ) parser . add_argument ( '--print_class_ids' , action = 'store_true' , help = 'Print the class IDs available for analysis in the specified workflow.' ) parser . add_argument ( '--class_ids' , action = 'store' , help = 'Send class IDs with the analysis request.' ) parser . add_argument ( '--print_actualized' , action = 'store_true' , help = 'Print the actualized workflow info.' ) parser . add_argument ( '--print_workflow' , action = 'store_true' , help = 'Print the workflow definition file info (before it is actualized, if requested)' ) # # parser.add_argument('--print_options', action='store_true', # help='Print the options recognized for each task') parser . add_argument ( '-s' , '--server' , action = 'store' , default = 'localhost' , help = 'The machine the server is running on. Defaults to %(default)s .' ) parser . add_argument ( '-P' , '--port' , type = int , action = 'store' , default = 5588 , help = 'The port to use.' ) parser . add_argument ( '--upload_port' , type = int , action = 'store' , default = 5000 , help = 'The upload port to use when specifying --upload_files.' ) parser . add_argument ( '-t' , '--timeout' , type = int , action = 'store' , default = 300 , help = 'The timeout (in seconds) to wait for a response from the server ' ) parser . add_argument ( '-i' , '--input' , action = 'append' , help = 'The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag' ) parser . add_argument ( '--input_list' , action = 'store' , help = 'A list of files to analyze. One file per line.' ) parser . add_argument ( '--text' , action = 'store_true' , help = 'Indicates that input (or input list) is a literal text string to send in the analysis request.' ) parser . add_argument ( '--options' , action = 'store' , help = 'A JSON formatted string of workflow options such as ' '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}] or ' '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}, where the former ' 'options are only applied to the SAD task, and the later are applied to all tasks ' ) parser . add_argument ( '--path' , action = 'store_true' , help = 'Send the path of the audio instead of a buffer. ' 'Server and client must share a filesystem to use this option' ) parser . add_argument ( '--upload_files' , action = 'store_true' , help = 'Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation.' ) parser . add_argument ( '--secure' , action = 'store_true' , help = 'Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set.' ) parser . add_argument ( '--certpath' , action = 'store' , help = 'Specifies the path of the certificate' ) parser . add_argument ( '--keypath' , action = 'store' , help = 'Specifies the path of the certificate key' ) parser . add_argument ( '--keypass' , action = 'store' , help = 'Specifies the certificate passphrase to unlock the encrypted certificate key' ) parser . add_argument ( '--cabundlepath' , action = 'store' , help = 'Specifies the path of the certificate authority' ) # # parser.add_argument('--heartbeat', action='store_true', # help='Listen for server heartbeats ') # parser.add_argument('--status', action='store_true', # help='get server status') parser . add_argument ( '--debug' , action = 'store_true' , help = 'Debug mode ' ) # Not supported since it needs an additional 3rd party lib: # parser.add_argument('--decoded', action='store_true', # help='Send audio file as decoded PCM16 samples instead of sending as serialized buffer. ' # 'Input file must be a wav file') args_bad = False args = parser . parse_args () # Simple logging config if args . debug : log_level = logging . DEBUG else : log_level = logging . INFO # log_level = logging.WARN logging . basicConfig ( level = log_level ) if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True if ( args . tasks or args . print_class_ids or args . print_actualized or args . print_workflow ): data_required = False else : data_required = True if args . upload_files and not args . path : print ( '--upload_files must be combined with --path' ) args_bad = True start = time . time () # Our workflow should consume one of the 4 data types (but not a combination of types) .... # data_input, audio_mode, send_pathname, audio, text, image, video = client_com.extract_input_data_type(args, fail_if_no_data=data_required) if args . text : # special case of handling text data expected_data_type = OliveInputDataType . TEXT_DATA_TYPE else : expected_data_type = OliveInputDataType . BINARY_DATA_TYPE # expected_data_type = OliveInputDataType.AUDIO_DATA_TYPE # if you only want to send audio data_input , transfer_mode , send_pathname = client_com . extract_input_data ( args , expected_data_type = expected_data_type , fail_if_no_data = data_required ) json_opts = None if args . options : if os . path . exists ( args . options ): with open ( args . options , 'r' ) as f : json_opts = f . read () else : json_opts = args . options print ( \"Using options: {} \" . format ( json_opts )) json_class_ids = None if args . class_ids : if os . path . exists ( args . class_ids ): with open ( args . class_ids , 'r' , encoding = 'utf-8' ) as f : json_class_ids = f . read () else : json_class_ids = args . class_ids print ( \"Using class_ids: {} \" . format ( json_class_ids )) # checking for if tls is enabled and correctly set up secure_connect = False if args . secure : if ( args . certpath is None or args . keypass is None or args . cabundlepath is None or args . keypath is None ): args_bad = True print ( '--secure requires --certpath, --keypass, --keypath, and --cabundlepath' ) else : secure_connect = True if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) enable_status_socket = False # Create the connection to the OLIVE server client = oc . AsyncOliveClient ( \"olivepy_workflow\" , args . server , args . port , args . timeout ) if not secure_connect : client . connect () else : client . secure_connect ( certfile = args . certpath , keyfile = args . keypath , password = args . keypass , ca_bundle_path = args . cabundlepath , monitor_status = enable_status_socket ) try : # if args.heartbeat: # # Register to be notified of heartbeats from the OLIVE server # client.add_heartbeat_listener(heartbeat_notification) # if args.status: # # Request the current server status # server_status_response = client.get_status() # if server_status_response.is_successful(): # print(\"OLIVE JSON Server status: {}\".format(server_status_response.to_json(indent=10))) # # # # Or you can access the GetStatusResult protobuf: # print(\"OLIVE Server status: pending: {}, busy: {}, finished: {}, version: {}\" # .format(server_status_response.get_response().num_pending, # server_status_response.get_response().num_busy, # server_status_response.get_response().num_finished, # server_status_response.get_response().version)) # first, create the workflow definition from the workflow file: workflow_def = ow . OliveWorkflowDefinition ( args . workflow ) if args . print_workflow : wdef_json = workflow_def . to_json ( indent = 1 ) print ( \"Workflow Definition: \\n {} \" . format ( wdef_json )) print ( \"\" ) # Submit that workflow definition to the client for actualization (instantiation): workflow = workflow_def . create_workflow ( client ) if args . print_actualized : # tasks_json = workflow.get_analysis_task_info() print ( \"Actualized Workflow: {} \" . format ( workflow . to_json ( indent = 1 ))) print ( \"\" ) if args . tasks : # Print the analysis tasks: print ( \"Analysis Tasks: {} \" . format ( workflow . get_analysis_tasks ())) for enroll_job_name in workflow . get_enrollment_job_names (): print ( \"Enrollment job ' {} ' has Tasks: {} \" . format ( enroll_job_name , workflow . get_enrollment_tasks ( enroll_job_name ))) for unenroll_job_name in workflow . get_unenrollment_job_names (): print ( \"Unenrollment job ' {} ' has Tasks: {} \" . format ( unenroll_job_name , workflow . get_unenrollment_tasks ( unenroll_job_name ))) if args . print_class_ids : # Print the class IDs available for the workflow tasks: # support other types: type=olive_pb2.WORKFLOW_ENROLLMENT_TYPE? class_status_response = workflow . get_analysis_class_ids () print ( \"Class Info: {} \" . format ( class_status_response . to_json ( indent = 1 ))) buffers = [] for input in data_input : buffers . append ( workflow . package_workflow_input ( input , expected_data_type )) if data_required : print ( \"Sending analysis request...\" ) response = workflow . analyze ( buffers , options = json_opts , class_ids = json_class_ids ) print ( \"Workflow analysis results:\" ) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) finally : client . disconnect () stop = time . time () print ( f 'took { stop - start } s' ) olivepy.client.workflow_enroll_client main () Returns: Type Description Source code in olivepy/client/workflow_enroll_client.py def main (): \"\"\" :return: \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyworkflowenroll' , description = \"Perform OLIVE enrollment using a Workflow \" \"Definition file\" ) # parser.add_argument('-C', '--client-id', action='store', default='olivepy_', # help='Experimental: the client_id to use') # Required positional option parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--print_jobs' , action = 'store_true' , help = 'Print the supported workflow enrollment jobs.' ) parser . add_argument ( '--job' , action = 'store' , help = 'Enroll/Unenroll an Class ID for a job(s) in the specified workflow. If not specified enroll or unenroll for ALL enrollment/unenrollment jobst' ) parser . add_argument ( '--enroll' , action = 'store' , help = 'Enroll using this (class) name. Should be used with the job argument to specify a target job to enroll with (if there are more than one enrollment jobs) ' ) parser . add_argument ( '--unenroll' , action = 'store' , help = 'Enroll using this (class) name. Should be used with the job argument to specify a job to unenroll (if there are more than one unenrollment jobs)' ) parser . add_argument ( '-i' , '--input' , action = 'store' , help = 'The data input to enroll. Either a pathname to an audio/image/video file or a string for text input' ) parser . add_argument ( '--input_list' , action = 'store' , help = 'A list of files to enroll. One file per line plus the class id to enroll.' ) parser . add_argument ( '--path' , action = 'store_true' , help = 'Send the path of the audio instead of a buffer. ' 'Server and client must share a filesystem to use this option' ) # Connection arguments parser . add_argument ( '-s' , '--server' , action = 'store' , default = 'localhost' , help = 'The machine the server is running on. Defaults to %(default)s .' ) parser . add_argument ( '-P' , '--port' , type = int , action = 'store' , default = 5588 , help = 'The port to use.' ) parser . add_argument ( '-t' , '--timeout' , type = int , action = 'store' , default = 10 , help = 'The timeout (in seconds) to wait for a response from the server ' ) parser . add_argument ( '--secure' , action = 'store_true' , help = 'Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set.' ) parser . add_argument ( '--certpath' , action = 'store' , help = 'Specifies the path of the certificate' ) parser . add_argument ( '--keypath' , action = 'store' , help = 'Specifies the path of the certificate key' ) parser . add_argument ( '--keypass' , action = 'store' , help = 'Specifies the certificate passphrase to unlock the encrypted certificate key' ) parser . add_argument ( '--cabundlepath' , action = 'store' , help = 'Specifies the path of the certificate authority' ) # not supporting batch enrollments: # parser.add_argument('--audio_list', action='store', # help='A list of audio files to analyze. One file per line') args_bad = False args = parser . parse_args () if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True require_data = True if args . unenroll or args . print_jobs : require_data = False expected_data_type = OliveInputDataType . BINARY_DATA_TYPE data_input , transfer_mode , send_pathname = client_com . extract_input_data ( args , expected_data_type = expected_data_type , fail_if_no_data = require_data , has_class_ids = True ) # if args.enroll: # # there must be only one input # if len(data_input) > 1: # args_bad = True # print(\"The enroll and audio_list argument are mutually exclusive. Pick one and run again\") # else: # data_input = [(data_input[0], args.enroll)] print ( \"enrolling {} files\" . format ( len ( data_input ))) # if len(data_input) > 1 and not audio: # args_bad = True # print(\"Non-audio files can not be enrolled from an input list\") # support other data types.... # audios = [] using_pem = False # TODO GET CLASS IDS FROM ENROLLMENT FILE enroll = False unenroll = False if args . enroll : action_str = \"Enrollment\" enroll = True if args . unenroll : print ( \"Enrollment and un-enrollment are mutually exclusive. Pick one and run again\" ) args_bad = True elif args . unenroll : action_str = \"Unenrollment\" unenroll = True elif len ( data_input ) > 1 : enroll = True elif not args . print_jobs : args_bad = True print ( \"Must use one of the options: --enroll, --unenroll, or --print_jobs \" ) action_str = \"\" # support enrollments from a file (list and/or PEM format)? # if not (audio or image or video): # # no input provided, make sure this is a status request and not an analysis task # if (enroll): # args_bad = True # print('The command requires data (audio, image, or video) input.') if args . job : jobs = [] jobs . extend ( str . split ( args . job , ',' )) # checking for if tls is enabled and correctly set up secure_connect = False if args . secure : if ( args . certpath is None or args . keypass is None or args . cabundlepath is None or args . keypath is None ): args_bad = True print ( '--secure requires --certpath, --keypass, --keypath, and --cabundlepath' ) else : secure_connect = True if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) # Create the connection to the OLIVE server client = oc . AsyncOliveClient ( \"olivepy_workflow\" , args . server , args . port , args . timeout ) if not secure_connect : client . connect () else : client . secure_connect ( certfile = args . certpath , keyfile = args . keypath , password = args . keypass , ca_bundle_path = args . cabundlepath ) try : # right now, we only support analysis, so that is what we do... # first, create the workflow definition from the workflow file: owd = ow . OliveWorkflowDefinition ( args . workflow ) # Submit that workflow definition to the client for actualization (instantiation): workflow = owd . create_workflow ( client ) if args . print_jobs : # Print available jobs: print ( \"Enrollment jobs ' {} '\" . format ( workflow . get_enrollment_job_names ())) print ( \"Un-Enrollment jobs ' {} '\" . format ( workflow . get_unenrollment_job_names ())) # for enroll_job_name in workflow.get_enrollment_job_names(): # print(\"Enrollment job '{}' has Tasks: {}\".format(enroll_job_name, workflow.get_enrollment_tasks(enroll_job_name))) # for unenroll_job_name in workflow.get_unenrollment_job_names(): # print(\"Unenrollment job '{}' has Tasks: {}\".format(unenroll_job_name, workflow.get_unenrollment_tasks(unenroll_job_name))) if not args . job : if enroll : print ( \"Enrolling for all jobs: {} \" . format ( workflow . get_enrollment_job_names ())) jobs = workflow . get_enrollment_job_names () if unenroll : print ( \"Unenrolling for all job: {} \" . format ( workflow . get_unenrollment_job_names ())) jobs = workflow . get_unenrollment_job_names () if len ( data_input ) > 0 : enroll_jobs = workflow . get_enrollment_job_names () if enroll_jobs is None : print ( \"ERROR: This workflow has no jobs that support enrollment\" ) quit ( 1 ) for t in jobs : if t not in enroll_jobs : print ( \"Error: Job ' {} ' can not be enrolled via this workflow. Only jobs(s) ' {} ' support enrollment.\" . format ( t , enroll_jobs )) quit ( 1 ) enroll_buffers = {} for classid in data_input . keys (): for input_msg in data_input [ classid ]: if classid not in enroll_buffers : enroll_buffers [ classid ] = [] # buffers.append(workflow.package_workflow_input(input, expected_data_type)) enroll_buffers [ classid ] . append ( workflow . package_workflow_input ( input_msg , expected_data_type )) # if audio: # # NOT SUPPORTING PEM # # # if using_pem: # # for filename, channel_dict in list(data_input.items()): # # for channel, regions in list(channel_dict.items()): # # try: # # if channel is None: # # ch_label = 0 # # else: # # ch_label = int(channel) # # # # buffers.append(workflow.package_audio(filename, mode=audio_mode, label=os.path.basename(filename), # # annotations=regions, selected_channel=ch_label)) # # # # except Exception as e: # # print(\"Failed to parse regions from (PEM) input file: {}\".format(e)) # # quit(1) # elif text: # print(\"Text enrollment not supported\") # elif video: # print(\"clg adding video file: {}\".format(filename)) # enroll_buffers[classid].append( # workflow.package_binary(filename, mode=audio_mode, label=os.path.basename(filename))) # elif image: # enroll_buffers[classid].append( # workflow.package_image(filename, mode=audio_mode, label=os.path.basename(filename))) print ( \"Workflow {} results:\" . format ( action_str . lower ())) for classid in enroll_buffers . keys (): buffers = enroll_buffers [ classid ] print ( \"enrolling {} files for class: {} \" . format ( len ( buffers ), classid )) response = workflow . enroll ( buffers , classid , jobs ) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) elif unenroll : # TODO use options unenroll_jobs = workflow . get_unenrollment_job_names () if unenroll_jobs is None : print ( \"ERROR: This workflow has no job that support unenrollment\" ) quit ( 1 ) for t in jobs : if t not in unenroll_jobs : print ( \"Error: Job ' {} ' can not be un-enrolled via this workflow. Only job(s) ' {} ' support \" \"un-enrollment.\" . format ( t , unenroll_jobs )) quit ( 1 ) response = workflow . unenroll ( args . unenroll , jobs ) print ( \"Workflow {} results:\" . format ( action_str . lower ())) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) finally : client . disconnect () parse_pem_file ( data_lines ) Parse a PEM file, grouping the results by audio file and channel Parameters: Name Type Description Default data_lines required Returns: Type Description a dictionary of audio files to score and the channel region: {'filename': {channel: [(start_region, end_region)]} } Source code in olivepy/client/workflow_enroll_client.py def parse_pem_file ( data_lines ): ''' Parse a PEM file, grouping the results by audio file and channel :param data_lines: :return: a dictionary of audio files to score and the channel region: {'filename': {channel: [(start_region, end_region)]} } ''' # We process by file and channel - the class/label is ignored regions = {} input_pem = Pem () input_pem . add_records_from_data_lines ( data_lines ) for id in input_pem . get_ids (): audio_id = os . path . expandvars ( id ) # Create a dictionary of the regions specified for the the current file regions [ audio_id ] = {} for rec in input_pem . get_records ( id ): # channel could be a list... channels = [] if type ( rec . channel ) is str : # convert to a list channels = map ( int , str . split ( rec . channel , ',' )) elif type ( rec . channel ) is int : channels . append ( rec . channel ) else : print ( \"Unsupported channel value: {} \" . format ( rec . channel )) for ch in channels : if ch not in regions [ audio_id ]: regions [ audio_id ][ ch ] = [] regions [ audio_id ][ ch ] . append (( rec . start_t , rec . end_t )) return regions","title":"`olivepy` `client` module"},{"location":"olivepy-docs/client.html#olivepy-client-module","text":"","title":"olivepy client module"},{"location":"olivepy-docs/client.html#olivepyclientanalyze_client","text":"","title":"olivepy.client.analyze_client"},{"location":"olivepy-docs/client.html#olivepyclientclient_common","text":"","title":"olivepy.client.client_common"},{"location":"olivepy-docs/client.html#olivepy.client.client_common.extract_input_data","text":"Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server Parameters: Name Type Description Default args required fail_if_no_data True has_class_ids False Returns: Type Description Source code in olivepy/client/client_common.py def extract_input_data ( args , expected_data_type = OliveInputDataType . AUDIO_DATA_TYPE , fail_if_no_data = True , has_class_ids = False ): \"\"\" Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server :param args: :param fail_if_no_data: :param has_class_ids: :return data_input, transfer_mode, send_pathname: \"\"\" send_pathname = True if args . path else False send_serialized = not send_pathname data_input = [] if send_pathname : # fixme more generic for image and video? transfer_mode = InputTransferType . PATH else : transfer_mode = InputTransferType . SERIALIZED audio = text = image = video = False if hasattr ( args , 'upload_files' ) and args . upload_files and args . path : if args . input : uploaded_input = upload_files ( args . input , args . server , args . upload_port ) data_input = uploaded_input elif args . input_list : data_input = parse_data_list2 ( args . input_list , transfer_mode , expected_data_type , has_class_ids ) data_input = upload_files ( data_input , args . server , args . upload_port ) elif args . input_list : # parse input list to make sure we actually have one or more files... data_input = parse_data_list2 ( args . input_list , transfer_mode , expected_data_type , has_class_ids ) # data_intpu --> [{filename: DATA_MSG, {channel: [start, end, class]}] , filename: Audio, {-1: (None, None, None)} if len ( data_input ) == 0 : args_bad = True print ( \"Data input list ' {} ' contains no valid files\" . format ( args . input_list )) elif args . input : # Do we want to support sending a path that is not local? In case they want to specify a path that is a # available for the server if isinstance ( args . input , list ): for input in args . input : input_data = convert_filename_to_data ( input , transfer_mode , expected_data_type ) if input_data : if has_class_ids : # data_input.append( (convert_filename_to_data(args.input, transfer_mode, expected_data_type), args.enroll)) data_input = { args . enroll : [ input_data ]} else : data_input . append ( input_data ) else : input_data = convert_filename_to_data ( args . input , transfer_mode , expected_data_type ) if input_data : if has_class_ids : # data_input.append( (convert_filename_to_data(args.input, transfer_mode, expected_data_type), args.enroll)) data_input = { args . enroll : [ input_data ]} else : data_input . append ( input_data ) # do some basic validation # If mo data input supplied, make sure that is okay with the other options provided if not data_input and fail_if_no_data : print ( 'The command requires data input(s).' ) exit ( 1 ) return data_input , transfer_mode , send_pathname","title":"extract_input_data()"},{"location":"olivepy-docs/client.html#olivepy.client.client_common.prepare_input_data","text":"Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server Parameters: Name Type Description Default enroll Whether this is an enrollment None input The singular input None input_list The list of inputs None path Send pathnames rather than raw data, use with upload_files to upload using http if the files don't share a filesystem False server The server host 'localhost' upload_files Requires path to be True, controls whether to upload files using http, this allows for processing more than 2 GB of data and will be more memory efficient False upload_port 5000 fail_if_no_data True has_class_ids False Returns: Type Description Source code in olivepy/client/client_common.py def prepare_input_data ( enroll = None , input = None , input_list = None , path = False , server = \"localhost\" , upload_files = False , upload_port = 5000 , expected_data_type = OliveInputDataType . AUDIO_DATA_TYPE , fail_if_no_data = True , has_class_ids = False ): \"\"\" Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server :param enroll: Whether this is an enrollment :param input: The singular input :param input_list: The list of inputs :param path: Send pathnames rather than raw data, use with upload_files to upload using http if the files don't share a filesystem :param server: The server host :param upload_files: Requires path to be True, controls whether to upload files using http, this allows for processing more than 2 GB of data and will be more memory efficient :param upload_port: :param fail_if_no_data: :param has_class_ids: :return data_input, transfer_mode, send_pathname: \"\"\" args = {} args . enroll = enroll args . path = path args . upload_files = True if upload_files and path else False args . input = input args . input_list = input_list if input is None else None args . server = server args . upload_port = upload_port return extract_input_data ( args , expected_data_type , fail_if_no_data , has_class_ids )","title":"prepare_input_data()"},{"location":"olivepy-docs/client.html#olivepyclientenroll_client","text":"","title":"olivepy.client.enroll_client"},{"location":"olivepy-docs/client.html#olivepyclientlearn_client","text":"","title":"olivepy.client.learn_client"},{"location":"olivepy-docs/client.html#olivepyclientstatus_client","text":"","title":"olivepy.client.status_client"},{"location":"olivepy-docs/client.html#olivepy.client.status_client.heartbeat_notification","text":"Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server Source code in olivepy/client/status_client.py def heartbeat_notification ( heatbeat ): \"\"\"Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server\"\"\" if heatbeat : if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats print ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) print ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) print ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) print ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) print ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) print ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) print ( \"Number active jobs: \" + str ( stats . pool_busy )) print ( \"Number pending jobs: \" + str ( stats . pool_pending )) print ( \"Number finished jobs: \" + str ( stats . pool_finished )) print ( \"Max number jobs: \" + str ( stats . max_num_jobs )) print ( \"Server version: \" + str ( stats . server_version )) print ( \" \\n \" ) else : print ( \"No OLIVE heartbeat received. Olive server or connection down\" )","title":"heartbeat_notification()"},{"location":"olivepy-docs/client.html#olivepyclientutils_client","text":"","title":"olivepy.client.utils_client"},{"location":"olivepy-docs/client.html#olivepy.client.utils_client.main","text":"Returns: Type Description Source code in olivepy/client/utils_client.py def main (): \"\"\" :return: \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyutils' ) parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--save_as_text' , action = 'store' , help = 'Save the workflow to a JSON formatted text file having this name.' ) parser . add_argument ( '--save_as_binary' , action = 'store' , help = 'Save the workflow to a binary formatted workflow file.' ) parser . add_argument ( '--print_workflow' , action = 'store_true' , help = 'Print the workflow definition file info (before it is actualized/sent to server)' ) args_bad = False args = parser . parse_args () if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True if not ( args . save_as_text or args . print_workflow or args . save_as_binary ): args_bad = True print ( 'The command requires one or more tasks.' ) if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) try : # first, create the workflow definition from the workflow file: owd = ow . OliveWorkflowDefinition ( args . workflow ) if args . save_as_text : print ( \"Saving Workflow Definition ' {} ' as ' {} '\" . format ( args . workflow , args . save_as_text )) owd . _save_as_json ( args . save_as_text ) if args . save_as_binary : print ( \"Saving Workflow Definition ' {} ' as ' {} '\" . format ( args . workflow , args . save_as_binary )) owd . _save_as_binary ( args . save_as_binary ) if args . print_workflow : wdef_json = owd . to_json ( indent = 1 ) print ( \"Workflow Definition Task Info: {} \" . format ( wdef_json )) print ( \"\" ) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e ))","title":"main()"},{"location":"olivepy-docs/client.html#olivepyclientworkflow_client","text":"","title":"olivepy.client.workflow_client"},{"location":"olivepy-docs/client.html#olivepy.client.workflow_client.heartbeat_notification","text":"Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server Source code in olivepy/client/workflow_client.py def heartbeat_notification ( heatbeat ): \"\"\" Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server \"\"\" if heatbeat : if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats print ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) print ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) print ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) print ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) print ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) print ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) print ( \"Number active jobs: \" + str ( stats . pool_busy )) print ( \"Number pending jobs: \" + str ( stats . pool_pending )) print ( \"Number finished jobs: \" + str ( stats . pool_finished )) print ( \"Max number jobs: \" + str ( stats . max_num_jobs )) print ( \"Server version: \" + str ( stats . server_version )) print ( \" \\n \" ) else : print ( \"Too long since a heatbeat message was received. Olive server or connection down\" )","title":"heartbeat_notification()"},{"location":"olivepy-docs/client.html#olivepy.client.workflow_client.main","text":"Client for interacting with the workflow API Source code in olivepy/client/workflow_client.py def main (): \"\"\" Client for interacting with the workflow API \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyworkflow' , description = \"Perform OLIVE analysis using a Workflow \" \"Definition file\" ) # Required positional option parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--tasks' , action = 'store_true' , help = 'Print the workflow analysis tasks.' ) parser . add_argument ( '--print_class_ids' , action = 'store_true' , help = 'Print the class IDs available for analysis in the specified workflow.' ) parser . add_argument ( '--class_ids' , action = 'store' , help = 'Send class IDs with the analysis request.' ) parser . add_argument ( '--print_actualized' , action = 'store_true' , help = 'Print the actualized workflow info.' ) parser . add_argument ( '--print_workflow' , action = 'store_true' , help = 'Print the workflow definition file info (before it is actualized, if requested)' ) # # parser.add_argument('--print_options', action='store_true', # help='Print the options recognized for each task') parser . add_argument ( '-s' , '--server' , action = 'store' , default = 'localhost' , help = 'The machine the server is running on. Defaults to %(default)s .' ) parser . add_argument ( '-P' , '--port' , type = int , action = 'store' , default = 5588 , help = 'The port to use.' ) parser . add_argument ( '--upload_port' , type = int , action = 'store' , default = 5000 , help = 'The upload port to use when specifying --upload_files.' ) parser . add_argument ( '-t' , '--timeout' , type = int , action = 'store' , default = 300 , help = 'The timeout (in seconds) to wait for a response from the server ' ) parser . add_argument ( '-i' , '--input' , action = 'append' , help = 'The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag' ) parser . add_argument ( '--input_list' , action = 'store' , help = 'A list of files to analyze. One file per line.' ) parser . add_argument ( '--text' , action = 'store_true' , help = 'Indicates that input (or input list) is a literal text string to send in the analysis request.' ) parser . add_argument ( '--options' , action = 'store' , help = 'A JSON formatted string of workflow options such as ' '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}] or ' '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}, where the former ' 'options are only applied to the SAD task, and the later are applied to all tasks ' ) parser . add_argument ( '--path' , action = 'store_true' , help = 'Send the path of the audio instead of a buffer. ' 'Server and client must share a filesystem to use this option' ) parser . add_argument ( '--upload_files' , action = 'store_true' , help = 'Must be specified with --path argument. This uploads the files to the server so the client and server do not need to share a filesystem. This can also be used to bypass the 2 GB request size limitation.' ) parser . add_argument ( '--secure' , action = 'store_true' , help = 'Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set.' ) parser . add_argument ( '--certpath' , action = 'store' , help = 'Specifies the path of the certificate' ) parser . add_argument ( '--keypath' , action = 'store' , help = 'Specifies the path of the certificate key' ) parser . add_argument ( '--keypass' , action = 'store' , help = 'Specifies the certificate passphrase to unlock the encrypted certificate key' ) parser . add_argument ( '--cabundlepath' , action = 'store' , help = 'Specifies the path of the certificate authority' ) # # parser.add_argument('--heartbeat', action='store_true', # help='Listen for server heartbeats ') # parser.add_argument('--status', action='store_true', # help='get server status') parser . add_argument ( '--debug' , action = 'store_true' , help = 'Debug mode ' ) # Not supported since it needs an additional 3rd party lib: # parser.add_argument('--decoded', action='store_true', # help='Send audio file as decoded PCM16 samples instead of sending as serialized buffer. ' # 'Input file must be a wav file') args_bad = False args = parser . parse_args () # Simple logging config if args . debug : log_level = logging . DEBUG else : log_level = logging . INFO # log_level = logging.WARN logging . basicConfig ( level = log_level ) if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True if ( args . tasks or args . print_class_ids or args . print_actualized or args . print_workflow ): data_required = False else : data_required = True if args . upload_files and not args . path : print ( '--upload_files must be combined with --path' ) args_bad = True start = time . time () # Our workflow should consume one of the 4 data types (but not a combination of types) .... # data_input, audio_mode, send_pathname, audio, text, image, video = client_com.extract_input_data_type(args, fail_if_no_data=data_required) if args . text : # special case of handling text data expected_data_type = OliveInputDataType . TEXT_DATA_TYPE else : expected_data_type = OliveInputDataType . BINARY_DATA_TYPE # expected_data_type = OliveInputDataType.AUDIO_DATA_TYPE # if you only want to send audio data_input , transfer_mode , send_pathname = client_com . extract_input_data ( args , expected_data_type = expected_data_type , fail_if_no_data = data_required ) json_opts = None if args . options : if os . path . exists ( args . options ): with open ( args . options , 'r' ) as f : json_opts = f . read () else : json_opts = args . options print ( \"Using options: {} \" . format ( json_opts )) json_class_ids = None if args . class_ids : if os . path . exists ( args . class_ids ): with open ( args . class_ids , 'r' , encoding = 'utf-8' ) as f : json_class_ids = f . read () else : json_class_ids = args . class_ids print ( \"Using class_ids: {} \" . format ( json_class_ids )) # checking for if tls is enabled and correctly set up secure_connect = False if args . secure : if ( args . certpath is None or args . keypass is None or args . cabundlepath is None or args . keypath is None ): args_bad = True print ( '--secure requires --certpath, --keypass, --keypath, and --cabundlepath' ) else : secure_connect = True if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) enable_status_socket = False # Create the connection to the OLIVE server client = oc . AsyncOliveClient ( \"olivepy_workflow\" , args . server , args . port , args . timeout ) if not secure_connect : client . connect () else : client . secure_connect ( certfile = args . certpath , keyfile = args . keypath , password = args . keypass , ca_bundle_path = args . cabundlepath , monitor_status = enable_status_socket ) try : # if args.heartbeat: # # Register to be notified of heartbeats from the OLIVE server # client.add_heartbeat_listener(heartbeat_notification) # if args.status: # # Request the current server status # server_status_response = client.get_status() # if server_status_response.is_successful(): # print(\"OLIVE JSON Server status: {}\".format(server_status_response.to_json(indent=10))) # # # # Or you can access the GetStatusResult protobuf: # print(\"OLIVE Server status: pending: {}, busy: {}, finished: {}, version: {}\" # .format(server_status_response.get_response().num_pending, # server_status_response.get_response().num_busy, # server_status_response.get_response().num_finished, # server_status_response.get_response().version)) # first, create the workflow definition from the workflow file: workflow_def = ow . OliveWorkflowDefinition ( args . workflow ) if args . print_workflow : wdef_json = workflow_def . to_json ( indent = 1 ) print ( \"Workflow Definition: \\n {} \" . format ( wdef_json )) print ( \"\" ) # Submit that workflow definition to the client for actualization (instantiation): workflow = workflow_def . create_workflow ( client ) if args . print_actualized : # tasks_json = workflow.get_analysis_task_info() print ( \"Actualized Workflow: {} \" . format ( workflow . to_json ( indent = 1 ))) print ( \"\" ) if args . tasks : # Print the analysis tasks: print ( \"Analysis Tasks: {} \" . format ( workflow . get_analysis_tasks ())) for enroll_job_name in workflow . get_enrollment_job_names (): print ( \"Enrollment job ' {} ' has Tasks: {} \" . format ( enroll_job_name , workflow . get_enrollment_tasks ( enroll_job_name ))) for unenroll_job_name in workflow . get_unenrollment_job_names (): print ( \"Unenrollment job ' {} ' has Tasks: {} \" . format ( unenroll_job_name , workflow . get_unenrollment_tasks ( unenroll_job_name ))) if args . print_class_ids : # Print the class IDs available for the workflow tasks: # support other types: type=olive_pb2.WORKFLOW_ENROLLMENT_TYPE? class_status_response = workflow . get_analysis_class_ids () print ( \"Class Info: {} \" . format ( class_status_response . to_json ( indent = 1 ))) buffers = [] for input in data_input : buffers . append ( workflow . package_workflow_input ( input , expected_data_type )) if data_required : print ( \"Sending analysis request...\" ) response = workflow . analyze ( buffers , options = json_opts , class_ids = json_class_ids ) print ( \"Workflow analysis results:\" ) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) finally : client . disconnect () stop = time . time () print ( f 'took { stop - start } s' )","title":"main()"},{"location":"olivepy-docs/client.html#olivepyclientworkflow_enroll_client","text":"","title":"olivepy.client.workflow_enroll_client"},{"location":"olivepy-docs/client.html#olivepy.client.workflow_enroll_client.main","text":"Returns: Type Description Source code in olivepy/client/workflow_enroll_client.py def main (): \"\"\" :return: \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyworkflowenroll' , description = \"Perform OLIVE enrollment using a Workflow \" \"Definition file\" ) # parser.add_argument('-C', '--client-id', action='store', default='olivepy_', # help='Experimental: the client_id to use') # Required positional option parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--print_jobs' , action = 'store_true' , help = 'Print the supported workflow enrollment jobs.' ) parser . add_argument ( '--job' , action = 'store' , help = 'Enroll/Unenroll an Class ID for a job(s) in the specified workflow. If not specified enroll or unenroll for ALL enrollment/unenrollment jobst' ) parser . add_argument ( '--enroll' , action = 'store' , help = 'Enroll using this (class) name. Should be used with the job argument to specify a target job to enroll with (if there are more than one enrollment jobs) ' ) parser . add_argument ( '--unenroll' , action = 'store' , help = 'Enroll using this (class) name. Should be used with the job argument to specify a job to unenroll (if there are more than one unenrollment jobs)' ) parser . add_argument ( '-i' , '--input' , action = 'store' , help = 'The data input to enroll. Either a pathname to an audio/image/video file or a string for text input' ) parser . add_argument ( '--input_list' , action = 'store' , help = 'A list of files to enroll. One file per line plus the class id to enroll.' ) parser . add_argument ( '--path' , action = 'store_true' , help = 'Send the path of the audio instead of a buffer. ' 'Server and client must share a filesystem to use this option' ) # Connection arguments parser . add_argument ( '-s' , '--server' , action = 'store' , default = 'localhost' , help = 'The machine the server is running on. Defaults to %(default)s .' ) parser . add_argument ( '-P' , '--port' , type = int , action = 'store' , default = 5588 , help = 'The port to use.' ) parser . add_argument ( '-t' , '--timeout' , type = int , action = 'store' , default = 10 , help = 'The timeout (in seconds) to wait for a response from the server ' ) parser . add_argument ( '--secure' , action = 'store_true' , help = 'Indicates a secure connection should be made. Requires --certpath, --keypass, --keypath, and --cabundlepath to be set.' ) parser . add_argument ( '--certpath' , action = 'store' , help = 'Specifies the path of the certificate' ) parser . add_argument ( '--keypath' , action = 'store' , help = 'Specifies the path of the certificate key' ) parser . add_argument ( '--keypass' , action = 'store' , help = 'Specifies the certificate passphrase to unlock the encrypted certificate key' ) parser . add_argument ( '--cabundlepath' , action = 'store' , help = 'Specifies the path of the certificate authority' ) # not supporting batch enrollments: # parser.add_argument('--audio_list', action='store', # help='A list of audio files to analyze. One file per line') args_bad = False args = parser . parse_args () if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True require_data = True if args . unenroll or args . print_jobs : require_data = False expected_data_type = OliveInputDataType . BINARY_DATA_TYPE data_input , transfer_mode , send_pathname = client_com . extract_input_data ( args , expected_data_type = expected_data_type , fail_if_no_data = require_data , has_class_ids = True ) # if args.enroll: # # there must be only one input # if len(data_input) > 1: # args_bad = True # print(\"The enroll and audio_list argument are mutually exclusive. Pick one and run again\") # else: # data_input = [(data_input[0], args.enroll)] print ( \"enrolling {} files\" . format ( len ( data_input ))) # if len(data_input) > 1 and not audio: # args_bad = True # print(\"Non-audio files can not be enrolled from an input list\") # support other data types.... # audios = [] using_pem = False # TODO GET CLASS IDS FROM ENROLLMENT FILE enroll = False unenroll = False if args . enroll : action_str = \"Enrollment\" enroll = True if args . unenroll : print ( \"Enrollment and un-enrollment are mutually exclusive. Pick one and run again\" ) args_bad = True elif args . unenroll : action_str = \"Unenrollment\" unenroll = True elif len ( data_input ) > 1 : enroll = True elif not args . print_jobs : args_bad = True print ( \"Must use one of the options: --enroll, --unenroll, or --print_jobs \" ) action_str = \"\" # support enrollments from a file (list and/or PEM format)? # if not (audio or image or video): # # no input provided, make sure this is a status request and not an analysis task # if (enroll): # args_bad = True # print('The command requires data (audio, image, or video) input.') if args . job : jobs = [] jobs . extend ( str . split ( args . job , ',' )) # checking for if tls is enabled and correctly set up secure_connect = False if args . secure : if ( args . certpath is None or args . keypass is None or args . cabundlepath is None or args . keypath is None ): args_bad = True print ( '--secure requires --certpath, --keypass, --keypath, and --cabundlepath' ) else : secure_connect = True if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) # Create the connection to the OLIVE server client = oc . AsyncOliveClient ( \"olivepy_workflow\" , args . server , args . port , args . timeout ) if not secure_connect : client . connect () else : client . secure_connect ( certfile = args . certpath , keyfile = args . keypath , password = args . keypass , ca_bundle_path = args . cabundlepath ) try : # right now, we only support analysis, so that is what we do... # first, create the workflow definition from the workflow file: owd = ow . OliveWorkflowDefinition ( args . workflow ) # Submit that workflow definition to the client for actualization (instantiation): workflow = owd . create_workflow ( client ) if args . print_jobs : # Print available jobs: print ( \"Enrollment jobs ' {} '\" . format ( workflow . get_enrollment_job_names ())) print ( \"Un-Enrollment jobs ' {} '\" . format ( workflow . get_unenrollment_job_names ())) # for enroll_job_name in workflow.get_enrollment_job_names(): # print(\"Enrollment job '{}' has Tasks: {}\".format(enroll_job_name, workflow.get_enrollment_tasks(enroll_job_name))) # for unenroll_job_name in workflow.get_unenrollment_job_names(): # print(\"Unenrollment job '{}' has Tasks: {}\".format(unenroll_job_name, workflow.get_unenrollment_tasks(unenroll_job_name))) if not args . job : if enroll : print ( \"Enrolling for all jobs: {} \" . format ( workflow . get_enrollment_job_names ())) jobs = workflow . get_enrollment_job_names () if unenroll : print ( \"Unenrolling for all job: {} \" . format ( workflow . get_unenrollment_job_names ())) jobs = workflow . get_unenrollment_job_names () if len ( data_input ) > 0 : enroll_jobs = workflow . get_enrollment_job_names () if enroll_jobs is None : print ( \"ERROR: This workflow has no jobs that support enrollment\" ) quit ( 1 ) for t in jobs : if t not in enroll_jobs : print ( \"Error: Job ' {} ' can not be enrolled via this workflow. Only jobs(s) ' {} ' support enrollment.\" . format ( t , enroll_jobs )) quit ( 1 ) enroll_buffers = {} for classid in data_input . keys (): for input_msg in data_input [ classid ]: if classid not in enroll_buffers : enroll_buffers [ classid ] = [] # buffers.append(workflow.package_workflow_input(input, expected_data_type)) enroll_buffers [ classid ] . append ( workflow . package_workflow_input ( input_msg , expected_data_type )) # if audio: # # NOT SUPPORTING PEM # # # if using_pem: # # for filename, channel_dict in list(data_input.items()): # # for channel, regions in list(channel_dict.items()): # # try: # # if channel is None: # # ch_label = 0 # # else: # # ch_label = int(channel) # # # # buffers.append(workflow.package_audio(filename, mode=audio_mode, label=os.path.basename(filename), # # annotations=regions, selected_channel=ch_label)) # # # # except Exception as e: # # print(\"Failed to parse regions from (PEM) input file: {}\".format(e)) # # quit(1) # elif text: # print(\"Text enrollment not supported\") # elif video: # print(\"clg adding video file: {}\".format(filename)) # enroll_buffers[classid].append( # workflow.package_binary(filename, mode=audio_mode, label=os.path.basename(filename))) # elif image: # enroll_buffers[classid].append( # workflow.package_image(filename, mode=audio_mode, label=os.path.basename(filename))) print ( \"Workflow {} results:\" . format ( action_str . lower ())) for classid in enroll_buffers . keys (): buffers = enroll_buffers [ classid ] print ( \"enrolling {} files for class: {} \" . format ( len ( buffers ), classid )) response = workflow . enroll ( buffers , classid , jobs ) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) elif unenroll : # TODO use options unenroll_jobs = workflow . get_unenrollment_job_names () if unenroll_jobs is None : print ( \"ERROR: This workflow has no job that support unenrollment\" ) quit ( 1 ) for t in jobs : if t not in unenroll_jobs : print ( \"Error: Job ' {} ' can not be un-enrolled via this workflow. Only job(s) ' {} ' support \" \"un-enrollment.\" . format ( t , unenroll_jobs )) quit ( 1 ) response = workflow . unenroll ( args . unenroll , jobs ) print ( \"Workflow {} results:\" . format ( action_str . lower ())) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) finally : client . disconnect ()","title":"main()"},{"location":"olivepy-docs/client.html#olivepy.client.workflow_enroll_client.parse_pem_file","text":"Parse a PEM file, grouping the results by audio file and channel Parameters: Name Type Description Default data_lines required Returns: Type Description a dictionary of audio files to score and the channel region: {'filename': {channel: [(start_region, end_region)]} } Source code in olivepy/client/workflow_enroll_client.py def parse_pem_file ( data_lines ): ''' Parse a PEM file, grouping the results by audio file and channel :param data_lines: :return: a dictionary of audio files to score and the channel region: {'filename': {channel: [(start_region, end_region)]} } ''' # We process by file and channel - the class/label is ignored regions = {} input_pem = Pem () input_pem . add_records_from_data_lines ( data_lines ) for id in input_pem . get_ids (): audio_id = os . path . expandvars ( id ) # Create a dictionary of the regions specified for the the current file regions [ audio_id ] = {} for rec in input_pem . get_records ( id ): # channel could be a list... channels = [] if type ( rec . channel ) is str : # convert to a list channels = map ( int , str . split ( rec . channel , ',' )) elif type ( rec . channel ) is int : channels . append ( rec . channel ) else : print ( \"Unsupported channel value: {} \" . format ( rec . channel )) for ch in channels : if ch not in regions [ audio_id ]: regions [ audio_id ][ ch ] = [] regions [ audio_id ][ ch ] . append (( rec . start_t , rec . end_t )) return regions","title":"parse_pem_file()"},{"location":"olivepy-docs/messaging.html","text":"olivepy messaging module olivepy.messaging.msgutil Contains data structures that map message_type enum values to protobuf types and vice versa. Should be updated whenever new message types are added. AllowableErrorFromServer ( Exception ) This exception means that the server could not complete a request; however, the reason it could not do isn't considered an error. This special case most often occurs when requesting analysis of a submission that contains no speech, in which case the analysis could not complete since there was not speech and not due to an error running the plugin. Otherwise, this is identical to Python's plain old Exception AudioTransferType ( Enum ) The method used to send audio to the OLIVE server. There are three options for sending audio to the server: AUDIO_PATH: Send the path of the audio file to the server. NOTE: If using this option, the path must be accessible to the server AUDIO_DECODED: Send the audio as a buffer of decoded samples (PCM-16). This option is not well supported by this client since it does not AUDIO_SERIALIZED: Send the file as a binary buffer ExceptionFromServer ( Exception ) This exception means that an error occured on the server side, and this error is being sent \"up the chain\" on the client side. Otherwise, it is identical to Python's plain old Exception InputTransferType ( Enum ) The method used to send audio/data to the OLIVE server. There are three options for sending data to the server: PATH: Send the path of the audio file to the server. NOTE: If using this option, the path must be accessible to the server DECODED: Send the audio as a buffer of decoded samples (PCM-16). This option is not well supported by this client since it does not SERIALIZED: Send the file as a binary buffer OliveInputDataType ( Enum ) The type of input data send to the OLIVE server. package_audio ( audio_msg , audio_data , annotations = None , selected_channel = None , mode =< InputTransferType . PATH : 1 > , num_channels = None , sample_rate = None , num_samples = None , validate_local_path = True , label = None ) Parameters: Name Type Description Default audio_msg Audio the Olive Audio message to populate required audio_data ~AnyStr either a filename or binary buffer required annotations a list of tuple start/end regions (in seconds) None selected_channel if audio_data is multi-channel then select this channel for processing None mode the submission mode: pathname, serialized, samples <InputTransferType.PATH: 1> num_channels the number of channels in the audio None sample_rate the sample rate of the audio None num_samples the number of samples in the audio. None validate_local_path if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. True Returns: Type Description a valid Audio message Source code in olivepy/messaging/msgutil.py def package_audio ( audio_msg : Audio , audio_data : AnyStr , annotations = None , selected_channel = None , mode = InputTransferType . PATH , num_channels = None , sample_rate = None , num_samples = None , validate_local_path = True , label = None ): \"\"\" :param audio_msg: the Olive Audio message to populate :param audio_data: either a filename or binary buffer :param annotations: a list of tuple start/end regions (in seconds) :param selected_channel: if audio_data is multi-channel then select this channel for processing :param mode: the submission mode: pathname, serialized, samples :param num_channels: the number of channels in the audio :param sample_rate: the sample rate of the audio :param num_samples: the number of samples in the audio. :param validate_local_path: if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. :return: a valid Audio message :raises Exception if unable to package the audio for the specified mode. \"\"\" if mode != InputTransferType . PATH and mode != InputTransferType . DECODED and mode != InputTransferType . SERIALIZED : raise Exception ( 'Called package_audio with an unknown mode. Must be PATH, DECODED, or SERIALIZED.' ) # only supporting pathname now if mode == InputTransferType . PATH : if validate_local_path : if not os . path . exists ( audio_data ): raise Exception ( \"Error creating an OLIVE Audio message, the Audio file ' {} ' does not exist.\" . format ( audio_data )) audio_msg . path = audio_data else : audio_buffer = audio_msg . audioSamples if isinstance ( audio_data , bytes ): # audio has already been converted to a buffer... no need to change audio_buffer . data = audio_data else : # Assume we have a filename with we will serialize (decoded samples not supported) if mode != InputTransferType . SERIALIZED : raise Exception ( \"Converting ' {} ' into a decoded buffer is not supported. Client must \" \"manually decode the file and pass bytes to package_audio()\" . format ( audio_data )) buffer = serialize_file ( audio_data ) audio_buffer . data = buffer if mode == InputTransferType . SERIALIZED : # olive.proto says these are all ignored for serialized buffers: # channels, rate, bitdepth, channels audio_buffer . serialized_file = True if mode == InputTransferType . DECODED : # This mode assumes the client has passed in a numpy array of samples, but we don't assume numpy is # installed for all clients so we don't do checks in ths this code # Get the data as shorts: # not if audio_data.dtype.kind == np.dtype(np.integer).kind: # audio_data = audio_data.astype( np.int16 ).flatten().tolist() # raise Exception(\"Error: Transferring decoded samples not supported\") problem = '' if num_channels is None or num_channels == 0 : problem += 'channel ' if sample_rate is None or sample_rate == 0 : problem += 'sample_rate ' if num_samples is None or num_samples == 0 : problem += 'num_samples' if problem != '' : raise Exception ( 'Error: can not create an OLIVE audio message from decoded samples because missing required argument(s): {} ' . format ( problem )) audio_buffer . serialized_file = False audio_buffer . channels = num_channels audio_buffer . rate = sample_rate audio_buffer . samples = num_samples audio_buffer . bit_depth = BIT_DEPTH_16 audio_buffer . encoding = PCM16 if annotations : for a in annotations : # np.float32(a[0] would be better but can we assume numpy is installed? region = audio_msg . regions . add () region . start_t = a [ 0 ] region . end_t = a [ 1 ] if selected_channel : # we can't do much validation, but if they selected a channel and specified the number of channels if num_channels : if selected_channel > num_channels : raise Exception ( \"Error: can not select channel ' {} ' if audio only contains ' {} ' channel(s)\" . format ( selected_channel , num_channels )) if selected_channel < 1 : raise Exception ( \"Error: invalid value for selected channel ' {} '. Channel must be 1 or higher \" . format ( selected_channel )) audio_msg . selected_channel = selected_channel if label : audio_msg . label = label return audio_msg package_binary_media ( binary_media_msg , media_data , annotations = None , mode =< InputTransferType . PATH : 1 > , validate_local_path = True , label = None , selected_channel = None ) Parameters: Name Type Description Default binary_media_msg BinaryMedia the Olive BinaryMedia message to populate required media_data ~AnyStr either a filename or binary buffer required annotations a list of tuple start/end regions (in seconds) None mode the submission mode: pathname, serialized, samples <InputTransferType.PATH: 1> validate_local_path if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. True Returns: Type Description a valid Audio message Source code in olivepy/messaging/msgutil.py def package_binary_media ( binary_media_msg : BinaryMedia , media_data : AnyStr , annotations = None , mode = InputTransferType . PATH , validate_local_path = True , label = None , selected_channel = None ): \"\"\" :param binary_media_msg: the Olive BinaryMedia message to populate :param media_data: either a filename or binary buffer :param annotations: a list of tuple start/end regions (in seconds) :param mode: the submission mode: pathname, serialized, samples :param validate_local_path: if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. :return: a valid Audio message :raises Exception if unable to package the audio for the specified mode. \"\"\" print ( \"adding binary media\" ) # todo support selected channel (if audio is to be handled form this data), label, and annotations if mode != InputTransferType . PATH and mode != InputTransferType . SERIALIZED : raise Exception ( 'Called package_visual_media with an unknown mode. Must be AUDIO_PATH, or AUDIO_SERIALIZED.' ) # only supporting pathname now if mode == InputTransferType . PATH : if validate_local_path : if not os . path . exists ( media_data ): raise Exception ( \"Error creating an OLIVE media message, the Audio file ' {} ' does not exist.\" . format ( media_data )) binary_media_msg . path = media_data else : media_buffer = binary_media_msg . buffer if isinstance ( media_data , bytes ): # audio has already been converted to a buffer... no need to change media_buffer . data = media_data else : # Assume we have a filename with we will serialize (decoded samples not supported) if mode != InputTransferType . SERIALIZED : raise Exception ( \"Converting ' {} ' into a decoded buffer is not supported.\" . format ( media_data )) buffer = serialize_file ( media_data ) media_buffer . data = buffer if label : binary_media_msg . label = label if selected_channel : binary_media_msg . selected_channel = selected_channel if annotations : classic_region = binary_media_msg . regions . add () for a in annotations : # np.float32(a[0] would be better but can we assume numpy is installed? region = classic_region . regions . add () # print(\"Adding region: {} to {}\".format(a[0], a[1])) region . start_t = a [ 0 ] region . end_t = a [ 1 ] return binary_media_msg serialize_file ( filename ) Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters: Name Type Description Default filename str the local path to the file to serialize required Returns: Type Description ~AnyStr the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples Source code in olivepy/messaging/msgutil.py def serialize_file ( filename : str ) -> AnyStr : \"\"\" Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() \\ when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized :param filename: the local path to the file to serialize :return: the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. \\ This buffer contains the raw content of the file, it does NOT contain encoded samples \"\"\" if not os . path . exists ( os . path . expanduser ( filename )): raise Exception ( \"Error serializing an audio file, the file ' {} ' does not exist.\" . format ( filename )) with open ( os . path . expanduser ( filename ), 'rb' ) as f : serialized_buffer = f . read () # return the buffer return serialized_buffer olivepy.messaging.olive_pb2 olivepy.messaging.response OliveClassStatusResponse ( OliveServerResponse ) The container/wrapper for WorkflowClassStatusResult from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server. get_workflow_type ( self ) Return the type of workflow done in this response (analysis, enrollment, adaptation) Returns: Type Description A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped Source code in olivepy/messaging/response.py def get_workflow_type ( self ): return WORKFLOW_ANALYSIS_TYPE parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) if self . is_error (): return status_result = [] for jc in self . _response . job_class : job_name = jc . job_name job_dict = dict () self . _job_names . add ( job_name ) job_dict [ KEY_JOB_NAME ] = job_name # we have a list of data items: job_dict [ KEY_TASkS ] = {} status_result . append ( job_dict ) for task_class in jc . task : task_class_dict = json . loads ( MessageToJson ( task_class , preserving_proto_field_name = True )) del task_class_dict [ 'task_name' ] if task_class . task_name not in job_dict [ KEY_TASkS ]: job_dict [ KEY_TASkS ][ task_class . task_name ] = [] job_dict [ KEY_TASkS ][ task_class . task_name ] . append ( task_class_dict ) #job_dict[KEY_TASkS].append({task_class.task_name: task_class_dict}) self . _json_result = status_result to_json ( self , indent = None ) Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" # consider setting preserving_proto_field_name to true # return json.dumps(json.loads(MessageToJson(self._response, preserving_proto_field_name=True)), indent=indent) if self . is_error (): return self . get_error () if indent and indent < 0 : return json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )) return json . dumps ( json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )), indent = indent , ensure_ascii = False ) OliveServerResponse The default container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server. get_response ( self ) The Protobuf message returned from the OLIVE server Returns: Type Description Source code in olivepy/messaging/response.py def get_response ( self ): \"\"\" The Protobuf message returned from the OLIVE server :return: \"\"\" # todo exception if none? return self . _response get_workflow_type ( self ) Return the type of workflow done in this response (analysis, enrollment, adaptation) Returns: Type Description A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped Source code in olivepy/messaging/response.py def get_workflow_type ( self ): \"\"\" Return the type of workflow done in this response (analysis, enrollment, adaptation) :return: A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped \"\"\" if not self . _response : raise Exception ( \"No valid response\" ) if isinstance ( self . _request , WorkflowAnalysisRequest ): return WORKFLOW_ANALYSIS_TYPE elif isinstance ( self . _request , WorkflowEnrollRequest ): return WORKFLOW_ENROLLMENT_TYPE elif isinstance ( self . _request , WorkflowAdaptRequest ): return WORKFLOW_ADAPT_TYPE elif isinstance ( self . _request , WorkflowUnenrollRequest ): return WORKFLOW_UNENROLLMENT_TYPE raise Exception ( \"Unknown Workflow Message: {} \" . format ( type ( self . _request ))) parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): \"\"\" Create this response from the :param request: :param response: :param message: :return: \"\"\" self . _request = request if message : # No results due to error self . _iserror = True self . _message = message # self._request = request if response is not None : try : if response . HasField ( \"error\" ): self . _iserror = True self . _message = response . error else : # we assume no errors self . _issuccessful = True except : # Some messages have no error field self . _issuccessful = True self . _response = response to_json ( self , indent = None ) Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the response as as JSON string: \"\"\" #consider setting preserving_proto_field_name to true if indent and indent < 0 : return json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )) return json . dumps ( json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )), indent = indent , ensure_ascii = False ) OliveWorkflowActualizedResponse ( OliveServerResponse ) Extracts info from an actualized workflow definition get_analysis_jobs ( self ) Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. Returns: Type Description a list of job names in the analysis Source code in olivepy/messaging/response.py def get_analysis_jobs ( self ): \"\"\" Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. :return: a list of job names in the analysis \"\"\" return [ job_dict [ KEY_JOB_NAME ] for job_dict in self . _json_result ] #todo get analysis job name(s) get_request_jobs ( self , workflow_type ) return the jobs in the original request for the specified analysis type Parameters: Name Type Description Default workflow_type the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) required Returns: Type Description the list of jobs for this type Source code in olivepy/messaging/response.py def get_request_jobs ( self , workflow_type ): \"\"\" return the jobs in the original request for the specified analysis type :param workflow_type: the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) :return: the list of jobs for this type \"\"\" if self . _request is not None : return get_workflow_jobs ( self . _request . workflow_definition , workflow_type ) raise Exception ( \"No jobs for the requested workflow type: {} \" . format ( workflow_type )) is_allowable_error ( self ) Returns: Type Description true if this response failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this response failed with an allowable error \"\"\" return self . _isallowable_error parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will walk the tree and deserialize any encoded messages if self . is_error (): # todo provide error info in JSON? return # make a new message for this type... if not isinstance ( self . _request , WorkflowActualizeRequest ): # we received an some other workflow message so se don't need to convert it to json... return # we only parse the analyze part now analysis_task = [] workflow_analysis_order_msg = None for order in self . _response . workflow . order : if order . workflow_type == WORKFLOW_ANALYSIS_TYPE : workflow_analysis_order_msg = order break if workflow_analysis_order_msg is None : # no analysis results return # for job in self._response.job_result: for job in workflow_analysis_order_msg . job_definition : # create a dictionary for each job result job_dict = dict () job_name = job . job_name self . _job_names . add ( job_name ) # and a dictionary of tasks: # job_dict[KEY_TASkS] = {} # add to our results - in most cases we will have just one job analysis_task . append ( job_dict ) # get data handling info for this job data_prop = job . data_properties job_dict [ 'Data Input' ] = json . loads ( MessageToJson ( data_prop , preserving_proto_field_name = True )) # if data_prop.mode == SPLIT: # # Hack to make split/mulit-channel mode more clear # job_dict['data']['mode'] = 'SPLIT: Process each channel as a job' for task in job . tasks : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # Deserialize message_data, and replace it in the task_result_dict task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ KEY_JOB_NAME ] = job_name task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) del task_result_dict [ 'message_data' ] job_dict [ task . consumer_result_label ] = task_result_dict self . _json_result = analysis_task to_json ( self , indent = None ) Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False ) # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=False)), indent=indent) OliveWorkflowAnalysisResponse ( OliveServerResponse ) The default container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server. get_analysis_jobs ( self ) Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. Returns: Type Description a list of job names in the analysis Source code in olivepy/messaging/response.py def get_analysis_jobs ( self ): \"\"\" Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. :return: a list of job names in the analysis \"\"\" return [ job_dict [ KEY_JOB_NAME ] for job_dict in self . _json_result ] #todo get analysis job name(s) get_analysis_task_result ( self , job_name , task_name ) Get the result(s) for the specified job_name and task_name, also include the data used for this task. If the workflow analyzes each channel in multi-channel data then there can be multiple jobs with the same name. Parameters: Name Type Description Default job_name for convenience can be None, since there is normally only one job. But if the workflow has multiple jobs then a valid name must be specified. required task_name the name to the task required Returns: Type Description a list of dictionaries, where each dictionary in the list includes the results for the specified task and a list of the data analyzed by this task, such as [ {task_name:{}, data:[] }] Source code in olivepy/messaging/response.py def get_analysis_task_result ( self , job_name , task_name ): \"\"\" Get the result(s) for the specified job_name and task_name, also include the data used for this task. If the workflow analyzes each channel in multi-channel data then there can be multiple jobs with the same name. :param job_name: for convenience can be None, since there is normally only one job. But if the workflow has multiple jobs then a valid name must be specified. :param task_name: the name to the task :return: a list of dictionaries, where each dictionary in the list includes the results for the specified task and a list of the data analyzed by this task, such as [ {task_name:{}, data:[] }] \"\"\" if job_name is None : job_name = self . _get_default_job_name () results = [] for job_dict in self . _json_result : if job_name == job_dict [ KEY_JOB_NAME ]: task_dict = dict () # there may be one or more result for task_name task_dict [ task_name ] = job_dict [ KEY_TASkS ][ task_name ] task_dict [ KEY_DATA ] = job_dict [ KEY_DATA ] results . append ( task_dict ) return results get_request_jobs ( self , workflow_type ) return the jobs in the original request for the specified analysis type Parameters: Name Type Description Default workflow_type the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) required Returns: Type Description the list of jobs for this type Source code in olivepy/messaging/response.py def get_request_jobs ( self , workflow_type ): \"\"\" return the jobs in the original request for the specified analysis type :param workflow_type: the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) :return: the list of jobs for this type \"\"\" if self . _request is not None : return get_workflow_jobs ( self . _request . workflow_definition , workflow_type ) raise Exception ( \"No jobs for the requested workflow type: {} \" . format ( workflow_type )) is_allowable_error ( self ) Returns: Type Description true if this response failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this response failed with an allowable error \"\"\" return self . _isallowable_error parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will walk the tree and deserialize any encoded messages if self . is_error (): self . _json_result = {} self . _json_result [ 'error' ] = self . get_error () return # make a new message for this type... if isinstance ( self . _request , WorkflowActualizeRequest ): # we received an actualized workflow, so se don't need to convert it to json... return # this should only contain an analysis request... but check just in case: wk_type = self . get_workflow_type () if wk_type == WORKFLOW_ANALYSIS_TYPE or wk_type == WORKFLOW_ENROLLMENT_TYPE or wk_type == WORKFLOW_UNENROLLMENT_TYPE : # analysis is a list of dictionary elements, which looks like: [ {job_name: X, data: [], tasks: {}} ] # there is a dictionary for each job, but due to the way jobs work in OLIVE for mulit-channel data we # consider a jobs to be unique by a combination of job_name plus data, so multiples dictionary elements may # have the same job_name, but will have different data properties (channel numbers) # get the analysis job order from the original request: analysis_result = [] # or enrollment/unenrollment result # analysis_result['jobs'] = [] # analysis_result['data inputs'] = [] job_requests = get_workflow_jobs ( self . _request . workflow_definition , wk_type ) for job in self . _response . job_result : # create a dictionary for each job result job_dict = dict () job_name = job . job_name self . _job_names . add ( job_name ) # job_dict[job_name] = dict() job_dict [ KEY_JOB_NAME ] = job_name if job . error : job_dict [ 'error' ] = job . error # we have a list of data items: job_dict [ KEY_DATA ] = [] # and a dictionary of tasks: (although note it is possible to have multiple tasks with the same name, so a task has a list of results) job_dict [ KEY_TASkS ] = {} # add to our results - in most cases we will have just one job analysis_result . append ( job_dict ) # get the tasks for the current (and likely, only) job: task_requests = get_workflow_job_tasks ( job_requests , job_name ) # task_result_dict = dict() # I don't think this can happen yet: # if job.HasField('error'): # Allowable job error # for task in job . task_results : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # check if this task failed with an error if task . HasField ( 'error' ): # Allowable error if KEY_ERROR in job_dict : job_dict [ KEY_ERROR ] = job_dict [ KEY_ERROR ] + \",\" + task . error else : job_dict [ KEY_ERROR ] = task . error self . _isallowable_error = True # should have an empty message data; del task_result_dict [ 'message_data' ] if job_name not in self . _allowable_failed_job_tasks : self . _allowable_failed_job_tasks [ job_name ] = [] self . _allowable_failed_job_tasks [ job_name ] . append ( task . task_name ) else : # Deserialize message_data, and replace it in the task_result_dict if task . message_type in msgutil . debug_message_map : # Get the pimiento message (debug only - these messages are not guaranteed to be supported print ( \"CLG special msg type: {} \" . format ( msgutil . MessageType . Name ( task . message_type ))) pimiento_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) if task . message_type == DATA_OUTPUT_TRANSFORMER_RESULT : if pimiento_msg . data_type == TEXT : # only supported type for now... pie_data_msg = WorkflowTextResult () pie_data_msg . ParseFromString ( pimiento_msg . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( pie_data_msg , preserving_proto_field_name = True )) else : print ( \"Unsupported debug message type: {} \" . format ( msgutil . InputDataType . Name ( pimiento_msg . data_type ))) elif task . message_type == SCORE_OUTPUT_TRANSFORMER_RESULT : # these should be standard trait message pie_score_msg = self . _extract_serialized_message ( pimiento_msg . message_type , pimiento_msg . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( pie_score_msg , preserving_proto_field_name = True )) else : task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) # Should we create special handlers for analysis results, like we sort global score results, # but what should be do with an AUDIO_MODIFICATION_RESULT? if task . message_type == GLOBAL_SCORER_RESULT : # Sort region scores task_result_dict [ 'analysis' ][ 'score' ] = sorted ( task_result_dict [ 'analysis' ][ 'score' ], key = sort_global_scores , reverse = True ) # messageData has been replaced with the actual task del task_result_dict [ 'message_data' ] # taskName is the key, so remove it: del task_result_dict [ 'task_name' ] # check if we need to add the plugin/domain name if task . task_name in task_requests : orig_task = task_requests [ task . task_name ] if orig_task . message_type in msgutil . plugin_message_map : # Get the original task task_req_msg = self . _extract_serialized_message ( orig_task . message_type , orig_task . message_data ) task_result_dict [ 'plugin' ] = task_req_msg . plugin task_result_dict [ 'domain' ] = task_req_msg . domain if orig_task . message_type == CLASS_MODIFICATION_REQUEST or orig_task . message_type == CLASS_REMOVAL_REQUEST : # add class ID task_result_dict [ 'class_id' ] = self . _request . class_id # print(\"adding {} as {}\".format(task_result_dict, task.task_name)) # fixme: there can be multiple taks with the same name if conditions in a workflow caused the task to be ran twice if task . task_name not in job_dict [ KEY_TASkS ]: job_dict [ KEY_TASkS ][ task . task_name ] = [] job_dict [ KEY_TASkS ][ task . task_name ] . append ( task_result_dict ) #job_dict[KEY_TASkS].append({task.task_name: task_result_dict}) # A job usually has one data input/output, but we handle as if there are multiple for data_result in job . data_results : data_result_dict = json . loads ( MessageToJson ( data_result , preserving_proto_field_name = True )) # Deserialize the data portion data_type_msg = self . _extract_serialized_message ( data_result . msg_type , data_result . result_data ) del data_result_dict [ 'result_data' ] # del data_result_dict['dataId'] # redundant into, used as the key # data_result_dict['data'] = json.loads(MessageToJson(data_type_msg)) data_result_dict . update ( json . loads ( MessageToJson ( data_type_msg , preserving_proto_field_name = True ))) job_dict [ KEY_DATA ] . append ( data_result_dict ) # analysis_result['data inputs'].append(data_result_dict) self . _json_result = analysis_result to_json ( self , indent = None ) Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" # return json.dumps(self._json_result, indent=indent, ensure_ascii=False) if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=True)), indent=indent, ensure_ascii=False) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False ) OliveWorkflowEnrollmentResponse ( OliveServerResponse ) The container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient) for enrollment. This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server. is_allowable_error ( self ) Returns: Type Description true if this message failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this message failed with an allowable error \"\"\" return self . _isallowable_error parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will the tree and deserialize any encoded messages if self . is_error (): # todo provide error info in JSON return # make a new message for this type... if isinstance ( self . _request , WorkflowActualizeRequest ): # we received an actualized workflow, so se don't need to convert it to json... return # this should only contain an analysis request... but check just in case: type = self . get_workflow_type () if type == WORKFLOW_ENROLLMENT_TYPE : # not much info in an enrollment response... enroll_result = dict () if self . _response . HasField ( 'error' ): enroll_result [ 'error' ] = self . _response . error else : enroll_result [ 'successful' ] = True self . _json_result = enroll_result to_json ( self , indent = None ) Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" #consider setting preserving_proto_field_name to true # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=True)), indent=indent) if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False ) get_workflow_job_names ( workflow_definition , workflow_type ) parse a workflow definition, returning a list of job definition name (job_name) return [job_name] for the requested workflow_type Source code in olivepy/messaging/response.py def get_workflow_job_names ( workflow_definition , workflow_type ): \"\"\" parse a workflow definition, returning a list of job definition name (job_name) return [job_name] for the requested workflow_type \"\"\" rtn_job_names = list () if workflow_definition is not None : for order in workflow_definition . order : if order . workflow_type == workflow_type : for job in order . job_definition : rtn_job_names . append ( job . job_name ) return rtn_job_names get_workflow_job_tasks ( jobs , job_name = None ) Fetch the tasks from a job Parameters: Name Type Description Default jobs a dictionary of WorkflowTasks, indexed by a job names required job_name find tasks that belong to a job having this name. This can be None if there is only one job None Returns: Type Description a dictionary of WorkflowTask indexed by the task's consumer_result_label for the specified job. An exception is thrown if there are multiple jobs but no job_name was specified Source code in olivepy/messaging/response.py def get_workflow_job_tasks ( jobs , job_name = None ): \"\"\" Fetch the tasks from a job :param jobs: a dictionary of WorkflowTasks, indexed by a job names :param job_name: find tasks that belong to a job having this name. This can be None if there is only one job :return: a dictionary of WorkflowTask indexed by the task's consumer_result_label for the specified job. An exception is thrown if there are multiple jobs but no job_name was specified \"\"\" if job_name is None : if len ( jobs ) == 1 : # hack to make it easier to fetch data since most workflows only have one job job_name = list ( jobs . keys ())[ 0 ] else : raise Exception ( \"Must specify a job name when there are multiple JobDefinitions in a Workflow\" ) rtn_tasks = dict () # and job_name is None: if job_name in jobs : for workflow_task in jobs [ job_name ]: rtn_tasks [ workflow_task . consumer_result_label ] = workflow_task # CLG: I don't think we need to generate a message/warning if a task name isn't in the original jobs since we can # have dynamic jobs and with streaming jobs are very dynamic # else: # print(\"Job '{}' not one of the expected job names: {}\".format(job_name, list(jobs.keys()))) return rtn_tasks get_workflow_jobs ( workflow_definition , workflow_type ) parse a workflow definition, returning a dictionary indexed job (definition) name (job_name) and a list of WorkflowTask elements. Parameters: Name Type Description Default workflow_definition find jobs in this workflow definition required workflow_type the type of workflow order (analysis, enrollment, unenrollment) return {job_name: [WorkflowTask]} for the requested workflow_type required Source code in olivepy/messaging/response.py def get_workflow_jobs ( workflow_definition , workflow_type ): \"\"\" parse a workflow definition, returning a dictionary indexed job (definition) name (job_name) and a list of \\ WorkflowTask elements. :param workflow_definition: find jobs in this workflow definition :param workflow_type: the type of workflow order (analysis, enrollment, unenrollment) return {job_name: [WorkflowTask]} for the requested workflow_type \"\"\" rtn_jobs = dict () if workflow_definition is not None : for order in workflow_definition . order : if order . workflow_type == workflow_type : for job in order . job_definition : rtn_jobs [ job . job_name ] = job . tasks return rtn_jobs","title":"`olivepy` `messaging` module"},{"location":"olivepy-docs/messaging.html#olivepy-messaging-module","text":"","title":"olivepy messaging module"},{"location":"olivepy-docs/messaging.html#olivepymessagingmsgutil","text":"Contains data structures that map message_type enum values to protobuf types and vice versa. Should be updated whenever new message types are added.","title":"olivepy.messaging.msgutil"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.AllowableErrorFromServer","text":"This exception means that the server could not complete a request; however, the reason it could not do isn't considered an error. This special case most often occurs when requesting analysis of a submission that contains no speech, in which case the analysis could not complete since there was not speech and not due to an error running the plugin. Otherwise, this is identical to Python's plain old Exception","title":"AllowableErrorFromServer"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.AudioTransferType","text":"The method used to send audio to the OLIVE server. There are three options for sending audio to the server: AUDIO_PATH: Send the path of the audio file to the server. NOTE: If using this option, the path must be accessible to the server AUDIO_DECODED: Send the audio as a buffer of decoded samples (PCM-16). This option is not well supported by this client since it does not AUDIO_SERIALIZED: Send the file as a binary buffer","title":"AudioTransferType"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.ExceptionFromServer","text":"This exception means that an error occured on the server side, and this error is being sent \"up the chain\" on the client side. Otherwise, it is identical to Python's plain old Exception","title":"ExceptionFromServer"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.InputTransferType","text":"The method used to send audio/data to the OLIVE server. There are three options for sending data to the server: PATH: Send the path of the audio file to the server. NOTE: If using this option, the path must be accessible to the server DECODED: Send the audio as a buffer of decoded samples (PCM-16). This option is not well supported by this client since it does not SERIALIZED: Send the file as a binary buffer","title":"InputTransferType"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.OliveInputDataType","text":"The type of input data send to the OLIVE server.","title":"OliveInputDataType"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.package_audio","text":"Parameters: Name Type Description Default audio_msg Audio the Olive Audio message to populate required audio_data ~AnyStr either a filename or binary buffer required annotations a list of tuple start/end regions (in seconds) None selected_channel if audio_data is multi-channel then select this channel for processing None mode the submission mode: pathname, serialized, samples <InputTransferType.PATH: 1> num_channels the number of channels in the audio None sample_rate the sample rate of the audio None num_samples the number of samples in the audio. None validate_local_path if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. True Returns: Type Description a valid Audio message Source code in olivepy/messaging/msgutil.py def package_audio ( audio_msg : Audio , audio_data : AnyStr , annotations = None , selected_channel = None , mode = InputTransferType . PATH , num_channels = None , sample_rate = None , num_samples = None , validate_local_path = True , label = None ): \"\"\" :param audio_msg: the Olive Audio message to populate :param audio_data: either a filename or binary buffer :param annotations: a list of tuple start/end regions (in seconds) :param selected_channel: if audio_data is multi-channel then select this channel for processing :param mode: the submission mode: pathname, serialized, samples :param num_channels: the number of channels in the audio :param sample_rate: the sample rate of the audio :param num_samples: the number of samples in the audio. :param validate_local_path: if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. :return: a valid Audio message :raises Exception if unable to package the audio for the specified mode. \"\"\" if mode != InputTransferType . PATH and mode != InputTransferType . DECODED and mode != InputTransferType . SERIALIZED : raise Exception ( 'Called package_audio with an unknown mode. Must be PATH, DECODED, or SERIALIZED.' ) # only supporting pathname now if mode == InputTransferType . PATH : if validate_local_path : if not os . path . exists ( audio_data ): raise Exception ( \"Error creating an OLIVE Audio message, the Audio file ' {} ' does not exist.\" . format ( audio_data )) audio_msg . path = audio_data else : audio_buffer = audio_msg . audioSamples if isinstance ( audio_data , bytes ): # audio has already been converted to a buffer... no need to change audio_buffer . data = audio_data else : # Assume we have a filename with we will serialize (decoded samples not supported) if mode != InputTransferType . SERIALIZED : raise Exception ( \"Converting ' {} ' into a decoded buffer is not supported. Client must \" \"manually decode the file and pass bytes to package_audio()\" . format ( audio_data )) buffer = serialize_file ( audio_data ) audio_buffer . data = buffer if mode == InputTransferType . SERIALIZED : # olive.proto says these are all ignored for serialized buffers: # channels, rate, bitdepth, channels audio_buffer . serialized_file = True if mode == InputTransferType . DECODED : # This mode assumes the client has passed in a numpy array of samples, but we don't assume numpy is # installed for all clients so we don't do checks in ths this code # Get the data as shorts: # not if audio_data.dtype.kind == np.dtype(np.integer).kind: # audio_data = audio_data.astype( np.int16 ).flatten().tolist() # raise Exception(\"Error: Transferring decoded samples not supported\") problem = '' if num_channels is None or num_channels == 0 : problem += 'channel ' if sample_rate is None or sample_rate == 0 : problem += 'sample_rate ' if num_samples is None or num_samples == 0 : problem += 'num_samples' if problem != '' : raise Exception ( 'Error: can not create an OLIVE audio message from decoded samples because missing required argument(s): {} ' . format ( problem )) audio_buffer . serialized_file = False audio_buffer . channels = num_channels audio_buffer . rate = sample_rate audio_buffer . samples = num_samples audio_buffer . bit_depth = BIT_DEPTH_16 audio_buffer . encoding = PCM16 if annotations : for a in annotations : # np.float32(a[0] would be better but can we assume numpy is installed? region = audio_msg . regions . add () region . start_t = a [ 0 ] region . end_t = a [ 1 ] if selected_channel : # we can't do much validation, but if they selected a channel and specified the number of channels if num_channels : if selected_channel > num_channels : raise Exception ( \"Error: can not select channel ' {} ' if audio only contains ' {} ' channel(s)\" . format ( selected_channel , num_channels )) if selected_channel < 1 : raise Exception ( \"Error: invalid value for selected channel ' {} '. Channel must be 1 or higher \" . format ( selected_channel )) audio_msg . selected_channel = selected_channel if label : audio_msg . label = label return audio_msg","title":"package_audio()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.package_binary_media","text":"Parameters: Name Type Description Default binary_media_msg BinaryMedia the Olive BinaryMedia message to populate required media_data ~AnyStr either a filename or binary buffer required annotations a list of tuple start/end regions (in seconds) None mode the submission mode: pathname, serialized, samples <InputTransferType.PATH: 1> validate_local_path if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. True Returns: Type Description a valid Audio message Source code in olivepy/messaging/msgutil.py def package_binary_media ( binary_media_msg : BinaryMedia , media_data : AnyStr , annotations = None , mode = InputTransferType . PATH , validate_local_path = True , label = None , selected_channel = None ): \"\"\" :param binary_media_msg: the Olive BinaryMedia message to populate :param media_data: either a filename or binary buffer :param annotations: a list of tuple start/end regions (in seconds) :param mode: the submission mode: pathname, serialized, samples :param validate_local_path: if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. :return: a valid Audio message :raises Exception if unable to package the audio for the specified mode. \"\"\" print ( \"adding binary media\" ) # todo support selected channel (if audio is to be handled form this data), label, and annotations if mode != InputTransferType . PATH and mode != InputTransferType . SERIALIZED : raise Exception ( 'Called package_visual_media with an unknown mode. Must be AUDIO_PATH, or AUDIO_SERIALIZED.' ) # only supporting pathname now if mode == InputTransferType . PATH : if validate_local_path : if not os . path . exists ( media_data ): raise Exception ( \"Error creating an OLIVE media message, the Audio file ' {} ' does not exist.\" . format ( media_data )) binary_media_msg . path = media_data else : media_buffer = binary_media_msg . buffer if isinstance ( media_data , bytes ): # audio has already been converted to a buffer... no need to change media_buffer . data = media_data else : # Assume we have a filename with we will serialize (decoded samples not supported) if mode != InputTransferType . SERIALIZED : raise Exception ( \"Converting ' {} ' into a decoded buffer is not supported.\" . format ( media_data )) buffer = serialize_file ( media_data ) media_buffer . data = buffer if label : binary_media_msg . label = label if selected_channel : binary_media_msg . selected_channel = selected_channel if annotations : classic_region = binary_media_msg . regions . add () for a in annotations : # np.float32(a[0] would be better but can we assume numpy is installed? region = classic_region . regions . add () # print(\"Adding region: {} to {}\".format(a[0], a[1])) region . start_t = a [ 0 ] region . end_t = a [ 1 ] return binary_media_msg","title":"package_binary_media()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.serialize_file","text":"Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters: Name Type Description Default filename str the local path to the file to serialize required Returns: Type Description ~AnyStr the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples Source code in olivepy/messaging/msgutil.py def serialize_file ( filename : str ) -> AnyStr : \"\"\" Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() \\ when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized :param filename: the local path to the file to serialize :return: the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. \\ This buffer contains the raw content of the file, it does NOT contain encoded samples \"\"\" if not os . path . exists ( os . path . expanduser ( filename )): raise Exception ( \"Error serializing an audio file, the file ' {} ' does not exist.\" . format ( filename )) with open ( os . path . expanduser ( filename ), 'rb' ) as f : serialized_buffer = f . read () # return the buffer return serialized_buffer","title":"serialize_file()"},{"location":"olivepy-docs/messaging.html#olivepymessagingolive_pb2","text":"","title":"olivepy.messaging.olive_pb2"},{"location":"olivepy-docs/messaging.html#olivepymessagingresponse","text":"","title":"olivepy.messaging.response"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveClassStatusResponse","text":"The container/wrapper for WorkflowClassStatusResult from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server.","title":"OliveClassStatusResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveClassStatusResponse.get_workflow_type","text":"Return the type of workflow done in this response (analysis, enrollment, adaptation) Returns: Type Description A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped Source code in olivepy/messaging/response.py def get_workflow_type ( self ): return WORKFLOW_ANALYSIS_TYPE","title":"get_workflow_type()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveClassStatusResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) if self . is_error (): return status_result = [] for jc in self . _response . job_class : job_name = jc . job_name job_dict = dict () self . _job_names . add ( job_name ) job_dict [ KEY_JOB_NAME ] = job_name # we have a list of data items: job_dict [ KEY_TASkS ] = {} status_result . append ( job_dict ) for task_class in jc . task : task_class_dict = json . loads ( MessageToJson ( task_class , preserving_proto_field_name = True )) del task_class_dict [ 'task_name' ] if task_class . task_name not in job_dict [ KEY_TASkS ]: job_dict [ KEY_TASkS ][ task_class . task_name ] = [] job_dict [ KEY_TASkS ][ task_class . task_name ] . append ( task_class_dict ) #job_dict[KEY_TASkS].append({task_class.task_name: task_class_dict}) self . _json_result = status_result","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveClassStatusResponse.to_json","text":"Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" # consider setting preserving_proto_field_name to true # return json.dumps(json.loads(MessageToJson(self._response, preserving_proto_field_name=True)), indent=indent) if self . is_error (): return self . get_error () if indent and indent < 0 : return json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )) return json . dumps ( json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )), indent = indent , ensure_ascii = False )","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse","text":"The default container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server.","title":"OliveServerResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse.get_response","text":"The Protobuf message returned from the OLIVE server Returns: Type Description Source code in olivepy/messaging/response.py def get_response ( self ): \"\"\" The Protobuf message returned from the OLIVE server :return: \"\"\" # todo exception if none? return self . _response","title":"get_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse.get_workflow_type","text":"Return the type of workflow done in this response (analysis, enrollment, adaptation) Returns: Type Description A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped Source code in olivepy/messaging/response.py def get_workflow_type ( self ): \"\"\" Return the type of workflow done in this response (analysis, enrollment, adaptation) :return: A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped \"\"\" if not self . _response : raise Exception ( \"No valid response\" ) if isinstance ( self . _request , WorkflowAnalysisRequest ): return WORKFLOW_ANALYSIS_TYPE elif isinstance ( self . _request , WorkflowEnrollRequest ): return WORKFLOW_ENROLLMENT_TYPE elif isinstance ( self . _request , WorkflowAdaptRequest ): return WORKFLOW_ADAPT_TYPE elif isinstance ( self . _request , WorkflowUnenrollRequest ): return WORKFLOW_UNENROLLMENT_TYPE raise Exception ( \"Unknown Workflow Message: {} \" . format ( type ( self . _request )))","title":"get_workflow_type()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): \"\"\" Create this response from the :param request: :param response: :param message: :return: \"\"\" self . _request = request if message : # No results due to error self . _iserror = True self . _message = message # self._request = request if response is not None : try : if response . HasField ( \"error\" ): self . _iserror = True self . _message = response . error else : # we assume no errors self . _issuccessful = True except : # Some messages have no error field self . _issuccessful = True self . _response = response","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse.to_json","text":"Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the response as as JSON string: \"\"\" #consider setting preserving_proto_field_name to true if indent and indent < 0 : return json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )) return json . dumps ( json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )), indent = indent , ensure_ascii = False )","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse","text":"Extracts info from an actualized workflow definition","title":"OliveWorkflowActualizedResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.get_analysis_jobs","text":"Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. Returns: Type Description a list of job names in the analysis Source code in olivepy/messaging/response.py def get_analysis_jobs ( self ): \"\"\" Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. :return: a list of job names in the analysis \"\"\" return [ job_dict [ KEY_JOB_NAME ] for job_dict in self . _json_result ] #todo get analysis job name(s)","title":"get_analysis_jobs()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.get_request_jobs","text":"return the jobs in the original request for the specified analysis type Parameters: Name Type Description Default workflow_type the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) required Returns: Type Description the list of jobs for this type Source code in olivepy/messaging/response.py def get_request_jobs ( self , workflow_type ): \"\"\" return the jobs in the original request for the specified analysis type :param workflow_type: the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) :return: the list of jobs for this type \"\"\" if self . _request is not None : return get_workflow_jobs ( self . _request . workflow_definition , workflow_type ) raise Exception ( \"No jobs for the requested workflow type: {} \" . format ( workflow_type ))","title":"get_request_jobs()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.is_allowable_error","text":"Returns: Type Description true if this response failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this response failed with an allowable error \"\"\" return self . _isallowable_error","title":"is_allowable_error()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will walk the tree and deserialize any encoded messages if self . is_error (): # todo provide error info in JSON? return # make a new message for this type... if not isinstance ( self . _request , WorkflowActualizeRequest ): # we received an some other workflow message so se don't need to convert it to json... return # we only parse the analyze part now analysis_task = [] workflow_analysis_order_msg = None for order in self . _response . workflow . order : if order . workflow_type == WORKFLOW_ANALYSIS_TYPE : workflow_analysis_order_msg = order break if workflow_analysis_order_msg is None : # no analysis results return # for job in self._response.job_result: for job in workflow_analysis_order_msg . job_definition : # create a dictionary for each job result job_dict = dict () job_name = job . job_name self . _job_names . add ( job_name ) # and a dictionary of tasks: # job_dict[KEY_TASkS] = {} # add to our results - in most cases we will have just one job analysis_task . append ( job_dict ) # get data handling info for this job data_prop = job . data_properties job_dict [ 'Data Input' ] = json . loads ( MessageToJson ( data_prop , preserving_proto_field_name = True )) # if data_prop.mode == SPLIT: # # Hack to make split/mulit-channel mode more clear # job_dict['data']['mode'] = 'SPLIT: Process each channel as a job' for task in job . tasks : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # Deserialize message_data, and replace it in the task_result_dict task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ KEY_JOB_NAME ] = job_name task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) del task_result_dict [ 'message_data' ] job_dict [ task . consumer_result_label ] = task_result_dict self . _json_result = analysis_task","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.to_json","text":"Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False ) # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=False)), indent=indent)","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse","text":"The default container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server.","title":"OliveWorkflowAnalysisResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.get_analysis_jobs","text":"Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. Returns: Type Description a list of job names in the analysis Source code in olivepy/messaging/response.py def get_analysis_jobs ( self ): \"\"\" Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. :return: a list of job names in the analysis \"\"\" return [ job_dict [ KEY_JOB_NAME ] for job_dict in self . _json_result ] #todo get analysis job name(s)","title":"get_analysis_jobs()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.get_analysis_task_result","text":"Get the result(s) for the specified job_name and task_name, also include the data used for this task. If the workflow analyzes each channel in multi-channel data then there can be multiple jobs with the same name. Parameters: Name Type Description Default job_name for convenience can be None, since there is normally only one job. But if the workflow has multiple jobs then a valid name must be specified. required task_name the name to the task required Returns: Type Description a list of dictionaries, where each dictionary in the list includes the results for the specified task and a list of the data analyzed by this task, such as [ {task_name:{}, data:[] }] Source code in olivepy/messaging/response.py def get_analysis_task_result ( self , job_name , task_name ): \"\"\" Get the result(s) for the specified job_name and task_name, also include the data used for this task. If the workflow analyzes each channel in multi-channel data then there can be multiple jobs with the same name. :param job_name: for convenience can be None, since there is normally only one job. But if the workflow has multiple jobs then a valid name must be specified. :param task_name: the name to the task :return: a list of dictionaries, where each dictionary in the list includes the results for the specified task and a list of the data analyzed by this task, such as [ {task_name:{}, data:[] }] \"\"\" if job_name is None : job_name = self . _get_default_job_name () results = [] for job_dict in self . _json_result : if job_name == job_dict [ KEY_JOB_NAME ]: task_dict = dict () # there may be one or more result for task_name task_dict [ task_name ] = job_dict [ KEY_TASkS ][ task_name ] task_dict [ KEY_DATA ] = job_dict [ KEY_DATA ] results . append ( task_dict ) return results","title":"get_analysis_task_result()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.get_request_jobs","text":"return the jobs in the original request for the specified analysis type Parameters: Name Type Description Default workflow_type the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) required Returns: Type Description the list of jobs for this type Source code in olivepy/messaging/response.py def get_request_jobs ( self , workflow_type ): \"\"\" return the jobs in the original request for the specified analysis type :param workflow_type: the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) :return: the list of jobs for this type \"\"\" if self . _request is not None : return get_workflow_jobs ( self . _request . workflow_definition , workflow_type ) raise Exception ( \"No jobs for the requested workflow type: {} \" . format ( workflow_type ))","title":"get_request_jobs()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.is_allowable_error","text":"Returns: Type Description true if this response failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this response failed with an allowable error \"\"\" return self . _isallowable_error","title":"is_allowable_error()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will walk the tree and deserialize any encoded messages if self . is_error (): self . _json_result = {} self . _json_result [ 'error' ] = self . get_error () return # make a new message for this type... if isinstance ( self . _request , WorkflowActualizeRequest ): # we received an actualized workflow, so se don't need to convert it to json... return # this should only contain an analysis request... but check just in case: wk_type = self . get_workflow_type () if wk_type == WORKFLOW_ANALYSIS_TYPE or wk_type == WORKFLOW_ENROLLMENT_TYPE or wk_type == WORKFLOW_UNENROLLMENT_TYPE : # analysis is a list of dictionary elements, which looks like: [ {job_name: X, data: [], tasks: {}} ] # there is a dictionary for each job, but due to the way jobs work in OLIVE for mulit-channel data we # consider a jobs to be unique by a combination of job_name plus data, so multiples dictionary elements may # have the same job_name, but will have different data properties (channel numbers) # get the analysis job order from the original request: analysis_result = [] # or enrollment/unenrollment result # analysis_result['jobs'] = [] # analysis_result['data inputs'] = [] job_requests = get_workflow_jobs ( self . _request . workflow_definition , wk_type ) for job in self . _response . job_result : # create a dictionary for each job result job_dict = dict () job_name = job . job_name self . _job_names . add ( job_name ) # job_dict[job_name] = dict() job_dict [ KEY_JOB_NAME ] = job_name if job . error : job_dict [ 'error' ] = job . error # we have a list of data items: job_dict [ KEY_DATA ] = [] # and a dictionary of tasks: (although note it is possible to have multiple tasks with the same name, so a task has a list of results) job_dict [ KEY_TASkS ] = {} # add to our results - in most cases we will have just one job analysis_result . append ( job_dict ) # get the tasks for the current (and likely, only) job: task_requests = get_workflow_job_tasks ( job_requests , job_name ) # task_result_dict = dict() # I don't think this can happen yet: # if job.HasField('error'): # Allowable job error # for task in job . task_results : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # check if this task failed with an error if task . HasField ( 'error' ): # Allowable error if KEY_ERROR in job_dict : job_dict [ KEY_ERROR ] = job_dict [ KEY_ERROR ] + \",\" + task . error else : job_dict [ KEY_ERROR ] = task . error self . _isallowable_error = True # should have an empty message data; del task_result_dict [ 'message_data' ] if job_name not in self . _allowable_failed_job_tasks : self . _allowable_failed_job_tasks [ job_name ] = [] self . _allowable_failed_job_tasks [ job_name ] . append ( task . task_name ) else : # Deserialize message_data, and replace it in the task_result_dict if task . message_type in msgutil . debug_message_map : # Get the pimiento message (debug only - these messages are not guaranteed to be supported print ( \"CLG special msg type: {} \" . format ( msgutil . MessageType . Name ( task . message_type ))) pimiento_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) if task . message_type == DATA_OUTPUT_TRANSFORMER_RESULT : if pimiento_msg . data_type == TEXT : # only supported type for now... pie_data_msg = WorkflowTextResult () pie_data_msg . ParseFromString ( pimiento_msg . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( pie_data_msg , preserving_proto_field_name = True )) else : print ( \"Unsupported debug message type: {} \" . format ( msgutil . InputDataType . Name ( pimiento_msg . data_type ))) elif task . message_type == SCORE_OUTPUT_TRANSFORMER_RESULT : # these should be standard trait message pie_score_msg = self . _extract_serialized_message ( pimiento_msg . message_type , pimiento_msg . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( pie_score_msg , preserving_proto_field_name = True )) else : task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) # Should we create special handlers for analysis results, like we sort global score results, # but what should be do with an AUDIO_MODIFICATION_RESULT? if task . message_type == GLOBAL_SCORER_RESULT : # Sort region scores task_result_dict [ 'analysis' ][ 'score' ] = sorted ( task_result_dict [ 'analysis' ][ 'score' ], key = sort_global_scores , reverse = True ) # messageData has been replaced with the actual task del task_result_dict [ 'message_data' ] # taskName is the key, so remove it: del task_result_dict [ 'task_name' ] # check if we need to add the plugin/domain name if task . task_name in task_requests : orig_task = task_requests [ task . task_name ] if orig_task . message_type in msgutil . plugin_message_map : # Get the original task task_req_msg = self . _extract_serialized_message ( orig_task . message_type , orig_task . message_data ) task_result_dict [ 'plugin' ] = task_req_msg . plugin task_result_dict [ 'domain' ] = task_req_msg . domain if orig_task . message_type == CLASS_MODIFICATION_REQUEST or orig_task . message_type == CLASS_REMOVAL_REQUEST : # add class ID task_result_dict [ 'class_id' ] = self . _request . class_id # print(\"adding {} as {}\".format(task_result_dict, task.task_name)) # fixme: there can be multiple taks with the same name if conditions in a workflow caused the task to be ran twice if task . task_name not in job_dict [ KEY_TASkS ]: job_dict [ KEY_TASkS ][ task . task_name ] = [] job_dict [ KEY_TASkS ][ task . task_name ] . append ( task_result_dict ) #job_dict[KEY_TASkS].append({task.task_name: task_result_dict}) # A job usually has one data input/output, but we handle as if there are multiple for data_result in job . data_results : data_result_dict = json . loads ( MessageToJson ( data_result , preserving_proto_field_name = True )) # Deserialize the data portion data_type_msg = self . _extract_serialized_message ( data_result . msg_type , data_result . result_data ) del data_result_dict [ 'result_data' ] # del data_result_dict['dataId'] # redundant into, used as the key # data_result_dict['data'] = json.loads(MessageToJson(data_type_msg)) data_result_dict . update ( json . loads ( MessageToJson ( data_type_msg , preserving_proto_field_name = True ))) job_dict [ KEY_DATA ] . append ( data_result_dict ) # analysis_result['data inputs'].append(data_result_dict) self . _json_result = analysis_result","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.to_json","text":"Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" # return json.dumps(self._json_result, indent=indent, ensure_ascii=False) if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=True)), indent=indent, ensure_ascii=False) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False )","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowEnrollmentResponse","text":"The container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient) for enrollment. This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server.","title":"OliveWorkflowEnrollmentResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowEnrollmentResponse.is_allowable_error","text":"Returns: Type Description true if this message failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this message failed with an allowable error \"\"\" return self . _isallowable_error","title":"is_allowable_error()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowEnrollmentResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will the tree and deserialize any encoded messages if self . is_error (): # todo provide error info in JSON return # make a new message for this type... if isinstance ( self . _request , WorkflowActualizeRequest ): # we received an actualized workflow, so se don't need to convert it to json... return # this should only contain an analysis request... but check just in case: type = self . get_workflow_type () if type == WORKFLOW_ENROLLMENT_TYPE : # not much info in an enrollment response... enroll_result = dict () if self . _response . HasField ( 'error' ): enroll_result [ 'error' ] = self . _response . error else : enroll_result [ 'successful' ] = True self . _json_result = enroll_result","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowEnrollmentResponse.to_json","text":"Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" #consider setting preserving_proto_field_name to true # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=True)), indent=indent) if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False )","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.get_workflow_job_names","text":"parse a workflow definition, returning a list of job definition name (job_name) return [job_name] for the requested workflow_type Source code in olivepy/messaging/response.py def get_workflow_job_names ( workflow_definition , workflow_type ): \"\"\" parse a workflow definition, returning a list of job definition name (job_name) return [job_name] for the requested workflow_type \"\"\" rtn_job_names = list () if workflow_definition is not None : for order in workflow_definition . order : if order . workflow_type == workflow_type : for job in order . job_definition : rtn_job_names . append ( job . job_name ) return rtn_job_names","title":"get_workflow_job_names()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.get_workflow_job_tasks","text":"Fetch the tasks from a job Parameters: Name Type Description Default jobs a dictionary of WorkflowTasks, indexed by a job names required job_name find tasks that belong to a job having this name. This can be None if there is only one job None Returns: Type Description a dictionary of WorkflowTask indexed by the task's consumer_result_label for the specified job. An exception is thrown if there are multiple jobs but no job_name was specified Source code in olivepy/messaging/response.py def get_workflow_job_tasks ( jobs , job_name = None ): \"\"\" Fetch the tasks from a job :param jobs: a dictionary of WorkflowTasks, indexed by a job names :param job_name: find tasks that belong to a job having this name. This can be None if there is only one job :return: a dictionary of WorkflowTask indexed by the task's consumer_result_label for the specified job. An exception is thrown if there are multiple jobs but no job_name was specified \"\"\" if job_name is None : if len ( jobs ) == 1 : # hack to make it easier to fetch data since most workflows only have one job job_name = list ( jobs . keys ())[ 0 ] else : raise Exception ( \"Must specify a job name when there are multiple JobDefinitions in a Workflow\" ) rtn_tasks = dict () # and job_name is None: if job_name in jobs : for workflow_task in jobs [ job_name ]: rtn_tasks [ workflow_task . consumer_result_label ] = workflow_task # CLG: I don't think we need to generate a message/warning if a task name isn't in the original jobs since we can # have dynamic jobs and with streaming jobs are very dynamic # else: # print(\"Job '{}' not one of the expected job names: {}\".format(job_name, list(jobs.keys()))) return rtn_tasks","title":"get_workflow_job_tasks()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.get_workflow_jobs","text":"parse a workflow definition, returning a dictionary indexed job (definition) name (job_name) and a list of WorkflowTask elements. Parameters: Name Type Description Default workflow_definition find jobs in this workflow definition required workflow_type the type of workflow order (analysis, enrollment, unenrollment) return {job_name: [WorkflowTask]} for the requested workflow_type required Source code in olivepy/messaging/response.py def get_workflow_jobs ( workflow_definition , workflow_type ): \"\"\" parse a workflow definition, returning a dictionary indexed job (definition) name (job_name) and a list of \\ WorkflowTask elements. :param workflow_definition: find jobs in this workflow definition :param workflow_type: the type of workflow order (analysis, enrollment, unenrollment) return {job_name: [WorkflowTask]} for the requested workflow_type \"\"\" rtn_jobs = dict () if workflow_definition is not None : for order in workflow_definition . order : if order . workflow_type == workflow_type : for job in order . job_definition : rtn_jobs [ job . job_name ] = job . tasks return rtn_jobs","title":"get_workflow_jobs()"},{"location":"olivepy-docs/utils.html","text":"olivepy utils module olivepy.utils.pem Encapsulate pem file IO Abstracts reading from and writing to PEM files and should provide common operations on PEM records such as duration(). The str() method of this class will result in a string that is legal PEM format. All operations are performed in memory so don't use any extraordinarily huge PEM files. Each line in a PEM file has: filename, channel, class_label, start_t, end_t Pem get_maximum_duration ( self ) Get duration of maxium duration record in PEM. Intended only for cases where PEM contains only one ID. Source code in olivepy/utils/pem.py def get_maximum_duration ( self ): \"\"\" Get duration of maxium duration record in PEM. Intended only for cases where PEM contains only one ID. \"\"\" duration = 0 for id in list ( self . __record_map . keys ()): for rec in self . __record_map [ id ]: this_duration = float ( rec . duration ()) if ( this_duration > duration ): duration = this_duration return duration get_minimum_duration ( self ) Get duration of minimum duration record in PEM. Intended only for cases where PEM contains only one ID. Source code in olivepy/utils/pem.py def get_minimum_duration ( self ): \"\"\" Get duration of minimum duration record in PEM. Intended only for cases where PEM contains only one ID. \"\"\" duration = self . get_total_duration () for id in list ( self . __record_map . keys ()): for rec in self . __record_map [ id ]: this_duration = float ( rec . duration ()) if ( this_duration < duration ): duration = this_duration return duration PemRecord The underlying PEM container __init__ ( self , id , channel , label , start_t , end_t , decimal = False ) special Parameters: Name Type Description Default id generally the filename required channel the channel (if stereo). May be a string list (i.e. \"1,2\"). No validation is done for the channel value required label a \"class\" label for this segment. Examples include speaker, language, speech, etc required start_t the start time in seconds required end_t the end time in seconds required decimal if true, value is stored as a float False Source code in olivepy/utils/pem.py def __init__ ( self , id , channel , label , start_t , end_t , decimal = False ): ''' :param id: generally the filename :param channel: the channel (if stereo). May be a string list (i.e. \"1,2\"). No validation is done for the channel value :param label: a \"class\" label for this segment. Examples include speaker, language, speech, etc :param start_t: the start time in seconds :param end_t: the end time in seconds :param decimal: if true, value is stored as a float ''' self . id = id self . channel = channel self . label = label # Using Decimal instead of floats made the code 100x slower # Use floats for now in the intrest of speed if decimal : self . start_t = start_t self . end_t = end_t else : self . start_t = float ( start_t ) self . end_t = float ( end_t ) if ( self . start_t > self . end_t ): raise Exception ( \"Start is after end in PemRecord: {self} \" . format ( ** vars ())) split_channels ( self ) Split the channel into an array, so that if a channel value of '1,2' is supplied it is returned as an array [1,2] Returns: Type Description an array of channel numbers Source code in olivepy/utils/pem.py def split_channels ( self ): ''' Split the channel into an array, so that if a channel value of '1,2' is supplied it is returned as an array [1,2] :return: an array of channel numbers ''' channels = [] if type ( self . channel ) is str : # convert to a list channels = list ( map ( int , str . split ( self . channel , ',' ))) elif type ( self . channel ) is int : channels . append ( self . channel ) else : print ( \"Unsupported channel value: {} \" . format ( self . channel )) return channels olivepy.utils.utils parse_annotation_file ( filename ) Parse a file for the names of files of audio files and their regions to use for adaptation. Parameters: Name Type Description Default filename the path and name of the file that contains the input. This file must have one or more lines having 4 columns: filename, class, start_region_ms, end_region_ms required Returns: Type Description the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} Source code in olivepy/utils/utils.py def parse_annotation_file ( filename ): \"\"\" Parse a file for the names of files of audio files and their regions to use for adaptation. :param filename: the path and name of the file that contains the input. This file must have one or more lines having 4 columns: filename, class, start_region_ms, end_region_ms :return: the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} \"\"\" data_lines = [] file_annotations = {} if not os . path . exists ( filename ): raise Exception ( \"The annotation file ' {} ' does not exist\" . format ( filename )) with open ( filename ) as f : data_lines . extend ([ line . strip () for line in f . readlines ()]) # process the file for line in data_lines : pieces = line . split () if len ( pieces ) != 4 : raise Exception ( \"The annotation file does not contain data in the correct format, found line ' {} '\" . format ( line )) adapt_audio_path = pieces [ 0 ] # assume a relative file is used, so the full path must be specified since being sent to server # This is being sent to server. If full path is given, do nothing. Otherwise make absolute. # TODO: this will not work from UNIX to Windows or other way around. # TODO: should use Python's abspath here, don't you think? if adapt_audio_path [ 0 ] != '/' and adapt_audio_path [ 1 ] != ':' : adapt_audio_path = os . path . join ( os . getcwd (), adapt_audio_path ) # todo validate file is valid... if adapt_audio_path not in file_annotations : file_annotations [ adapt_audio_path ] = [] class_id = pieces [ 1 ] start = float ( pieces [ 2 ]) end = float ( pieces [ 3 ]) file_annotations [ adapt_audio_path ] . append (( start , end , class_id )) return file_annotations parse_json_options ( option_str ) Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name Parameters: Name Type Description Default option_str the options to parse required Source code in olivepy/utils/utils.py def parse_json_options ( option_str ): \"\"\" Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name :param option_str: the options to parse :return a list of OptionValue objects created from the JSON option input \"\"\" # Examples of inputs to handle: # [{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' # '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}' # '[{\"task\":\"LID\", \"options\": {\"filter_length\":99, \"interpolate\":11.0, \"test_name\":\"midge\"}}]' # '[{\"job\":\"SAD LID Job\", \"task\":\"LID\", \"options\": {\"filter_length\":99, \"interpolate\":11.0, \"test_name\":\"midge\"}}]' # Parse options json_opts = json . loads ( option_str ) out_opts = [] # Options can be a list of task specific options # currently we don't support task specific options so just create one dictionary of name/value options if isinstance ( json_opts , list ): for item in json_opts : in_opts = item [ 'options' ] for opt in in_opts : # print(\"\\t{} = {}, value type: {}\".format(opt, in_opts[opt], type(in_opts[opt]))) opt_msg = olivepb . OptionValue () opt_msg . name = opt opt_msg . value = str ( in_opts [ opt ]) # optionally check if this option is restricted to a job/task: if 'task' in item : opt_msg . task_filter_name = item [ 'task' ] if 'job' in item : opt_msg . job_filter_name = item [ 'job' ] out_opts . append ( opt_msg ) else : # or options that are applied to each task, which is just a simple dictionary # like: {\"filter_length\":99, \"interpolate\":1.0} # OLIVE wil internally ignore these options if the keyname does not match one of the option name # a plugin supports for the requested trait (i.e. plugin.get_region_scoring_opts() for opt in json_opts : opt_msg = olivepb . OptionValue () opt_msg . name = opt opt_msg . value = str ( json_opts [ opt ]) out_opts . append ( opt_msg ) # print(\"\\t{} = {}, value type: {}\".format(opt, json_opts[opt], type(json_opts[opt]))) print ( \"Final json options: {} \" . format ( out_opts )) return out_opts parse_json_options_as_dict ( option_str ) Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name Parameters: Name Type Description Default option_str the options to parse required Returns: Type Description a dictionary of options name/value pairs Source code in olivepy/utils/utils.py def parse_json_options_as_dict ( option_str ): \"\"\" Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name :param option_str: the options to parse :return: a dictionary of options name/value pairs \"\"\" # Parse options json_opts = json . loads ( option_str ) out_opts = dict () # Options can be a list of task specific options # currently we don't support task specific options so just create one dictionary of name/value options if isinstance ( json_opts , list ): for item in json_opts : in_opts = item [ 'options' ] print ( \"Found {} options for task: {} \" . format ( len ( in_opts ), item [ 'task' ])) out_opts . update ( in_opts ) for opt in in_opts : print ( \" \\t {} = {} , value type: {} \" . format ( opt , in_opts [ opt ], type ( in_opts [ opt ]))) else : # or options that are applied to each task, which is just a simple dictionary # like: {\"filter_length\":99, \"interpolate\":1.0} # OLIVE wil internally ignore these options if the keyname does not match one of the option name # a plugin supports for the requested trait (i.e. plugin.get_region_scoring_opts() out_opts = json_opts for opt in json_opts : print ( \" \\t {} = {} , value type: {} \" . format ( opt , json_opts [ opt ], type ( json_opts [ opt ]))) print ( \"Final json options: {} \" . format ( out_opts )) return out_opts parse_pem_file ( data_lines ) Parse a PEM file, grouping the results by audio file and channel. Parameters: Name Type Description Default data_lines the data line to parse required Returns: Type Description a dictionary of audio files to score and the channel region: # {'filename': {channel: {class_id : [(start_region, end_region, class_id)]} } } Source code in olivepy/utils/utils.py def parse_pem_file ( data_lines ): \"\"\" Parse a PEM file, grouping the results by audio file and channel. :param data_lines: the data line to parse :return: a dictionary of audio files to score and the channel region: # {'filename': {channel: {class_id : [(start_region, end_region, class_id)]} } } \"\"\" # We process by file and channel - the class/label is ignored regions = {} input_pem = Pem () input_pem . add_records_from_data_lines ( data_lines ) for id in input_pem . get_ids (): audio_id = os . path . expandvars ( id ) # Create a dictionary of the regions specified for the the current file regions [ audio_id ] = {} for rec in input_pem . get_records ( id ): # channel could be a list... channels = [] if type ( rec . channel ) is str : # convert to a list channels = map ( int , str . split ( rec . channel , ',' )) elif type ( rec . channel ) is int : channels . append ( rec . channel ) else : print ( \"Unsupported channel value: {} \" . format ( rec . channel )) for ch in channels : if ch not in regions [ audio_id ]: regions [ audio_id ][ ch ] = {} class_id = rec . label if class_id not in regions [ audio_id ][ ch ]: regions [ audio_id ][ ch ][ class_id ] = [] regions [ audio_id ][ ch ][ class_id ] . append (( rec . start_t , rec . end_t )) return regions","title":"`olivepy` `utils` module"},{"location":"olivepy-docs/utils.html#olivepy-utils-module","text":"","title":"olivepy utils module"},{"location":"olivepy-docs/utils.html#olivepyutilspem","text":"Encapsulate pem file IO Abstracts reading from and writing to PEM files and should provide common operations on PEM records such as duration(). The str() method of this class will result in a string that is legal PEM format. All operations are performed in memory so don't use any extraordinarily huge PEM files. Each line in a PEM file has: filename, channel, class_label, start_t, end_t","title":"olivepy.utils.pem"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.Pem","text":"","title":"Pem"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.Pem.get_maximum_duration","text":"Get duration of maxium duration record in PEM. Intended only for cases where PEM contains only one ID. Source code in olivepy/utils/pem.py def get_maximum_duration ( self ): \"\"\" Get duration of maxium duration record in PEM. Intended only for cases where PEM contains only one ID. \"\"\" duration = 0 for id in list ( self . __record_map . keys ()): for rec in self . __record_map [ id ]: this_duration = float ( rec . duration ()) if ( this_duration > duration ): duration = this_duration return duration","title":"get_maximum_duration()"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.Pem.get_minimum_duration","text":"Get duration of minimum duration record in PEM. Intended only for cases where PEM contains only one ID. Source code in olivepy/utils/pem.py def get_minimum_duration ( self ): \"\"\" Get duration of minimum duration record in PEM. Intended only for cases where PEM contains only one ID. \"\"\" duration = self . get_total_duration () for id in list ( self . __record_map . keys ()): for rec in self . __record_map [ id ]: this_duration = float ( rec . duration ()) if ( this_duration < duration ): duration = this_duration return duration","title":"get_minimum_duration()"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.PemRecord","text":"The underlying PEM container","title":"PemRecord"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.PemRecord.__init__","text":"Parameters: Name Type Description Default id generally the filename required channel the channel (if stereo). May be a string list (i.e. \"1,2\"). No validation is done for the channel value required label a \"class\" label for this segment. Examples include speaker, language, speech, etc required start_t the start time in seconds required end_t the end time in seconds required decimal if true, value is stored as a float False Source code in olivepy/utils/pem.py def __init__ ( self , id , channel , label , start_t , end_t , decimal = False ): ''' :param id: generally the filename :param channel: the channel (if stereo). May be a string list (i.e. \"1,2\"). No validation is done for the channel value :param label: a \"class\" label for this segment. Examples include speaker, language, speech, etc :param start_t: the start time in seconds :param end_t: the end time in seconds :param decimal: if true, value is stored as a float ''' self . id = id self . channel = channel self . label = label # Using Decimal instead of floats made the code 100x slower # Use floats for now in the intrest of speed if decimal : self . start_t = start_t self . end_t = end_t else : self . start_t = float ( start_t ) self . end_t = float ( end_t ) if ( self . start_t > self . end_t ): raise Exception ( \"Start is after end in PemRecord: {self} \" . format ( ** vars ()))","title":"__init__()"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.PemRecord.split_channels","text":"Split the channel into an array, so that if a channel value of '1,2' is supplied it is returned as an array [1,2] Returns: Type Description an array of channel numbers Source code in olivepy/utils/pem.py def split_channels ( self ): ''' Split the channel into an array, so that if a channel value of '1,2' is supplied it is returned as an array [1,2] :return: an array of channel numbers ''' channels = [] if type ( self . channel ) is str : # convert to a list channels = list ( map ( int , str . split ( self . channel , ',' ))) elif type ( self . channel ) is int : channels . append ( self . channel ) else : print ( \"Unsupported channel value: {} \" . format ( self . channel )) return channels","title":"split_channels()"},{"location":"olivepy-docs/utils.html#olivepyutilsutils","text":"","title":"olivepy.utils.utils"},{"location":"olivepy-docs/utils.html#olivepy.utils.utils.parse_annotation_file","text":"Parse a file for the names of files of audio files and their regions to use for adaptation. Parameters: Name Type Description Default filename the path and name of the file that contains the input. This file must have one or more lines having 4 columns: filename, class, start_region_ms, end_region_ms required Returns: Type Description the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} Source code in olivepy/utils/utils.py def parse_annotation_file ( filename ): \"\"\" Parse a file for the names of files of audio files and their regions to use for adaptation. :param filename: the path and name of the file that contains the input. This file must have one or more lines having 4 columns: filename, class, start_region_ms, end_region_ms :return: the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} \"\"\" data_lines = [] file_annotations = {} if not os . path . exists ( filename ): raise Exception ( \"The annotation file ' {} ' does not exist\" . format ( filename )) with open ( filename ) as f : data_lines . extend ([ line . strip () for line in f . readlines ()]) # process the file for line in data_lines : pieces = line . split () if len ( pieces ) != 4 : raise Exception ( \"The annotation file does not contain data in the correct format, found line ' {} '\" . format ( line )) adapt_audio_path = pieces [ 0 ] # assume a relative file is used, so the full path must be specified since being sent to server # This is being sent to server. If full path is given, do nothing. Otherwise make absolute. # TODO: this will not work from UNIX to Windows or other way around. # TODO: should use Python's abspath here, don't you think? if adapt_audio_path [ 0 ] != '/' and adapt_audio_path [ 1 ] != ':' : adapt_audio_path = os . path . join ( os . getcwd (), adapt_audio_path ) # todo validate file is valid... if adapt_audio_path not in file_annotations : file_annotations [ adapt_audio_path ] = [] class_id = pieces [ 1 ] start = float ( pieces [ 2 ]) end = float ( pieces [ 3 ]) file_annotations [ adapt_audio_path ] . append (( start , end , class_id )) return file_annotations","title":"parse_annotation_file()"},{"location":"olivepy-docs/utils.html#olivepy.utils.utils.parse_json_options","text":"Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name Parameters: Name Type Description Default option_str the options to parse required Source code in olivepy/utils/utils.py def parse_json_options ( option_str ): \"\"\" Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name :param option_str: the options to parse :return a list of OptionValue objects created from the JSON option input \"\"\" # Examples of inputs to handle: # [{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' # '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}' # '[{\"task\":\"LID\", \"options\": {\"filter_length\":99, \"interpolate\":11.0, \"test_name\":\"midge\"}}]' # '[{\"job\":\"SAD LID Job\", \"task\":\"LID\", \"options\": {\"filter_length\":99, \"interpolate\":11.0, \"test_name\":\"midge\"}}]' # Parse options json_opts = json . loads ( option_str ) out_opts = [] # Options can be a list of task specific options # currently we don't support task specific options so just create one dictionary of name/value options if isinstance ( json_opts , list ): for item in json_opts : in_opts = item [ 'options' ] for opt in in_opts : # print(\"\\t{} = {}, value type: {}\".format(opt, in_opts[opt], type(in_opts[opt]))) opt_msg = olivepb . OptionValue () opt_msg . name = opt opt_msg . value = str ( in_opts [ opt ]) # optionally check if this option is restricted to a job/task: if 'task' in item : opt_msg . task_filter_name = item [ 'task' ] if 'job' in item : opt_msg . job_filter_name = item [ 'job' ] out_opts . append ( opt_msg ) else : # or options that are applied to each task, which is just a simple dictionary # like: {\"filter_length\":99, \"interpolate\":1.0} # OLIVE wil internally ignore these options if the keyname does not match one of the option name # a plugin supports for the requested trait (i.e. plugin.get_region_scoring_opts() for opt in json_opts : opt_msg = olivepb . OptionValue () opt_msg . name = opt opt_msg . value = str ( json_opts [ opt ]) out_opts . append ( opt_msg ) # print(\"\\t{} = {}, value type: {}\".format(opt, json_opts[opt], type(json_opts[opt]))) print ( \"Final json options: {} \" . format ( out_opts )) return out_opts","title":"parse_json_options()"},{"location":"olivepy-docs/utils.html#olivepy.utils.utils.parse_json_options_as_dict","text":"Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name Parameters: Name Type Description Default option_str the options to parse required Returns: Type Description a dictionary of options name/value pairs Source code in olivepy/utils/utils.py def parse_json_options_as_dict ( option_str ): \"\"\" Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name :param option_str: the options to parse :return: a dictionary of options name/value pairs \"\"\" # Parse options json_opts = json . loads ( option_str ) out_opts = dict () # Options can be a list of task specific options # currently we don't support task specific options so just create one dictionary of name/value options if isinstance ( json_opts , list ): for item in json_opts : in_opts = item [ 'options' ] print ( \"Found {} options for task: {} \" . format ( len ( in_opts ), item [ 'task' ])) out_opts . update ( in_opts ) for opt in in_opts : print ( \" \\t {} = {} , value type: {} \" . format ( opt , in_opts [ opt ], type ( in_opts [ opt ]))) else : # or options that are applied to each task, which is just a simple dictionary # like: {\"filter_length\":99, \"interpolate\":1.0} # OLIVE wil internally ignore these options if the keyname does not match one of the option name # a plugin supports for the requested trait (i.e. plugin.get_region_scoring_opts() out_opts = json_opts for opt in json_opts : print ( \" \\t {} = {} , value type: {} \" . format ( opt , json_opts [ opt ], type ( json_opts [ opt ]))) print ( \"Final json options: {} \" . format ( out_opts )) return out_opts","title":"parse_json_options_as_dict()"},{"location":"olivepy-docs/utils.html#olivepy.utils.utils.parse_pem_file","text":"Parse a PEM file, grouping the results by audio file and channel. Parameters: Name Type Description Default data_lines the data line to parse required Returns: Type Description a dictionary of audio files to score and the channel region: # {'filename': {channel: {class_id : [(start_region, end_region, class_id)]} } } Source code in olivepy/utils/utils.py def parse_pem_file ( data_lines ): \"\"\" Parse a PEM file, grouping the results by audio file and channel. :param data_lines: the data line to parse :return: a dictionary of audio files to score and the channel region: # {'filename': {channel: {class_id : [(start_region, end_region, class_id)]} } } \"\"\" # We process by file and channel - the class/label is ignored regions = {} input_pem = Pem () input_pem . add_records_from_data_lines ( data_lines ) for id in input_pem . get_ids (): audio_id = os . path . expandvars ( id ) # Create a dictionary of the regions specified for the the current file regions [ audio_id ] = {} for rec in input_pem . get_records ( id ): # channel could be a list... channels = [] if type ( rec . channel ) is str : # convert to a list channels = map ( int , str . split ( rec . channel , ',' )) elif type ( rec . channel ) is int : channels . append ( rec . channel ) else : print ( \"Unsupported channel value: {} \" . format ( rec . channel )) for ch in channels : if ch not in regions [ audio_id ]: regions [ audio_id ][ ch ] = {} class_id = rec . label if class_id not in regions [ audio_id ][ ch ]: regions [ audio_id ][ ch ][ class_id ] = [] regions [ audio_id ][ ch ][ class_id ] . append (( rec . start_t , rec . end_t )) return regions","title":"parse_pem_file()"},{"location":"plugins/asr-dynapy-v4.html","text":"asr-dynapy-v4 (Automatic Speech Recognition) Version Changelog Plugin Version Change v4.0.0 Initial plugin release, based on v3.0.0, but with bug fixes and merged in some specialty features like Streaming, and the addition of memory-saving look-ahead model architectures for some domains. Published with OLIVE 5.5.0 v4.1.0 Additional domains targed for distant speech, and new language support. v4.2.0 Updated Ukrainian and Mandarin distant-microphone domains. Released with OLIVE 5.6.0 Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin builds from its predecessor, asr-dynapy-v2, and features bug fixes and compatibility updates, as well as 3 additional domains bringing new language capabilities. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks (TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models. Domains (Supported Languages) Distant Speech targeted domains These domains have been augmented with additional audio data sources to better handle distortions and other effects caused by non-close-talk recording conditions. These augmentations have also improved performance over the telephone-trained domains below for many/most situations, but will also consume more memory than the \"tdnnLookaheadRnnlm\" under similar circumstances. mandarin-tdnnChainStaticSrilm-distant-v2 (new) Mandarin domain augmented for distant speech. russian-tdnnChainLookaheadRnnlm-distant-v2 Russian domain augmented for distant speech. spanish-tdnnChainStaticRnnlm-distant-v2 Spanish domain augmented for distant speech. ukrainian-tdnnChainStaticSrilm-distant-v2 (new) Ukrainian domain augmented for distant speech. Telephony Speech trained domains with Lookahead models english-tdnnLookaheadRnnlm-tel-v2 English domain trained with conversational telephony speech. This domain features lookahead archicture for memory footpring benefits and reports word posterior scores. farsi-tdnnLookaheadRnnlm-tel-v1 Farsi domain trained with conversational telephony speech. This domain now features lookahead models and reports word posterior scores. french-tdnnLookaheadRnnlm-tel-v1 French domain augmented with African-accented French data. This domain features lookahead models and reports word posterior scores. iraqiArabic-tdnnLookaheadRnnlm-tel-v1 Iraqi Arabic dialect domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. korean-tdnnLookahead-tel-v1 Korean domain trained with conversational telephony speech. Features lookahead models and reports word posterior scores. levantineArabic-tdnnLookaheadRnnlm-tel-v1 Levantine Arabic dialect domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. mandarin-tdnnLookaheadRnnlm-tel-v1 Mandarin Chinese domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. pashto-tdnnLookaheadRnnlm-tel-v1 Pashto domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. russian-tdnnLookaheadRnnlm-tel-v1 Russian domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. spanish-tdnnLookaheadRnnlm-tel-v1 Spanish domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.2+ Limitations As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Out of Vocabulary (OOV) Words, Names The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications. Confidences vs Word Posteriors Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points. Resources The Russian domain, rus-tdnnChain-tel-v2, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain. Arabic Script Languages Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Non-Verbal Recognizer Output Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Automatic Speech Recognition (ASR)"},{"location":"plugins/asr-dynapy-v4.html#asr-dynapy-v4-automatic-speech-recognition","text":"","title":"asr-dynapy-v4 (Automatic Speech Recognition)"},{"location":"plugins/asr-dynapy-v4.html#version-changelog","text":"Plugin Version Change v4.0.0 Initial plugin release, based on v3.0.0, but with bug fixes and merged in some specialty features like Streaming, and the addition of memory-saving look-ahead model architectures for some domains. Published with OLIVE 5.5.0 v4.1.0 Additional domains targed for distant speech, and new language support. v4.2.0 Updated Ukrainian and Mandarin distant-microphone domains. Released with OLIVE 5.6.0","title":"Version Changelog"},{"location":"plugins/asr-dynapy-v4.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin builds from its predecessor, asr-dynapy-v2, and features bug fixes and compatibility updates, as well as 3 additional domains bringing new language capabilities. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks (TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models.","title":"Description"},{"location":"plugins/asr-dynapy-v4.html#domains-supported-languages","text":"","title":"Domains (Supported Languages)"},{"location":"plugins/asr-dynapy-v4.html#distant-speech-targeted-domains","text":"These domains have been augmented with additional audio data sources to better handle distortions and other effects caused by non-close-talk recording conditions. These augmentations have also improved performance over the telephone-trained domains below for many/most situations, but will also consume more memory than the \"tdnnLookaheadRnnlm\" under similar circumstances. mandarin-tdnnChainStaticSrilm-distant-v2 (new) Mandarin domain augmented for distant speech. russian-tdnnChainLookaheadRnnlm-distant-v2 Russian domain augmented for distant speech. spanish-tdnnChainStaticRnnlm-distant-v2 Spanish domain augmented for distant speech. ukrainian-tdnnChainStaticSrilm-distant-v2 (new) Ukrainian domain augmented for distant speech.","title":"Distant Speech targeted domains"},{"location":"plugins/asr-dynapy-v4.html#telephony-speech-trained-domains-with-lookahead-models","text":"english-tdnnLookaheadRnnlm-tel-v2 English domain trained with conversational telephony speech. This domain features lookahead archicture for memory footpring benefits and reports word posterior scores. farsi-tdnnLookaheadRnnlm-tel-v1 Farsi domain trained with conversational telephony speech. This domain now features lookahead models and reports word posterior scores. french-tdnnLookaheadRnnlm-tel-v1 French domain augmented with African-accented French data. This domain features lookahead models and reports word posterior scores. iraqiArabic-tdnnLookaheadRnnlm-tel-v1 Iraqi Arabic dialect domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. korean-tdnnLookahead-tel-v1 Korean domain trained with conversational telephony speech. Features lookahead models and reports word posterior scores. levantineArabic-tdnnLookaheadRnnlm-tel-v1 Levantine Arabic dialect domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. mandarin-tdnnLookaheadRnnlm-tel-v1 Mandarin Chinese domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. pashto-tdnnLookaheadRnnlm-tel-v1 Pashto domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. russian-tdnnLookaheadRnnlm-tel-v1 Russian domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. spanish-tdnnLookaheadRnnlm-tel-v1 Spanish domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores.","title":"Telephony Speech trained domains with Lookahead models"},{"location":"plugins/asr-dynapy-v4.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-dynapy-v4.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Outputs"},{"location":"plugins/asr-dynapy-v4.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-dynapy-v4.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/asr-dynapy-v4.html#limitations","text":"As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-dynapy-v4.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-dynapy-v4.html#out-of-vocabulary-oov-words-names","text":"The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications.","title":"Out of Vocabulary (OOV) Words, Names"},{"location":"plugins/asr-dynapy-v4.html#confidences-vs-word-posteriors","text":"Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores.","title":"Confidences vs Word Posteriors"},{"location":"plugins/asr-dynapy-v4.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points.","title":"Overlap"},{"location":"plugins/asr-dynapy-v4.html#resources","text":"The Russian domain, rus-tdnnChain-tel-v2, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain.","title":"Resources"},{"location":"plugins/asr-dynapy-v4.html#arabic-script-languages","text":"Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Arabic Script Languages"},{"location":"plugins/asr-dynapy-v4.html#non-verbal-recognizer-output","text":"Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations.","title":"Non-Verbal Recognizer Output"},{"location":"plugins/asr-dynapy-v4.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-dynapy-v4.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-dynapy-v4.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Global Options"},{"location":"plugins/asr-end2end-v1.html","text":"asr-end2end-v1 (Automatic Speech Recognition) Version Changelog Plugin Version Change v1.0.0 Initial plugin release of the end-to-end models, tested and published with OLIVE 5.5.0 v1.0.1 Updated plugin, improved overall stability and performance, updated sample rate configuration to allow boosted performance when using 16 kHz audio, bug fixes and workflow-compatibility fixes. v1.1.0 Rolling change to plugin to fix output for certain languages to standardize output case. v1.2.0 Updated plugin to include a bug fix for certain languages that could output words/regions with a length of 0s, and introducing new languages (Japenese, Khmer, Korean) and augmented domains for many languages. Adds overlap to the input \"chunking\" algorithm to smooth/remove errors at processing break points. v1.3.1 Fixed minor issues. Introduced new domains. Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is the first release of SRI's end-to-end ASR plugin which uses a wav2vec v2.0 model for mapping the microphone samples to the letters/characters in the target language. Currently, this plugin supports 10 languages which overlaps with the language capabilities of the alternative asr-dynapy-v4 plugin. The output format is identical to the asr-dynapy output which is detailed below. This first version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. The -augmented- domains have also been trained on degraded speech as well as clean speech and they are expected to perform much better on noisy/reverberant recordings. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance. Domains english-augmented-v1 (both 8k and 16k data) * NOTE: This model has a known deficiency where \"lau\" is stripped from output, causing words like \"laugh\" to be reported as \"gh\". This will be remedied in a future plugin release. english-spanish-v1 * Experimental domain meant for English and Spanish code-switching farsi-v1 french-v1 iraqiArabic-v1 japanese-augmented-v1 khmer-augmented-v2 korean-augmented-v1 levantineArabic-v1 mandarin-augmented-v2 pashto-v1 russian-augmented-v1 spanish-augmented-v1 ukrainian-augmented-v1 Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 AND 1.00000000 input-audio.wav 0.210 0.340 WE'RE 1.00000000 input-audio.wav 0.330 0.460 GOING 1.00000000 input-audio.wav 0.450 0.520 TO 1.00000000 input-audio.wav 0.510 0.940 FLY 1.00000000 input-audio.wav 1.080 1.300 WAS 1.00000000 input-audio.wav 1.290 1.390 THAT 1.00000000 input-audio.wav 1.290 1.390 IT 1.00000000 input-audio.wav 1.380 1.510 WE'RE 1.00000000 input-audio.wav 1.500 1.660 GOING 1.00000000 input-audio.wav 1.650 1.720 TO 1.00000000 input-audio.wav 1.710 1.930 FLY 1.00000000 input-audio.wav 1.920 2.110 OVER 1.00000000 input-audio.wav 2.100 2.380 SAINT 1.00000000 input-audio.wav 2.370 2.950 LOUIS 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 1.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 1.00000000 input-audio.wav 0.870 0.970 \u7684 1.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 1.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 1.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 1.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 1.00000000 input-audio.wav 3.130 3.340 \u7684 1.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 1.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 1.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.5+ Limitations As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. When chunking the input, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. There is error-reconciliation code to attempt to compensate for these overlapped sections, but it is still possible for issues to be encountered at these \"chunk\" boundaries. Arabic Script Languages Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments GPU Support This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"asr-end2end-v1 (Automatic Speech Recognition)"},{"location":"plugins/asr-end2end-v1.html#asr-end2end-v1-automatic-speech-recognition","text":"","title":"asr-end2end-v1 (Automatic Speech Recognition)"},{"location":"plugins/asr-end2end-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release of the end-to-end models, tested and published with OLIVE 5.5.0 v1.0.1 Updated plugin, improved overall stability and performance, updated sample rate configuration to allow boosted performance when using 16 kHz audio, bug fixes and workflow-compatibility fixes. v1.1.0 Rolling change to plugin to fix output for certain languages to standardize output case. v1.2.0 Updated plugin to include a bug fix for certain languages that could output words/regions with a length of 0s, and introducing new languages (Japenese, Khmer, Korean) and augmented domains for many languages. Adds overlap to the input \"chunking\" algorithm to smooth/remove errors at processing break points. v1.3.1 Fixed minor issues. Introduced new domains.","title":"Version Changelog"},{"location":"plugins/asr-end2end-v1.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is the first release of SRI's end-to-end ASR plugin which uses a wav2vec v2.0 model for mapping the microphone samples to the letters/characters in the target language. Currently, this plugin supports 10 languages which overlaps with the language capabilities of the alternative asr-dynapy-v4 plugin. The output format is identical to the asr-dynapy output which is detailed below. This first version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. The -augmented- domains have also been trained on degraded speech as well as clean speech and they are expected to perform much better on noisy/reverberant recordings. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance.","title":"Description"},{"location":"plugins/asr-end2end-v1.html#domains","text":"english-augmented-v1 (both 8k and 16k data) * NOTE: This model has a known deficiency where \"lau\" is stripped from output, causing words like \"laugh\" to be reported as \"gh\". This will be remedied in a future plugin release. english-spanish-v1 * Experimental domain meant for English and Spanish code-switching farsi-v1 french-v1 iraqiArabic-v1 japanese-augmented-v1 khmer-augmented-v2 korean-augmented-v1 levantineArabic-v1 mandarin-augmented-v2 pashto-v1 russian-augmented-v1 spanish-augmented-v1 ukrainian-augmented-v1","title":"Domains"},{"location":"plugins/asr-end2end-v1.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-end2end-v1.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 AND 1.00000000 input-audio.wav 0.210 0.340 WE'RE 1.00000000 input-audio.wav 0.330 0.460 GOING 1.00000000 input-audio.wav 0.450 0.520 TO 1.00000000 input-audio.wav 0.510 0.940 FLY 1.00000000 input-audio.wav 1.080 1.300 WAS 1.00000000 input-audio.wav 1.290 1.390 THAT 1.00000000 input-audio.wav 1.290 1.390 IT 1.00000000 input-audio.wav 1.380 1.510 WE'RE 1.00000000 input-audio.wav 1.500 1.660 GOING 1.00000000 input-audio.wav 1.650 1.720 TO 1.00000000 input-audio.wav 1.710 1.930 FLY 1.00000000 input-audio.wav 1.920 2.110 OVER 1.00000000 input-audio.wav 2.100 2.380 SAINT 1.00000000 input-audio.wav 2.370 2.950 LOUIS 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 1.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 1.00000000 input-audio.wav 0.870 0.970 \u7684 1.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 1.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 1.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 1.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 1.00000000 input-audio.wav 3.130 3.340 \u7684 1.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 1.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 1.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Outputs"},{"location":"plugins/asr-end2end-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-end2end-v1.html#compatibility","text":"OLIVE 5.5+","title":"Compatibility"},{"location":"plugins/asr-end2end-v1.html#limitations","text":"As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-end2end-v1.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-end2end-v1.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. When chunking the input, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. There is error-reconciliation code to attempt to compensate for these overlapped sections, but it is still possible for issues to be encountered at these \"chunk\" boundaries.","title":"Overlap"},{"location":"plugins/asr-end2end-v1.html#arabic-script-languages","text":"Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Arabic Script Languages"},{"location":"plugins/asr-end2end-v1.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-end2end-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-end2end-v1.html#gpu-support","text":"This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/asr-end2end-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Global Options"},{"location":"plugins/asr-end2end-v3.html","text":"asr-end2end-v3 (Automatic Speech Recognition) Version Changelog Plugin Version Change v3.0.0 Initial plugin release of the fully quantized ctranslate2 framework end-to-end models, tested and published with OLIVE 5.7.1 Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is an iteration of SRI's end-to-end ASR plugin which now uses fully quantized faster-wav2vec models for dramatic memory usage reduction over the asr-end2end-v2 plugin, which was already a significant reduction over the original plugin. The output format is identical to the existing ASR plugin output which is detailed below. This version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. The -augmented- domains have also been trained on degraded speech as well as clean speech and they are expected to perform much better on noisy/reverberant recordings. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance. Domains english-augmented-v3 (both 8k and 16k data) english-spanish-v1 * Experimental domain meant for English and Spanish code-switching farsi-v1 french-v1 iraqiArabic-v1 japanese-augmented-v2 khmer-augmented-v2 korean-augmented-v1 levantineArabic-v1 mandarin-augmented-v2 pashto-v1 russian-augmented-v1 spanish-augmented-v1 thai-augmented-v1 ukrainian-augmented-v1 Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 AND 1.00000000 input-audio.wav 0.210 0.340 WE'RE 1.00000000 input-audio.wav 0.330 0.460 GOING 1.00000000 input-audio.wav 0.450 0.520 TO 1.00000000 input-audio.wav 0.510 0.940 FLY 1.00000000 input-audio.wav 1.080 1.300 WAS 1.00000000 input-audio.wav 1.290 1.390 THAT 1.00000000 input-audio.wav 1.290 1.390 IT 1.00000000 input-audio.wav 1.380 1.510 WE'RE 1.00000000 input-audio.wav 1.500 1.660 GOING 1.00000000 input-audio.wav 1.650 1.720 TO 1.00000000 input-audio.wav 1.710 1.930 FLY 1.00000000 input-audio.wav 1.920 2.110 OVER 1.00000000 input-audio.wav 2.100 2.380 SAINT 1.00000000 input-audio.wav 2.370 2.950 LOUIS 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 1.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 1.00000000 input-audio.wav 0.870 0.970 \u7684 1.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 1.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 1.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 1.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 1.00000000 input-audio.wav 3.130 3.340 \u7684 1.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 1.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 1.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.5+ Limitations As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. When chunking the input, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. There is error-reconciliation code to attempt to compensate for these overlapped sections, but it is still possible for issues to be encountered at these \"chunk\" boundaries. Arabic Script Languages Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments GPU Support This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"asr-end2end-v3 (Automatic Speech Recognition)"},{"location":"plugins/asr-end2end-v3.html#asr-end2end-v3-automatic-speech-recognition","text":"","title":"asr-end2end-v3 (Automatic Speech Recognition)"},{"location":"plugins/asr-end2end-v3.html#version-changelog","text":"Plugin Version Change v3.0.0 Initial plugin release of the fully quantized ctranslate2 framework end-to-end models, tested and published with OLIVE 5.7.1","title":"Version Changelog"},{"location":"plugins/asr-end2end-v3.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is an iteration of SRI's end-to-end ASR plugin which now uses fully quantized faster-wav2vec models for dramatic memory usage reduction over the asr-end2end-v2 plugin, which was already a significant reduction over the original plugin. The output format is identical to the existing ASR plugin output which is detailed below. This version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. The -augmented- domains have also been trained on degraded speech as well as clean speech and they are expected to perform much better on noisy/reverberant recordings. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance.","title":"Description"},{"location":"plugins/asr-end2end-v3.html#domains","text":"english-augmented-v3 (both 8k and 16k data) english-spanish-v1 * Experimental domain meant for English and Spanish code-switching farsi-v1 french-v1 iraqiArabic-v1 japanese-augmented-v2 khmer-augmented-v2 korean-augmented-v1 levantineArabic-v1 mandarin-augmented-v2 pashto-v1 russian-augmented-v1 spanish-augmented-v1 thai-augmented-v1 ukrainian-augmented-v1","title":"Domains"},{"location":"plugins/asr-end2end-v3.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-end2end-v3.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 AND 1.00000000 input-audio.wav 0.210 0.340 WE'RE 1.00000000 input-audio.wav 0.330 0.460 GOING 1.00000000 input-audio.wav 0.450 0.520 TO 1.00000000 input-audio.wav 0.510 0.940 FLY 1.00000000 input-audio.wav 1.080 1.300 WAS 1.00000000 input-audio.wav 1.290 1.390 THAT 1.00000000 input-audio.wav 1.290 1.390 IT 1.00000000 input-audio.wav 1.380 1.510 WE'RE 1.00000000 input-audio.wav 1.500 1.660 GOING 1.00000000 input-audio.wav 1.650 1.720 TO 1.00000000 input-audio.wav 1.710 1.930 FLY 1.00000000 input-audio.wav 1.920 2.110 OVER 1.00000000 input-audio.wav 2.100 2.380 SAINT 1.00000000 input-audio.wav 2.370 2.950 LOUIS 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 1.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 1.00000000 input-audio.wav 0.870 0.970 \u7684 1.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 1.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 1.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 1.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 1.00000000 input-audio.wav 3.130 3.340 \u7684 1.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 1.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 1.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Outputs"},{"location":"plugins/asr-end2end-v3.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-end2end-v3.html#compatibility","text":"OLIVE 5.5+","title":"Compatibility"},{"location":"plugins/asr-end2end-v3.html#limitations","text":"As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-end2end-v3.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-end2end-v3.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. When chunking the input, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. There is error-reconciliation code to attempt to compensate for these overlapped sections, but it is still possible for issues to be encountered at these \"chunk\" boundaries.","title":"Overlap"},{"location":"plugins/asr-end2end-v3.html#arabic-script-languages","text":"Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Arabic Script Languages"},{"location":"plugins/asr-end2end-v3.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-end2end-v3.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-end2end-v3.html#gpu-support","text":"This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/asr-end2end-v3.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Global Options"},{"location":"plugins/asr-end2end-v4.html","text":"asr-end2end-v4 (Automatic Speech Recognition) Version Changelog Plugin Version Change v4.0.0 Plugin release of (1) the wav2vec2-bert (w2v2-bert) models for higher transcription accuracy and (2) fully quantized ctranslate2-converted wav2vec2 (w2v2) models for fast inference for all supported languages, tested and published with OLIVE 6.0.0 Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is an iteration of SRI's end-to-end ASR plugin which now uses a more recent variation of w2v2, w2v2-bert, which provides improved transcription accuracy compared to the standard w2v2 since it has been pre-trained on a much larger amount of audio (4.5M hours vs. 440K hours) and uses conformers instead of transformers which are known to capture both the global and local interactions in the audio. This plugin further supports fully quantized faster w2v2 models for much faster inference with reduced memory usage compared to the w2v2-bert models, at the expense of transcription accuracy. The output format is identical to the existing ASR plugin output which is detailed below. This version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is language-specific, and is only capable of transcribing speech in that one language. For each language, an accuracy and a speed domain is provided, expect for the Levantine Arabic and the experimental English-Spanish code-switching domains. Both w2v2 and w2v2-bert models provided similar transcription accuracy in these cases. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of 8k and 16k training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. All domains have also been trained on degraded speech as well as clean speech and they are expected to perform much better on noisy/reverberant recordings. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All -accuracy- and -speed- domains are based on the w2v2-bert and w2v2 model architectures, respectively. An n-gram language model is used during decoding to improve the ASR performance. Domains english-accuracy-v1 english-speed-v1 english-spanish-speed-v1 * Experimental domain meant for English and Spanish code-switching farsi-accuracy-v1 farsi-speed-v1 french-accuracy-v1 french-speed-v1 iraqiArabic-accuracy-v1 iraqiArabic-speed-v1 japanese-accuracy-v1 japanese-speed-v1 khmer-accuracy-v1 khmer-speed-v1 koran-accuracy-v1 korean-speed-v1 levantineArabic-speed-v1 mandarin-accuracy-v1 mandarin-speed-v1 pashto-accuracy-v1 pashto-speed-v1 russian-accuracy-v1 russian-speed-v1 spanish-accuracy-v1 spanish-speed-v1 thai-accuracy-v1 thai-speed-v1 ukrainian-accuracy-v1 ukrainian-speed-v1 Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 1.00000000 input-audio.wav 0.210 0.340 we're 1.00000000 input-audio.wav 0.330 0.460 going 1.00000000 input-audio.wav 0.450 0.520 to 1.00000000 input-audio.wav 0.510 0.940 fly 1.00000000 input-audio.wav 1.080 1.300 was 1.00000000 input-audio.wav 1.290 1.390 that 1.00000000 input-audio.wav 1.290 1.390 it 1.00000000 input-audio.wav 1.380 1.510 we're 1.00000000 input-audio.wav 1.500 1.660 going 1.00000000 input-audio.wav 1.650 1.720 to 1.00000000 input-audio.wav 1.710 1.930 fly 1.00000000 input-audio.wav 1.920 2.110 over 1.00000000 input-audio.wav 2.100 2.380 saint 1.00000000 input-audio.wav 2.370 2.950 louis 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 1.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 1.00000000 input-audio.wav 0.870 0.970 \u7684 1.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 1.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 1.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 1.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 1.00000000 input-audio.wav 3.130 3.340 \u7684 1.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 1.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 1.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 6.0+ Limitations As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. In an effort to address this issue, for the first time, SRI releases an accuracy and a speed domain for each language to fulfill different customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. When chunking the input, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. There is error-reconciliation code to attempt to compensate for these overlapped sections, but it is still possible for issues to be encountered at these \"chunk\" boundaries. Arabic Script Languages Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments GPU Support This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Automatic Speech Recognition (ASR) end2end/GPU"},{"location":"plugins/asr-end2end-v4.html#asr-end2end-v4-automatic-speech-recognition","text":"","title":"asr-end2end-v4 (Automatic Speech Recognition)"},{"location":"plugins/asr-end2end-v4.html#version-changelog","text":"Plugin Version Change v4.0.0 Plugin release of (1) the wav2vec2-bert (w2v2-bert) models for higher transcription accuracy and (2) fully quantized ctranslate2-converted wav2vec2 (w2v2) models for fast inference for all supported languages, tested and published with OLIVE 6.0.0","title":"Version Changelog"},{"location":"plugins/asr-end2end-v4.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is an iteration of SRI's end-to-end ASR plugin which now uses a more recent variation of w2v2, w2v2-bert, which provides improved transcription accuracy compared to the standard w2v2 since it has been pre-trained on a much larger amount of audio (4.5M hours vs. 440K hours) and uses conformers instead of transformers which are known to capture both the global and local interactions in the audio. This plugin further supports fully quantized faster w2v2 models for much faster inference with reduced memory usage compared to the w2v2-bert models, at the expense of transcription accuracy. The output format is identical to the existing ASR plugin output which is detailed below. This version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is language-specific, and is only capable of transcribing speech in that one language. For each language, an accuracy and a speed domain is provided, expect for the Levantine Arabic and the experimental English-Spanish code-switching domains. Both w2v2 and w2v2-bert models provided similar transcription accuracy in these cases. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of 8k and 16k training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. All domains have also been trained on degraded speech as well as clean speech and they are expected to perform much better on noisy/reverberant recordings. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All -accuracy- and -speed- domains are based on the w2v2-bert and w2v2 model architectures, respectively. An n-gram language model is used during decoding to improve the ASR performance.","title":"Description"},{"location":"plugins/asr-end2end-v4.html#domains","text":"english-accuracy-v1 english-speed-v1 english-spanish-speed-v1 * Experimental domain meant for English and Spanish code-switching farsi-accuracy-v1 farsi-speed-v1 french-accuracy-v1 french-speed-v1 iraqiArabic-accuracy-v1 iraqiArabic-speed-v1 japanese-accuracy-v1 japanese-speed-v1 khmer-accuracy-v1 khmer-speed-v1 koran-accuracy-v1 korean-speed-v1 levantineArabic-speed-v1 mandarin-accuracy-v1 mandarin-speed-v1 pashto-accuracy-v1 pashto-speed-v1 russian-accuracy-v1 russian-speed-v1 spanish-accuracy-v1 spanish-speed-v1 thai-accuracy-v1 thai-speed-v1 ukrainian-accuracy-v1 ukrainian-speed-v1","title":"Domains"},{"location":"plugins/asr-end2end-v4.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-end2end-v4.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 1.00000000 input-audio.wav 0.210 0.340 we're 1.00000000 input-audio.wav 0.330 0.460 going 1.00000000 input-audio.wav 0.450 0.520 to 1.00000000 input-audio.wav 0.510 0.940 fly 1.00000000 input-audio.wav 1.080 1.300 was 1.00000000 input-audio.wav 1.290 1.390 that 1.00000000 input-audio.wav 1.290 1.390 it 1.00000000 input-audio.wav 1.380 1.510 we're 1.00000000 input-audio.wav 1.500 1.660 going 1.00000000 input-audio.wav 1.650 1.720 to 1.00000000 input-audio.wav 1.710 1.930 fly 1.00000000 input-audio.wav 1.920 2.110 over 1.00000000 input-audio.wav 2.100 2.380 saint 1.00000000 input-audio.wav 2.370 2.950 louis 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 1.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 1.00000000 input-audio.wav 0.870 0.970 \u7684 1.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 1.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 1.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 1.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 1.00000000 input-audio.wav 3.130 3.340 \u7684 1.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 1.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 1.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Outputs"},{"location":"plugins/asr-end2end-v4.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-end2end-v4.html#compatibility","text":"OLIVE 6.0+","title":"Compatibility"},{"location":"plugins/asr-end2end-v4.html#limitations","text":"As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. In an effort to address this issue, for the first time, SRI releases an accuracy and a speed domain for each language to fulfill different customer needs.","title":"Limitations"},{"location":"plugins/asr-end2end-v4.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-end2end-v4.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. When chunking the input, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. There is error-reconciliation code to attempt to compensate for these overlapped sections, but it is still possible for issues to be encountered at these \"chunk\" boundaries.","title":"Overlap"},{"location":"plugins/asr-end2end-v4.html#arabic-script-languages","text":"Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Arabic Script Languages"},{"location":"plugins/asr-end2end-v4.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-end2end-v4.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-end2end-v4.html#gpu-support","text":"This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/asr-end2end-v4.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Global Options"},{"location":"plugins/asr-whisper-v1.html","text":"asr-whisper-v1 (Automatic Speech Recognition) Version Changelog Plugin Version Change v1.0.0 Initial plugin release of the fine-tuned Whisper ASR models, tested and published with OLIVE 5.6.0 Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is the first release of SRI's whisper-based ASR plugin which uses an in-house fine-tuned whisper model for mapping acoustic features to the letters/characters in the target language. Currently, this plugin supports 6 languages, namely English, Spanish, Russian, Mandarin, Ukrainian and Khmer. The output format is identical to the asr-dynapy output with the exception of asr-whisper returning segment-level output (with segment boundaries) instead of word-level output (with word boundaries). Please see the details below. This first version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance. Domains english-v1 (both 8k and 16k data) khmer-augmented-v1 mandarin-v1 russian-v1 spanish-v1 ukrainian-v1 Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.00 11.00 of a three days fever from all which considerations we may conclude as a whole that these things which cannot make good the advantages they promise 1.00000000 input-audio.wav 11.00 15.42 which are never made perfect by the assembly of all good things 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.00 11.00 \u6ca1 \u4ec0\u4e48 \u610f\u89c1 \u7684 \u5c31 \u6536\u62fe \u4e2a \u7eff\u5316 \u5c31 \u6536\u62fe \u5916\u8fb9 \u7684 \u7eff\u5316 \u5916\u8fb9\u513f \u90a3\u4e2a \u7eff\u5316 \u5c31 \u6709\u7684 \u8bf7 \u68c0\u67e5 \u4e00\u70b9\u513f \u6709\u7684 \u90a3\u4e2a \u5e03\u62c9\u6559 \u4e0d \u5230 \u7684 \u6709\u7684 \u5c31 \u53bb \u6559 \u7136\u540e 1.00000000 input-audio.wav 11.00 14.12 \u8ba9 \u5b83 \u90a3\u4e2a \u82b1\u513f \u5e72 \u7c97 \u7684 \u90a3\u4e2a \u5c31 \u628a \u5b83 \u5254 \u51fa\u6765 1.00000000 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.6+ Limitations ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments GPU Support This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"asr-whisper-v1 (Automatic Speech Recognition)"},{"location":"plugins/asr-whisper-v1.html#asr-whisper-v1-automatic-speech-recognition","text":"","title":"asr-whisper-v1 (Automatic Speech Recognition)"},{"location":"plugins/asr-whisper-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release of the fine-tuned Whisper ASR models, tested and published with OLIVE 5.6.0","title":"Version Changelog"},{"location":"plugins/asr-whisper-v1.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is the first release of SRI's whisper-based ASR plugin which uses an in-house fine-tuned whisper model for mapping acoustic features to the letters/characters in the target language. Currently, this plugin supports 6 languages, namely English, Spanish, Russian, Mandarin, Ukrainian and Khmer. The output format is identical to the asr-dynapy output with the exception of asr-whisper returning segment-level output (with segment boundaries) instead of word-level output (with word boundaries). Please see the details below. This first version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance.","title":"Description"},{"location":"plugins/asr-whisper-v1.html#domains","text":"english-v1 (both 8k and 16k data) khmer-augmented-v1 mandarin-v1 russian-v1 spanish-v1 ukrainian-v1","title":"Domains"},{"location":"plugins/asr-whisper-v1.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-whisper-v1.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.00 11.00 of a three days fever from all which considerations we may conclude as a whole that these things which cannot make good the advantages they promise 1.00000000 input-audio.wav 11.00 15.42 which are never made perfect by the assembly of all good things 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.00 11.00 \u6ca1 \u4ec0\u4e48 \u610f\u89c1 \u7684 \u5c31 \u6536\u62fe \u4e2a \u7eff\u5316 \u5c31 \u6536\u62fe \u5916\u8fb9 \u7684 \u7eff\u5316 \u5916\u8fb9\u513f \u90a3\u4e2a \u7eff\u5316 \u5c31 \u6709\u7684 \u8bf7 \u68c0\u67e5 \u4e00\u70b9\u513f \u6709\u7684 \u90a3\u4e2a \u5e03\u62c9\u6559 \u4e0d \u5230 \u7684 \u6709\u7684 \u5c31 \u53bb \u6559 \u7136\u540e 1.00000000 input-audio.wav 11.00 14.12 \u8ba9 \u5b83 \u90a3\u4e2a \u82b1\u513f \u5e72 \u7c97 \u7684 \u90a3\u4e2a \u5c31 \u628a \u5b83 \u5254 \u51fa\u6765 1.00000000","title":"Outputs"},{"location":"plugins/asr-whisper-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-whisper-v1.html#compatibility","text":"OLIVE 5.6+","title":"Compatibility"},{"location":"plugins/asr-whisper-v1.html#limitations","text":"ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-whisper-v1.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-whisper-v1.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points.","title":"Overlap"},{"location":"plugins/asr-whisper-v1.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-whisper-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-whisper-v1.html#gpu-support","text":"This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/asr-whisper-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Global Options"},{"location":"plugins/asr-whisper-v2.html","text":"asr-whisper-v1 (Automatic Speech Recognition) Version Changelog Plugin Version Change v2.0.0 Initial plugin release of the faster-whisper-based fine-tuned Whisper ASR models, tested and published with OLIVE 5.7.0. Now includes Japanese support. Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin builds on the first release of SRI's whisper-based ASR plugin which uses an in-house fine-tuned whisper model for mapping acoustic features to the letters/characters in the target language, and is now based on the faster-whisper architecture, providing memory usage and speed improvements over the initial release of the plugin. Currently, this plugin supports 7 languages, namely English, Spanish, Russian, Mandarin, Ukrainian, Japanese, and Khmer. The output format is identical to the asr-dynapy output with the exception of asr-whisper returning segment-level output (with segment boundaries) instead of word-level output (with word boundaries). Please see the details below. This first version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance. Domains english-v1 (both 8k and 16k data) japanese-v1 khmer-augmented-v2 mandarin-augmented-v2 russian-v1 spanish-v1 ukrainian-v1 Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.00 11.00 of a three days fever from all which considerations we may conclude as a whole that these things which cannot make good the advantages they promise 1.00000000 input-audio.wav 11.00 15.42 which are never made perfect by the assembly of all good things 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.00 11.00 \u6ca1 \u4ec0\u4e48 \u610f\u89c1 \u7684 \u5c31 \u6536\u62fe \u4e2a \u7eff\u5316 \u5c31 \u6536\u62fe \u5916\u8fb9 \u7684 \u7eff\u5316 \u5916\u8fb9\u513f \u90a3\u4e2a \u7eff\u5316 \u5c31 \u6709\u7684 \u8bf7 \u68c0\u67e5 \u4e00\u70b9\u513f \u6709\u7684 \u90a3\u4e2a \u5e03\u62c9\u6559 \u4e0d \u5230 \u7684 \u6709\u7684 \u5c31 \u53bb \u6559 \u7136\u540e 1.00000000 input-audio.wav 11.00 14.12 \u8ba9 \u5b83 \u90a3\u4e2a \u82b1\u513f \u5e72 \u7c97 \u7684 \u90a3\u4e2a \u5c31 \u628a \u5b83 \u5254 \u51fa\u6765 1.00000000 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.7+ Limitations ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments GPU Support This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"asr-whisper-v1 (Automatic Speech Recognition)"},{"location":"plugins/asr-whisper-v2.html#asr-whisper-v1-automatic-speech-recognition","text":"","title":"asr-whisper-v1 (Automatic Speech Recognition)"},{"location":"plugins/asr-whisper-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release of the faster-whisper-based fine-tuned Whisper ASR models, tested and published with OLIVE 5.7.0. Now includes Japanese support.","title":"Version Changelog"},{"location":"plugins/asr-whisper-v2.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin builds on the first release of SRI's whisper-based ASR plugin which uses an in-house fine-tuned whisper model for mapping acoustic features to the letters/characters in the target language, and is now based on the faster-whisper architecture, providing memory usage and speed improvements over the initial release of the plugin. Currently, this plugin supports 7 languages, namely English, Spanish, Russian, Mandarin, Ukrainian, Japanese, and Khmer. The output format is identical to the asr-dynapy output with the exception of asr-whisper returning segment-level output (with segment boundaries) instead of word-level output (with word boundaries). Please see the details below. This first version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance.","title":"Description"},{"location":"plugins/asr-whisper-v2.html#domains","text":"english-v1 (both 8k and 16k data) japanese-v1 khmer-augmented-v2 mandarin-augmented-v2 russian-v1 spanish-v1 ukrainian-v1","title":"Domains"},{"location":"plugins/asr-whisper-v2.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-whisper-v2.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.00 11.00 of a three days fever from all which considerations we may conclude as a whole that these things which cannot make good the advantages they promise 1.00000000 input-audio.wav 11.00 15.42 which are never made perfect by the assembly of all good things 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.00 11.00 \u6ca1 \u4ec0\u4e48 \u610f\u89c1 \u7684 \u5c31 \u6536\u62fe \u4e2a \u7eff\u5316 \u5c31 \u6536\u62fe \u5916\u8fb9 \u7684 \u7eff\u5316 \u5916\u8fb9\u513f \u90a3\u4e2a \u7eff\u5316 \u5c31 \u6709\u7684 \u8bf7 \u68c0\u67e5 \u4e00\u70b9\u513f \u6709\u7684 \u90a3\u4e2a \u5e03\u62c9\u6559 \u4e0d \u5230 \u7684 \u6709\u7684 \u5c31 \u53bb \u6559 \u7136\u540e 1.00000000 input-audio.wav 11.00 14.12 \u8ba9 \u5b83 \u90a3\u4e2a \u82b1\u513f \u5e72 \u7c97 \u7684 \u90a3\u4e2a \u5c31 \u628a \u5b83 \u5254 \u51fa\u6765 1.00000000","title":"Outputs"},{"location":"plugins/asr-whisper-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-whisper-v2.html#compatibility","text":"OLIVE 5.7+","title":"Compatibility"},{"location":"plugins/asr-whisper-v2.html#limitations","text":"ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-whisper-v2.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-whisper-v2.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points.","title":"Overlap"},{"location":"plugins/asr-whisper-v2.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-whisper-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-whisper-v2.html#gpu-support","text":"This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/asr-whisper-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Global Options"},{"location":"plugins/asr.html","text":"redirect: plugins/asr-end2end-v4.md","title":"Asr"},{"location":"plugins/asr.html#redirect-pluginsasr-end2end-v4md","text":"","title":"redirect: plugins/asr-end2end-v4.md"},{"location":"plugins/dfa-end2end-commercial-v1.html","text":"dfa-end2end-commercial-v1 (Deep Fake Audio Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.7.1 Description Deepfake audio plugins classify whether a given speech sample comes from a human or from a synthetic speech generator. Synthetic speech refers to artificially generated audio that mimics human speech using advanced algorithms and machine learning techniques. Unlike real speech, which originates from an actual human voice, synthetic speech is created by machines and can be manipulated to sound convincingly like any individual. There are various types of synthetic speech, including text-to-speech (TTS), voice cloning, and neural audio synthesis. Detecting synthetic speech is crucial to preventing misinformation, and safeguarding against malicious activities such as impersonation and fraud. This plugin detects whether a given audio sample came from a person or from a synthetic system. The plugin uses a model combining a fine-tuned pretrained large speech model (Multi-Resolution HuBERT) and a AASIST spectro-temporal graph attention network to extract embedding representations from an input audio. These are then fed into a calibrated PLDA backend to score the audio sample as either synthetic (from TTS or VC) or coming from a person. Domains This plugin has one domain: multi-v1 . multi This domain uses a PLDA backend scorer trained with multiple deep fake approaches like voice conversion, or synthetic speech, to output the log-likelihood ratios. This domain can handle audio with 8kHz sampling rate and higher, although it was optimized for 16kHz sampling rate. Inputs Audio file or buffer and an optional identifier. Outputs The scores represent whether a given audio is detected as being \"synthetic\", generated speech or not. The score itself comes from the learned embedding in a Multi-Resolution HuBERT AASIST network. The embedding is passed through a PLDA backend scorer which outputs the log-likelihood ratios (as commonly seen in other OLIVE plugins) representing the probability that the audio is synthetic and not from a human speaker. The scores are log-likelihood ratios where a score of greater than \"0\" is considered a \"synthetic\" or deep-fake audio detection and a score below \"0\" is considered audio from a bonafide human talker. The threshold in the configuration file determines the final classification. For example, if the threshold is equal to 0, then positive scores are classified as synthetic while negative scores are classified as real. An example output excerpt: input-audio-1.wav synthetic -1.9423 input-audio-2.wav synthetic 1.2817 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region indicates synthetic speech. RegionScorerRequest Compatibility OLIVE 5.7.1+ Limitations Known or potential limitations of the plugin are outlined below. Minimum Audio Duration Audio samples are assumed to be at least 4 seconds long. Any segment shorter than 4 seconds is extended with zero padding to reach at least 4 seconds. If the audio is longer than 4 seconds, the plugin steps through the waveform with 4-second windows to compute embeddings. In this case the embeddings are averaged to compute a score as final output. Minimum Sampling Rate This plugin supports audio sampling rates of 8kHz and higher. The recommended sampling rate for input audio is 16kHz or higher. Audio with a lower sampling rate will be internally upsampled to 16kHz. However, this upsampling process carries some risks, such as missing bandwidth and artifacts in the upper frequencies that could be useful for deepfake detection Types of Speech Generators The plugin performs best on TTS generators that are based on Neural Networks. The plugin has more difficulty with Voice Conversion generators that use lower level, waveform-specific manipulations. Impersonators This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from professional impersonators are not within the scope of this plugin. Processing Speed and Memory The plugin requires more resources than other audio deepfake plugins. It reaches slightly better than 1xRT when run on CPU (single thread), and much higher speeds (16xRT or more) when run on GPU. Comments Detecting fake speech generators is a cat-and-mouse game. The field is (as of writing) moving incredibly fast with new advances and models constantly released. We strove to develop a system that is robust to unseen generators but cannot guarantee that we captured the universe of in-the-wild deepfake audio generators. GPU Support This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default, this plugin will run on GPU only. Global Options The following global scoring options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 score_offset An offset added to scores to allow 0.0 to be a tuned decision point. Higher score offset values will shift output scores towards \"synthetic\", making speech more likely to be detected as deep fake. 0.0 -10.0 - 10.0 Region Options The following region scoring options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 score_offset An offset added to scores to allow 0.0 to be a tuned decision point. Higher score offset values will shift output scores towards \"synthetic\", making speech more likely to be detected as deep fake. 0.0 -10.0 - 10.0 min_region The minimum length of a synthetic region in order to be recognized. 1.0 0.3 - 3.0 threshold Threshold for deepfake region detection. The higher the threshold is, less deepfake regions will be selected 0.0 -0.5 - 0.5","title":"dfa-end2end-commercial-v1 (Deep Fake Audio Detection)"},{"location":"plugins/dfa-end2end-commercial-v1.html#dfa-end2end-commercial-v1-deep-fake-audio-detection","text":"","title":"dfa-end2end-commercial-v1 (Deep Fake Audio Detection)"},{"location":"plugins/dfa-end2end-commercial-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.7.1","title":"Version Changelog"},{"location":"plugins/dfa-end2end-commercial-v1.html#description","text":"Deepfake audio plugins classify whether a given speech sample comes from a human or from a synthetic speech generator. Synthetic speech refers to artificially generated audio that mimics human speech using advanced algorithms and machine learning techniques. Unlike real speech, which originates from an actual human voice, synthetic speech is created by machines and can be manipulated to sound convincingly like any individual. There are various types of synthetic speech, including text-to-speech (TTS), voice cloning, and neural audio synthesis. Detecting synthetic speech is crucial to preventing misinformation, and safeguarding against malicious activities such as impersonation and fraud. This plugin detects whether a given audio sample came from a person or from a synthetic system. The plugin uses a model combining a fine-tuned pretrained large speech model (Multi-Resolution HuBERT) and a AASIST spectro-temporal graph attention network to extract embedding representations from an input audio. These are then fed into a calibrated PLDA backend to score the audio sample as either synthetic (from TTS or VC) or coming from a person.","title":"Description"},{"location":"plugins/dfa-end2end-commercial-v1.html#domains","text":"This plugin has one domain: multi-v1 . multi This domain uses a PLDA backend scorer trained with multiple deep fake approaches like voice conversion, or synthetic speech, to output the log-likelihood ratios. This domain can handle audio with 8kHz sampling rate and higher, although it was optimized for 16kHz sampling rate.","title":"Domains"},{"location":"plugins/dfa-end2end-commercial-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/dfa-end2end-commercial-v1.html#outputs","text":"The scores represent whether a given audio is detected as being \"synthetic\", generated speech or not. The score itself comes from the learned embedding in a Multi-Resolution HuBERT AASIST network. The embedding is passed through a PLDA backend scorer which outputs the log-likelihood ratios (as commonly seen in other OLIVE plugins) representing the probability that the audio is synthetic and not from a human speaker. The scores are log-likelihood ratios where a score of greater than \"0\" is considered a \"synthetic\" or deep-fake audio detection and a score below \"0\" is considered audio from a bonafide human talker. The threshold in the configuration file determines the final classification. For example, if the threshold is equal to 0, then positive scores are classified as synthetic while negative scores are classified as real. An example output excerpt: input-audio-1.wav synthetic -1.9423 input-audio-2.wav synthetic 1.2817","title":"Outputs"},{"location":"plugins/dfa-end2end-commercial-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region indicates synthetic speech. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/dfa-end2end-commercial-v1.html#compatibility","text":"OLIVE 5.7.1+","title":"Compatibility"},{"location":"plugins/dfa-end2end-commercial-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/dfa-end2end-commercial-v1.html#minimum-audio-duration","text":"Audio samples are assumed to be at least 4 seconds long. Any segment shorter than 4 seconds is extended with zero padding to reach at least 4 seconds. If the audio is longer than 4 seconds, the plugin steps through the waveform with 4-second windows to compute embeddings. In this case the embeddings are averaged to compute a score as final output.","title":"Minimum Audio Duration"},{"location":"plugins/dfa-end2end-commercial-v1.html#minimum-sampling-rate","text":"This plugin supports audio sampling rates of 8kHz and higher. The recommended sampling rate for input audio is 16kHz or higher. Audio with a lower sampling rate will be internally upsampled to 16kHz. However, this upsampling process carries some risks, such as missing bandwidth and artifacts in the upper frequencies that could be useful for deepfake detection","title":"Minimum Sampling Rate"},{"location":"plugins/dfa-end2end-commercial-v1.html#types-of-speech-generators","text":"The plugin performs best on TTS generators that are based on Neural Networks. The plugin has more difficulty with Voice Conversion generators that use lower level, waveform-specific manipulations.","title":"Types of Speech Generators"},{"location":"plugins/dfa-end2end-commercial-v1.html#impersonators","text":"This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from professional impersonators are not within the scope of this plugin.","title":"Impersonators"},{"location":"plugins/dfa-end2end-commercial-v1.html#processing-speed-and-memory","text":"The plugin requires more resources than other audio deepfake plugins. It reaches slightly better than 1xRT when run on CPU (single thread), and much higher speeds (16xRT or more) when run on GPU.","title":"Processing Speed and Memory"},{"location":"plugins/dfa-end2end-commercial-v1.html#comments","text":"Detecting fake speech generators is a cat-and-mouse game. The field is (as of writing) moving incredibly fast with new advances and models constantly released. We strove to develop a system that is robust to unseen generators but cannot guarantee that we captured the universe of in-the-wild deepfake audio generators.","title":"Comments"},{"location":"plugins/dfa-end2end-commercial-v1.html#gpu-support","text":"This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default, this plugin will run on GPU only.","title":"GPU Support"},{"location":"plugins/dfa-end2end-commercial-v1.html#global-options","text":"The following global scoring options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 score_offset An offset added to scores to allow 0.0 to be a tuned decision point. Higher score offset values will shift output scores towards \"synthetic\", making speech more likely to be detected as deep fake. 0.0 -10.0 - 10.0","title":"Global Options"},{"location":"plugins/dfa-end2end-commercial-v1.html#region-options","text":"The following region scoring options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 score_offset An offset added to scores to allow 0.0 to be a tuned decision point. Higher score offset values will shift output scores towards \"synthetic\", making speech more likely to be detected as deep fake. 0.0 -10.0 - 10.0 min_region The minimum length of a synthetic region in order to be recognized. 1.0 0.3 - 3.0 threshold Threshold for deepfake region detection. The higher the threshold is, less deepfake regions will be selected 0.0 -0.5 - 0.5","title":"Region Options"},{"location":"plugins/dfa-end2end-v1.html","text":"dfa-end2end-v1 (Deep Fake Audio Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.7.1 Description Deepfake audio plugins classify whether a given speech sample comes from a human or from a synthetic speech generator. Synthetic speech refers to artificially generated audio that mimics human speech using advanced algorithms and machine learning techniques. Unlike real speech, which originates from an actual human voice, synthetic speech is created by machines and can be manipulated to sound convincingly like any individual. There are various types of synthetic speech, including text-to-speech (TTS), voice cloning, and neural audio synthesis. Detecting synthetic speech is crucial to preventing misinformation, and safeguarding against malicious activities such as impersonation and fraud. This plugin detects whether a given audio sample came from a person or from a synthetic system. The plugin uses a model combining a fine-tuned pretrained large speech model (Multi-Resolution HuBERT) and a AASIST spectro-temporal graph attention network to extract embedding representations from an input audio. These are then fed into a calibrated PLDA backend to score the audio sample as either synthetic (from TTS or VC) or coming from a person. Domains This plugin has one domain: multicond-v1 . multicond This domain uses a PLDA backend scorer trained with multiple deep fake approaches like voice conversion, or synthetic speech, to output the log-likelihood ratios. This domain can handle audio with 8kHz sampling rate and higher, although it was optimized for 16kHz sampling rate. Inputs Audio file or buffer and an optional identifier. Outputs The scores represent whether a given audio is detected as being \"synthetic\", generated speech or not. The score itself comes from the learned embedding in a Multi-Resolution HuBERT AASIST network. The embedding is passed through a PLDA backend scorer which outputs the log-likelihood ratios (as commonly seen in other OLIVE plugins) representing the probability that the audio is synthetic and not from a human speaker. The scores are log-likelihood ratios where a score of greater than \"0\" is considered a \"synthetic\" or deep-fake audio detection and a score below \"0\" is considered audio from a bonafide human talker. The threshold in the configuration file determines the final classification. For example, if the threshold is equal to 0, then positive scores are classified as synthetic while negative scores are classified as real. An example output excerpt: input-audio-1.wav synthetic -1.9423 input-audio-2.wav synthetic 1.2817 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest Compatibility OLIVE 5.7.1+ Limitations Known or potential limitations of the plugin are outlined below. Minimum Audio Duration Audio samples are assumed to be at least 4 seconds long. Any segment shorter than 4 seconds is extended with zero padding to reach at least 4 seconds. If the audio is longer than 4 seconds, the plugin steps through the waveform with 4-second windows to compute embeddings. In this case the embeddings are averaged to compute a score as final output. Minimum Sampling Rate This plugin supports audio sampling rates of 8kHz and higher. The recommended sampling rate for input audio is 16kHz or higher. Audio with a lower sampling rate will be internally upsampled to 16kHz. However, this upsampling process carries some risks, such as missing bandwidth and artifacts in the upper frequencies that could be useful for deepfake detection Types of Speech Generators The plugin performs best on TTS generators that are based on Neural Networks. The plugin has more difficulty with Voice Conversion generators that use lower level, waveform-specific manipulations. Impersonators This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from professional impersonators are not within the scope of this plugin. Processing Speed and Memory The plugin requires more resources than other audio deepfake plugins. It reaches slightly better than 1xRT when run on CPU (single thread), and much higher speeds (16xRT or more) when run on GPU. Comments Detecting fake speech generators is a cat-and-mouse game. The field is (as of writing) moving incredibly fast with new advances and models constantly released. We strove to develop a system that is robust to unseen generators but cannot guarantee that we captured the universe of in-the-wild deepfake audio generators. GPU Support This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default, this plugin will run on GPU only. Global Options The following global scoring options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 score_offset An offset added to scores to allow 0.0 to be a tuned decision point. Higher score offset values will shift output scores towards \"synthetic\", making speech more likely to be detected as deep fake. 0.0 -10.0 - 10.0","title":"Deep Fake Audio Detection (DFA)"},{"location":"plugins/dfa-end2end-v1.html#dfa-end2end-v1-deep-fake-audio-detection","text":"","title":"dfa-end2end-v1 (Deep Fake Audio Detection)"},{"location":"plugins/dfa-end2end-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.7.1","title":"Version Changelog"},{"location":"plugins/dfa-end2end-v1.html#description","text":"Deepfake audio plugins classify whether a given speech sample comes from a human or from a synthetic speech generator. Synthetic speech refers to artificially generated audio that mimics human speech using advanced algorithms and machine learning techniques. Unlike real speech, which originates from an actual human voice, synthetic speech is created by machines and can be manipulated to sound convincingly like any individual. There are various types of synthetic speech, including text-to-speech (TTS), voice cloning, and neural audio synthesis. Detecting synthetic speech is crucial to preventing misinformation, and safeguarding against malicious activities such as impersonation and fraud. This plugin detects whether a given audio sample came from a person or from a synthetic system. The plugin uses a model combining a fine-tuned pretrained large speech model (Multi-Resolution HuBERT) and a AASIST spectro-temporal graph attention network to extract embedding representations from an input audio. These are then fed into a calibrated PLDA backend to score the audio sample as either synthetic (from TTS or VC) or coming from a person.","title":"Description"},{"location":"plugins/dfa-end2end-v1.html#domains","text":"This plugin has one domain: multicond-v1 . multicond This domain uses a PLDA backend scorer trained with multiple deep fake approaches like voice conversion, or synthetic speech, to output the log-likelihood ratios. This domain can handle audio with 8kHz sampling rate and higher, although it was optimized for 16kHz sampling rate.","title":"Domains"},{"location":"plugins/dfa-end2end-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/dfa-end2end-v1.html#outputs","text":"The scores represent whether a given audio is detected as being \"synthetic\", generated speech or not. The score itself comes from the learned embedding in a Multi-Resolution HuBERT AASIST network. The embedding is passed through a PLDA backend scorer which outputs the log-likelihood ratios (as commonly seen in other OLIVE plugins) representing the probability that the audio is synthetic and not from a human speaker. The scores are log-likelihood ratios where a score of greater than \"0\" is considered a \"synthetic\" or deep-fake audio detection and a score below \"0\" is considered audio from a bonafide human talker. The threshold in the configuration file determines the final classification. For example, if the threshold is equal to 0, then positive scores are classified as synthetic while negative scores are classified as real. An example output excerpt: input-audio-1.wav synthetic -1.9423 input-audio-2.wav synthetic 1.2817","title":"Outputs"},{"location":"plugins/dfa-end2end-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/dfa-end2end-v1.html#compatibility","text":"OLIVE 5.7.1+","title":"Compatibility"},{"location":"plugins/dfa-end2end-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/dfa-end2end-v1.html#minimum-audio-duration","text":"Audio samples are assumed to be at least 4 seconds long. Any segment shorter than 4 seconds is extended with zero padding to reach at least 4 seconds. If the audio is longer than 4 seconds, the plugin steps through the waveform with 4-second windows to compute embeddings. In this case the embeddings are averaged to compute a score as final output.","title":"Minimum Audio Duration"},{"location":"plugins/dfa-end2end-v1.html#minimum-sampling-rate","text":"This plugin supports audio sampling rates of 8kHz and higher. The recommended sampling rate for input audio is 16kHz or higher. Audio with a lower sampling rate will be internally upsampled to 16kHz. However, this upsampling process carries some risks, such as missing bandwidth and artifacts in the upper frequencies that could be useful for deepfake detection","title":"Minimum Sampling Rate"},{"location":"plugins/dfa-end2end-v1.html#types-of-speech-generators","text":"The plugin performs best on TTS generators that are based on Neural Networks. The plugin has more difficulty with Voice Conversion generators that use lower level, waveform-specific manipulations.","title":"Types of Speech Generators"},{"location":"plugins/dfa-end2end-v1.html#impersonators","text":"This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from professional impersonators are not within the scope of this plugin.","title":"Impersonators"},{"location":"plugins/dfa-end2end-v1.html#processing-speed-and-memory","text":"The plugin requires more resources than other audio deepfake plugins. It reaches slightly better than 1xRT when run on CPU (single thread), and much higher speeds (16xRT or more) when run on GPU.","title":"Processing Speed and Memory"},{"location":"plugins/dfa-end2end-v1.html#comments","text":"Detecting fake speech generators is a cat-and-mouse game. The field is (as of writing) moving incredibly fast with new advances and models constantly released. We strove to develop a system that is robust to unseen generators but cannot guarantee that we captured the universe of in-the-wild deepfake audio generators.","title":"Comments"},{"location":"plugins/dfa-end2end-v1.html#gpu-support","text":"This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default, this plugin will run on GPU only.","title":"GPU Support"},{"location":"plugins/dfa-end2end-v1.html#global-options","text":"The following global scoring options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 score_offset An offset added to scores to allow 0.0 to be a tuned decision point. Higher score offset values will shift output scores towards \"synthetic\", making speech more likely to be detected as deep fake. 0.0 -10.0 - 10.0","title":"Global Options"},{"location":"plugins/dfa-global-v1.html","text":"dfa-global-v1 (Deep Fake Audio Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.5.0 v1.1.0 General plugin updates and improvements, released with OLIVE 5.7.1 Description Deepfake audio plugins classify whether a given speech sample comes from a human or from a synthetic generator. Speech generators often leave many acoustic and phonetic artifacts in the audio. Traditional synthetic speech detectors typically rely on deep learning models trained with acoustic features alone to classify whether a given speech sample comes from a human or from a synthetic generator. These traditional techniques do not leverage any phonetic in determining whether a sample was generated or real. This plugin integrates both phonetic and acoustic information into the model to detect whether a given audio sample came from a person or from a synthetic system. The plugin uses a Residual Neural Network (ResNet) trained on phonetically-rich bottleneck features from bi-lingual automatic speech recognition (ASR) models along with Linear Frequency Cepstral Coefficients (LFCC) acoustic features to train deep learning models for better discrimination between real and synthetic speech. The LFCC features expose the network to the complete frequency range to reveal potential artifacts left by a synthetic generator. This is in contrast to the MFCC features used in most OLIVE tasks that focus more on the frequency range of human speech. Domains This plugin has two domains differentiated by the backend each uses for classification. multi-plda-v1 tends to offer better discrimination, where multi-gb-v1 is typically better calibrated across conditions. This means that multi-plda-v1 will likely provide better performance when the audio conditions are clean and are conditions the model has been exposed to. Conditions that vary widely from what the model was trained on may benefit from using the multi-gb-v1 domain instead. multi-plda-v1 This domain uses a PLDA backend scorer trained with multiple deep fake approaches like voice conversion, or synthetic speech, to output the log-likelihood ratios. This domain can handle audio with 8kHz sampling rate or higher. This domain (PLDA backend) tends to provide better discrimination compared to the Gaussian backend domain. multi-gb-v1 This domain uses a Gaussian backend (GB) scorer trained with multiple deep fake approaches like voice conversion, or synthetic speech, to output the log-likelihood ratios. This domain can handle audio with 8kHz sampling rate or higher. This domain (GB backend) tends to offer better calibration across conditions including unseen conditions (due to using multi-class calibration). Inputs Audio file or buffer and an optional identifier. Outputs The scores represent whether a given audio is detected as being \"synthetic\", generated speech or not. The score itself comes from the learned embedding in a ResNet network. The embedding is passed through a backend scorer (PLDA/GB) which outputs the log-likelihood ratios (as commonly seen in other OLIVE plugins) representing the probability that the audio is fake and not from a human speaker. The scores are log-likelihood ratios where a score of greater than \"0\" is considered a \"synthetic\" or deep-fake audio detection and a score below \"0\" is considered audio from a bonafide human talker. The threshold in the configuration file determines the final classification. For example, if the threshold is equal to 0, then positive scores are classified as real while negative scores are classified as fake. An example output excerpt: input-audio-1.wav synthetic -1.9423 input-audio-2.wav synthetic 1.2817 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest Compatibility OLIVE 5.5+ Limitations Known or potential limitations of the plugin are outlined below. Minimum Audio Duration Audio samples are assumed to be at least 1 seconds long. Any segment shorter than 1 second is extended with \"repeat\" padding, where the waveform is repeated as many times as is necessary to reach at least 1 seconds. If the audio is longer than 1 seconds, the plugin steps through the waveform with 1-second windows using a step size determined in the config file. In this case the final output is the averaged score across all of the windows. Types of Speech Generators The plugin performs best on TTS generators that are based on Neural Networks. The plugin has more difficulty with Voice Conversion generators that use lower level, waveform-specific manipulations. Impersonators This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from professional impersonators are not within the scope of this plugin. Comments Detecting fake speech generators is a cat-and-mouse game. The field is (as of writing) moving incredibly fast with new advances and models constantly released. We strove to develop a system that is robust to unseen generators but cannot guarantee that we captured the universe of in-the-wild deepfake audio generators. Processing Speed and Memory Adaptation is computationally expensive and it requires more resources than global calibration. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 score_offset An offset added to scores to allow 0.0 to be a tuned decision point. Higher score offset values will shift output scores towards \"synthetic\", making speech more likely to be detected as deep fake. 0.0 -10.0 - 10.0","title":"dfa-global-v1 (Deep Fake Audio Detection)"},{"location":"plugins/dfa-global-v1.html#dfa-global-v1-deep-fake-audio-detection","text":"","title":"dfa-global-v1 (Deep Fake Audio Detection)"},{"location":"plugins/dfa-global-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.5.0 v1.1.0 General plugin updates and improvements, released with OLIVE 5.7.1","title":"Version Changelog"},{"location":"plugins/dfa-global-v1.html#description","text":"Deepfake audio plugins classify whether a given speech sample comes from a human or from a synthetic generator. Speech generators often leave many acoustic and phonetic artifacts in the audio. Traditional synthetic speech detectors typically rely on deep learning models trained with acoustic features alone to classify whether a given speech sample comes from a human or from a synthetic generator. These traditional techniques do not leverage any phonetic in determining whether a sample was generated or real. This plugin integrates both phonetic and acoustic information into the model to detect whether a given audio sample came from a person or from a synthetic system. The plugin uses a Residual Neural Network (ResNet) trained on phonetically-rich bottleneck features from bi-lingual automatic speech recognition (ASR) models along with Linear Frequency Cepstral Coefficients (LFCC) acoustic features to train deep learning models for better discrimination between real and synthetic speech. The LFCC features expose the network to the complete frequency range to reveal potential artifacts left by a synthetic generator. This is in contrast to the MFCC features used in most OLIVE tasks that focus more on the frequency range of human speech.","title":"Description"},{"location":"plugins/dfa-global-v1.html#domains","text":"This plugin has two domains differentiated by the backend each uses for classification. multi-plda-v1 tends to offer better discrimination, where multi-gb-v1 is typically better calibrated across conditions. This means that multi-plda-v1 will likely provide better performance when the audio conditions are clean and are conditions the model has been exposed to. Conditions that vary widely from what the model was trained on may benefit from using the multi-gb-v1 domain instead. multi-plda-v1 This domain uses a PLDA backend scorer trained with multiple deep fake approaches like voice conversion, or synthetic speech, to output the log-likelihood ratios. This domain can handle audio with 8kHz sampling rate or higher. This domain (PLDA backend) tends to provide better discrimination compared to the Gaussian backend domain. multi-gb-v1 This domain uses a Gaussian backend (GB) scorer trained with multiple deep fake approaches like voice conversion, or synthetic speech, to output the log-likelihood ratios. This domain can handle audio with 8kHz sampling rate or higher. This domain (GB backend) tends to offer better calibration across conditions including unseen conditions (due to using multi-class calibration).","title":"Domains"},{"location":"plugins/dfa-global-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/dfa-global-v1.html#outputs","text":"The scores represent whether a given audio is detected as being \"synthetic\", generated speech or not. The score itself comes from the learned embedding in a ResNet network. The embedding is passed through a backend scorer (PLDA/GB) which outputs the log-likelihood ratios (as commonly seen in other OLIVE plugins) representing the probability that the audio is fake and not from a human speaker. The scores are log-likelihood ratios where a score of greater than \"0\" is considered a \"synthetic\" or deep-fake audio detection and a score below \"0\" is considered audio from a bonafide human talker. The threshold in the configuration file determines the final classification. For example, if the threshold is equal to 0, then positive scores are classified as real while negative scores are classified as fake. An example output excerpt: input-audio-1.wav synthetic -1.9423 input-audio-2.wav synthetic 1.2817","title":"Outputs"},{"location":"plugins/dfa-global-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/dfa-global-v1.html#compatibility","text":"OLIVE 5.5+","title":"Compatibility"},{"location":"plugins/dfa-global-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/dfa-global-v1.html#minimum-audio-duration","text":"Audio samples are assumed to be at least 1 seconds long. Any segment shorter than 1 second is extended with \"repeat\" padding, where the waveform is repeated as many times as is necessary to reach at least 1 seconds. If the audio is longer than 1 seconds, the plugin steps through the waveform with 1-second windows using a step size determined in the config file. In this case the final output is the averaged score across all of the windows.","title":"Minimum Audio Duration"},{"location":"plugins/dfa-global-v1.html#types-of-speech-generators","text":"The plugin performs best on TTS generators that are based on Neural Networks. The plugin has more difficulty with Voice Conversion generators that use lower level, waveform-specific manipulations.","title":"Types of Speech Generators"},{"location":"plugins/dfa-global-v1.html#impersonators","text":"This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from professional impersonators are not within the scope of this plugin.","title":"Impersonators"},{"location":"plugins/dfa-global-v1.html#comments","text":"Detecting fake speech generators is a cat-and-mouse game. The field is (as of writing) moving incredibly fast with new advances and models constantly released. We strove to develop a system that is robust to unseen generators but cannot guarantee that we captured the universe of in-the-wild deepfake audio generators.","title":"Comments"},{"location":"plugins/dfa-global-v1.html#processing-speed-and-memory","text":"Adaptation is computationally expensive and it requires more resources than global calibration.","title":"Processing Speed and Memory"},{"location":"plugins/dfa-global-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 score_offset An offset added to scores to allow 0.0 to be a tuned decision point. Higher score offset values will shift output scores towards \"synthetic\", making speech more likely to be detected as deep fake. 0.0 -10.0 - 10.0","title":"Global Options"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html","text":"dfa-speakerSpecific-phonetic-v1 (Deep Fake Audio Detection - Speaker Specific) Version Changelog Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.5.0 v1.0.2 Bug fix for input audio files \\<0.5s Description Deep fake speech generators often leave many acoustic and phonetic artifacts in the audio. Traditional synthetic speech detectors typically rely on deep learning models trained with acoustic features alone to classify whether a given speech sample comes from a human or from a synthetic generator. While \u201cdeep fakes\u201d often target known individuals, the common detection techniques do not leverage any phonetic or speaker-specific information in determining whether a sample was generated or real. This plugin integrates phonetic and speaker information into the model to counter spoofing attacks aimed at the most vulnerable individuals. Therefore, it will detect if an audio is a bonafide file or if the audio is a deepfake representation of a specific enrolled speaker (the Speaker Of Interest). Like Speaker Identification (SID), DFA-SpeakerSpecific-Phonetic needs information from the Speaker Of Interest, in the form of the creation of a speaker specific enrollment. In this version, the number of speakers that we are analyzing for each scoring query is limited to one. Finally, the plugin allows the user to adapt the model towards the Speaker Of Interest if there is enough enrollment data for that specific speaker. Currently, the minimum number of utterances enrolled into the system to perform the optional Speaker of Interest adaptation is three. Domains multicondition-16k-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech and can handle audio with 16kHz sampling rate or higher. Downsamples all input to 16kHz. multicondition-8k-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech and can handle audio with 8kHz sampling rate or higher. Downsamples all input to 8kHz. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. Multiple speakers and multiple utterances of the same speaker can be enrolled at the same time. For scoring, an audio buffer or file to evaluate, and a label indicating the speaker-of-interest (enrolled speaker) to compare with. One and only one speaker-of-interest class must be provided - if there are more than one speaker in the list at score time, it will stop processing and provide an error message. Outputs The dfa-speakerSpecific-phonetic plugin returns a list of files with a score for the enrolled speaker. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a bonafide file and a score below \"0\" is considered a deep-fake audio. The plugin uses a global score calibration by default. If desired, the user can choose to instead enable the enable_soi_adaptation option (described below) to adapt the score calibration from speaker-of-interest specific information, as long as there are three or more enrollments from the target speaker. Example output: My_journey_from_Marine_to_actor___Adam_Driver-nCwwVjPNloY_spk0_30sec_006.wav Adam 0.73312998 My_journey_from_Marine_to_actor___Adam_Driver-nCwwVjPNloY_spk0_30sec_012_8k.wav Adam 0.92221069 LAI_VoiceJ_Kylo_Explains_Star_Wars-_ZZlYYC24LY_spk0.wav Adam -0.44476557 LAI_VoiceJ_Kylo_Ren_in_Star_Trek-T-yyabjI_RE_spk0.wav Adam -1.08671188 Enrollments Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 30 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Note that at least 3 audio samples are needed to perform the model adaptation for the speaker-of-interest. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.5+ Limitations Known or potential limitations of the plugin are outlined below. Speaker ID functionality Unlike SID, this plugin has not been designed to identify speakers and, therefore, scoring candidate utterances against models from obviously different speakers can provide unexpected or undesired results. Impersonators This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from profesional impersonators are not within the scope of this plugin. Processing Speed and Memory Adaptation is computationally expensive and it requires more resources than global calibration. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 1 second by default). Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 threshold An offset applied to scores to allow 0.0 to be a tuned decision point. Higher threshold values will lower output scores, and result in less audio labeled \"fake\" 1.5 0.0 - 10.0 enable_soi_adaptation Speaker Of Interest calibration adaptation. If True, the plugin adapts the model using enrolled data of the speaker-of-interest False True or False","title":"dfa-speakerSpecific-phonetic-v1 (Deep Fake Audio Detection - Speaker Specific)"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#dfa-speakerspecific-phonetic-v1-deep-fake-audio-detection-speaker-specific","text":"","title":"dfa-speakerSpecific-phonetic-v1 (Deep Fake Audio Detection - Speaker Specific)"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.5.0 v1.0.2 Bug fix for input audio files \\<0.5s","title":"Version Changelog"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#description","text":"Deep fake speech generators often leave many acoustic and phonetic artifacts in the audio. Traditional synthetic speech detectors typically rely on deep learning models trained with acoustic features alone to classify whether a given speech sample comes from a human or from a synthetic generator. While \u201cdeep fakes\u201d often target known individuals, the common detection techniques do not leverage any phonetic or speaker-specific information in determining whether a sample was generated or real. This plugin integrates phonetic and speaker information into the model to counter spoofing attacks aimed at the most vulnerable individuals. Therefore, it will detect if an audio is a bonafide file or if the audio is a deepfake representation of a specific enrolled speaker (the Speaker Of Interest). Like Speaker Identification (SID), DFA-SpeakerSpecific-Phonetic needs information from the Speaker Of Interest, in the form of the creation of a speaker specific enrollment. In this version, the number of speakers that we are analyzing for each scoring query is limited to one. Finally, the plugin allows the user to adapt the model towards the Speaker Of Interest if there is enough enrollment data for that specific speaker. Currently, the minimum number of utterances enrolled into the system to perform the optional Speaker of Interest adaptation is three.","title":"Description"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#domains","text":"multicondition-16k-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech and can handle audio with 16kHz sampling rate or higher. Downsamples all input to 16kHz. multicondition-8k-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech and can handle audio with 8kHz sampling rate or higher. Downsamples all input to 8kHz.","title":"Domains"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. Multiple speakers and multiple utterances of the same speaker can be enrolled at the same time. For scoring, an audio buffer or file to evaluate, and a label indicating the speaker-of-interest (enrolled speaker) to compare with. One and only one speaker-of-interest class must be provided - if there are more than one speaker in the list at score time, it will stop processing and provide an error message.","title":"Inputs"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#outputs","text":"The dfa-speakerSpecific-phonetic plugin returns a list of files with a score for the enrolled speaker. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a bonafide file and a score below \"0\" is considered a deep-fake audio. The plugin uses a global score calibration by default. If desired, the user can choose to instead enable the enable_soi_adaptation option (described below) to adapt the score calibration from speaker-of-interest specific information, as long as there are three or more enrollments from the target speaker. Example output: My_journey_from_Marine_to_actor___Adam_Driver-nCwwVjPNloY_spk0_30sec_006.wav Adam 0.73312998 My_journey_from_Marine_to_actor___Adam_Driver-nCwwVjPNloY_spk0_30sec_012_8k.wav Adam 0.92221069 LAI_VoiceJ_Kylo_Explains_Star_Wars-_ZZlYYC24LY_spk0.wav Adam -0.44476557 LAI_VoiceJ_Kylo_Ren_in_Star_Trek-T-yyabjI_RE_spk0.wav Adam -1.08671188","title":"Outputs"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#enrollments","text":"Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 30 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Note that at least 3 audio samples are needed to perform the model adaptation for the speaker-of-interest.","title":"Enrollments"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#compatibility","text":"OLIVE 5.5+","title":"Compatibility"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#speaker-id-functionality","text":"Unlike SID, this plugin has not been designed to identify speakers and, therefore, scoring candidate utterances against models from obviously different speakers can provide unexpected or undesired results.","title":"Speaker ID functionality"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#impersonators","text":"This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from profesional impersonators are not within the scope of this plugin.","title":"Impersonators"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#processing-speed-and-memory","text":"Adaptation is computationally expensive and it requires more resources than global calibration.","title":"Processing Speed and Memory"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 1 second by default).","title":"Minimum Speech Duration"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 threshold An offset applied to scores to allow 0.0 to be a tuned decision point. Higher threshold values will lower output scores, and result in less audio labeled \"fake\" 1.5 0.0 - 10.0 enable_soi_adaptation Speaker Of Interest calibration adaptation. If True, the plugin adapts the model using enrolled data of the speaker-of-interest False True or False","title":"Global Options"},{"location":"plugins/dfa-speakerSpecific-v1.html","text":"dfa-speakerSpecific-v1 (Deep Fake Audio Detection - Speaker Specific) Version Changelog Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.3.0 Description Common \u201cdeep fake\u201d detection techniques apply general approaches based on the detection of synthetic speech or audio artifacts. While \u201cdeep fakes\u201d very often target known individuals, the common detection techniques do not leverage the actual known-samples of the target\u2019s speech, (also called the speaker of interest) in determining whether a sample was generated or real. This is a very valuable piece of information that can be used to assist in detection deep-fake speech from an individual, as well as in building an effective system for the general population. This plugin for DeepFake Audio Detection incorporates information from the speaker-of-interest into the model to avoid specific attacks for certain vulnerable people. Therefore, it will detect if an audio is a bonafide file or if the audio is a deepfake representation of a specific enrolled speaker (the Speaker Of Interest). Like Speaker Identification (SID), DFA-SpeakerSpecific needs information from the Speaker Of Interest, in the form of the creation of a speaker specific enrollment. In this version, the number of speakers that we are analyzing for each scoring query is limited to one. Finally, the plugin allows the user to adapt the model towards the Speaker Of Interest if there is enough enrollment data for that specific speaker. Currently, the minimum number of utterances enrolled into the system to perform the adaptation is four. Domains multicondition-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. Multiple speakers and multiple utterances of the same speaker can be enrolled at the same time. For scoring, an audio buffer or file to evaluate, and a label indicating the speaker-of-interest (enrolled speaker) to compare with. One and only one speaker-of-interest class must be provided - if there are more than one speaker in the list at score time, it will stop processing and provide an error message. Outputs The DFA-speakerSpecific plugin returns a list of files with a score for the enrolled speaker. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a bonafide file and a score below \"0\" is considered a deep-fake audio. The DFA-speakerSpecific plugin adapts the score calibration by default is there are enough utterances of the enrolled speaker. Otherwise, the user can choose to use a global score calibration by disabling the enable_soi_adaptation option (described below) or add more data of the speaker-of-interest. Example output: Airplane-imTrEFnrVCs_spk0.wav JohnTravolta -240.64154053 Car-ajtyqj81b6E_spk0.wav JohnTravolta -135.29077148 Texas-7n1qnUOz4Uk_spk0_30sec_026.wav JohnTravolta 78.67844391 Texas-7n1qnUOz4Uk_spk0_30sec_011.wav JohnTravolta 97.31453705 Enrollments Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 30 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Note that at least 4 audio samples are needed to perform the model adaptation for the speaker-of-interest. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.3+ Limitations Known or potential limitations of the plugin are outlined below. Speaker ID functionality Unlike SID, this plugin has not been designed to identify speakers and, therefore, scoring candidate utterances against models from obviously different speakers can provide unexpected or undesired results. Impersonators This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from profesional impersonators are not within the scope of this plugin. Processing Speed and Memory Adaptation is computational expensive and it requires more resources than global calibration. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 threshold Threshold to determine if audio will be labeled as 'real' or 'fake'. Higher value results in more audio labeled as fake. 3.0 0.0 - 10.0 enable_soi_adaptation Speaker Of Interest calibration adaptation. If True, the plugin adapts the model using enrolled data of the speaker-of-interest True True False","title":"dfa-speakerSpecific-v1 (Deep Fake Audio Detection - Speaker Specific)"},{"location":"plugins/dfa-speakerSpecific-v1.html#dfa-speakerspecific-v1-deep-fake-audio-detection-speaker-specific","text":"","title":"dfa-speakerSpecific-v1 (Deep Fake Audio Detection - Speaker Specific)"},{"location":"plugins/dfa-speakerSpecific-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/dfa-speakerSpecific-v1.html#description","text":"Common \u201cdeep fake\u201d detection techniques apply general approaches based on the detection of synthetic speech or audio artifacts. While \u201cdeep fakes\u201d very often target known individuals, the common detection techniques do not leverage the actual known-samples of the target\u2019s speech, (also called the speaker of interest) in determining whether a sample was generated or real. This is a very valuable piece of information that can be used to assist in detection deep-fake speech from an individual, as well as in building an effective system for the general population. This plugin for DeepFake Audio Detection incorporates information from the speaker-of-interest into the model to avoid specific attacks for certain vulnerable people. Therefore, it will detect if an audio is a bonafide file or if the audio is a deepfake representation of a specific enrolled speaker (the Speaker Of Interest). Like Speaker Identification (SID), DFA-SpeakerSpecific needs information from the Speaker Of Interest, in the form of the creation of a speaker specific enrollment. In this version, the number of speakers that we are analyzing for each scoring query is limited to one. Finally, the plugin allows the user to adapt the model towards the Speaker Of Interest if there is enough enrollment data for that specific speaker. Currently, the minimum number of utterances enrolled into the system to perform the adaptation is four.","title":"Description"},{"location":"plugins/dfa-speakerSpecific-v1.html#domains","text":"multicondition-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech.","title":"Domains"},{"location":"plugins/dfa-speakerSpecific-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. Multiple speakers and multiple utterances of the same speaker can be enrolled at the same time. For scoring, an audio buffer or file to evaluate, and a label indicating the speaker-of-interest (enrolled speaker) to compare with. One and only one speaker-of-interest class must be provided - if there are more than one speaker in the list at score time, it will stop processing and provide an error message.","title":"Inputs"},{"location":"plugins/dfa-speakerSpecific-v1.html#outputs","text":"The DFA-speakerSpecific plugin returns a list of files with a score for the enrolled speaker. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a bonafide file and a score below \"0\" is considered a deep-fake audio. The DFA-speakerSpecific plugin adapts the score calibration by default is there are enough utterances of the enrolled speaker. Otherwise, the user can choose to use a global score calibration by disabling the enable_soi_adaptation option (described below) or add more data of the speaker-of-interest. Example output: Airplane-imTrEFnrVCs_spk0.wav JohnTravolta -240.64154053 Car-ajtyqj81b6E_spk0.wav JohnTravolta -135.29077148 Texas-7n1qnUOz4Uk_spk0_30sec_026.wav JohnTravolta 78.67844391 Texas-7n1qnUOz4Uk_spk0_30sec_011.wav JohnTravolta 97.31453705","title":"Outputs"},{"location":"plugins/dfa-speakerSpecific-v1.html#enrollments","text":"Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 30 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Note that at least 4 audio samples are needed to perform the model adaptation for the speaker-of-interest.","title":"Enrollments"},{"location":"plugins/dfa-speakerSpecific-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/dfa-speakerSpecific-v1.html#compatibility","text":"OLIVE 5.3+","title":"Compatibility"},{"location":"plugins/dfa-speakerSpecific-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/dfa-speakerSpecific-v1.html#speaker-id-functionality","text":"Unlike SID, this plugin has not been designed to identify speakers and, therefore, scoring candidate utterances against models from obviously different speakers can provide unexpected or undesired results.","title":"Speaker ID functionality"},{"location":"plugins/dfa-speakerSpecific-v1.html#impersonators","text":"This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from profesional impersonators are not within the scope of this plugin.","title":"Impersonators"},{"location":"plugins/dfa-speakerSpecific-v1.html#processing-speed-and-memory","text":"Adaptation is computational expensive and it requires more resources than global calibration.","title":"Processing Speed and Memory"},{"location":"plugins/dfa-speakerSpecific-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/dfa-speakerSpecific-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 threshold Threshold to determine if audio will be labeled as 'real' or 'fake'. Higher value results in more audio labeled as fake. 3.0 0.0 - 10.0 enable_soi_adaptation Speaker Of Interest calibration adaptation. If True, the plugin adapts the model using enrolled data of the speaker-of-interest True True False","title":"Global Options"},{"location":"plugins/dfa.html","text":"redirect: plugins/dfa-end2end-v1.md","title":"Dfa"},{"location":"plugins/dfa.html#redirect-pluginsdfa-end2end-v1md","text":"","title":"redirect: plugins/dfa-end2end-v1.md"},{"location":"plugins/dia.html","text":"redirect: plugins/sdd-embed-v2.md","title":"Dia"},{"location":"plugins/dia.html#redirect-pluginssdd-embed-v2md","text":"","title":"redirect: plugins/sdd-embed-v2.md"},{"location":"plugins/enh.html","text":"redirect: plugins/enh-mmse-v1.md","title":"Enh"},{"location":"plugins/enh.html#redirect-pluginsenh-mmse-v1md","text":"","title":"redirect: plugins/enh-mmse-v1.md"},{"location":"plugins/fdi-pyEmbed-v1.html","text":"fdi-pyEmbed-v1 (Face Detection Image) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.2.0 GPU usage enabled. Bug fix for high memory usage under certain circumstances Description Face Detection Image plugins process an input image and attempt to localize one or more faces within the frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be a face. Unlike Face Recognition plugins, a Face Detection plugin is only looking for faces generically - it doesn't care about the identity of the face or faces, and will report any faces detected. Domains multi-v1 A general purpose image processing domain. Inputs An image file to process. Outputs Face Detection Image plugins are 'bounding box' scorers - the output of a bounding box scorer is a class, a corresponding score, and 4 points associated with this class and score that attempt to localize the detected class (in this case a face) within the image frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) Where the bounding box itself is defined by the four coordinates in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) An example output could look like this: input_image.png face 0.9974257349967957 (154, 78, 657, 745) Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon) Compatibility OLIVE 5.7+ Limitations Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Face Detection Image (FDI)"},{"location":"plugins/fdi-pyEmbed-v1.html#fdi-pyembed-v1-face-detection-image","text":"","title":"fdi-pyEmbed-v1 (Face Detection Image)"},{"location":"plugins/fdi-pyEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.2.0 GPU usage enabled. Bug fix for high memory usage under certain circumstances","title":"Version Changelog"},{"location":"plugins/fdi-pyEmbed-v1.html#description","text":"Face Detection Image plugins process an input image and attempt to localize one or more faces within the frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be a face. Unlike Face Recognition plugins, a Face Detection plugin is only looking for faces generically - it doesn't care about the identity of the face or faces, and will report any faces detected.","title":"Description"},{"location":"plugins/fdi-pyEmbed-v1.html#domains","text":"multi-v1 A general purpose image processing domain.","title":"Domains"},{"location":"plugins/fdi-pyEmbed-v1.html#inputs","text":"An image file to process.","title":"Inputs"},{"location":"plugins/fdi-pyEmbed-v1.html#outputs","text":"Face Detection Image plugins are 'bounding box' scorers - the output of a bounding box scorer is a class, a corresponding score, and 4 points associated with this class and score that attempt to localize the detected class (in this case a face) within the image frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) Where the bounding box itself is defined by the four coordinates in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) An example output could look like this: input_image.png face 0.9974257349967957 (154, 78, 657, 745)","title":"Outputs"},{"location":"plugins/fdi-pyEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon)","title":"Functionality (Traits)"},{"location":"plugins/fdi-pyEmbed-v1.html#compatibility","text":"OLIVE 5.7+","title":"Compatibility"},{"location":"plugins/fdi-pyEmbed-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/fdi-pyEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/fdi-pyEmbed-v1.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/fdi-pyEmbed-v1.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/fdi.html","text":"redirect: plugins/fdi-pyEmbed-v1.md","title":"Fdi"},{"location":"plugins/fdi.html#redirect-pluginsfdi-pyembed-v1md","text":"","title":"redirect: plugins/fdi-pyEmbed-v1.md"},{"location":"plugins/fdv-pyEmbed-v1.html","text":"fdv-pyEmbed-v1 (Face Detection Video) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.2.0 GPU usage enabled. Bug fix for high memory usage under certain circumstances Description Face Detection Video plugins process an input video and attempt to localize in both space and time one or more enrolled faces within the video frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to belong to one of the enrolled targets, and an associated start and end time region. Unlike Face Recognition plugins, a Face Detection plugin is only looking for faces generically - it doesn't care about the identity of the face or faces, and will report any faces detected. Domains multi-v1 A general purpose video processing domain. Inputs An video file to process. Outputs Face Recognition Video plugins are 'bounding box' scorers - the output of a video-processing bounding box scorer is a class, a corresponding score, an associated start and end timestamp denoting when the bounding box is valid, and 4 points associated with this class, time, and score that attempt to localize the detected class (in this case the name of the enrolled face) within the video frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) (<start_seconds>, <end_seconds>) Where the bounding box itself is defined by the four coordinates grouped in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) And the timestamps are the final two grouped numbers. An example output could look like this: test-videos/input_video.mp4 face 0.9974257349967957 (154, 78, 657, 745) (978.31, 978.84) Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon) Compatibility OLIVE 5.3+ Limitations Due to the intensity of resources required for processing videos, this plugin has a few limitations or behaviors that need to be considered. Large Video Files When a video file is opened and decoded into individual frames in memory, it can expand in size by considerable amounts. Because of this expansion, care should be taken to minimize other overheards when processing video files - such as by submitting video files for scoring via a file path instead of as a serialized buffer whenever possible. Realistic expectations should be held when attempting to process large video files when available memory is limited. Please plan on making a minimum of 16GB of memory available for video processing; ideally more for larger files. Resolution Scaling The current crop of OLIVE video processing plugins do not process video at full resolution - as the video files are opened, they are rescaled to 640 x 480 pixel resolution, and processed at this size. Our internal testing has shown this does not significantly degrade performance with these plugins, but drastically reduces required memory resources and improves our processing capabilities as a result. Note that there is currently no retention of the original aspect ratio, so some files, such as those with a very wide, very square, or portrait-orientation aspect ratio may not be processed exactly as expected due to scaling to 640 x 480 exactly. Frame Rate (vs Temporal Resolution) Processing every individual video frame at the videos native frame rate is enormously expensive. To avoid this resource cost and improve the processing speed and reduce the resource requirements of running these plugins, plugins currently process 4 frames per second. This limits the precision of the start and end timestamps for face regions, and makes it possible, though unlikely, for very quickly appearing/disappearing faces to be missed. Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Face Detection Video (FDV)"},{"location":"plugins/fdv-pyEmbed-v1.html#fdv-pyembed-v1-face-detection-video","text":"","title":"fdv-pyEmbed-v1 (Face Detection Video)"},{"location":"plugins/fdv-pyEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.2.0 GPU usage enabled. Bug fix for high memory usage under certain circumstances","title":"Version Changelog"},{"location":"plugins/fdv-pyEmbed-v1.html#description","text":"Face Detection Video plugins process an input video and attempt to localize in both space and time one or more enrolled faces within the video frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to belong to one of the enrolled targets, and an associated start and end time region. Unlike Face Recognition plugins, a Face Detection plugin is only looking for faces generically - it doesn't care about the identity of the face or faces, and will report any faces detected.","title":"Description"},{"location":"plugins/fdv-pyEmbed-v1.html#domains","text":"multi-v1 A general purpose video processing domain.","title":"Domains"},{"location":"plugins/fdv-pyEmbed-v1.html#inputs","text":"An video file to process.","title":"Inputs"},{"location":"plugins/fdv-pyEmbed-v1.html#outputs","text":"Face Recognition Video plugins are 'bounding box' scorers - the output of a video-processing bounding box scorer is a class, a corresponding score, an associated start and end timestamp denoting when the bounding box is valid, and 4 points associated with this class, time, and score that attempt to localize the detected class (in this case the name of the enrolled face) within the video frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) (<start_seconds>, <end_seconds>) Where the bounding box itself is defined by the four coordinates grouped in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) And the timestamps are the final two grouped numbers. An example output could look like this: test-videos/input_video.mp4 face 0.9974257349967957 (154, 78, 657, 745) (978.31, 978.84)","title":"Outputs"},{"location":"plugins/fdv-pyEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon)","title":"Functionality (Traits)"},{"location":"plugins/fdv-pyEmbed-v1.html#compatibility","text":"OLIVE 5.3+","title":"Compatibility"},{"location":"plugins/fdv-pyEmbed-v1.html#limitations","text":"Due to the intensity of resources required for processing videos, this plugin has a few limitations or behaviors that need to be considered.","title":"Limitations"},{"location":"plugins/fdv-pyEmbed-v1.html#large-video-files","text":"When a video file is opened and decoded into individual frames in memory, it can expand in size by considerable amounts. Because of this expansion, care should be taken to minimize other overheards when processing video files - such as by submitting video files for scoring via a file path instead of as a serialized buffer whenever possible. Realistic expectations should be held when attempting to process large video files when available memory is limited. Please plan on making a minimum of 16GB of memory available for video processing; ideally more for larger files.","title":"Large Video Files"},{"location":"plugins/fdv-pyEmbed-v1.html#resolution-scaling","text":"The current crop of OLIVE video processing plugins do not process video at full resolution - as the video files are opened, they are rescaled to 640 x 480 pixel resolution, and processed at this size. Our internal testing has shown this does not significantly degrade performance with these plugins, but drastically reduces required memory resources and improves our processing capabilities as a result. Note that there is currently no retention of the original aspect ratio, so some files, such as those with a very wide, very square, or portrait-orientation aspect ratio may not be processed exactly as expected due to scaling to 640 x 480 exactly.","title":"Resolution Scaling"},{"location":"plugins/fdv-pyEmbed-v1.html#frame-rate-vs-temporal-resolution","text":"Processing every individual video frame at the videos native frame rate is enormously expensive. To avoid this resource cost and improve the processing speed and reduce the resource requirements of running these plugins, plugins currently process 4 frames per second. This limits the precision of the start and end timestamps for face regions, and makes it possible, though unlikely, for very quickly appearing/disappearing faces to be missed.","title":"Frame Rate (vs Temporal Resolution)"},{"location":"plugins/fdv-pyEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/fdv-pyEmbed-v1.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/fdv-pyEmbed-v1.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/fdv.html","text":"redirect: plugins/fdv-pyEmbed-v1.md","title":"Fdv"},{"location":"plugins/fdv.html#redirect-pluginsfdv-pyembed-v1md","text":"","title":"redirect: plugins/fdv-pyEmbed-v1.md"},{"location":"plugins/for.html","text":"redirect: plugins/for-forensic-v1.md","title":"For"},{"location":"plugins/for.html#redirect-pluginsfor-forensic-v1md","text":"","title":"redirect: plugins/for-forensic-v1.md"},{"location":"plugins/fri-pyEmbed-v1.html","text":"fri-pyEmbed-v1 (Face Recognition Image) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.2.0 GPU usage enabled. Bug fix for high memory usage under certain circumstances Description Face Recognition Image plugins process an input image and attempt to localize one or more enrolled faces within the frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be the face of one of the enrolled faces. Unlike Face Detection plugins, a Face Recognition plugin is only looking for the faces of enrolled persons of interest. If faces are detected that the system is not confident belong to one of the enrolled targets, they will not be reported. Domains multi-v1 A general purpose image processing domain. Inputs An image file to process. Enrollments Face Recognition plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's representation - in this case, an image of a person's face. A new enrollment is created with the first class modification, which consists of essentially sending the system an image sample from a person of interest. This enrollment can be augmented with subsequent class modification requests by adding more images with the same class label. Note that currently only images can be used for face enrollment requests; it is not yet possible to enroll faces via video. Outputs Face Recognition Image plugins are 'bounding box' scorers - the output of a bounding box scorer is a class, a corresponding score, and 4 points associated with this class and score that attempt to localize the detected class (in this case a face) within the image frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) Where the bounding box itself is defined by the four coordinates in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) An example output could look like this: input_image.png Marcus 0.5474257 (154, 78, 657, 745) Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon) Compatibility OLIVE 5.7+ Limitations Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Face Recognition Image (FRI)"},{"location":"plugins/fri-pyEmbed-v1.html#fri-pyembed-v1-face-recognition-image","text":"","title":"fri-pyEmbed-v1 (Face Recognition Image)"},{"location":"plugins/fri-pyEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.2.0 GPU usage enabled. Bug fix for high memory usage under certain circumstances","title":"Version Changelog"},{"location":"plugins/fri-pyEmbed-v1.html#description","text":"Face Recognition Image plugins process an input image and attempt to localize one or more enrolled faces within the frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be the face of one of the enrolled faces. Unlike Face Detection plugins, a Face Recognition plugin is only looking for the faces of enrolled persons of interest. If faces are detected that the system is not confident belong to one of the enrolled targets, they will not be reported.","title":"Description"},{"location":"plugins/fri-pyEmbed-v1.html#domains","text":"multi-v1 A general purpose image processing domain.","title":"Domains"},{"location":"plugins/fri-pyEmbed-v1.html#inputs","text":"An image file to process.","title":"Inputs"},{"location":"plugins/fri-pyEmbed-v1.html#enrollments","text":"Face Recognition plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's representation - in this case, an image of a person's face. A new enrollment is created with the first class modification, which consists of essentially sending the system an image sample from a person of interest. This enrollment can be augmented with subsequent class modification requests by adding more images with the same class label. Note that currently only images can be used for face enrollment requests; it is not yet possible to enroll faces via video.","title":"Enrollments"},{"location":"plugins/fri-pyEmbed-v1.html#outputs","text":"Face Recognition Image plugins are 'bounding box' scorers - the output of a bounding box scorer is a class, a corresponding score, and 4 points associated with this class and score that attempt to localize the detected class (in this case a face) within the image frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) Where the bounding box itself is defined by the four coordinates in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) An example output could look like this: input_image.png Marcus 0.5474257 (154, 78, 657, 745)","title":"Outputs"},{"location":"plugins/fri-pyEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon)","title":"Functionality (Traits)"},{"location":"plugins/fri-pyEmbed-v1.html#compatibility","text":"OLIVE 5.7+","title":"Compatibility"},{"location":"plugins/fri-pyEmbed-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/fri-pyEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/fri-pyEmbed-v1.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/fri-pyEmbed-v1.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/fri.html","text":"redirect: plugins/fri-pyEmbed-v1.md","title":"Fri"},{"location":"plugins/fri.html#redirect-pluginsfri-pyembed-v1md","text":"","title":"redirect: plugins/fri-pyEmbed-v1.md"},{"location":"plugins/frv-pyEmbed-v1.html","text":"fdv-pyEmbed-v1 (Face Recognition Video) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.2.0 GPU usage enabled. Bug fix for high memory usage under certain circumstances Description Face Recognition Video plugins process an input video and attempt to localize in both space and time one or more faces within the video frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be a face, and an associated start and end time region. Unlike Face Detection plugins, a Face Recognition plugin is only looking for the faces of enrolled persons of interest. If faces are detected that the system is not confident belong to one of the enrolled targets, they will not be reported. Domains multi-v1 A general purpose video processing domain. Inputs An video file to process. Enrollments Face Recognition plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's representation - in this case, an image of a person's face. A new enrollment is created with the first class modification, which consists of essentially sending the system an image sample from a person of interest. This enrollment can be augmented with subsequent class modification requests by adding more images with the same class label. Note that currently only images can be used for face enrollment requests; it is not yet possible to enroll faces via video. Outputs Face Recognition Video plugins are 'bounding box' scorers - the output of a video-processing bounding box scorer is a class, a corresponding score, an associated start and end timestamp denoting when the bounding box is valid, and 4 points associated with this class, time, and score that attempt to localize the detected class (in this case a face) within the video frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) (<start_seconds>, <end_seconds>) Where the bounding box itself is defined by the four coordinates grouped in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) And the timestamps are the final two grouped numbers. An example output could look like this: test-videos/input_video.mp4 Marcus 0.5474257349967957 (154, 78, 657, 745) (978.31, 978.84) Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon) Compatibility OLIVE 5.7+ Limitations Due to the intensity of resources required for processing videos, this plugin has a few limitations or behaviors that need to be considered. Large Video Files When a video file is opened and decoded into individual frames in memory, it can expand in size by considerable amounts. Because of this expansion, care should be taken to minimize other overheards when processing video files - such as by submitting video files for scoring via a file path instead of as a serialized buffer whenever possible. Realistic expectations should be held when attempting to process large video files when available memory is limited. Please plan on making a minimum of 16GB of memory available for video processing; ideally more for larger files. Resolution Scaling The current crop of OLIVE video processing plugins do not process video at full resolution - as the video files are opened, they are rescaled to 640 x 480 pixel resolution, and processed at this size. Our internal testing has shown this does not significantly degrade performance with these plugins, but drastically reduces required memory resources and improves our processing capabilities as a result. Note that there is currently no retention of the original aspect ratio, so some files, such as those with a very wide, very square, or portrait-orientation aspect ratio may not be processed exactly as expected due to scaling to 640 x 480 exactly. Frame Rate (vs Temporal Resolution) Processing every individual video frame at the videos native frame rate is enormously expensive. To avoid this resource cost and improve the processing speed and reduce the resource requirements of running these plugins, plugins currently process 4 frames per second. This limits the precision of the start and end timestamps for face regions, and makes it possible, though unlikely, for very quickly appearing/disappearing faces to be missed. Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Face Recognition Video (FRV)"},{"location":"plugins/frv-pyEmbed-v1.html#fdv-pyembed-v1-face-recognition-video","text":"","title":"fdv-pyEmbed-v1 (Face Recognition Video)"},{"location":"plugins/frv-pyEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.2.0 GPU usage enabled. Bug fix for high memory usage under certain circumstances","title":"Version Changelog"},{"location":"plugins/frv-pyEmbed-v1.html#description","text":"Face Recognition Video plugins process an input video and attempt to localize in both space and time one or more faces within the video frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be a face, and an associated start and end time region. Unlike Face Detection plugins, a Face Recognition plugin is only looking for the faces of enrolled persons of interest. If faces are detected that the system is not confident belong to one of the enrolled targets, they will not be reported.","title":"Description"},{"location":"plugins/frv-pyEmbed-v1.html#domains","text":"multi-v1 A general purpose video processing domain.","title":"Domains"},{"location":"plugins/frv-pyEmbed-v1.html#inputs","text":"An video file to process.","title":"Inputs"},{"location":"plugins/frv-pyEmbed-v1.html#enrollments","text":"Face Recognition plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's representation - in this case, an image of a person's face. A new enrollment is created with the first class modification, which consists of essentially sending the system an image sample from a person of interest. This enrollment can be augmented with subsequent class modification requests by adding more images with the same class label. Note that currently only images can be used for face enrollment requests; it is not yet possible to enroll faces via video.","title":"Enrollments"},{"location":"plugins/frv-pyEmbed-v1.html#outputs","text":"Face Recognition Video plugins are 'bounding box' scorers - the output of a video-processing bounding box scorer is a class, a corresponding score, an associated start and end timestamp denoting when the bounding box is valid, and 4 points associated with this class, time, and score that attempt to localize the detected class (in this case a face) within the video frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) (<start_seconds>, <end_seconds>) Where the bounding box itself is defined by the four coordinates grouped in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) And the timestamps are the final two grouped numbers. An example output could look like this: test-videos/input_video.mp4 Marcus 0.5474257349967957 (154, 78, 657, 745) (978.31, 978.84)","title":"Outputs"},{"location":"plugins/frv-pyEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon)","title":"Functionality (Traits)"},{"location":"plugins/frv-pyEmbed-v1.html#compatibility","text":"OLIVE 5.7+","title":"Compatibility"},{"location":"plugins/frv-pyEmbed-v1.html#limitations","text":"Due to the intensity of resources required for processing videos, this plugin has a few limitations or behaviors that need to be considered.","title":"Limitations"},{"location":"plugins/frv-pyEmbed-v1.html#large-video-files","text":"When a video file is opened and decoded into individual frames in memory, it can expand in size by considerable amounts. Because of this expansion, care should be taken to minimize other overheards when processing video files - such as by submitting video files for scoring via a file path instead of as a serialized buffer whenever possible. Realistic expectations should be held when attempting to process large video files when available memory is limited. Please plan on making a minimum of 16GB of memory available for video processing; ideally more for larger files.","title":"Large Video Files"},{"location":"plugins/frv-pyEmbed-v1.html#resolution-scaling","text":"The current crop of OLIVE video processing plugins do not process video at full resolution - as the video files are opened, they are rescaled to 640 x 480 pixel resolution, and processed at this size. Our internal testing has shown this does not significantly degrade performance with these plugins, but drastically reduces required memory resources and improves our processing capabilities as a result. Note that there is currently no retention of the original aspect ratio, so some files, such as those with a very wide, very square, or portrait-orientation aspect ratio may not be processed exactly as expected due to scaling to 640 x 480 exactly.","title":"Resolution Scaling"},{"location":"plugins/frv-pyEmbed-v1.html#frame-rate-vs-temporal-resolution","text":"Processing every individual video frame at the videos native frame rate is enormously expensive. To avoid this resource cost and improve the processing speed and reduce the resource requirements of running these plugins, plugins currently process 4 frames per second. This limits the precision of the start and end timestamps for face regions, and makes it possible, though unlikely, for very quickly appearing/disappearing faces to be missed.","title":"Frame Rate (vs Temporal Resolution)"},{"location":"plugins/frv-pyEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/frv-pyEmbed-v1.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/frv-pyEmbed-v1.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/frv.html","text":"redirect: plugins/frv-pyEmbed-v1.md","title":"Frv"},{"location":"plugins/frv.html#redirect-pluginsfrv-pyembed-v1md","text":"","title":"redirect: plugins/frv-pyEmbed-v1.md"},{"location":"plugins/gdd-embedplda-v1.html","text":"gdd-embedplda-v1 (Gender Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0 v1.1.0 Fixed domain label, release with OLIVE 5.7.1 Description Gender Detection plugins will detect and label the gender of the speaker for regions of speech in a submitted audio segment. This is in contrast to Gender Identification (GID) plugins, which label the entire segment with a single gender. So, unlike Gender Identification (GID), GDD is capable of handling audio where multiple speakers of a different gender are speaking, and will provide timestamp region labels to point to label male and female regions. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Inputs Audio file or buffer and an optional identifier. Outputs GDD plugins return a list of regions with a score for the detected gender within that region. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 Female 5.40590000 input-audio.wav 43.500 77.500 Female 5.29558277 input-audio.wav 77.500 78.500 Male 2.63369179 input-audio.wav 78.500 80.500 Male 2.25519705 input-audio.wav 85.500 86.500 Female 2.06612849 input-audio.wav 97.500 98.500 Female 3.74665093 input-audio.wav 98.500 99.500 Male 2.22936487 input-audio.wav 105.500 106.500 Male 2.72254372 input-audio.wav 107.500 108.500 Female 2.60355234 input-audio.wav 108.500 110.500 Female 2.76414633 input-audio.wav 109.500 113.140 Male 2.85003138 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected gender and corresponding score for this gender RegionScorerRequest Compatibility OLIVE 5.2+ Limitations Known or potential limitations of the plugin are outlined below. Minimum Speech Duration The system will only attempt to perform gender detection if the submitted audio segment contains more than 2 seconds of detected speech. Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 1.5 -10.0 to 10.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for gender. 2.0 1.0 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"Gender Detection (GDD)"},{"location":"plugins/gdd-embedplda-v1.html#gdd-embedplda-v1-gender-detection","text":"","title":"gdd-embedplda-v1 (Gender Detection)"},{"location":"plugins/gdd-embedplda-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0 v1.1.0 Fixed domain label, release with OLIVE 5.7.1","title":"Version Changelog"},{"location":"plugins/gdd-embedplda-v1.html#description","text":"Gender Detection plugins will detect and label the gender of the speaker for regions of speech in a submitted audio segment. This is in contrast to Gender Identification (GID) plugins, which label the entire segment with a single gender. So, unlike Gender Identification (GID), GDD is capable of handling audio where multiple speakers of a different gender are speaking, and will provide timestamp region labels to point to label male and female regions.","title":"Description"},{"location":"plugins/gdd-embedplda-v1.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB.","title":"Domains"},{"location":"plugins/gdd-embedplda-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/gdd-embedplda-v1.html#outputs","text":"GDD plugins return a list of regions with a score for the detected gender within that region. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 Female 5.40590000 input-audio.wav 43.500 77.500 Female 5.29558277 input-audio.wav 77.500 78.500 Male 2.63369179 input-audio.wav 78.500 80.500 Male 2.25519705 input-audio.wav 85.500 86.500 Female 2.06612849 input-audio.wav 97.500 98.500 Female 3.74665093 input-audio.wav 98.500 99.500 Male 2.22936487 input-audio.wav 105.500 106.500 Male 2.72254372 input-audio.wav 107.500 108.500 Female 2.60355234 input-audio.wav 108.500 110.500 Female 2.76414633 input-audio.wav 109.500 113.140 Male 2.85003138","title":"Outputs"},{"location":"plugins/gdd-embedplda-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected gender and corresponding score for this gender RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/gdd-embedplda-v1.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/gdd-embedplda-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/gdd-embedplda-v1.html#minimum-speech-duration","text":"The system will only attempt to perform gender detection if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/gdd-embedplda-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/gdd-embedplda-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 1.5 -10.0 to 10.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for gender. 2.0 1.0 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/gdd.html","text":"redirect: plugins/gdd-embedplda-v1.md","title":"Gdd"},{"location":"plugins/gdd.html#redirect-pluginsgdd-embedplda-v1md","text":"","title":"redirect: plugins/gdd-embedplda-v1.md"},{"location":"plugins/gid-embedplda-v1.html","text":"gid-embedplda-v1 (Gender Identification) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, shares models with gdd-embedplda-v1 released with 5.4.0 v1.0.1 Code streamlining and minor bug fixes, released with 5.5.0 v1.0.2 Fixed domain label, released with 5.7.1 Description Gender Identification (GID) plugins attempt to identify the gender of the talker in the audio submitted to the plugin for scoring. The goal is to distinguish between male and female speakers as a quick triage of incoming data, or to serve as a front-end for other plugins that may have a gender-specific workflow. This plugin is powered by the PLDA Embeddings models originally released with gdd-embedplda-v1 . Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Inputs Audio file or buffer and an optional identifier. Outputs Gender ID plugins report a score for each gender, in the format shown below. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. input-audio.wav Female -3.212653 input-audio.wav Male 5.40590000 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each gender. GlobalScorerRequest Compatibility OLIVE 5.4+ Limitations Labeling Granularity GID plugins assume that an audio segment contains only a single gender and may be scored as a unit. If a segment contains multiple speakers who may or may not be of a different gender, the entire segment will still be scored as a unit. Ensuring that audio submitted for scoring consists of a single speaker is important for reliable results. Age All current Gender ID plugins are trained only on adult male and female speech. Speech from children will likely be confused with female speech, or be otherwise volatile. Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Gender Identification (GID)"},{"location":"plugins/gid-embedplda-v1.html#gid-embedplda-v1-gender-identification","text":"","title":"gid-embedplda-v1 (Gender Identification)"},{"location":"plugins/gid-embedplda-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, shares models with gdd-embedplda-v1 released with 5.4.0 v1.0.1 Code streamlining and minor bug fixes, released with 5.5.0 v1.0.2 Fixed domain label, released with 5.7.1","title":"Version Changelog"},{"location":"plugins/gid-embedplda-v1.html#description","text":"Gender Identification (GID) plugins attempt to identify the gender of the talker in the audio submitted to the plugin for scoring. The goal is to distinguish between male and female speakers as a quick triage of incoming data, or to serve as a front-end for other plugins that may have a gender-specific workflow. This plugin is powered by the PLDA Embeddings models originally released with gdd-embedplda-v1 .","title":"Description"},{"location":"plugins/gid-embedplda-v1.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB.","title":"Domains"},{"location":"plugins/gid-embedplda-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/gid-embedplda-v1.html#outputs","text":"Gender ID plugins report a score for each gender, in the format shown below. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. input-audio.wav Female -3.212653 input-audio.wav Male 5.40590000","title":"Outputs"},{"location":"plugins/gid-embedplda-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each gender. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/gid-embedplda-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/gid-embedplda-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/gid-embedplda-v1.html#labeling-granularity","text":"GID plugins assume that an audio segment contains only a single gender and may be scored as a unit. If a segment contains multiple speakers who may or may not be of a different gender, the entire segment will still be scored as a unit. Ensuring that audio submitted for scoring consists of a single speaker is important for reliable results.","title":"Labeling Granularity"},{"location":"plugins/gid-embedplda-v1.html#age","text":"All current Gender ID plugins are trained only on adult male and female speech. Speech from children will likely be confused with female speech, or be otherwise volatile.","title":"Age"},{"location":"plugins/gid-embedplda-v1.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/gid.html","text":"redirect: plugins/gid-embedplda-v1.md","title":"Gid"},{"location":"plugins/gid.html#redirect-pluginsgid-embedplda-v1md","text":"","title":"redirect: plugins/gid-embedplda-v1.md"},{"location":"plugins/kws.html","text":"Keyword Spotting (KWS) Released Plugins All of our current QBE plugins match the description on this page. Refer to the list below for the currently supported Speaker Detection plugins. Legacy Plugins (OLIVE 4.x Compatible) OLIVE Version Plugin Description OLIVE 4.12+ kws-dynapy-v1 DynaPy-based keyword spotting plugin improving the overall KWS system infrastructure and performance. Description Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by text input from the user at score time. It is based on automatic speech recognition (ASR) technology, featuring speech-to-text transcription and language modeling. This means that every KWS domain is language dependent, and that keywords can only be detected if they exist in the underlying ASR system's dictionary, making out-of-vocabulary keywords a potential problem. Inputs An audio file or buffer and a list of desired keywords or keyphrases to detect. Outputs When one or more of the enrolled keywords has been detected in the submitted audio, KWS returns a region or list of timestamped regions, each with a score for the keyword that has been detected. Enrollments KWS plugins do not support enrollments; instead, the set of classes the plugin is searching for is provided as a list of text keywords with each scoring request. Note that each domain is language dependent, and that a word or phrase in a language other than the one the domain is trained in is likely to be out-of-vocabulary. This means the word or phrase will be difficult or impossible to recall. For details on how to set or pass these keywords, please refer to the appropriate sections within the OLIVE CLI User Guide or OLIVE API Documentation. Limitations As was previously mentioned, traditional KWS relies on an underlying ASR system, making each KWS domain completely language dependent. This places several real restrictions on the users. First, this means that for keywords to be detectable, the words must be part of the ASR system's dictionary. This may make it difficult to find some keywords or phrases, like names, brands, slang or other colloquialisms, if they are out-of-vocabulary. This also makes it more difficult to deal with speakers or situations that may involve code switching. This reliance on ASR also makes KWS plugins quite heavy with respect to resource requirements, and also quite slow compared to other plugin types. An additional limitation stemming from the language dependence of the system is that if a user would like to detect keywords in a new language that isn't currently offered by SRI, a new domain for that language would need to be created, which requires a large amount of transcribed audio in that language. Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Keyword Spotting (KWS)"},{"location":"plugins/kws.html#keyword-spotting-kws","text":"","title":"Keyword Spotting (KWS)"},{"location":"plugins/kws.html#released-plugins","text":"All of our current QBE plugins match the description on this page. Refer to the list below for the currently supported Speaker Detection plugins.","title":"Released Plugins"},{"location":"plugins/kws.html#legacy-plugins-olive-4x-compatible","text":"OLIVE Version Plugin Description OLIVE 4.12+ kws-dynapy-v1 DynaPy-based keyword spotting plugin improving the overall KWS system infrastructure and performance.","title":"Legacy Plugins (OLIVE 4.x Compatible)"},{"location":"plugins/kws.html#description","text":"Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by text input from the user at score time. It is based on automatic speech recognition (ASR) technology, featuring speech-to-text transcription and language modeling. This means that every KWS domain is language dependent, and that keywords can only be detected if they exist in the underlying ASR system's dictionary, making out-of-vocabulary keywords a potential problem.","title":"Description"},{"location":"plugins/kws.html#inputs","text":"An audio file or buffer and a list of desired keywords or keyphrases to detect.","title":"Inputs"},{"location":"plugins/kws.html#outputs","text":"When one or more of the enrolled keywords has been detected in the submitted audio, KWS returns a region or list of timestamped regions, each with a score for the keyword that has been detected.","title":"Outputs"},{"location":"plugins/kws.html#enrollments","text":"KWS plugins do not support enrollments; instead, the set of classes the plugin is searching for is provided as a list of text keywords with each scoring request. Note that each domain is language dependent, and that a word or phrase in a language other than the one the domain is trained in is likely to be out-of-vocabulary. This means the word or phrase will be difficult or impossible to recall. For details on how to set or pass these keywords, please refer to the appropriate sections within the OLIVE CLI User Guide or OLIVE API Documentation.","title":"Enrollments"},{"location":"plugins/kws.html#limitations","text":"As was previously mentioned, traditional KWS relies on an underlying ASR system, making each KWS domain completely language dependent. This places several real restrictions on the users. First, this means that for keywords to be detectable, the words must be part of the ASR system's dictionary. This may make it difficult to find some keywords or phrases, like names, brands, slang or other colloquialisms, if they are out-of-vocabulary. This also makes it more difficult to deal with speakers or situations that may involve code switching. This reliance on ASR also makes KWS plugins quite heavy with respect to resource requirements, and also quite slow compared to other plugin types. An additional limitation stemming from the language dependence of the system is that if a user would like to detect keywords in a new language that isn't currently offered by SRI, a new domain for that language would need to be created, which requires a large amount of transcribed audio in that language.","title":"Limitations"},{"location":"plugins/kws.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/ldd-embed-v2.html","text":"ldd-embed-v2 (Language Detection) Version Changelog Plugin Version Change v2.0.0 This is a re-release of ldd-embedpldaSmolive-v1.1.1, intended to streamline and simplify some plugin names, fix an inaccurate domain name, and improve maintainability. Released with OLIVE 6.0.0. Description This plugin is based heavily on its predecessor, ldd-embedplda-v1, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one or more of the enrolled languages is found. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API. Domains multi-smart-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. The smart in the name refers to the domain's ability to attempt to load quantized 8-bit integer models for processing with significant speed and memory improvements, with the ability to back off to unquantized float models if the hardware doesn't support it. Inputs Audio file or buffer and an optional identifier. Outputs LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 French 11.40590000 input-audio.wav 43.500 77.500 French 11.29558277 input-audio.wav 77.500 78.500 Modern Standard Arabic 2.63369179 input-audio.wav 78.500 80.500 Iraqi Arabic 2.25519705 input-audio.wav 85.500 86.500 French 2.06612849 input-audio.wav 97.500 98.500 French 3.74665093 input-audio.wav 98.500 99.500 Mandarin Chinese 2.22936487 input-audio.wav 105.500 106.500 Spanish 2.72254372 input-audio.wav 107.500 108.500 French 2.60355234 input-audio.wav 108.500 110.500 French 2.76414633 input-audio.wav 109.500 113.140 English 2.85003138 input-audio.wav 113.760 116.260 French 2.50716114 input-audio.wav 120.260 140.260 Korean 14.93032360 input-audio.wav 143.260 157.260 Korean 12.62243176 input-audio.wav 158.260 161.260 Mandarin Chinese 3.24917603 input-audio.wav 161.260 162.260 French 2.73345900 input-audio.wav 165.260 177.260 Korean 12.32051945 input-audio.wav 178.260 180.260 Iraqi Arabic 2.41706276 input-audio.wav 186.320 188.820 Iraqi Arabic 2.85040617 input-audio.wav 193.820 194.820 Spanish 2.21501803 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 6.0+ Limitations Known or potential limitations of the plugin are outlined below. Quantized Model Hardware Compatibility There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech and false alarms. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Comments Language/Dialect Detection Granularity LDD plugins attempt to distinguish dialects (ie., Tunisian Arabic and Egyptian Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Mandarin English Pashto French Portuguese Farsi (Iranian Persian) Russian Japanese Spanish Korean Tagalog Arabic (Modern Standard Arabic and Levantine Arabic) Ukrainian Khmer Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editting domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Segmentation By Classification Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to win_sec min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 max_class_per_frame Determines whether the plugin is forced to output a proposed language for each speech region, even if the top scoring language is below the detection threshold True True or False max_class_per_file Determines the maximum number of languages the plugin will consider possible to detect in a single file/scoring request. This parameter helps control short, spurious false detections. If the likely number of languages is known a priori, setting this value close to that can help boost performance. If it is set too low, however, you may hinder the plugin's performance, since it may become impossible for the plugin to be correct. If there are 3 languages being spoken in a file, for example, and this is set to 2, then it is impossible for the plugin to detect and output all three languages. 3 2 - 6 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0 Additional Option Notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels","title":"Lanugage Detection (LDD) Low Resource"},{"location":"plugins/ldd-embed-v2.html#ldd-embed-v2-language-detection","text":"","title":"ldd-embed-v2 (Language Detection)"},{"location":"plugins/ldd-embed-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 This is a re-release of ldd-embedpldaSmolive-v1.1.1, intended to streamline and simplify some plugin names, fix an inaccurate domain name, and improve maintainability. Released with OLIVE 6.0.0.","title":"Version Changelog"},{"location":"plugins/ldd-embed-v2.html#description","text":"This plugin is based heavily on its predecessor, ldd-embedplda-v1, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one or more of the enrolled languages is found. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API.","title":"Description"},{"location":"plugins/ldd-embed-v2.html#domains","text":"multi-smart-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. The smart in the name refers to the domain's ability to attempt to load quantized 8-bit integer models for processing with significant speed and memory improvements, with the ability to back off to unquantized float models if the hardware doesn't support it.","title":"Domains"},{"location":"plugins/ldd-embed-v2.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/ldd-embed-v2.html#outputs","text":"LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 French 11.40590000 input-audio.wav 43.500 77.500 French 11.29558277 input-audio.wav 77.500 78.500 Modern Standard Arabic 2.63369179 input-audio.wav 78.500 80.500 Iraqi Arabic 2.25519705 input-audio.wav 85.500 86.500 French 2.06612849 input-audio.wav 97.500 98.500 French 3.74665093 input-audio.wav 98.500 99.500 Mandarin Chinese 2.22936487 input-audio.wav 105.500 106.500 Spanish 2.72254372 input-audio.wav 107.500 108.500 French 2.60355234 input-audio.wav 108.500 110.500 French 2.76414633 input-audio.wav 109.500 113.140 English 2.85003138 input-audio.wav 113.760 116.260 French 2.50716114 input-audio.wav 120.260 140.260 Korean 14.93032360 input-audio.wav 143.260 157.260 Korean 12.62243176 input-audio.wav 158.260 161.260 Mandarin Chinese 3.24917603 input-audio.wav 161.260 162.260 French 2.73345900 input-audio.wav 165.260 177.260 Korean 12.32051945 input-audio.wav 178.260 180.260 Iraqi Arabic 2.41706276 input-audio.wav 186.320 188.820 Iraqi Arabic 2.85040617 input-audio.wav 193.820 194.820 Spanish 2.21501803","title":"Outputs"},{"location":"plugins/ldd-embed-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/ldd-embed-v2.html#compatibility","text":"OLIVE 6.0+","title":"Compatibility"},{"location":"plugins/ldd-embed-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/ldd-embed-v2.html#quantized-model-hardware-compatibility","text":"There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function.","title":"Quantized Model Hardware Compatibility"},{"location":"plugins/ldd-embed-v2.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/ldd-embed-v2.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/ldd-embed-v2.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech and false alarms. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/ldd-embed-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/ldd-embed-v2.html#languagedialect-detection-granularity","text":"LDD plugins attempt to distinguish dialects (ie., Tunisian Arabic and Egyptian Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/ldd-embed-v2.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Mandarin English Pashto French Portuguese Farsi (Iranian Persian) Russian Japanese Spanish Korean Tagalog Arabic (Modern Standard Arabic and Levantine Arabic) Ukrainian Khmer Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/ldd-embed-v2.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese","title":"Supported Languages"},{"location":"plugins/ldd-embed-v2.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language.","title":"Enrollments"},{"location":"plugins/ldd-embed-v2.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editting domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line.","title":"Configuring Languages"},{"location":"plugins/ldd-embed-v2.html#segmentation-by-classification","text":"Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/ldd-embed-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to win_sec min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 max_class_per_frame Determines whether the plugin is forced to output a proposed language for each speech region, even if the top scoring language is below the detection threshold True True or False max_class_per_file Determines the maximum number of languages the plugin will consider possible to detect in a single file/scoring request. This parameter helps control short, spurious false detections. If the likely number of languages is known a priori, setting this value close to that can help boost performance. If it is set too low, however, you may hinder the plugin's performance, since it may become impossible for the plugin to be correct. If there are 3 languages being spoken in a file, for example, and this is set to 2, then it is impossible for the plugin to detect and output all three languages. 3 2 - 6 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/ldd-embed-v2.html#additional-option-notes","text":"","title":"Additional Option Notes"},{"location":"plugins/ldd-embed-v2.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/ldd-embed-v2.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels","title":"win_sec and step_sec"},{"location":"plugins/ldd.html","text":"redirect: plugins/ldd-embed-v2.md","title":"Ldd"},{"location":"plugins/ldd.html#redirect-pluginsldd-embed-v2md","text":"","title":"redirect: plugins/ldd-embed-v2.md"},{"location":"plugins/lid-hdplda-v2.html","text":"lid-hdplda-v2 (Language Identification) Version Changelog Plugin Version Change v2.0.0 Initial release of plugin with GPU support - otherwise functionally identical to v1.0.2. Released with OLIVE 5.6.0. v2.0.1 Updated language reporting names and domain settings to be more consistent with other language-based OLIVE plugins. Streamlined domains. Released with OLIVE 5.7.1. v2.0.2 Compatibility fix. Released with OLIVE 6.0.0. Description LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. In contrast to its predecessor, instead of training the PLDA parameters in a generative way, this plugin discriminatively trains the PDLA parameters. In addition, two PLDA models are trained, one to generate scores for clusters of highly related languages, and a second one to generate scores conditional to each cluster. For example, there is a \u201cSpanish cluster\u201d and inside that cluster we have languages as Castilian Spanish, Catalan, Galego etc\u2026 We call this approach Hierarchical Discriminative PLDA, or HDPLDA. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to over 100 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest Compatibility OLIVE 5.4+ Limitations Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Language/Dialect Detection Granularity LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load. Enrollments Some recent LID plugins allow class modifications. Due to the more complex structure of the model training process for the HDPLDA architecture, this plugin does not support user-enrollable or user-augmentable classes. The language model set for this plugin is fixed, though the provided languages can still be enabled or disabled (see below) as desired. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail. Note that internally, this plugin uses ISO-639-3 Language Codes to refer to each language. They are translated to English language names before being reported by OLIVE for human consumption, but it's important to know the language code when enabling or disabling a language. Refer to the link above to look up language codes, or see below for a list of the included languages and a mapping of the internal codes to the reported language name. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Reported Language Name arb, aeb, acm, afb, alv, arz Arabic (dialects merged and reported as Arabic) cmn Mandarin eng English fas Farsi fra French jpn Japanese khm Khmer kor Korean por Portuguese pus Pashto spa Spanish tgl Tagalog amh Amharic rus Russian ukr Ukrainian vie Vietnamese yue Cantonese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Reported Language Name abk Abkhazian aeb TunisianArabic acm MesopotamianArabic afb GulfArabic arb ModernStandardArabic alv LevantineArabic arz EgyptianArabic asm Assamese ben Bengali bod Tibetan bul Bulgarian cmn Mandarin dan Danish deu German ell Greek eng English eus Basque fas Farsi est Estonian fin Finnish gaz WestCentralOromo fra French hat HaitianCreole hau Hausa heb Hebrew hun Hungarian hye Armenian fao Faroese isl Icelandic ita Italian indsun Indonesian/Sundanese jav Javanese jpn Japanese kat Georgian khm Khmer kor Korean lav Latvian lin Lingala lit Lithuanian ltz Luxembourgish guj Gujarati mar Marathi hbs Serbo/Croatian mkd Macedonian mlg Malagasy mlt Maltese mon Mongolian mri Maori mya Burmese nan MinNanChinese afr Afrikaans nld Dutch npi Nepali bre Breton oci Occitan pan Punjabi por Portuguese pus Pashto ron Romanian sin Sinhala ces Czech pol Polish slk Slovak slv Slovenian nde Ndebele sna Shona snd Sindhi som Somali cat Catalan glg Galician spa Spanish sqi Albanian swa Swahili nno NorwegianNynorsk swe Swedish bak Bashkir kaz Kazakh tat Tatar kan Kannada mal Malayalam tam Tamil tel Telugu tgk Tajik ceb Cebuano tgl Tagalog thalao Thai amh Amharic tir Tigrinya tuk Turkmen aze Azerbaijani tur Turkish bel Belarusian rus Russian ukr Ukrainian urdhin Urdu/Hindi uzb Uzbek vie Vietnamese wuu WuChinese yid Yiddish ymm MaayMaay yor Yoruba yue Cantonese Global Options This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"Language Identification (LID)"},{"location":"plugins/lid-hdplda-v2.html#lid-hdplda-v2-language-identification","text":"","title":"lid-hdplda-v2 (Language Identification)"},{"location":"plugins/lid-hdplda-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial release of plugin with GPU support - otherwise functionally identical to v1.0.2. Released with OLIVE 5.6.0. v2.0.1 Updated language reporting names and domain settings to be more consistent with other language-based OLIVE plugins. Streamlined domains. Released with OLIVE 5.7.1. v2.0.2 Compatibility fix. Released with OLIVE 6.0.0.","title":"Version Changelog"},{"location":"plugins/lid-hdplda-v2.html#description","text":"LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. In contrast to its predecessor, instead of training the PLDA parameters in a generative way, this plugin discriminatively trains the PDLA parameters. In addition, two PLDA models are trained, one to generate scores for clusters of highly related languages, and a second one to generate scores conditional to each cluster. For example, there is a \u201cSpanish cluster\u201d and inside that cluster we have languages as Castilian Spanish, Catalan, Galego etc\u2026 We call this approach Hierarchical Discriminative PLDA, or HDPLDA.","title":"Description"},{"location":"plugins/lid-hdplda-v2.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to over 100 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/lid-hdplda-v2.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/lid-hdplda-v2.html#outputs","text":"Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102","title":"Outputs"},{"location":"plugins/lid-hdplda-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/lid-hdplda-v2.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/lid-hdplda-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile.","title":"Limitations"},{"location":"plugins/lid-hdplda-v2.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/lid-hdplda-v2.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/lid-hdplda-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/lid-hdplda-v2.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/lid-hdplda-v2.html#languagedialect-detection-granularity","text":"LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/lid-hdplda-v2.html#enrollments","text":"Some recent LID plugins allow class modifications. Due to the more complex structure of the model training process for the HDPLDA architecture, this plugin does not support user-enrollable or user-augmentable classes. The language model set for this plugin is fixed, though the provided languages can still be enabled or disabled (see below) as desired.","title":"Enrollments"},{"location":"plugins/lid-hdplda-v2.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail. Note that internally, this plugin uses ISO-639-3 Language Codes to refer to each language. They are translated to English language names before being reported by OLIVE for human consumption, but it's important to know the language code when enabling or disabling a language. Refer to the link above to look up language codes, or see below for a list of the included languages and a mapping of the internal codes to the reported language name.","title":"Configuring Languages"},{"location":"plugins/lid-hdplda-v2.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Reported Language Name arb, aeb, acm, afb, alv, arz Arabic (dialects merged and reported as Arabic) cmn Mandarin eng English fas Farsi fra French jpn Japanese khm Khmer kor Korean por Portuguese pus Pashto spa Spanish tgl Tagalog amh Amharic rus Russian ukr Ukrainian vie Vietnamese yue Cantonese","title":"Default Enabled Languages"},{"location":"plugins/lid-hdplda-v2.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Reported Language Name abk Abkhazian aeb TunisianArabic acm MesopotamianArabic afb GulfArabic arb ModernStandardArabic alv LevantineArabic arz EgyptianArabic asm Assamese ben Bengali bod Tibetan bul Bulgarian cmn Mandarin dan Danish deu German ell Greek eng English eus Basque fas Farsi est Estonian fin Finnish gaz WestCentralOromo fra French hat HaitianCreole hau Hausa heb Hebrew hun Hungarian hye Armenian fao Faroese isl Icelandic ita Italian indsun Indonesian/Sundanese jav Javanese jpn Japanese kat Georgian khm Khmer kor Korean lav Latvian lin Lingala lit Lithuanian ltz Luxembourgish guj Gujarati mar Marathi hbs Serbo/Croatian mkd Macedonian mlg Malagasy mlt Maltese mon Mongolian mri Maori mya Burmese nan MinNanChinese afr Afrikaans nld Dutch npi Nepali bre Breton oci Occitan pan Punjabi por Portuguese pus Pashto ron Romanian sin Sinhala ces Czech pol Polish slk Slovak slv Slovenian nde Ndebele sna Shona snd Sindhi som Somali cat Catalan glg Galician spa Spanish sqi Albanian swa Swahili nno NorwegianNynorsk swe Swedish bak Bashkir kaz Kazakh tat Tatar kan Kannada mal Malayalam tam Tamil tel Telugu tgk Tajik ceb Cebuano tgl Tagalog thalao Thai amh Amharic tir Tigrinya tuk Turkmen aze Azerbaijani tur Turkish bel Belarusian rus Russian ukr Ukrainian urdhin Urdu/Hindi uzb Uzbek vie Vietnamese wuu WuChinese yid Yiddish ymm MaayMaay yor Yoruba yue Cantonese","title":"Supported Languages"},{"location":"plugins/lid-hdplda-v2.html#global-options","text":"This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"Global Options"},{"location":"plugins/lid.html","text":"redirect: plugins/lid-hdplda-v1.md","title":"Lid"},{"location":"plugins/lid.html#redirect-pluginslid-hdplda-v1md","text":"","title":"redirect: plugins/lid-hdplda-v1.md"},{"location":"plugins/qbe-ftdnn-v2.html","text":"qbe-ftdnn-v2 (Query by Example Keyword Spotting) Version Changelog Plugin Version Change v2.0.0 Released with OLIVE 6.0.0 - This plugin is a re-release/update of qbe-ftdnnSmolive-v1, featuring bug fixes and the addition of stateless enrollment to allow client-side management of enrolled keywords to allow distributed server processing, multiple users Description This plugin is based heavily on its predecessor, qbe-tdnn-v5, with the important distinction that the model has been modified with both quantization and pruning, in addition to the adoption of factorized TDNN models. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Query by Example Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by audio sample enrollments. It has no language model or other constraints that accompany a traditional keyword spotting plugin, so it is language independent. This is a query by example plugin constructed with Factorized TDNN architecture models, with score calibration and test-adaptive merging of examples. It features dynamic time warping to compensate for speed and cadence differences when detecting keywords. This version also removes Kaldi dependency, replacing a poor-performing bottleneck feature extractor with an in-house developed and trained model, and reduces false alarms by filtering overlapping lower-confidence detections. Domains multi-smart-v1 Multi-condition domain meant for general-purpose audio conditions including telephone, broadband microphone, and other noisy situations without too many digital or PTT distortions. The smart in the name refers to the domain's ability to attempt to load quantized 8-bit integer models for processing with significant speed and memory improvements, with the ability to back off to unquantized float models if the hardware doesn't support it. Inputs For enrollment, an audio file or buffer with a corresponding keyword or query label. For scoring, an audio buffer or file. Outputs When one or more of the enrolled keywords has been detected in the submitted audio, QBE returns a region or list of timestamped regions (in seconds), each with a score for the keyword that has been detected. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006 Enrollments Query by Example plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new keyword to enroll. A new enrollment is created with the first class modification request, which consists of essentially sending the system an audio sample of a new keyword or key phrase, along with a label for that query. The label is not used at all by the system for detection, and is only a reference for the user to help recall what the query was. This means that it's completely acceptable to enroll a sample where a speaker is saying something like \"buenas noches\", and to label it \"good night - spanish\" or \"good night\" or even \"lorem ipsum\" in the system. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same query label. Stateless Enrollments / Vectorization This plugin now supports Stateless Enrollment . For more information on stateless enrollment, refer to the previous link. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected keyword and corresponding score for this keyword. RegionScorerRequest RegionScorerStereoRequest CLASS_MODIFIER \u2013 Enroll new keyword models or augment existing keyword models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 6.0+ Limitations Known or potential limitations of the plugin are outlined below. Quantized Model Hardware Compatibility There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function. Query/Keyword Recognizability The longer and more distinct the enrolled keyword or key phrase is, the better it will be recognized. Shorter keywords, that may occur often in speech (for example, enrolling the word 'a') or sound very similar to other words, may cause false alarms. In general, QBE plugins are language and speaker independent, and enrolled queries should be able to find speech from other talkers as well. However, if the enrolled keyword example is spoken by someone with a particular accent or non-standard pronunciation, it may not generalize with one or few samples. It's possible to enroll multiple samples and/or samples from multiple speakers to maximize the coverage of the query's model. Silence Sensitivity During Enrollment This version of the plugin is known to be very sensitive to including silence when enrolling new keyword queries, especially at the beginning or end of the query. Care should be taken to ensure that the boundaries of the enrollment submissions are as tight to the actual speech as possible. If excessive amounts of silence are included in the enrollment, the system could confuse this silence as part of the query, and the dynamic-time-warping algorithm may cause this to label keyword detections erroneously including large amounts of silence, and may also drastically increase the keyword search time. Future versions of the plugin will address this. Comments Very short keyword queries will be confusable with many other words, since the phonemes they consist of may be common or frequently occur as part of other words, or sound very similar to words or sounds that may commonly occur. The longer and more distinct a keyword is, the lower the likelihood of false alarms. Global Options This plugin does not feature user-configurable parameters.","title":"Query By Example (QBE)"},{"location":"plugins/qbe-ftdnn-v2.html#qbe-ftdnn-v2-query-by-example-keyword-spotting","text":"","title":"qbe-ftdnn-v2 (Query by Example Keyword Spotting)"},{"location":"plugins/qbe-ftdnn-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Released with OLIVE 6.0.0 - This plugin is a re-release/update of qbe-ftdnnSmolive-v1, featuring bug fixes and the addition of stateless enrollment to allow client-side management of enrolled keywords to allow distributed server processing, multiple users","title":"Version Changelog"},{"location":"plugins/qbe-ftdnn-v2.html#description","text":"This plugin is based heavily on its predecessor, qbe-tdnn-v5, with the important distinction that the model has been modified with both quantization and pruning, in addition to the adoption of factorized TDNN models. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Query by Example Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by audio sample enrollments. It has no language model or other constraints that accompany a traditional keyword spotting plugin, so it is language independent. This is a query by example plugin constructed with Factorized TDNN architecture models, with score calibration and test-adaptive merging of examples. It features dynamic time warping to compensate for speed and cadence differences when detecting keywords. This version also removes Kaldi dependency, replacing a poor-performing bottleneck feature extractor with an in-house developed and trained model, and reduces false alarms by filtering overlapping lower-confidence detections.","title":"Description"},{"location":"plugins/qbe-ftdnn-v2.html#domains","text":"multi-smart-v1 Multi-condition domain meant for general-purpose audio conditions including telephone, broadband microphone, and other noisy situations without too many digital or PTT distortions. The smart in the name refers to the domain's ability to attempt to load quantized 8-bit integer models for processing with significant speed and memory improvements, with the ability to back off to unquantized float models if the hardware doesn't support it.","title":"Domains"},{"location":"plugins/qbe-ftdnn-v2.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding keyword or query label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/qbe-ftdnn-v2.html#outputs","text":"When one or more of the enrolled keywords has been detected in the submitted audio, QBE returns a region or list of timestamped regions (in seconds), each with a score for the keyword that has been detected. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006","title":"Outputs"},{"location":"plugins/qbe-ftdnn-v2.html#enrollments","text":"Query by Example plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new keyword to enroll. A new enrollment is created with the first class modification request, which consists of essentially sending the system an audio sample of a new keyword or key phrase, along with a label for that query. The label is not used at all by the system for detection, and is only a reference for the user to help recall what the query was. This means that it's completely acceptable to enroll a sample where a speaker is saying something like \"buenas noches\", and to label it \"good night - spanish\" or \"good night\" or even \"lorem ipsum\" in the system. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same query label.","title":"Enrollments"},{"location":"plugins/qbe-ftdnn-v2.html#stateless-enrollments-vectorization","text":"This plugin now supports Stateless Enrollment . For more information on stateless enrollment, refer to the previous link.","title":"Stateless Enrollments / Vectorization"},{"location":"plugins/qbe-ftdnn-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected keyword and corresponding score for this keyword. RegionScorerRequest RegionScorerStereoRequest CLASS_MODIFIER \u2013 Enroll new keyword models or augment existing keyword models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/qbe-ftdnn-v2.html#compatibility","text":"OLIVE 6.0+","title":"Compatibility"},{"location":"plugins/qbe-ftdnn-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/qbe-ftdnn-v2.html#quantized-model-hardware-compatibility","text":"There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function.","title":"Quantized Model Hardware Compatibility"},{"location":"plugins/qbe-ftdnn-v2.html#querykeyword-recognizability","text":"The longer and more distinct the enrolled keyword or key phrase is, the better it will be recognized. Shorter keywords, that may occur often in speech (for example, enrolling the word 'a') or sound very similar to other words, may cause false alarms. In general, QBE plugins are language and speaker independent, and enrolled queries should be able to find speech from other talkers as well. However, if the enrolled keyword example is spoken by someone with a particular accent or non-standard pronunciation, it may not generalize with one or few samples. It's possible to enroll multiple samples and/or samples from multiple speakers to maximize the coverage of the query's model.","title":"Query/Keyword Recognizability"},{"location":"plugins/qbe-ftdnn-v2.html#silence-sensitivity-during-enrollment","text":"This version of the plugin is known to be very sensitive to including silence when enrolling new keyword queries, especially at the beginning or end of the query. Care should be taken to ensure that the boundaries of the enrollment submissions are as tight to the actual speech as possible. If excessive amounts of silence are included in the enrollment, the system could confuse this silence as part of the query, and the dynamic-time-warping algorithm may cause this to label keyword detections erroneously including large amounts of silence, and may also drastically increase the keyword search time. Future versions of the plugin will address this.","title":"Silence Sensitivity During Enrollment"},{"location":"plugins/qbe-ftdnn-v2.html#comments","text":"Very short keyword queries will be confusable with many other words, since the phonemes they consist of may be common or frequently occur as part of other words, or sound very similar to words or sounds that may commonly occur. The longer and more distinct a keyword is, the lower the likelihood of false alarms.","title":"Comments"},{"location":"plugins/qbe-ftdnn-v2.html#global-options","text":"This plugin does not feature user-configurable parameters.","title":"Global Options"},{"location":"plugins/qbe.html","text":"redirect: plugins/qbe-ftdnn-v2.md","title":"Qbe"},{"location":"plugins/qbe.html#redirect-pluginsqbe-ftdnn-v2md","text":"","title":"redirect: plugins/qbe-ftdnn-v2.md"},{"location":"plugins/red-transform-v1.html","text":"red-transform-v1 (Redaction - Voice Transformation) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0 v1.0.1 Compatibility update for OLIVE 5.7.0 runtime Description The red-transform plugin operates very similarly to the red-tone-v1 plugin, in that it alters the selected regions of the audio passed to it. In the red-tone-v1 plugin, this alteration was replacing it with a 'bleep' tone; in this new plugin the regions of submitted audio are instead passed through voice transformation with the goal of obscuring a speaker's identity. Those with very close knowledge of the original speaker may still be able to identify the transformed speaker through pronunciation, accent, and other unique identifiers that may not be sufficiently disguised by the transformation algorithm(s). Domains Several pre-set domains are available. Ideally, the audio containing the speaker to be obscured is a clean close microphone and users would select domain clean_close-v1. As the input audio quality decreases and/or distance between the speaker and the microphone increases, users will likely move down the domain list to find a balance between obscurement and intelligibility, with degraded_distant-v1. These domains were chosen as likely-useful compromises along the gammut of speed, intelligibility, and identity-obscurement. If you find your use case is typically not served well by either of these domains, and need something somewhere between the two, or maybe even less aggressive than the one included for degraded or distant speech, please get in touch with us and we can configure a new domain to better match your expected data, or to provide additional options to have on the shelf if desired. Moving further down the domain list (e.g. selecting degraded_distant-v1 over clean_close-v1) also lessens robustness to reverse engineering, but also process significantly faster than more aggressive domains. The included domains: clean_close-v1 This domain offers the highest amount of speaker obscurement, but suffers the most with respect to intelligibility in degraded audio conditions (noise, distant mic). It is the most aggressive regarding the transformations performed, is the slowest to process, and is designed for use in very clean audio conditions. degraded_distant-v1 This domain backs off some of the transformation being applied in an attempt to maximize intelligibility in noisy, degraded, and/or distant speech recordings or conditions, while still providing acceptable levels of speaker identity masking. It is significantly faster than the domain above. Inputs For redaction-voice transformation, an audio file and time-annotated regions corresponding to regions of speech to be obscured is required. Outputs The output of red-transform-v1 is an audio file that has been transformed according the domain specifications with the goal of creating audio where the speech is intelligible, but identifying features of the voice that could lead to recognizing the speaker have been obscured. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AUDIO_CONVERTER \u2013 Take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. AudioModificationRequestRequest Compatibility OLIVE 5.2+ Limitations Intelligibility & Audio conditions This plugin is very sensitive to audio conditions and can struggle to produce intelligible speech in degraded or distant microphone conditions. This is why multiple domains were provided, giving users the options of performing less intense transformation in difficult audio conditions where identity will already be partially obscured. Speed This plugin can be quite resource-intensive when it comes to processing speed. There can be significant speed differences between domains (e.g. domain degraded_distant-v1 is much faster than domain clean_close-v1, due to less processing). The clean_close-v1 domain may be near real-time processing speed, depending on hardware used for the transformation; and other domains should increase in speed from there. Usage This plugin was designed to be used in concert with SRI's Nightingale UI, and the specially designed \"Speaker Redaction\" tools within. For instructions on using the Speaker Redaction module, refer to the Speaker Redaction documentation.","title":"Redaction (RED) Transformation"},{"location":"plugins/red-transform-v1.html#red-transform-v1-redaction-voice-transformation","text":"","title":"red-transform-v1 (Redaction - Voice Transformation)"},{"location":"plugins/red-transform-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0 v1.0.1 Compatibility update for OLIVE 5.7.0 runtime","title":"Version Changelog"},{"location":"plugins/red-transform-v1.html#description","text":"The red-transform plugin operates very similarly to the red-tone-v1 plugin, in that it alters the selected regions of the audio passed to it. In the red-tone-v1 plugin, this alteration was replacing it with a 'bleep' tone; in this new plugin the regions of submitted audio are instead passed through voice transformation with the goal of obscuring a speaker's identity. Those with very close knowledge of the original speaker may still be able to identify the transformed speaker through pronunciation, accent, and other unique identifiers that may not be sufficiently disguised by the transformation algorithm(s).","title":"Description"},{"location":"plugins/red-transform-v1.html#domains","text":"Several pre-set domains are available. Ideally, the audio containing the speaker to be obscured is a clean close microphone and users would select domain clean_close-v1. As the input audio quality decreases and/or distance between the speaker and the microphone increases, users will likely move down the domain list to find a balance between obscurement and intelligibility, with degraded_distant-v1. These domains were chosen as likely-useful compromises along the gammut of speed, intelligibility, and identity-obscurement. If you find your use case is typically not served well by either of these domains, and need something somewhere between the two, or maybe even less aggressive than the one included for degraded or distant speech, please get in touch with us and we can configure a new domain to better match your expected data, or to provide additional options to have on the shelf if desired. Moving further down the domain list (e.g. selecting degraded_distant-v1 over clean_close-v1) also lessens robustness to reverse engineering, but also process significantly faster than more aggressive domains. The included domains: clean_close-v1 This domain offers the highest amount of speaker obscurement, but suffers the most with respect to intelligibility in degraded audio conditions (noise, distant mic). It is the most aggressive regarding the transformations performed, is the slowest to process, and is designed for use in very clean audio conditions. degraded_distant-v1 This domain backs off some of the transformation being applied in an attempt to maximize intelligibility in noisy, degraded, and/or distant speech recordings or conditions, while still providing acceptable levels of speaker identity masking. It is significantly faster than the domain above.","title":"Domains"},{"location":"plugins/red-transform-v1.html#inputs","text":"For redaction-voice transformation, an audio file and time-annotated regions corresponding to regions of speech to be obscured is required.","title":"Inputs"},{"location":"plugins/red-transform-v1.html#outputs","text":"The output of red-transform-v1 is an audio file that has been transformed according the domain specifications with the goal of creating audio where the speech is intelligible, but identifying features of the voice that could lead to recognizing the speaker have been obscured.","title":"Outputs"},{"location":"plugins/red-transform-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AUDIO_CONVERTER \u2013 Take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. AudioModificationRequestRequest","title":"Functionality (Traits)"},{"location":"plugins/red-transform-v1.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/red-transform-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/red-transform-v1.html#intelligibility-audio-conditions","text":"This plugin is very sensitive to audio conditions and can struggle to produce intelligible speech in degraded or distant microphone conditions. This is why multiple domains were provided, giving users the options of performing less intense transformation in difficult audio conditions where identity will already be partially obscured.","title":"Intelligibility &amp; Audio conditions"},{"location":"plugins/red-transform-v1.html#speed","text":"This plugin can be quite resource-intensive when it comes to processing speed. There can be significant speed differences between domains (e.g. domain degraded_distant-v1 is much faster than domain clean_close-v1, due to less processing). The clean_close-v1 domain may be near real-time processing speed, depending on hardware used for the transformation; and other domains should increase in speed from there.","title":"Speed"},{"location":"plugins/red-transform-v1.html#usage","text":"This plugin was designed to be used in concert with SRI's Nightingale UI, and the specially designed \"Speaker Redaction\" tools within. For instructions on using the Speaker Redaction module, refer to the Speaker Redaction documentation.","title":"Usage"},{"location":"plugins/red.html","text":"redirect: plugins/red-transform-v1.md","title":"Red"},{"location":"plugins/red.html#redirect-pluginsred-transform-v1md","text":"","title":"redirect: plugins/red-transform-v1.md"},{"location":"plugins/sad-dnn-v8.html","text":"sad-dnn-v8 (Speech Activity Detection) Version Changelog Plugin Version Change v8.0.0 Initial plugin release, functionally identical to v7.0.2, but updated to include GPU support with proper configuration. Tested and released with OLIVE 5.5.0 v8.1.0 Reduced memory usage for longer files, better GPU device handling, other minor bug fixes v8.2.0 Improvement to handling of new domain when performing adaptation. Tested and released with OLIVE 5.7.0 v8.3.0 Updates for Windows compatibility/thread safety. Released with OLIVE 5.7.1 Description Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring. Domains multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. fast-multi-v1 Multi-condition domain trained on the same data as the multi-v1 model above, but featuring configuration changes that allow it to process much more quickly, with a possible very slight trade off in accuracy in some circumstances. vtd-v1 Specialized voice-type discrimination plugin encompassing the technology and use case formerly covered by the separate VTD plugin of attempting to discriminate between a live human talking in a room versus a voice being played back from a device (i.e. news broadcast, speakerphone, etc.). The model is identical to the one provided with the vtd-dnn-v7 plugin, but provided here to minimize code duplication and plugin proliferation. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins. Adaptation Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide . Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions. Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Speech Disclaimer A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced. DTMF False Alarms It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech. Minimum Audio Length A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection. Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Speech Region Padding When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD. Supervised Adaptation Guidance When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Speech Activity Detection (SAD)"},{"location":"plugins/sad-dnn-v8.html#sad-dnn-v8-speech-activity-detection","text":"","title":"sad-dnn-v8 (Speech Activity Detection)"},{"location":"plugins/sad-dnn-v8.html#version-changelog","text":"Plugin Version Change v8.0.0 Initial plugin release, functionally identical to v7.0.2, but updated to include GPU support with proper configuration. Tested and released with OLIVE 5.5.0 v8.1.0 Reduced memory usage for longer files, better GPU device handling, other minor bug fixes v8.2.0 Improvement to handling of new domain when performing adaptation. Tested and released with OLIVE 5.7.0 v8.3.0 Updates for Windows compatibility/thread safety. Released with OLIVE 5.7.1","title":"Version Changelog"},{"location":"plugins/sad-dnn-v8.html#description","text":"Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring.","title":"Description"},{"location":"plugins/sad-dnn-v8.html#domains","text":"multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. fast-multi-v1 Multi-condition domain trained on the same data as the multi-v1 model above, but featuring configuration changes that allow it to process much more quickly, with a possible very slight trade off in accuracy in some circumstances. vtd-v1 Specialized voice-type discrimination plugin encompassing the technology and use case formerly covered by the separate VTD plugin of attempting to discriminate between a live human talking in a room versus a voice being played back from a device (i.e. news broadcast, speakerphone, etc.). The model is identical to the one provided with the vtd-dnn-v7 plugin, but provided here to minimize code duplication and plugin proliferation.","title":"Domains"},{"location":"plugins/sad-dnn-v8.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/sad-dnn-v8.html#outputs","text":"SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/sad-dnn-v8.html#adaptation","text":"Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide .","title":"Adaptation"},{"location":"plugins/sad-dnn-v8.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/sad-dnn-v8.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sad-dnn-v8.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/sad-dnn-v8.html#speech-disclaimer","text":"A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced.","title":"Speech Disclaimer"},{"location":"plugins/sad-dnn-v8.html#dtmf-false-alarms","text":"It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech.","title":"DTMF False Alarms"},{"location":"plugins/sad-dnn-v8.html#minimum-audio-length","text":"A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection.","title":"Minimum Audio Length"},{"location":"plugins/sad-dnn-v8.html#comments","text":"","title":"Comments"},{"location":"plugins/sad-dnn-v8.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/sad-dnn-v8.html#speech-region-padding","text":"When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD.","title":"Speech Region Padding"},{"location":"plugins/sad-dnn-v8.html#supervised-adaptation-guidance","text":"When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported.","title":"Supervised Adaptation Guidance"},{"location":"plugins/sad-dnn-v8.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/sad.html","text":"redirect: plugins/sad-dnn-v7.md","title":"Sad"},{"location":"plugins/sad.html#redirect-pluginssad-dnn-v7md","text":"","title":"redirect: plugins/sad-dnn-v7.md"},{"location":"plugins/sdd-embed-v2.html","text":"sdd-diarizeEmbedSmolive-v1 (Speaker Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.0.2 Feature-enhanced plugin, this version adds a float-32 full-resolution model to back off to when loading of the quantized 'smolive' model fails, which can happen on certain hardware that does not fully support the quantized model. Released with OLIVE 5.4.0. v1.0.3 Bug fix for short files v1.0.4 Bug fix re-enabling VB diarization, updated diarization parameters to more generalized. Tested/released with OLIVE 5.7.0. v1.0.5 Bug fix for compatibility with Windows. Released with OLIVE 5.7.1 v2.0.0 This is an updated version of sdd-diarizeEmbedSmolive-v1.0.6, including the addition of stateless enrollment, embedding extractor update, bug fixes. Released with OLIVE 6.0.0. Description The model in this plugin has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. This plugin bridges the gap between Speaker Detection and Speaker Diarization plugins. As it processes the submitted audio, the diarization stage will segment the submitted audio to determine 'who spoke when' by automatically clustering speech regions that it determines to be attributable to the same speaker, and labels them with class names such as 'unknownspk1', 'unknownspk2', etc. With those regions established, if there are speaker enrollments for different speakers of interest, the plugin will then attempt to determine if any of those \"unknown speaker\" regions belong to one or more of the enrolled speakers of interest, similar to a traditional Speaker Detection plugin. The output then combines these operations, by outputting the enrolled class name for regions where the system is confident that an enrolled speaker is detected, and retaining the \"unknown speaker\" labels where the system isn't confident that the speech comes from an enrolled speaker. Domains multi-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 unknownspk00 1.4000 /data/sid/audio/file1.wav 13.280 29.960 unknownspk01 1.4000 /data/sid/audio/file1.wav 30.350 32.030 unknownspk00 1.4000 /data/sid/audio/file2.wav 32.310 46.980 Phil 2.5333 /data/sid/audio/file2.wav 47.790 51.120 unknownspk02 1.4000 /data/sid/audio/file2.wav 54.340 55.400 unknownspk00 1.4000 Enrollments Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Stateless Enrollments / Vectorization This plugin now supports Stateless Enrollment . For more information on stateless enrollment, refer to the previous link. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 6.0+ Limitations Known or potential limitations of the plugin are outlined below. Quantized Model Hardware Compatibility There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Comments Segmentation By Diarization By default, this plugin uses blind speaker diarization to separate individual speakers within a file. This allows us to have much higher resolution when determining speaker boundaries than the previous plugins' \"segmentation by classification\" approach, and also avoids forcing a decision to be made between bounday resolution and processing speed. Under this segmentation scheme, the plugin first attempts to perform speaker diarization on the audio, clustering speakers that it believes are similar together, and labeling those regions appropriately unknownspk00 , unknownspk01 , etc. Once it has hypothesized individual speakear regions within the audio, it will then compare that audio against any enrolled speakers if they exist. If the plugin is confident enough that the speech of one of the \"unknowns\" actually belongs to an enrolled speaker, it updates the label and score for those regions accordingly. Segmentation By Classification (Legacy, Optional) This segmentation method is an optional, legacy method of clustering the speakers found within a file. Due to design considerations, it will always have to compromise between processing speed and diarization resolution. It should only be enabled by experienced users under specific circumstances. Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin performs when operating under SBC takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type. Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False dia_fixed_unknown_speaker_score The placeholder score assigned to unknown speaker regions to maintain region scoring format compatbility. 1.4 -1000 - 10000 enable_diarization Determines whether segmentation by diarization is performed, to use blind diarization to automatically cluster and segment the file into different speakers. If set to false, segmentation by classification is performed is performed instead, using the sliding window approach described above. True True or False enable_diarization_unknown_spk_output Determines whether labels for unknown speakers are output. If set to false, plugin only performs speaker detection and will not attempt to label unknown speakers. True True or False Additional option notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables only come into play if the enable_diarization option is set to false and the plugin is processing via segmentation by classification instead of segmentation by diarization. This is an optional, legacy mode for the plugin and will likely not be encountered. These options can be ignored unless you are intentionally enabling the legacy segmentation method. These options determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels output_only_highest_scoring_detected_speaker The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (/data/sid/audio/file2.wav) with scores for three different speakers previously enrolled, /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 /data/sid/audio/file2.wav 54.340 55.400 Sarah 2.430 and we change the threshold to 3.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: /data/sid/audio/file2.wav 41.130 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 However, with output_only_highest_scoring_detected_speaker = False, the system reports: /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"Speaker Detection (SDD) Low Resource"},{"location":"plugins/sdd-embed-v2.html#sdd-diarizeembedsmolive-v1-speaker-detection","text":"","title":"sdd-diarizeEmbedSmolive-v1 (Speaker Detection)"},{"location":"plugins/sdd-embed-v2.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.0.2 Feature-enhanced plugin, this version adds a float-32 full-resolution model to back off to when loading of the quantized 'smolive' model fails, which can happen on certain hardware that does not fully support the quantized model. Released with OLIVE 5.4.0. v1.0.3 Bug fix for short files v1.0.4 Bug fix re-enabling VB diarization, updated diarization parameters to more generalized. Tested/released with OLIVE 5.7.0. v1.0.5 Bug fix for compatibility with Windows. Released with OLIVE 5.7.1 v2.0.0 This is an updated version of sdd-diarizeEmbedSmolive-v1.0.6, including the addition of stateless enrollment, embedding extractor update, bug fixes. Released with OLIVE 6.0.0.","title":"Version Changelog"},{"location":"plugins/sdd-embed-v2.html#description","text":"The model in this plugin has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. This plugin bridges the gap between Speaker Detection and Speaker Diarization plugins. As it processes the submitted audio, the diarization stage will segment the submitted audio to determine 'who spoke when' by automatically clustering speech regions that it determines to be attributable to the same speaker, and labels them with class names such as 'unknownspk1', 'unknownspk2', etc. With those regions established, if there are speaker enrollments for different speakers of interest, the plugin will then attempt to determine if any of those \"unknown speaker\" regions belong to one or more of the enrolled speakers of interest, similar to a traditional Speaker Detection plugin. The output then combines these operations, by outputting the enrolled class name for regions where the system is confident that an enrolled speaker is detected, and retaining the \"unknown speaker\" labels where the system isn't confident that the speech comes from an enrolled speaker.","title":"Description"},{"location":"plugins/sdd-embed-v2.html#domains","text":"multi-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations.","title":"Domains"},{"location":"plugins/sdd-embed-v2.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sdd-embed-v2.html#outputs","text":"In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 unknownspk00 1.4000 /data/sid/audio/file1.wav 13.280 29.960 unknownspk01 1.4000 /data/sid/audio/file1.wav 30.350 32.030 unknownspk00 1.4000 /data/sid/audio/file2.wav 32.310 46.980 Phil 2.5333 /data/sid/audio/file2.wav 47.790 51.120 unknownspk02 1.4000 /data/sid/audio/file2.wav 54.340 55.400 unknownspk00 1.4000","title":"Outputs"},{"location":"plugins/sdd-embed-v2.html#enrollments","text":"Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sdd-embed-v2.html#stateless-enrollments-vectorization","text":"This plugin now supports Stateless Enrollment . For more information on stateless enrollment, refer to the previous link.","title":"Stateless Enrollments / Vectorization"},{"location":"plugins/sdd-embed-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sdd-embed-v2.html#compatibility","text":"OLIVE 6.0+","title":"Compatibility"},{"location":"plugins/sdd-embed-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sdd-embed-v2.html#quantized-model-hardware-compatibility","text":"There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function.","title":"Quantized Model Hardware Compatibility"},{"location":"plugins/sdd-embed-v2.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/sdd-embed-v2.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sdd-embed-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/sdd-embed-v2.html#segmentation-by-diarization","text":"By default, this plugin uses blind speaker diarization to separate individual speakers within a file. This allows us to have much higher resolution when determining speaker boundaries than the previous plugins' \"segmentation by classification\" approach, and also avoids forcing a decision to be made between bounday resolution and processing speed. Under this segmentation scheme, the plugin first attempts to perform speaker diarization on the audio, clustering speakers that it believes are similar together, and labeling those regions appropriately unknownspk00 , unknownspk01 , etc. Once it has hypothesized individual speakear regions within the audio, it will then compare that audio against any enrolled speakers if they exist. If the plugin is confident enough that the speech of one of the \"unknowns\" actually belongs to an enrolled speaker, it updates the label and score for those regions accordingly.","title":"Segmentation By Diarization"},{"location":"plugins/sdd-embed-v2.html#segmentation-by-classification-legacy-optional","text":"This segmentation method is an optional, legacy method of clustering the speakers found within a file. Due to design considerations, it will always have to compromise between processing speed and diarization resolution. It should only be enabled by experienced users under specific circumstances. Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin performs when operating under SBC takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification (Legacy, Optional)"},{"location":"plugins/sdd-embed-v2.html#options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False dia_fixed_unknown_speaker_score The placeholder score assigned to unknown speaker regions to maintain region scoring format compatbility. 1.4 -1000 - 10000 enable_diarization Determines whether segmentation by diarization is performed, to use blind diarization to automatically cluster and segment the file into different speakers. If set to false, segmentation by classification is performed is performed instead, using the sliding window approach described above. True True or False enable_diarization_unknown_spk_output Determines whether labels for unknown speakers are output. If set to false, plugin only performs speaker detection and will not attempt to label unknown speakers. True True or False","title":"Options"},{"location":"plugins/sdd-embed-v2.html#additional-option-notes","text":"","title":"Additional option notes"},{"location":"plugins/sdd-embed-v2.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/sdd-embed-v2.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables only come into play if the enable_diarization option is set to false and the plugin is processing via segmentation by classification instead of segmentation by diarization. This is an optional, legacy mode for the plugin and will likely not be encountered. These options can be ignored unless you are intentionally enabling the legacy segmentation method. These options determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"win_sec and step_sec"},{"location":"plugins/sdd-embed-v2.html#output_only_highest_scoring_detected_speaker","text":"The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (/data/sid/audio/file2.wav) with scores for three different speakers previously enrolled, /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 /data/sid/audio/file2.wav 54.340 55.400 Sarah 2.430 and we change the threshold to 3.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: /data/sid/audio/file2.wav 41.130 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 However, with output_only_highest_scoring_detected_speaker = False, the system reports: /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"output_only_highest_scoring_detected_speaker"},{"location":"plugins/sdd.html","text":"redirect: plugins/sdd-embed-v2.md","title":"Sdd"},{"location":"plugins/sdd.html#redirect-pluginssdd-embed-v2md","text":"","title":"redirect: plugins/sdd-embed-v2.md"},{"location":"plugins/shl-sbcEmbed-v1.html","text":"shl-sbcEmbed-v1 (Speaker Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 Updated to be compatible with OLIVE 5.1.0 v1.0.2 Bug fixes, released with OLIVE 5.2.0 Description Speaker Highlighting plugins will detect and label regions of speech in a submitted audio segment by searching for more speech within the audio that resembles the speaker in one or more user-provided 'seed' regions. Unlike Speaker Identification (SID), SHL is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations where the desired seed speaker is found. Unlike SID and SDD, Speaker Highlighting is not capable of performing any type of 'enrollment', and can only search for more examples of a speaker from within a given audio segment. There is no persistent speaker information retained for future trials, so a new target speaker seed must be supplied each time a new analysis is requested. Speaker Highlighting is meant to be used as a quick triage tool within a file, often longer files, where finding more or all speech from a given speaker is the goal. This can be done as a front-end for a task such as Speaker Redaction, where you'd like to remove, disguise, or otherwise process sections of a given speaker's voice, or as a 'helper' tool for Speaker Identification or Speaker Detection, to assist in building better speaker enrollment models for those plugins, by quickly finding additional candidate speech to add to an enrollment. Domains micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording. Inputs For scoring, an audio buffer or file, in addition to one or more timestamp regions denoting known locations of the speaker of interest. Outputs Speaker Highlighting returns a list of regions with an associated score for each region in the audio where the speaker is determined as 'detected'. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SHL plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/audio/file1.wav 8.320 13.110 speaker 0.5348 /data/audio/file1.wav 13.280 29.960 speaker 3.2122 /data/audio/file1.wav 30.350 32.030 speaker 5.5340 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Speaker Information Persistence Since Speaker Highlighting has no concept of enrollments, no information is retained between different audio analysis queries, and can only search for more of a given speaker within individual audio buffers or files. It can also only search audio for a single speaker at a time, since it assumes that all provided timestamped regions belong to the same speaker. Just as SID and SDD are sensitive to having \"good\" data provided as enrollment exemplars to perform properly, care must be applied when choosing the timestamp regions of the target speaker to 'seed' the system. If you provide a region that contains speech from multiple speakers, or is too short, or is noisy, or otherwise compromised, the performance of the system will degrade. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Comments Segmentation By Classification Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 Additional option notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"Speaker Highlighting (SHL)"},{"location":"plugins/shl-sbcEmbed-v1.html#shl-sbcembed-v1-speaker-detection","text":"","title":"shl-sbcEmbed-v1 (Speaker Detection)"},{"location":"plugins/shl-sbcEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 Updated to be compatible with OLIVE 5.1.0 v1.0.2 Bug fixes, released with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/shl-sbcEmbed-v1.html#description","text":"Speaker Highlighting plugins will detect and label regions of speech in a submitted audio segment by searching for more speech within the audio that resembles the speaker in one or more user-provided 'seed' regions. Unlike Speaker Identification (SID), SHL is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations where the desired seed speaker is found. Unlike SID and SDD, Speaker Highlighting is not capable of performing any type of 'enrollment', and can only search for more examples of a speaker from within a given audio segment. There is no persistent speaker information retained for future trials, so a new target speaker seed must be supplied each time a new analysis is requested. Speaker Highlighting is meant to be used as a quick triage tool within a file, often longer files, where finding more or all speech from a given speaker is the goal. This can be done as a front-end for a task such as Speaker Redaction, where you'd like to remove, disguise, or otherwise process sections of a given speaker's voice, or as a 'helper' tool for Speaker Identification or Speaker Detection, to assist in building better speaker enrollment models for those plugins, by quickly finding additional candidate speech to add to an enrollment.","title":"Description"},{"location":"plugins/shl-sbcEmbed-v1.html#domains","text":"micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording.","title":"Domains"},{"location":"plugins/shl-sbcEmbed-v1.html#inputs","text":"For scoring, an audio buffer or file, in addition to one or more timestamp regions denoting known locations of the speaker of interest.","title":"Inputs"},{"location":"plugins/shl-sbcEmbed-v1.html#outputs","text":"Speaker Highlighting returns a list of regions with an associated score for each region in the audio where the speaker is determined as 'detected'. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SHL plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/audio/file1.wav 8.320 13.110 speaker 0.5348 /data/audio/file1.wav 13.280 29.960 speaker 3.2122 /data/audio/file1.wav 30.350 32.030 speaker 5.5340","title":"Outputs"},{"location":"plugins/shl-sbcEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/shl-sbcEmbed-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/shl-sbcEmbed-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/shl-sbcEmbed-v1.html#speaker-information-persistence","text":"Since Speaker Highlighting has no concept of enrollments, no information is retained between different audio analysis queries, and can only search for more of a given speaker within individual audio buffers or files. It can also only search audio for a single speaker at a time, since it assumes that all provided timestamped regions belong to the same speaker. Just as SID and SDD are sensitive to having \"good\" data provided as enrollment exemplars to perform properly, care must be applied when choosing the timestamp regions of the target speaker to 'seed' the system. If you provide a region that contains speech from multiple speakers, or is too short, or is noisy, or otherwise compromised, the performance of the system will degrade.","title":"Speaker Information Persistence"},{"location":"plugins/shl-sbcEmbed-v1.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/shl-sbcEmbed-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/shl-sbcEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/shl-sbcEmbed-v1.html#segmentation-by-classification","text":"Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/shl-sbcEmbed-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0","title":"Global Options"},{"location":"plugins/shl-sbcEmbed-v1.html#additional-option-notes","text":"","title":"Additional option notes"},{"location":"plugins/shl-sbcEmbed-v1.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/shl-sbcEmbed-v1.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"win_sec and step_sec"},{"location":"plugins/shl.html","text":"redirect: plugins/shl-sbcEmbed-v1.md","title":"Shl"},{"location":"plugins/shl.html#redirect-pluginsshl-sbcembed-v1md","text":"","title":"redirect: plugins/shl-sbcEmbed-v1.md"},{"location":"plugins/sid-dplda-v3.html","text":"sid-dplda-v3 (Speaker Identification) Version Changelog Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v2.0.2, but updated to be include GPU support with proper configuration. Tested and released with OLIVE 5.5.0 v3.1.0 Bug fix for large memory usage on long input audio files. v3.2.0 Added 'fast-v1' domain, added enhancements for large enrollment batches. Released with OLIVE 5.7.1. v3.3.0 Added stateless enrollment support, bug fixes. Released with OLIVE 6.0.0. Description Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. This is a SID plugin leveraging dynamic calibration and discrimination via a DNN-powered DPLDA backend. This plugin accounts for the conditions of the trial to provide superior calibration performance out-of-the-box relative to prior plugins. This plugin features: Discriminative PLDA: SRI-pioneered approach to modeling of speaker variability using a DNN-trained backend and internal calibration defined by conditions of the audio. This approach is considerably faster and more reliable than the prior approach from SRI termed Trial-based Calibration (TBC). Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained. Domains multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. fast-v1 Specialized domain that shares actual model format with multi-v1, but features code changes that will perform speaker clustering and multiple rounds of scoring to enhance analysis speed when there are more than 500 enrolled speakers. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Enrollments SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Stateless Enrollments / Vectorization This plugin now supports Stateless Enrollment . For more information on stateless enrollment, refer to the previous link. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 6.0+ Limitations Known or potential limitations of the plugin are outlined below. Detection Granularity All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment. Minimum Speech Duration The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default). Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Minimum Speech The plugin will only process files with at least 0.5 seconds of detected speech (configurable). Advanced (Experimental) Usage for Exporting Embeddings A custom option to output embeddings, the speech regions, and duration of speech is available by setting output_ivs_dump_path=$OUTPUT_PATH where OUTPUT_PATH is a user defined directory to store items. During enrollment and evaluation, if the information for an audio file (based on md5sum) exists in this directory, it is loaded instead of re-computed. The plugin is an audio vectorizer and class exporter/importer. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 20.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.5 0.5 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Speaker Identification (SID)"},{"location":"plugins/sid-dplda-v3.html#sid-dplda-v3-speaker-identification","text":"","title":"sid-dplda-v3 (Speaker Identification)"},{"location":"plugins/sid-dplda-v3.html#version-changelog","text":"Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v2.0.2, but updated to be include GPU support with proper configuration. Tested and released with OLIVE 5.5.0 v3.1.0 Bug fix for large memory usage on long input audio files. v3.2.0 Added 'fast-v1' domain, added enhancements for large enrollment batches. Released with OLIVE 5.7.1. v3.3.0 Added stateless enrollment support, bug fixes. Released with OLIVE 6.0.0.","title":"Version Changelog"},{"location":"plugins/sid-dplda-v3.html#description","text":"Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. This is a SID plugin leveraging dynamic calibration and discrimination via a DNN-powered DPLDA backend. This plugin accounts for the conditions of the trial to provide superior calibration performance out-of-the-box relative to prior plugins. This plugin features: Discriminative PLDA: SRI-pioneered approach to modeling of speaker variability using a DNN-trained backend and internal calibration defined by conditions of the audio. This approach is considerably faster and more reliable than the prior approach from SRI termed Trial-based Calibration (TBC). Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained.","title":"Description"},{"location":"plugins/sid-dplda-v3.html#domains","text":"multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. fast-v1 Specialized domain that shares actual model format with multi-v1, but features code changes that will perform speaker clustering and multiple rounds of scoring to enhance analysis speed when there are more than 500 enrolled speakers.","title":"Domains"},{"location":"plugins/sid-dplda-v3.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sid-dplda-v3.html#outputs","text":"Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Outputs"},{"location":"plugins/sid-dplda-v3.html#enrollments","text":"SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sid-dplda-v3.html#stateless-enrollments-vectorization","text":"This plugin now supports Stateless Enrollment . For more information on stateless enrollment, refer to the previous link.","title":"Stateless Enrollments / Vectorization"},{"location":"plugins/sid-dplda-v3.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sid-dplda-v3.html#compatibility","text":"OLIVE 6.0+","title":"Compatibility"},{"location":"plugins/sid-dplda-v3.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sid-dplda-v3.html#detection-granularity","text":"All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment.","title":"Detection Granularity"},{"location":"plugins/sid-dplda-v3.html#minimum-speech-duration","text":"The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sid-dplda-v3.html#comments","text":"","title":"Comments"},{"location":"plugins/sid-dplda-v3.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/sid-dplda-v3.html#minimum-speech","text":"The plugin will only process files with at least 0.5 seconds of detected speech (configurable).","title":"Minimum Speech"},{"location":"plugins/sid-dplda-v3.html#advanced-experimental-usage-for-exporting-embeddings","text":"A custom option to output embeddings, the speech regions, and duration of speech is available by setting output_ivs_dump_path=$OUTPUT_PATH where OUTPUT_PATH is a user defined directory to store items. During enrollment and evaluation, if the information for an audio file (based on md5sum) exists in this directory, it is loaded instead of re-computed. The plugin is an audio vectorizer and class exporter/importer.","title":"Advanced (Experimental) Usage for Exporting Embeddings"},{"location":"plugins/sid-dplda-v3.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 20.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.5 0.5 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/sid.html","text":"redirect: plugins/sid-dplda-v2.md","title":"Sid"},{"location":"plugins/sid.html#redirect-pluginssid-dplda-v2md","text":"","title":"redirect: plugins/sid-dplda-v2.md"},{"location":"plugins/tmt-ctranslate-v1.html","text":"tmt-ctranslate-v1 (Text Machine Translation) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.6.0. Carries over the functionality of several domains from tmt-neural-v1 with improvements to speed, memory usage, hardware compatibility, and stability, and adds new language capabilities as well v1.2.0 New domains released, including improved Mandarin to English translation based on text directly (rather than speech), meant for chat-style translation, as well as Ukrainian to English, English to Ukrainian, and an updated English to Russian model. Updated character limits to be domain specific to be more compatible with Mandarin and provide better translation. Tested and released for OLIVE 5.6.0 v1.3.0 Improved CPU/GPU device handling and compatibility between OLIVE's multi-processing ethos with ctranslate2's. Tested and released with OLIVE 5.7.0 v1.3.1 Bug fixes for Windows compatibility, fixed an issue with one domain's vocabulary file. Released with OLIVE 5.7.1 v1.3.2 Bug fixes, speed improvements for multi-line inputs. Released with OLIVE 6.0.0. Description Text Machine Translation plugins perform translation of text from one language to another, typically from one language, specified by the domain, to English. All TMT domains are language-dependent, and each one is meant to work only with a single, specific language. TMT plugins may have some limitations or special processing considerations depending on the language(s) involved and their native alphabets, as well as the training data used to train the underlying models. The goal of this plugin is to ingest text in one language, defined by the specified domain, and translate it to another language. Both of these languages are specified in the domain names - the first language listed is the \"source\" language, that will be translated from . The second language listed is the \"target\" language, that the plugin will attempt to translate to . An example input string, in Spanish: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras Note that some punctuation and special characters will be stripped from the input during preprocessing. An example output string translation of the above example, as provided by the spanish-english-v1 domain: manual of photography to learn everything essential about unk cameras Words that the system does not recognize or can't translate may be marked up with an unk tag for some domains, as can be seen in the example output below. Note that all system output will be lowercase for case sensitive languages, and apart from the unk tag, all output will be devoid of punctuation as the models have been primarily trained with punctuation-less ASR output and are meant for translating ASR output more than text documents. Most domains perform best when given individual sentences as input. An important exception to the above punctuation statement is the mandarin-english-text-v1 domain listed below. This is a new/advanced domain that has been trained with more text awareness and is capable of both considering punctuation as input in its ingestion step, and of outputting punctuation in the resulting translation. Domains (Supported Languages) english-japanese-v2 Translates English text into Japanese text. english-mandarin-v2 Translates English text into Mandarin text. english-russian-v2 Translates English text into Russian text. english-spanish-v2 Translates English text into Spanish text. english-ukrainian-v2 Translates English text into Ukrainian text. iraqiArabic-english-v1 Translates Iraqi Arabic text into English text. japanese-english-v2 Translates Japanese text into English text. mandarin-english-text-v2 Translates Mandarin text into English text. Capable of leveraging punctuation information during translation and producing punctuation in the output. Best able to deal with multi-sentence input. Not compatible with or released for OLIVE on Windows at this time mandarin-english-v2 Translates Mandarin text into English text. russian-english-v2 Translates Russian text into English text. spanish-english-v2 Translates Spanish text into English text. ukrainian-english-v2 Translates Ukrainian text into English text. Inputs For scoring, a text string or text-populated file is required. There is no verification performed by OLIVE or by TMT plugins that the text passed as input is actually in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Note that output may fail or be very confusing if the input language does not match the domain's capabilities. An example input string: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras There is a bit of preprocessing of input that occurs before a string is sent to translation. Thus, all input is lower-cased and frequent punctuation marks such as commas, exclamation marks or question marks are stripped from the string. However, no spelling error correction of any kind is performed. As described above - one exception to this is the mandarin-english-text-v1 domain, which bypasses this preprocessing as it can handle the punctuation in the input. Outputs The output format for TMT plugins is simply text. Words that the system does not recognize or can't translate will be marked up with an unk tag, as can be seen in the example output below. Note that all system output will be lowercase, and apart from the unk tag, all output will be devoid of punctuation. An example output string translation of the input example: manual of photography to learn everything essential about unk cameras Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. TextTransformer \u2013 Plugin accepts and analyzes text string inputs, and outputs a new text string as output. In the case of TMT plugins, a text string in the source language should be provided as input, with the expectation that the output will be a text string translated to the desired destination language. TextTransformRequest TextTransformResult Compatibility OLIVE 5.6+ Limitations Like ASR, TMT plugins are language dependent and also largely text domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the translation models underlying each domain. Several factors contribute to what might limit the vocabulary of a translation model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, TMT plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. The size-on-disk of each domain can be quite large depending on the technology of the individual plugin, and some technologies/domains may have additional constraints. Multi-Language Memory Constraints Each domain currently uses a significant amount of memory once the translation models have been loaded by using the plugin. Without substantial memory resources available on the machine hosting OLIVE, it's possible to quickly run out of available memory and run into strange behavior and/or failures - especially when running TMT alongside other memory-intensive plugins like ASR. Language Dependence Each domain of this TMT plugin is language specific, and is only capable of translating text from one single language to one other. There is no filter or any sort of verification performed by OLIVE to ensure that the text passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input text if the source language is unknown, though it is often easy to spot mismatched input languages by the high number of output words that appears with the \" unk \" tag for domains that support it. Spelling Errors Note that there is no spell-checking or other types of spelling related pre-processing that occurs on the input data. Therefore any spelling mistakes in the input are likely to cause the system to output unk tags. Out of Vocabulary (OOV) Words, Names The individual words that the plugin is capable of recognizing and translating is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be translated by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary or to the translation model. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications. Input Expectations This plugin has been trained to translate a single sentence at a time - to maximize the performance of this plugin, each scoring request input should be a single sentence. When longer segments, like paragraphs, pages, or even whole files are input, performance may be less than optimal, but the plugin will 'split' the data into 25-word chunks that it treats as sentences to minimize the negative impact. This is less than ideal, but performs much better on the whole than treating larger inputs as a single sentence and leaving them as-is. Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options This plugin does not expose any options to the user.","title":"Text Machine Translation (TMT)"},{"location":"plugins/tmt-ctranslate-v1.html#tmt-ctranslate-v1-text-machine-translation","text":"","title":"tmt-ctranslate-v1 (Text Machine Translation)"},{"location":"plugins/tmt-ctranslate-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.6.0. Carries over the functionality of several domains from tmt-neural-v1 with improvements to speed, memory usage, hardware compatibility, and stability, and adds new language capabilities as well v1.2.0 New domains released, including improved Mandarin to English translation based on text directly (rather than speech), meant for chat-style translation, as well as Ukrainian to English, English to Ukrainian, and an updated English to Russian model. Updated character limits to be domain specific to be more compatible with Mandarin and provide better translation. Tested and released for OLIVE 5.6.0 v1.3.0 Improved CPU/GPU device handling and compatibility between OLIVE's multi-processing ethos with ctranslate2's. Tested and released with OLIVE 5.7.0 v1.3.1 Bug fixes for Windows compatibility, fixed an issue with one domain's vocabulary file. Released with OLIVE 5.7.1 v1.3.2 Bug fixes, speed improvements for multi-line inputs. Released with OLIVE 6.0.0.","title":"Version Changelog"},{"location":"plugins/tmt-ctranslate-v1.html#description","text":"Text Machine Translation plugins perform translation of text from one language to another, typically from one language, specified by the domain, to English. All TMT domains are language-dependent, and each one is meant to work only with a single, specific language. TMT plugins may have some limitations or special processing considerations depending on the language(s) involved and their native alphabets, as well as the training data used to train the underlying models. The goal of this plugin is to ingest text in one language, defined by the specified domain, and translate it to another language. Both of these languages are specified in the domain names - the first language listed is the \"source\" language, that will be translated from . The second language listed is the \"target\" language, that the plugin will attempt to translate to . An example input string, in Spanish: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras Note that some punctuation and special characters will be stripped from the input during preprocessing. An example output string translation of the above example, as provided by the spanish-english-v1 domain: manual of photography to learn everything essential about unk cameras Words that the system does not recognize or can't translate may be marked up with an unk tag for some domains, as can be seen in the example output below. Note that all system output will be lowercase for case sensitive languages, and apart from the unk tag, all output will be devoid of punctuation as the models have been primarily trained with punctuation-less ASR output and are meant for translating ASR output more than text documents. Most domains perform best when given individual sentences as input. An important exception to the above punctuation statement is the mandarin-english-text-v1 domain listed below. This is a new/advanced domain that has been trained with more text awareness and is capable of both considering punctuation as input in its ingestion step, and of outputting punctuation in the resulting translation.","title":"Description"},{"location":"plugins/tmt-ctranslate-v1.html#domains-supported-languages","text":"english-japanese-v2 Translates English text into Japanese text. english-mandarin-v2 Translates English text into Mandarin text. english-russian-v2 Translates English text into Russian text. english-spanish-v2 Translates English text into Spanish text. english-ukrainian-v2 Translates English text into Ukrainian text. iraqiArabic-english-v1 Translates Iraqi Arabic text into English text. japanese-english-v2 Translates Japanese text into English text. mandarin-english-text-v2 Translates Mandarin text into English text. Capable of leveraging punctuation information during translation and producing punctuation in the output. Best able to deal with multi-sentence input. Not compatible with or released for OLIVE on Windows at this time mandarin-english-v2 Translates Mandarin text into English text. russian-english-v2 Translates Russian text into English text. spanish-english-v2 Translates Spanish text into English text. ukrainian-english-v2 Translates Ukrainian text into English text.","title":"Domains (Supported Languages)"},{"location":"plugins/tmt-ctranslate-v1.html#inputs","text":"For scoring, a text string or text-populated file is required. There is no verification performed by OLIVE or by TMT plugins that the text passed as input is actually in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Note that output may fail or be very confusing if the input language does not match the domain's capabilities. An example input string: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras There is a bit of preprocessing of input that occurs before a string is sent to translation. Thus, all input is lower-cased and frequent punctuation marks such as commas, exclamation marks or question marks are stripped from the string. However, no spelling error correction of any kind is performed. As described above - one exception to this is the mandarin-english-text-v1 domain, which bypasses this preprocessing as it can handle the punctuation in the input.","title":"Inputs"},{"location":"plugins/tmt-ctranslate-v1.html#outputs","text":"The output format for TMT plugins is simply text. Words that the system does not recognize or can't translate will be marked up with an unk tag, as can be seen in the example output below. Note that all system output will be lowercase, and apart from the unk tag, all output will be devoid of punctuation. An example output string translation of the input example: manual of photography to learn everything essential about unk cameras","title":"Outputs"},{"location":"plugins/tmt-ctranslate-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. TextTransformer \u2013 Plugin accepts and analyzes text string inputs, and outputs a new text string as output. In the case of TMT plugins, a text string in the source language should be provided as input, with the expectation that the output will be a text string translated to the desired destination language. TextTransformRequest TextTransformResult","title":"Functionality (Traits)"},{"location":"plugins/tmt-ctranslate-v1.html#compatibility","text":"OLIVE 5.6+","title":"Compatibility"},{"location":"plugins/tmt-ctranslate-v1.html#limitations","text":"Like ASR, TMT plugins are language dependent and also largely text domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the translation models underlying each domain. Several factors contribute to what might limit the vocabulary of a translation model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, TMT plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. The size-on-disk of each domain can be quite large depending on the technology of the individual plugin, and some technologies/domains may have additional constraints.","title":"Limitations"},{"location":"plugins/tmt-ctranslate-v1.html#multi-language-memory-constraints","text":"Each domain currently uses a significant amount of memory once the translation models have been loaded by using the plugin. Without substantial memory resources available on the machine hosting OLIVE, it's possible to quickly run out of available memory and run into strange behavior and/or failures - especially when running TMT alongside other memory-intensive plugins like ASR.","title":"Multi-Language Memory Constraints"},{"location":"plugins/tmt-ctranslate-v1.html#language-dependence","text":"Each domain of this TMT plugin is language specific, and is only capable of translating text from one single language to one other. There is no filter or any sort of verification performed by OLIVE to ensure that the text passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input text if the source language is unknown, though it is often easy to spot mismatched input languages by the high number of output words that appears with the \" unk \" tag for domains that support it.","title":"Language Dependence"},{"location":"plugins/tmt-ctranslate-v1.html#spelling-errors","text":"Note that there is no spell-checking or other types of spelling related pre-processing that occurs on the input data. Therefore any spelling mistakes in the input are likely to cause the system to output unk tags.","title":"Spelling Errors"},{"location":"plugins/tmt-ctranslate-v1.html#out-of-vocabulary-oov-words-names","text":"The individual words that the plugin is capable of recognizing and translating is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be translated by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary or to the translation model. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications.","title":"Out of Vocabulary (OOV) Words, Names"},{"location":"plugins/tmt-ctranslate-v1.html#input-expectations","text":"This plugin has been trained to translate a single sentence at a time - to maximize the performance of this plugin, each scoring request input should be a single sentence. When longer segments, like paragraphs, pages, or even whole files are input, performance may be less than optimal, but the plugin will 'split' the data into 25-word chunks that it treats as sentences to minimize the negative impact. This is less than ideal, but performs much better on the whole than treating larger inputs as a single sentence and leaving them as-is.","title":"Input Expectations"},{"location":"plugins/tmt-ctranslate-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/tmt-ctranslate-v1.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/tmt-ctranslate-v1.html#global-options","text":"This plugin does not expose any options to the user.","title":"Global Options"},{"location":"plugins/tmt.html","text":"redirect: plugins/tmt-ctranslate-v1.md","title":"Tmt"},{"location":"plugins/tmt.html#redirect-pluginstmt-ctranslate-v1md","text":"","title":"redirect: plugins/tmt-ctranslate-v1.md"},{"location":"plugins/tpd-dynapy-v5.html","text":"tpd-dynapy-v5 (Topic Detection) Version Changelog Plugin Version Change v5.0.0 Initial plugin release, tested to be compatible with OLIVE 5.2.0 v5.0.1 Released with OLIVE 5.3.0, minor parameter change to improve output stability v5.1.0 Modernization and compatibilty update to allow re-release with OLIVE 6.0.0 Description Topic Detection (TPD) plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic embeddings framework with PLDA backend scoring and multi-class calibration. This plugin improves on prior TPD technology using updated ASR models, and XLM-RoBERTa word embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided. Domains This plugin includes two Mandarin Chinese domains - they are both using monolingual word embedding extractors with the difference being the size of their respective embedding sizes, 768 for cmn-tel-v1 vs. 1024 for cmn-tel-large-v1. The base domain is significantly faster and uses significantly less memory during processing than the 'large' domain. The larger word embeddings used by the 'large' domain afforded slightly better TPD performance compared to the standard model in SRI's pilot experiments, and may have the potential to improve performance more in user data. eng-tel-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-tel-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-large-v1 Same as above, except this one uses higher-dimensional word embeddings. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.2+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech. Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language. Comments or Usage Notes The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below. Base Topic Data eng-cts-v1 BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK cmn-cts-v1 ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Topic Detection (TPD) ASR-based"},{"location":"plugins/tpd-dynapy-v5.html#tpd-dynapy-v5-topic-detection","text":"","title":"tpd-dynapy-v5 (Topic Detection)"},{"location":"plugins/tpd-dynapy-v5.html#version-changelog","text":"Plugin Version Change v5.0.0 Initial plugin release, tested to be compatible with OLIVE 5.2.0 v5.0.1 Released with OLIVE 5.3.0, minor parameter change to improve output stability v5.1.0 Modernization and compatibilty update to allow re-release with OLIVE 6.0.0","title":"Version Changelog"},{"location":"plugins/tpd-dynapy-v5.html#description","text":"Topic Detection (TPD) plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic embeddings framework with PLDA backend scoring and multi-class calibration. This plugin improves on prior TPD technology using updated ASR models, and XLM-RoBERTa word embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided.","title":"Description"},{"location":"plugins/tpd-dynapy-v5.html#domains","text":"This plugin includes two Mandarin Chinese domains - they are both using monolingual word embedding extractors with the difference being the size of their respective embedding sizes, 768 for cmn-tel-v1 vs. 1024 for cmn-tel-large-v1. The base domain is significantly faster and uses significantly less memory during processing than the 'large' domain. The larger word embeddings used by the 'large' domain afforded slightly better TPD performance compared to the standard model in SRI's pilot experiments, and may have the potential to improve performance more in user data. eng-tel-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-tel-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-large-v1 Same as above, except this one uses higher-dimensional word embeddings.","title":"Domains"},{"location":"plugins/tpd-dynapy-v5.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-dynapy-v5.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-dynapy-v5.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic.","title":"Enrollments"},{"location":"plugins/tpd-dynapy-v5.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-dynapy-v5.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/tpd-dynapy-v5.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-dynapy-v5.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-dynapy-v5.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-dynapy-v5.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech.","title":"Minimum Speech Duration"},{"location":"plugins/tpd-dynapy-v5.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language.","title":"Language Dependence"},{"location":"plugins/tpd-dynapy-v5.html#comments-or-usage-notes","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below.","title":"Comments or Usage Notes"},{"location":"plugins/tpd-dynapy-v5.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-dynapy-v5.html#eng-cts-v1","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1"},{"location":"plugins/tpd-dynapy-v5.html#rus-cts-v1","text":"ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK","title":"rus-cts-v1"},{"location":"plugins/tpd-dynapy-v5.html#cmn-cts-v1","text":"ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK","title":"cmn-cts-v1"},{"location":"plugins/tpd-dynapy-v5.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/tpd-fusion-v1.html","text":"tpd-fusion-v1 (Topic Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, tested to be compatible with OLIVE 5.2.0 v1.0.1 Released with OLIVE 5.3.0, minor parameter change to improve output stability v1.1.0 Modernization and compatibilty update to allow re-release with OLIVE 6.0.0 Description Topic Detection (TPD) plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin features a bimodal TPD framework that combines the word (XLM-RoBERTa) and acoustic topic embeddings before the PLDA backend scoring and multi-class calibration. The word and acoustic embeddings are extracted separately using the embedding extractors of the unimodal TPD plugins, tpd-dynapy and tpd-embed, respectively and fused by applying linear discriminant analysis before applying the PLDA scoring. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided. Domains This plugin includes two Mandarin Chinese domains - they are both using monolingual word embedding extractors with the difference being the size of their respective embedding sizes, 768 for cmn-tel-v1 vs. 1024 for cmn-tel-large-v1. The base domain is significantly faster and uses significantly less memory during processing than the 'large' domain. The larger word embeddings used by the 'large' domain afforded slightly better TPD performance compared to the standard model in SRI's pilot experiments, and may have the potential to improve performance more in user data. eng-tel-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-tel-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-large-v1 Same as above, except this one uses higher-dimensional word embeddings. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.2+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech. Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language. Comments or Usage Notes The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below. Base Topic Data eng-cts-v1 BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK cmn-cts-v1 ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Topic Detection (TPD) ASR and Acoustic Fusion"},{"location":"plugins/tpd-fusion-v1.html#tpd-fusion-v1-topic-detection","text":"","title":"tpd-fusion-v1 (Topic Detection)"},{"location":"plugins/tpd-fusion-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, tested to be compatible with OLIVE 5.2.0 v1.0.1 Released with OLIVE 5.3.0, minor parameter change to improve output stability v1.1.0 Modernization and compatibilty update to allow re-release with OLIVE 6.0.0","title":"Version Changelog"},{"location":"plugins/tpd-fusion-v1.html#description","text":"Topic Detection (TPD) plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin features a bimodal TPD framework that combines the word (XLM-RoBERTa) and acoustic topic embeddings before the PLDA backend scoring and multi-class calibration. The word and acoustic embeddings are extracted separately using the embedding extractors of the unimodal TPD plugins, tpd-dynapy and tpd-embed, respectively and fused by applying linear discriminant analysis before applying the PLDA scoring. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided.","title":"Description"},{"location":"plugins/tpd-fusion-v1.html#domains","text":"This plugin includes two Mandarin Chinese domains - they are both using monolingual word embedding extractors with the difference being the size of their respective embedding sizes, 768 for cmn-tel-v1 vs. 1024 for cmn-tel-large-v1. The base domain is significantly faster and uses significantly less memory during processing than the 'large' domain. The larger word embeddings used by the 'large' domain afforded slightly better TPD performance compared to the standard model in SRI's pilot experiments, and may have the potential to improve performance more in user data. eng-tel-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-tel-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-large-v1 Same as above, except this one uses higher-dimensional word embeddings.","title":"Domains"},{"location":"plugins/tpd-fusion-v1.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-fusion-v1.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-fusion-v1.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic.","title":"Enrollments"},{"location":"plugins/tpd-fusion-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-fusion-v1.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/tpd-fusion-v1.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-fusion-v1.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-fusion-v1.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-fusion-v1.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech.","title":"Minimum Speech Duration"},{"location":"plugins/tpd-fusion-v1.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language.","title":"Language Dependence"},{"location":"plugins/tpd-fusion-v1.html#comments-or-usage-notes","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below.","title":"Comments or Usage Notes"},{"location":"plugins/tpd-fusion-v1.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-fusion-v1.html#eng-cts-v1","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1"},{"location":"plugins/tpd-fusion-v1.html#rus-cts-v1","text":"ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK","title":"rus-cts-v1"},{"location":"plugins/tpd-fusion-v1.html#cmn-cts-v1","text":"ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK","title":"cmn-cts-v1"},{"location":"plugins/tpd-fusion-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/vtd-dnn-v7.html","text":"vtd-dnn-v7 (Voice Type Discrimination) Version Changelog Plugin Version Change v7.0.1 Initial plugin release, this plugin shares a codebase with sad-dnn-v7.0.1, with only the models and parameters configured for live speech detection. Released with OLIVE 5.1.0 v7.0.2 Bug fixes from v7.0.1, released with OLIVE 5.2.0 Description Voice type discrimination (VTD) plugins are designed to detect the presence of speech coming from a live human talker. The goal of VTD is to be able to distinguish not only live-produced, human speech from silence or noise, but also from speech being played over an electronic speaker, such as from a television or phone. When live speech is detected, it is labeled with the timestamps corresponding to its location in the audio. Like SAD, this plug-in may be used either as a frame scorer or region scorer. Domains vtd-v1 Domain designed to detect live speech indoors, differentiating within room live-speech from background distractors like TV, radio, door sound, telephone ringing, traffic etc. Inputs An audio file or buffer and optional identifier and/or optional regions. Outputs The current VTD plugin is capable of performing both frame scoring and region scoring . For frame scoring, typically, a log-likelihood ratio (LLR) score of live-speech vs. non-speech/non-live-speech per each 10ms frame of the input audio segment is output (i.e. 100 audio frames per second). An LLR of greater than \u201c0\u201d indicates that the likelihood of live-speech is greater than the likelihood of non-speech or non-live-speech and \u201c0\u201d is generally used as the threshold for detecting these live-speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech or non-live-speech. VTD plugins may also post-process frame scores to return speech regions, though this is often done on the client-side for flexibility. An excerpt example of what this typically looks like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by adding a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 seconds, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of VTD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a place holder number to retain formatting compatibility with other region-scoring type plugins. Adaptation VTD does not currently support adaptation. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest FrameScorerStereoRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest RegionScorerStereoRequest? This plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, VTD will return results only for those specified regions. Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Speech Intelligibility The VTD plugin detects any and all live speech, whether it is intelligible or not. Live-speech Detection Difficulties It is especially difficult to detect live-speech or differentiate live-speech from pre-recorded or electronic-speaker-produced speech 1) when the microphone is placed very close to distractor sources like TV, radio, etc., 2) when these distractors are played at an unusually high volume or 3) in cases where the microphone is very distant from the source and the signal is weak. Minimum Audio Length A minimum waveform duration of 0.3 seconds is suggested to produce a meaningful live-speech detection. Comments Speech Region Padding When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false live-speech segments. Reduce the threshold value if there are too many missed live-speech segments. 0.0 -4.0 to 4.0","title":"Voice Type Discrimination (VTD)"},{"location":"plugins/vtd-dnn-v7.html#vtd-dnn-v7-voice-type-discrimination","text":"","title":"vtd-dnn-v7 (Voice Type Discrimination)"},{"location":"plugins/vtd-dnn-v7.html#version-changelog","text":"Plugin Version Change v7.0.1 Initial plugin release, this plugin shares a codebase with sad-dnn-v7.0.1, with only the models and parameters configured for live speech detection. Released with OLIVE 5.1.0 v7.0.2 Bug fixes from v7.0.1, released with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/vtd-dnn-v7.html#description","text":"Voice type discrimination (VTD) plugins are designed to detect the presence of speech coming from a live human talker. The goal of VTD is to be able to distinguish not only live-produced, human speech from silence or noise, but also from speech being played over an electronic speaker, such as from a television or phone. When live speech is detected, it is labeled with the timestamps corresponding to its location in the audio. Like SAD, this plug-in may be used either as a frame scorer or region scorer.","title":"Description"},{"location":"plugins/vtd-dnn-v7.html#domains","text":"vtd-v1 Domain designed to detect live speech indoors, differentiating within room live-speech from background distractors like TV, radio, door sound, telephone ringing, traffic etc.","title":"Domains"},{"location":"plugins/vtd-dnn-v7.html#inputs","text":"An audio file or buffer and optional identifier and/or optional regions.","title":"Inputs"},{"location":"plugins/vtd-dnn-v7.html#outputs","text":"The current VTD plugin is capable of performing both frame scoring and region scoring . For frame scoring, typically, a log-likelihood ratio (LLR) score of live-speech vs. non-speech/non-live-speech per each 10ms frame of the input audio segment is output (i.e. 100 audio frames per second). An LLR of greater than \u201c0\u201d indicates that the likelihood of live-speech is greater than the likelihood of non-speech or non-live-speech and \u201c0\u201d is generally used as the threshold for detecting these live-speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech or non-live-speech. VTD plugins may also post-process frame scores to return speech regions, though this is often done on the client-side for flexibility. An excerpt example of what this typically looks like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by adding a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 seconds, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of VTD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a place holder number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/vtd-dnn-v7.html#adaptation","text":"VTD does not currently support adaptation.","title":"Adaptation"},{"location":"plugins/vtd-dnn-v7.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest FrameScorerStereoRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest RegionScorerStereoRequest? This plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, VTD will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/vtd-dnn-v7.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/vtd-dnn-v7.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/vtd-dnn-v7.html#speech-intelligibility","text":"The VTD plugin detects any and all live speech, whether it is intelligible or not.","title":"Speech Intelligibility"},{"location":"plugins/vtd-dnn-v7.html#live-speech-detection-difficulties","text":"It is especially difficult to detect live-speech or differentiate live-speech from pre-recorded or electronic-speaker-produced speech 1) when the microphone is placed very close to distractor sources like TV, radio, etc., 2) when these distractors are played at an unusually high volume or 3) in cases where the microphone is very distant from the source and the signal is weak.","title":"Live-speech Detection Difficulties"},{"location":"plugins/vtd-dnn-v7.html#minimum-audio-length","text":"A minimum waveform duration of 0.3 seconds is suggested to produce a meaningful live-speech detection.","title":"Minimum Audio Length"},{"location":"plugins/vtd-dnn-v7.html#comments","text":"","title":"Comments"},{"location":"plugins/vtd-dnn-v7.html#speech-region-padding","text":"When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD.","title":"Speech Region Padding"},{"location":"plugins/vtd-dnn-v7.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false live-speech segments. Reduce the threshold value if there are too many missed live-speech segments. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/vtd.html","text":"redirect: plugins/vtd-dnn-v7.md","title":"Vtd"},{"location":"plugins/vtd.html#redirect-pluginsvtd-dnn-v7md","text":"","title":"redirect: plugins/vtd-dnn-v7.md"}]}; var search = { index: new Promise(resolve => setTimeout(() => resolve(local_index), 0)) }