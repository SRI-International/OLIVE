const local_index = {"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Open Language Interface for Voice Exploitation (OLIVE) 5.1.0 OLIVE is a suite of audio processing software tools to enable a wide range of audio analysis for several mission types, including large scale audio triage, targeted close forensic analysis, detection of speech, speakers, languages, and keywords of interest. OLIVE 5.1.0 contains the first official release of OLIVE Workflows - a powerful new feature allowing multiple OLIVE plugins to be exercised with a single analysis request, allowing reduced overhead and integration learning curve, as well as additional flexibility compared to manual integration with the OLIVE Enterprise API . OLIVE 5.1.0 also launches the first supported delivery of the Raven Batch Web UI , as part of the OLIVE Multi software package. For more information about the OLIVE 5.1 plugins that are currently released and their capabilities, refer to the OLIVE 5.1 Release Plugins list, that links to more information about each. There are many facets to OLIVE, several of which are documented here. Feel free to navigate through the documentation using the bar along the left side of each page. The links along the right side of each page provide navigation within that particular page. If you're unsure of the best place to look, this may help you choose where to get started: User Centric Documentation Installation - Details for installing the OLIVE software package and getting up and running. OLIVE Martini Docker-based Installation or Standalone Docker-based Installation - Details for installing and setting up the OLIVE software package for docker-container-based deliveries. \"Standalone\" refers to the OLIVE Docker Container alone, while OLIVE Martini is a new delivery package that includes a web-based Raven Batch GUI, and other utilities. Server - For reference for setting up the OLIVE environment and firing up an OLIVE server instance, along with important details about how the Server works with the OLIVE Runtime and the OLIVE Runtime contents. OLIVE Nightingale GUI - Details for launching and interacting with the OLIVE tools through the the optional OLIVE Nightingale GUI. Speaker Redaction GUI Task - Details how to step through the process of using the OLIVE GUI for Speaker Redaction. Command Line Interface Documentation - If you're interested in exploring the capabilities of OLIVE and its plugins as an evaluation tool just using the command line. Supported Audio Formats - Outlines the audio formats OLIVE can currently process and which situations these restrictions apply. Developer Documentation OLIVE Integration Guide - Start here if you're not sure what level of integration is appropriate. Provides links to further information and guidance for getting started with integrating OLIVE capabilities at various levels. Workflow Integration - Details for getting started submitting enrollment and analysis requests using OLIVE workflows, a powerful new tool allowing you to leverage several plugins with a single server request/API message. Workflow integration is recommended for most integrations, unless very specific functionality is required that's not yet supported by workflows. Enterprise API Information - If interested in details about the mechanisms that allow integrating the OLIVE Server and backend audio processing capabilities to an existing system or GUI. Integrating the (Java) Client API - Provides code examples (currently only in Java) for how to perform several OLIVE tasks and integrate the OLIVE Java Reference Implementation to create a new client or augment a client with OLIVE functionality. Creating an API Reference Implementation - Contains information to help guide the creation of a new API Reference Implementation if Java is not the target language for client integration. Enterprise API Message Definitions - Provides more of the low-level details on the available API messages, their structures and contents. Plugins Release Plugins - List of the plugins released with this version of OLIVE, with links to more information on the specific plugins themselves. General Plugin Info - Information regarding the actual technologies and capabilities provided by OLIVE (such as speech activity detection, language identification, etc.), and details on implementing these technologies. OLIVE Plugin Resource Requirements - Information about the processing speed and memory requirements for a selection of OLIVE plugins. Glossary - Definitions of important terms and concepts. Contact Us - Information for how to reach out to the OLIVE team Care was made to provide links to important contextual information regarding important terms, acronyms, API messages, etc. throughout the documentation. If you find any section to be unclear or lacking in important details, please let us know which area(s) require improvement. For any questions, comments, or concerns about this documentation contact olive-support@sri.com .","title":"Overview"},{"location":"index.html#open-language-interface-for-voice-exploitation-olive-510","text":"OLIVE is a suite of audio processing software tools to enable a wide range of audio analysis for several mission types, including large scale audio triage, targeted close forensic analysis, detection of speech, speakers, languages, and keywords of interest. OLIVE 5.1.0 contains the first official release of OLIVE Workflows - a powerful new feature allowing multiple OLIVE plugins to be exercised with a single analysis request, allowing reduced overhead and integration learning curve, as well as additional flexibility compared to manual integration with the OLIVE Enterprise API . OLIVE 5.1.0 also launches the first supported delivery of the Raven Batch Web UI , as part of the OLIVE Multi software package. For more information about the OLIVE 5.1 plugins that are currently released and their capabilities, refer to the OLIVE 5.1 Release Plugins list, that links to more information about each. There are many facets to OLIVE, several of which are documented here. Feel free to navigate through the documentation using the bar along the left side of each page. The links along the right side of each page provide navigation within that particular page. If you're unsure of the best place to look, this may help you choose where to get started: User Centric Documentation Installation - Details for installing the OLIVE software package and getting up and running. OLIVE Martini Docker-based Installation or Standalone Docker-based Installation - Details for installing and setting up the OLIVE software package for docker-container-based deliveries. \"Standalone\" refers to the OLIVE Docker Container alone, while OLIVE Martini is a new delivery package that includes a web-based Raven Batch GUI, and other utilities. Server - For reference for setting up the OLIVE environment and firing up an OLIVE server instance, along with important details about how the Server works with the OLIVE Runtime and the OLIVE Runtime contents. OLIVE Nightingale GUI - Details for launching and interacting with the OLIVE tools through the the optional OLIVE Nightingale GUI. Speaker Redaction GUI Task - Details how to step through the process of using the OLIVE GUI for Speaker Redaction. Command Line Interface Documentation - If you're interested in exploring the capabilities of OLIVE and its plugins as an evaluation tool just using the command line. Supported Audio Formats - Outlines the audio formats OLIVE can currently process and which situations these restrictions apply. Developer Documentation OLIVE Integration Guide - Start here if you're not sure what level of integration is appropriate. Provides links to further information and guidance for getting started with integrating OLIVE capabilities at various levels. Workflow Integration - Details for getting started submitting enrollment and analysis requests using OLIVE workflows, a powerful new tool allowing you to leverage several plugins with a single server request/API message. Workflow integration is recommended for most integrations, unless very specific functionality is required that's not yet supported by workflows. Enterprise API Information - If interested in details about the mechanisms that allow integrating the OLIVE Server and backend audio processing capabilities to an existing system or GUI. Integrating the (Java) Client API - Provides code examples (currently only in Java) for how to perform several OLIVE tasks and integrate the OLIVE Java Reference Implementation to create a new client or augment a client with OLIVE functionality. Creating an API Reference Implementation - Contains information to help guide the creation of a new API Reference Implementation if Java is not the target language for client integration. Enterprise API Message Definitions - Provides more of the low-level details on the available API messages, their structures and contents. Plugins Release Plugins - List of the plugins released with this version of OLIVE, with links to more information on the specific plugins themselves. General Plugin Info - Information regarding the actual technologies and capabilities provided by OLIVE (such as speech activity detection, language identification, etc.), and details on implementing these technologies. OLIVE Plugin Resource Requirements - Information about the processing speed and memory requirements for a selection of OLIVE plugins. Glossary - Definitions of important terms and concepts. Contact Us - Information for how to reach out to the OLIVE team Care was made to provide links to important contextual information regarding important terms, acronyms, API messages, etc. throughout the documentation. If you find any section to be unclear or lacking in important details, please let us know which area(s) require improvement. For any questions, comments, or concerns about this documentation contact olive-support@sri.com .","title":"Open Language Interface for Voice Exploitation (OLIVE) 5.1.0"},{"location":"apiBuildReferenceImp.html","text":"Developing an OLIVE API Reference Implementation If SRI\u2019s Java client library does not meet your needs or you need to create an implementation in a language other than Java, then the following information is helpful for creating a new reference implementation of the OLIVE Enterprise API. Things to know before you start Before you start, it is a good idea to first develop an understanding of the current OLIVE Java Client API. Reading the API Primer page and reviewing the provided Java Client API source files are all good places to start. As an alternative, To enable users to quickly put together client codes, the OLIVE Java API hides many of the low level implementation details such as assemby of request messages, submission of requests over a port, etc. This section describes some of these implementation details you need to consider when creating a brand new OLIVE API. 1. Communicating with the OLIVE Server A messaging system called ZeroMQ , or ZMQ for short, is used as the backbone for communicating with the OLIVE Server. In order to communicate with a running server, you must initialize ZeroMQ (ZMQ) sockets in your client code. There are two sockets: request socket: This is the socket over which you send requests and receive replies, all of which are serialized protobuf messages. You must conect to this socket using the ZMQ.DEALER configuration. status socket: This socket provides a simple heartbeat broadcast from the server. You may optionally monitor this socket to determine the up/down status of the server. Connect to this socket using the ZMQ.PUB configuration. The code for initializing a ZMQ context and creating/connecting the necessary sockets can differ by language. Below is a ZMQ initialization example in Java. ZMQ . Context context = ZMQ . context (); ZMQ . Socket request_socket = context . socket ( ZMQ . DEALER ); ZMQ . Socket status_socket = context . socket ( ZMQ . SUB ); request_socket . connect ( \"tcp://myserver:6678\" ); status_socket . connect ( \"tcp://myserver:6679\" ); status_socket . subscribe ( \"\" ); // Don\u2019t forget this After connecting the sockets, you can begin sending requests and receiving responses over the request port. The specifics of how this operation is performed are described in the next section. 2. Serialization of messages to and from the OLIVE Server Messages are exchanged between the client and server in serialized form, over the request port. Serialization is provided by the Google Protobufs library. You should familiarize yourself with protobufs before beginning your integration. In order to utilize protobufs, you must first take the scenic.proto message definition file (provided in the example code package or available upon request ) and use protoc (or protobuf.net) to automatically generate classes that represent the OLIVE API messages. For more information see the Google Protocol Buffers Documentation . For example for the OLIVE Java Client API, protoc is used to produce the Java file Scenic.java, located in src/main/java/com/sri/scenic/api within the example code package. This file contains the class Scenic which is used in the Java API for all message related classes and definitions. Once you have compiled the OLIVE messages into your code base you can begin your integration. As you may have seen in the OLIVE Java Client API code, (Scenic.java), every OLIVE message is an instance of a class named Envelope . As its name implies, Envelope acts as a container for enclosing messages. Messages are instances of a class named ScenicMessage . An Envelope can contain multiple instances of ScenicMessage , allowing you to batch your communications to the server. Envelope and ScenicMessage are special because they are used for every communication across the request_port and are basically just wrappers. They\u2019re analogous to the envelope and paper when writing someone a letter. The remaining OLIVE messages comprise the actual API requests and responses. The remaining messages each have an entry in the MessageType enum, allowing you to request certain types when retrieving data from database as well as dynamically deserializing data returned by the server. Message Building Example To get started, your integration will probably need to retrieve some information from the server. For example, you may wish to know the list of available plugins. An example pseudocode excerpt accomplishing this is shown below. PluginDirectoryRequest . Builder req = PluginDirectoryRequest . newBuilder () String id = getUUIDString () ScenicMessage msg = ScenicMessage . newBuilder () . setMessageType ( MessageType . PLUGIN_DIRECTORY_REQUEST ) . addMessageData ( req . build (). toByteString ()) . setMessageId ( id ) . build (); Envelope env = Envelope . newBuilder () . setSenderId ( \u201c third - party - integration \u201d ) . addMessage ( msg ). build (); // Now send the message to the server request_socket . send ( env . toByteArray ()); Envelope resp = Envelope . parseFrom ( request_socket . recv ()); for ( ScenicMessage sm : resp . getMessageList ()) { // For purposes of this example, we assume the above message was // the first and only sent so we can assume things about the response // message, namely that it corresponds to our request. assert ( id == msg . getMessageId ()) \u2019 assert ( msg . getMessageType () == MessageType . PLUGIN_DIRECTORY_RESULT ); if ( sm . hasError ()) { System . out . println ( \u201c Dang : \u201c + sm . getError ()); continue ; } PluginDirectoryResult rep = PluginDirectoryResult . parseFrom ( sm . getMessageData ( 0 )); for ( Plugin p : rep . getPluginsList ()){ System . out . println ( p . getId () + \": \" + p . getDesc () ); } } Please note the following: We could have put other requests in the Envelope. Their responses may or may not have come back in the same envelope, but they would have come back in order. We are guaranteed that our messages are received in order by the server and responses sent in order. However, for messages such as scoring requests and class modification requests (enrollment), which are highly asynchronous, there is no guarantee about the order in which they will finish. The sure-fire way to ensure that you process a message from the server correctly is to base your actions on the message id (which you originally assigned in your request). To properly deserialize the data contained within a ScenicMessage , you must check or otherwise be sure of the MessageType . Some OLIVE plugins need to be preloaded by the server in order to fulfill a request. In such cases either the API or the client program must first send a Load Plugin Domain Request to have the targeted plugin preloaded. An Analysis (Scoring) Message Example Now let\u2019s assume we wish to perform language identification on an audio file. We can create a LID like request as follows: // Variable init String plugin = \u201c lid - embed - v2 \u201d String domain = \u201c multi - v1 \u201d String audioFilePath = \u201c / home / user / audio / file1 . wav \u201d // Build audio object Scenic . Audio . Builder audio = Scenic . Audio . newBuilder (). setPath ( audioFilePath . toAbsolutePath (). toString ; // Create LID request // If specifically processing stereo audio files and wish to score // both channels, please use the FrameScorerStereoRequest or // GlobalScorerStereoRequest messages, that will be responded to with // an FrameScoreStereo or GlobalScoreStereo message, containing score results // for both channels of the submitted audio. // If submitting a stereo audio file using the standard xScorerRequest functions, // and you don\u2019t desire to score both channels independently, there are two // options: // - Specify the channel you wish to be scored -> you will receive results // for that channel only. // - Do not specify a channel -> you will receive a single set of results // corresponding to the merged mono representation of the stereo file. Scenic . GlobalScorerRequest . Builder req = Scenic . GlobalScorerRequest . newBuilder () . setAudio ( audio ) . setPlugin ( plugin ) . setDomain ( domain ); Note that this example relies on code written in our Java Client API, but the general steps to perform the task are the same. Now we wrap the request in a ScenicMessage and Envelope like we did in the last example and send it across the request socket. Analysis requests as well as Enrollment requests take significant time to process on the server. It\u2019s likely you\u2019ll want your integration to be doing other things while it is waiting for the response, such as issuing further analysis requests. This is fully supported. However, you don\u2019t know when or in what order the responses to your analyze request will emerge from the server. Therefore, it\u2019s advantageous to track the message ids that you\u2019ve issued in a map of the form message_id -> request message , so that you know the request to which a newly received response pertains. Let\u2019s assume we\u2019ve received a GlobalScorerResult message and have deserialized it into a variable named res . We could process the result as follows: // Currently OLIVE (SCENIC) will only send back one score reply per score // request. Future releases may be able to send back multiple. // Because of this, we must iterate though all of the scores. List < Scenic . GlobalScore > scores = res . getScoreList (); for ( Scenic . GlobalScore gs : scores ){ system . out . println ( \"LID Score: class \" + gs . getClassId () \u201c = \u201c + gs . getScore ()); } For more details regarding the specifications and breadth of the possible requests, their replies, and the structure of each of these data objects, including how results are represented, please refer to the OLIVE API Message Reference documentation. \u2003 Request a new OLIVE API Reference Implementation If there is a need for an OLIVE API in another language, SRI International would be an ideal candidate to undertake the task because of its rich experience having already done it in Java. However, it will still be a complex software engineering undertaking which will definitely take up a significant amount of project resources. For more information about what this would entail, please reach out to olive-support@sri.com.","title":"Building an API Reference Implementation"},{"location":"apiBuildReferenceImp.html#developing-an-olive-api-reference-implementation","text":"If SRI\u2019s Java client library does not meet your needs or you need to create an implementation in a language other than Java, then the following information is helpful for creating a new reference implementation of the OLIVE Enterprise API.","title":"Developing an OLIVE API Reference Implementation"},{"location":"apiBuildReferenceImp.html#things-to-know-before-you-start","text":"Before you start, it is a good idea to first develop an understanding of the current OLIVE Java Client API. Reading the API Primer page and reviewing the provided Java Client API source files are all good places to start. As an alternative, To enable users to quickly put together client codes, the OLIVE Java API hides many of the low level implementation details such as assemby of request messages, submission of requests over a port, etc. This section describes some of these implementation details you need to consider when creating a brand new OLIVE API.","title":"Things to know before you start"},{"location":"apiBuildReferenceImp.html#1-communicating-with-the-olive-server","text":"A messaging system called ZeroMQ , or ZMQ for short, is used as the backbone for communicating with the OLIVE Server. In order to communicate with a running server, you must initialize ZeroMQ (ZMQ) sockets in your client code. There are two sockets: request socket: This is the socket over which you send requests and receive replies, all of which are serialized protobuf messages. You must conect to this socket using the ZMQ.DEALER configuration. status socket: This socket provides a simple heartbeat broadcast from the server. You may optionally monitor this socket to determine the up/down status of the server. Connect to this socket using the ZMQ.PUB configuration. The code for initializing a ZMQ context and creating/connecting the necessary sockets can differ by language. Below is a ZMQ initialization example in Java. ZMQ . Context context = ZMQ . context (); ZMQ . Socket request_socket = context . socket ( ZMQ . DEALER ); ZMQ . Socket status_socket = context . socket ( ZMQ . SUB ); request_socket . connect ( \"tcp://myserver:6678\" ); status_socket . connect ( \"tcp://myserver:6679\" ); status_socket . subscribe ( \"\" ); // Don\u2019t forget this After connecting the sockets, you can begin sending requests and receiving responses over the request port. The specifics of how this operation is performed are described in the next section.","title":"1. Communicating with the OLIVE Server"},{"location":"apiBuildReferenceImp.html#2-serialization-of-messages-to-and-from-the-olive-server","text":"Messages are exchanged between the client and server in serialized form, over the request port. Serialization is provided by the Google Protobufs library. You should familiarize yourself with protobufs before beginning your integration. In order to utilize protobufs, you must first take the scenic.proto message definition file (provided in the example code package or available upon request ) and use protoc (or protobuf.net) to automatically generate classes that represent the OLIVE API messages. For more information see the Google Protocol Buffers Documentation . For example for the OLIVE Java Client API, protoc is used to produce the Java file Scenic.java, located in src/main/java/com/sri/scenic/api within the example code package. This file contains the class Scenic which is used in the Java API for all message related classes and definitions. Once you have compiled the OLIVE messages into your code base you can begin your integration. As you may have seen in the OLIVE Java Client API code, (Scenic.java), every OLIVE message is an instance of a class named Envelope . As its name implies, Envelope acts as a container for enclosing messages. Messages are instances of a class named ScenicMessage . An Envelope can contain multiple instances of ScenicMessage , allowing you to batch your communications to the server. Envelope and ScenicMessage are special because they are used for every communication across the request_port and are basically just wrappers. They\u2019re analogous to the envelope and paper when writing someone a letter. The remaining OLIVE messages comprise the actual API requests and responses. The remaining messages each have an entry in the MessageType enum, allowing you to request certain types when retrieving data from database as well as dynamically deserializing data returned by the server.","title":"2.  Serialization of messages to and from the OLIVE Server"},{"location":"apiBuildReferenceImp.html#message-building-example","text":"To get started, your integration will probably need to retrieve some information from the server. For example, you may wish to know the list of available plugins. An example pseudocode excerpt accomplishing this is shown below. PluginDirectoryRequest . Builder req = PluginDirectoryRequest . newBuilder () String id = getUUIDString () ScenicMessage msg = ScenicMessage . newBuilder () . setMessageType ( MessageType . PLUGIN_DIRECTORY_REQUEST ) . addMessageData ( req . build (). toByteString ()) . setMessageId ( id ) . build (); Envelope env = Envelope . newBuilder () . setSenderId ( \u201c third - party - integration \u201d ) . addMessage ( msg ). build (); // Now send the message to the server request_socket . send ( env . toByteArray ()); Envelope resp = Envelope . parseFrom ( request_socket . recv ()); for ( ScenicMessage sm : resp . getMessageList ()) { // For purposes of this example, we assume the above message was // the first and only sent so we can assume things about the response // message, namely that it corresponds to our request. assert ( id == msg . getMessageId ()) \u2019 assert ( msg . getMessageType () == MessageType . PLUGIN_DIRECTORY_RESULT ); if ( sm . hasError ()) { System . out . println ( \u201c Dang : \u201c + sm . getError ()); continue ; } PluginDirectoryResult rep = PluginDirectoryResult . parseFrom ( sm . getMessageData ( 0 )); for ( Plugin p : rep . getPluginsList ()){ System . out . println ( p . getId () + \": \" + p . getDesc () ); } } Please note the following: We could have put other requests in the Envelope. Their responses may or may not have come back in the same envelope, but they would have come back in order. We are guaranteed that our messages are received in order by the server and responses sent in order. However, for messages such as scoring requests and class modification requests (enrollment), which are highly asynchronous, there is no guarantee about the order in which they will finish. The sure-fire way to ensure that you process a message from the server correctly is to base your actions on the message id (which you originally assigned in your request). To properly deserialize the data contained within a ScenicMessage , you must check or otherwise be sure of the MessageType . Some OLIVE plugins need to be preloaded by the server in order to fulfill a request. In such cases either the API or the client program must first send a Load Plugin Domain Request to have the targeted plugin preloaded.","title":"Message Building Example"},{"location":"apiBuildReferenceImp.html#an-analysis-scoring-message-example","text":"Now let\u2019s assume we wish to perform language identification on an audio file. We can create a LID like request as follows: // Variable init String plugin = \u201c lid - embed - v2 \u201d String domain = \u201c multi - v1 \u201d String audioFilePath = \u201c / home / user / audio / file1 . wav \u201d // Build audio object Scenic . Audio . Builder audio = Scenic . Audio . newBuilder (). setPath ( audioFilePath . toAbsolutePath (). toString ; // Create LID request // If specifically processing stereo audio files and wish to score // both channels, please use the FrameScorerStereoRequest or // GlobalScorerStereoRequest messages, that will be responded to with // an FrameScoreStereo or GlobalScoreStereo message, containing score results // for both channels of the submitted audio. // If submitting a stereo audio file using the standard xScorerRequest functions, // and you don\u2019t desire to score both channels independently, there are two // options: // - Specify the channel you wish to be scored -> you will receive results // for that channel only. // - Do not specify a channel -> you will receive a single set of results // corresponding to the merged mono representation of the stereo file. Scenic . GlobalScorerRequest . Builder req = Scenic . GlobalScorerRequest . newBuilder () . setAudio ( audio ) . setPlugin ( plugin ) . setDomain ( domain ); Note that this example relies on code written in our Java Client API, but the general steps to perform the task are the same. Now we wrap the request in a ScenicMessage and Envelope like we did in the last example and send it across the request socket. Analysis requests as well as Enrollment requests take significant time to process on the server. It\u2019s likely you\u2019ll want your integration to be doing other things while it is waiting for the response, such as issuing further analysis requests. This is fully supported. However, you don\u2019t know when or in what order the responses to your analyze request will emerge from the server. Therefore, it\u2019s advantageous to track the message ids that you\u2019ve issued in a map of the form message_id -> request message , so that you know the request to which a newly received response pertains. Let\u2019s assume we\u2019ve received a GlobalScorerResult message and have deserialized it into a variable named res . We could process the result as follows: // Currently OLIVE (SCENIC) will only send back one score reply per score // request. Future releases may be able to send back multiple. // Because of this, we must iterate though all of the scores. List < Scenic . GlobalScore > scores = res . getScoreList (); for ( Scenic . GlobalScore gs : scores ){ system . out . println ( \"LID Score: class \" + gs . getClassId () \u201c = \u201c + gs . getScore ()); } For more details regarding the specifications and breadth of the possible requests, their replies, and the structure of each of these data objects, including how results are represented, please refer to the OLIVE API Message Reference documentation.","title":"An Analysis (Scoring) Message Example"},{"location":"apiBuildReferenceImp.html#request-a-new-olive-api-reference-implementation","text":"If there is a need for an OLIVE API in another language, SRI International would be an ideal candidate to undertake the task because of its rich experience having already done it in Java. However, it will still be a complex software engineering undertaking which will definitely take up a significant amount of project resources. For more information about what this would entail, please reach out to olive-support@sri.com.","title":"Request a new OLIVE API Reference Implementation"},{"location":"apiCode.html","text":"Integrating a Client API with OLIVE Java Client IDE Setup Guide If you would like to import the provided OLIVE Java API sample code and reference implementation into an IDE to explore the code and/or get started with integrating this code, please refer over to the Java Client IDE Setup Guide page. If you've already done this step, plan on integrating this functionality just using the provided JAR file, or just wish to browse code samples, continue below. The OLIVE Java API While the OLIVE system allows client integration via Protobuf messages sent from a variety of languages such as Java, Python, C++ and C#, and machine types such as Windows, Linux, and MacOS, the currently available OLIVE reference API implementation is Java-based only, and is also referred to as the OLIVE Java Client API. Therefore all instructions and code examples presented in this section currently assume that client programs are also in Java. Pseudocode and Python versions of the included code will be rolled into this page as they are available. Fundamentally any OLIVE API implementation is based on exchanging Protobuf messages between the client and server. These messages are defined in the API Message Reference Documentation. If you would like to create a reference implementation of the OLIVE API in another language, please refer to the information in the Developing an OLIVE API Reference Implementation page, that should prove to be helpful. If you would like an API reference implementation in another language to be created, please reach out to olive-support@sri.com to discuss your needs with the team. Basic Recipe for using the OLIVE Java Client API This section covers the first steps towards building a client based on implementing SRI's OLIVE API Java Reference implementation in order to use the OLIVE Java API for speech processing. The sections immediately following cover setting up an OLIVE server instance to connect to, then move into a number of steps necessary to connect a client to this server, and finally cover how to build and submit scoring and enrollment requests from the client program. The first step towards integrating a client with the provided OLIVE Java Reference Implementation is to review the API Message Reference to understand the available request and response messages, then dive in by setting up your IDE to work with this code. We have created a step-by-step guide for this configuration and setup process. Note that these steps are not necessary for implementing the actual functionality from this software package, but could be very useful for exploring and learning the code. If only the functionality from this reference implementation is desired, SRI can provide an appropriate JAR file to include in your project. OLIVE server As mentioned above, the OLIVE Enterprise API operates on a client/server model, making the OLIVE server an essential part of the OLIVE system. You must run the OLIVE server and manage its lifecycle as part of your integration effort. For more information about the server's duties, how to interact with it, what options and functionality are available to it, and general setup and troubleshooting information, please refer to the OLIVE Server Information Page . Establish Server Connection Before making any task request, a client must establish a connection with the server. By default, the OLIVE server listens on ports 5588 (request port) and 5589 (status port) for client connection and status requests. These ports are configurable, but if the server has not been instructed to change its listening ports, the code below should establish a connection. Using the OLIVE Java API, connection to the server can be made with a single call, as shown below. This call is available from the API Server class , included in the package com.sri.scenic.api.Server. Java Server server = new Server (); server . connect ( \"scenic-client\" , //client-id \"localhost\" , //address of server 5588 , //request-port 5589 , //status-port 10000 //timeout for failed connection request ); Python --- Coming Soon --- Psudocode connect ( server - host - name , 5588 , 5589 ) The request port (5588 by default) is used for call and response messages (Protobuf messages). Each request message sent to this port is guaranteed a response from the server (this is why the messages in the API Message Reference are often suffixed with 'Request' and 'Result'). There is no need to poll the server for information on a submitted request, as the result/response for the a request is returned to the client as soon as it is available. The status port (5589 by default) is used by the Server to publish health and status messages (Heartbeat) to client(s). Clients can not send requests on this port. Request Available Plugins In order to submit most server requests, the client must specify the plugin and domain to handle the request. To obtain the handle of a targeted plugin, the client first requests a list of all currently available valid plugins from the server. From the returned plugins list, the client looks up the specific plugin handle by the plugin's unique name (id) and its associated trait for the task to be performed. This handle can then be used in a future request message sent to the server. In the Java API Reference Implementation, this is accomplished by the utilities requestPlugin() and findPluginDomainByTrait(), within the included ClientUtils package. The steps are shown below. Java // ask the server for a list of currently available plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // Look up a specific plugin from the plugin list using // the plugin's unique name (pluginName and domainName) and associated trait e.g. FRAME_SCORER Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Scenic . TraitType . FRAME_SCORER , pluginList ); Python def request_plugins ( self ): request = PluginDirectoryRequest () # Wrap message in an Envelop request = self . _wrap_message ( request , FRAME_SCORER_REQUEST ) # Now send the message logging . debug ( \"Sending a Plugin request message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message...\" ) envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): server_msg = envelope . message [ i ] if broker_msg . HasField ( \"error\" ): print ( server_msg . error ) else : plugin_dir_msg = PluginDirectoryResult () plugin_dir_msg . ParseFromString ( server_msg . message_data [ 0 ]) # There is likely to be multiple plugin/domains print ( \"Server has {} plugins \" . format ( len ( plugin_dir_msg . plugins ))) for p in plugin_dir_msg . plugins : print ( \"Plugin {} has {} domains\" . format ( p . id , len ( p . domain ))) Pseudocode plugin - list = getListOfAvailablePlugins ( server ); targeted - plugin = lookupPlugin ( plugin - list , pluginName , pluginTrait ); The targeted plugin handle, pd in the above example, can then be used with other utilities within the Client API to submit requests, or to otherwise interact with the plugin. For example, this code below shows how one might check what Traits that each plugin in the pluginList returned above support: Java for ( Pair < Scenic . Plugin , Scenic . Domain > pair : pluginList ){ for ( Scenic . Trait t : pair . getFirst (). getTraitList ()) { if ( t . getType () == Scenic . TraitType . FRAME_SCORER ){ log . info ( \"Plugin {} supports frame scoring\" , pair . getFirst (). getId ()); } if ( t . getType () == Scenic . TraitType . GLOBAL_SCORER ){ // Supports global scoring (i.e. SID or LID) log . info ( \"Plugin {} supports global scoring\" , pair . getFirst (). getId ()); } if ( t . getType () == Scenic . TraitType . REGION_SCORER ){ // Supports region scoring (i.e. KWS) log . info ( \"Plugin {} supports region scoring\" , pair . getFirst (). getId ()); } } } Python -- coming soon -- Pseudocode --coming soon-- Audio Submission Guidelines One of the core client activities is submitting Audio with a request. In the OLIVE API, three ways are provided for a client to package audio data to send to the OLIVE server: file path buffer of raw audio sample data serialized file buffer object The enum AudioTransferType is used to specify they type of audio transfer to use. When the client and the OLIVE server share the same file system, the easiest way for the client to send audio data to the server is by specifying the audio's file path on disk. The OLIVE Client API provides the utility below to package audio files which are accessible to the server locally: Java packageAudioAsPath () // AudioTransferType.SEND_AS_PATH When the client and the server don't share the same file system, as in the case of a client making a remote connection to the OLIVE server, it is necessary to send the client's local audio files as a file buffer. To help package the client's audio data in a raw buffer, the OLIVE Client API provides the utility below: Java packageAudioAsRawBuffer () // AudioTransferType.SEND_SAMPLES_BUFFER When submitting audio to the server as a buffer of raw samples, it is important to include information characterizing the audio, such as the bit depth, audio encoding, sample rate, and number of channels, to ensure the server knows how to properly treat the buffer it receives. A third utility, shown below, also packages the client's audio data in a buffer. This utility passes the original file to the server in its entirety in one contiguous buffer, leaving the audio file header intact. Java packageAudioAsSerializedBuffer () // AudioTransferType.SEND_SERIALIZED_BUFFER Sending audio data as a serialized file buffer ensures that all audio header information is provided intact to the server. This allows the server to properly decode and process the audio once its received, since it can directly access the bit depth, encoding type, sample rate and other necessary information from the header itself. The tradeoff with serialized files is that there may be additional overhead needed to process the audio into a consumable form. If the client and server reside on the same hardware and file system, it is advisable to simply pass filepaths when possible. This saves the memory overhead burden of both the client and server loading audio into memory. If using common audio types, like 16-bit PCM .wav files, it may also be possible to simply pass a non-serialized file buffer. The OLIVE Java Client API provides utilities such as requestFrameScore(), requestEnrollClass(), etc. to handle various client message requests that were covered above. In these utilities, the enum argument transferType is used to select in what way the audio data is to be sent. For example, when transferType is set to AudioTransferType.SEND_SERIALIZED_BUFFER , audio data will be interpreted as if it were sent as a serialized buffer. Synchronous vs. Asynchronous Message Submission The OLIVE Client API allows the client to choose between processing a task request synchronously or asynchronously. Processing a task request synchronously means the client will block and wait for the task result to return before proceeding to other task requests. On the other hand, asynchronous processing means the client will not wait for the result to come back before moving on, allowing several jobs to be submitted in parallel. The examples below generally show submitting requests asynchronously. The argument async in the API utilities requestFrameRequest(), requestEnrollClass(), etc., can be used to select if the client intends to wait for the task result to return. When async is set to true , the client will not block when a request is sent to the server, so other task requests can be made before the results are received asynchronously and handled by the callback. Construct Request Message Information contained in a task request message may be different depending on the type of task to be performed. For a scoring task, the request message usually contains specific plugin and domain names, and the audio to be scored. For an adaptation or enrollment task, it also contains class ID information. To make a task request, the client program must first assemble the necessary information into a request message, and then send the message to the server. These 2 steps are shown in pseudo code below. Pseudocode request = packageRequest ( pluginName , pluginDomain , requestType , any other information specific to requestType ) sendRequest ( server , request , audio ) Client API Code Samples The OLIVE Reference API includes functionality pre-coded to accommodate many of the available request messages. Utilities such as requestFrameScore(), requestEnrollClass(), etc. not only do the packaging of request messages, but also take care of sending the request messages to the server, all in one call. They are available from the SRI Java API package sri.com.scenic.api.ClientUtils. The required parameters to send requests using these scoring utilities include: server handle (server - see here ) the plugin handle (pd - see here ) the name of the audio file to submit to the server (filename) channel number of the audio to be processed when audio has more than 1 channel (channelNumber) a callback function for handling results returned either asynchronously or synchronously (rc) whether the client will block for task result to return (async - see here ) an enum of how to submit audio to the server (transferType) optional lists of annotations of the submitted audio (regions) optional list of parameters for customizing plugin behavior (options) optional list of class IDs for filtering the results (classIDs) Performing an enrollment request adds an additional parameter: the ID of the class to be enrolled The primary requests covered below are: Frame score requests - used to make most SAD requests (note that some SAD plugins return regions, not frame scores) Global score requests - used to make LID, SID, or Gender score requests Region score requests - used to make QbE, KWS, Diarization, and sometimes SAD score requests Enrollment requests - used to enroll speakers or other class types for plugins that support the ClassEnroller trait Frame Score Request The example below provides sample code for a function MyFrameScoreRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Frame scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. Note that only a plugin that support the FrameScorer trait, cna handle this request. All SAD plugins support this train, while some also support the RegionScorer trait. For an example of how to call this code with a specific plugin, refer to the SAD Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyFrameScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pp , filename , 1 , rc , true , true , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } Python --- Coming Soon --- Pseudocode -- Coming Soon -- Global Score Request The example below provides sample code for a function MyGlobalScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Global scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. Note also that the code required to submit a GlobalScorerRequest message doesn't care what type of plugin is going to be doing the scoring, as long as the plugin implements the GlobalScorer Trait. This means that the exact same code can be used for submitting audio to global scoring SID plugins, LID plugins, or any other global scoring plugin. For an example of how to call this code with a specific plugin, refer to the SID Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyGlobalScorerRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a call back to handle the global scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ System . out . println ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { System . out . println ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // make the global scoring reqeust return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , true , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } Python def request_lid ( self , plugin , domain , filename , classes ): ''' Request a LID analysis of 'filename' :param plugin: the name of the LID plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param languages: optional list of languages trigraphs used to filter (restrict) results returned from the server. This list should only include languages supported by the specified plugin :return: the LID analysis as a list of (global) scores ''' request = GlobalScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio # send the name of the file to the server: audio . path = filename # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) request = self . _wrap_message ( request , GLOBAL_SCORER_REQUEST ) # Now send the message logging . debug ( \"Sending a LID (global score request) message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): broker_msg = envelope . message [ i ] if broker_msg . HasField ( \"error\" ): print broker_msg . error else : global_score_msg = GlobalScorerResult () global_score_msg . ParseFromString ( broker_msg . message_data [ 0 ]) # Assume there is only one result set (for 'speech'): frame_score_msg.result[0] print ( \"Received {} global scores\" . format ( len ( global_score_msg . result [ 0 ] . score ))) return global_score_msg . result [ 0 ] return None Pseudocode --- Coming Soon --- Region Score Request The example below provides sample code for a function MyRegionScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Region scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. For an example of how to call this code with a specific plugin, refer to the KWS Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyRegionScorerRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a call back to handle the global scoring request Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // scoring result received if ( ! r . hasError ()){ log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"SDD Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // make the region scoring reqeust return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } Python --- Coming soon --- Pseudocode -- Coming soon -- Enrollment Request The example below provides sample code for a function MyEnrollmentRequest that takes a server connection, a plugin/domain handle, the name of the class (speaker) to enroll, and a path to an audio file as arguments, and uses this information to build and submit a Region Scoring request to the connected server. Examples below show an enrollment request with one file, followed by an enrollment with multiple files (for one class). Enrollment Request with a single audio file/enrollment. Java public static boolean MyEnrollmentRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String classID , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it synchronous so we know enrollment is complete boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , classID , enrollmentFileName , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); /** can do something else now, enrollment is complete **/ return true ; } Python --- Coming soon --- Pseudocode --- Coming soon --- Batch enrollment request with multiple files. Java public static boolean MyEnrollmentUsingMultipleFilesRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String classID , ArrayList < String > audioFiles , int enrollmentFileCount ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****server processed 1 enrollment file*****\" ); responseCount ++ ; } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; for ( String audioFilename : audioFiles ) { // For simplicity, enrollments requests are done synchronously boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , classID , audioFilename , 0 , enrollmentCallback , false , AudioTransferType . SEND_AS_FILE , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out enrollment file processed } } // wait for all enrollment files to be processed while ( responseCount != enrollmentFileCount ) { try { TimeUnit . SECONDS . sleep ( 10 ); } catch ( InterruptedException e ) { System . out . println ( \"*****Timeout Exception*****\" ); } } return true ; // all enrollment files are processed, now we can proceed to do other things if necessary } Python --- Coming soon --- Pseudocode --- Coming soon --- The function call definitions for some of these scoring and enrollment request utilities follows: Java boolean requestFrameScore ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename , int channelNumber , Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a frame scoring request to the OLIVE server, e.g. SAD scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestGlobalScore ( Server server , Pair < Scenic . Plugin , Scenic . Domain > plugin , Scenic . TraitType trait , String filename , int channelNumber , Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a global scoring request to the OLIVE server, e.g. LID scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestRegionScores ( Server server , Pair < Scenic . Plugin , Scenic . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a region scoring request to the OLIVE server, e.g. KWS scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestEnrollClass ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String id , String wavePath , int channelNumber , Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options ) This call submits an enrollment request to the OLIVE server, e.g. enrolling speakers in a SID plugin. A class ID (id) is required for enrollment. If selected regions of an audio are used for enrollment, they are passed along in a list (regions). To see examples of how these utilities are used, consult sample client code provided in the next section. Plugin Specific Code Examples This section shows examples of using the functions just outlined to make calls to specific plugins, and demonstrate how the same code can be reused for several purposes - for example, requestGlobalScore is valid to request scoring from both SID and LID plugins. All client examples below can be found in the OLIVE-API-examples tree in the src/main/java/com/sri/scenic/api/client folder. SAD Scoring Example SID Enrollment and Scoring Example LID Enrollment and Scoring Example (synchronized approach) LID Enrollment and Scoring Example (asynchronized approach) KWS Scoring Example SDD Enrollment and Scoring Example SAD Supervised Adaptation Example TPD Enrollment and Scoring Example SAD Scoring Request This shows a full implementation of a client program which sends a frame scoring request to a SAD plugin. Upon return of the result, it outputs the received frame scores. Included also is a second version where a threshold is used to filter out frame scores which are higher. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.ArrayBlockingQueue ; import java.util.concurrent.BlockingQueue ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MySADFrameScorer { private static Logger log = LoggerFactory . getLogger ( MySADFrameScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"tel-v1\" ; private static String pluginName = \"sad-dnn-v6_rc5\" ; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // from the list of plugins, find the targeted plugin for the task Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Scenic . TraitType . FRAME_SCORER , pluginList ); // formulate SAD frame scoring request and send to server MyFrameScoreRequest ( server , pd , audioFileName ); } public static boolean MyFrameScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pp , filename , 1 , rc , true , true , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } } The code above outputs all frame scores of the input audio, this can generate massive amount of output, especially when the audio is long. One good way to trim down the output is to filter out regions with frame scores higher than a preset threshold value. The following shows how this can be done in the @Override section of the callback routine, using a threshold of 0.0. Java // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); // filter to output speech regions with scores > 0.0 int speech_start = 0 ; int speech_end = 0 ; double sum_scores = 0.0 ; int num_frames = 0 ; for ( int i = 0 ; i < scores . length ; i ++ ) { if ( scores [ i ] > 0.0 ) { if ( speech_end == speech_start ) { speech_start = i ; } speech_end = i + 1 ; sum_scores = sum_scores + scores [ i ] ; num_frames = num_frames + 1 ; } else { if ( speech_end > speech_start ) { int start = ( int ) ( 100 * speech_start / ( double ) rate ); int end = ( int ) ( 100 * speech_end / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , sum_scores / num_frames )); } speech_start = i ; speech_end = i ; sum_scores = 0.0 ; num_frames = 0 ; } } } } System . exit ( 0 ); } }; SID Enrollment and Scoring Request This example is a full implementation of a client program which sends an enrollment request to a SID plugin, followed by a scoring request to the same SID plugin. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.atomic.AtomicInteger ; public class MySIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multicond-v1\" ; private static String pluginName = \"sid-embed-v5b\" ; private static String enrollmentFileName ; private static String speakerName = \"Mr.X\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static String outputDirName = \"./\" ;; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain SID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform SID enrollment task MySIDEnrollmentAndScoreRequest ( server , pd , speakerName , enrollmentFileName , scoreWaveFileName ); } public static boolean MySIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String enrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it a synchronized call, so we know the speaker is enrolled before we make the score request boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); if ( enrolled ){ // Create a call back to handle the SID scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ System . out . println ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { System . out . println ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // SID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , AudioTransferType . SEND_SERIALIZED_BUFFER , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } } LID Enrollment and Scoring Request Synchronized Approach A LID enrollment of a new language may involve multiple enrollment files. The synchronized version of this client waits for the server to complete the enrollment request with each enrollment file sequentially, before requesting LID scores on an input file. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MysyncLIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MysyncLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v1-rc6\" ; private static String audioList ; private static ArrayList < String > audioFiles = new ArrayList < String > (); private static String audioFilename ; private static int enrollmentFileCount = 0 ; private static String languageName = \"JKL\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String audioList = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // read enrollment files from input list if ( ! Files . exists ( Paths . get ( audioList ). toAbsolutePath ())) { System . err . println ( \"ERROR: '\" + audioList + \"' does not exist\" ); } BufferedReader br = new BufferedReader ( new FileReader ( audioList )); while (( audioFilename = br . readLine ()) != null ){ audioFiles . add ( audioFilename ); } enrollmentFileCount = audioFiles . size (); // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain LID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform LID enrollment task MyLIDEnrollmentAndScoreRequest ( server , pd , languageName , audioFiles , enrollmentFileCount , scoreWaveFileName ); } public static boolean MyLIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String languageName , ArrayList < String > audioFiles , int enrollmentFileCount , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****Enrollment Succeeded*****\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment requests, with one enrollment file at a time // make a synchronized enrollment call, so we know the language is enrolled before we make the score request for ( String audioFilename : audioFiles ) { boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , languageName , audioFilename , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out name of enrollment file used } } // do the score request // First create a call back to handle the LID scoring result Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ log . info ( \"\\t{} = {}\" , gs . getClassId (), gs . getScore ()); } } else { log . error ( \"Global scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } } Asynchronized Approach This example provides an alternate LID client example that performs asynchronized enrollment and does not wait for the last enrollment request to complete before making another. Instead, it keeps count on server responses to enrollment requests made, and makes sure that all enrollment requests are completed before making a LID scoring request on an input file. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MyasyncLIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MyasyncLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v1-rc6\" ; private static String audioList ; private static ArrayList < String > audioFiles = new ArrayList < String > (); private static String audioFilename ; private static int enrollmentFileCount = 0 ; private static int responseCount = 0 ; private static String languageName = \"MNO\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static String outputDirName = \"./\" ;; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment list is passed in as an argument String audioList = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // read enrollment files from input list and keep count of enrollment files to be processed if ( ! Files . exists ( Paths . get ( audioList ). toAbsolutePath ())) { System . err . println ( \"ERROR: '\" + audioList + \"' does not exist\" ); } BufferedReader br = new BufferedReader ( new FileReader ( audioList )); while (( audioFilename = br . readLine ()) != null ){ audioFiles . add ( audioFilename ); } enrollmentFileCount = audioFiles . size (); // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain LID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform LID enrollment task MyLIDEnrollmentAndScoreRequest ( server , pd , languageName , audioFiles , enrollmentFileCount , scoreWaveFileName ); } public static boolean MyLIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String languageName , ArrayList < String > audioFiles , int enrollmentFileCount , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****server processed 1 enrollment file*****\" ); responseCount ++ ; } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; for ( String audioFilename : audioFiles ) { boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , languageName , audioFilename , 0 , enrollmentCallback , true , true , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out enrollment file processed } } // wait for all enrollments to complete while ( responseCount != enrollmentFileCount ) { try { TimeUnit . SECONDS . sleep ( 10 ); } catch ( InterruptedException e ) { System . out . println ( \"*****Timeout Exception*****\" ); } } // We next do the score request // First create a call back to handle the LID scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ log . info ( \"\\t{} = {}\" , gs . getClassId (), gs . getScore ()); } } else { log . error ( \"Global scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , true , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } } KWS Scoring Request This portrays a full implementation of a KWS scoring request, against the list of default keywords already trained. Note that unlike the previous examples that requested GlobalScores, this client issues a RegionScoreRequest because KWS is a \"RegionScorer\". Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MyKWSRegionScorer { private static Logger log = LoggerFactory . getLogger ( MyKWSRegionScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"r-tdnn-tel-v1\" ; private static String pluginName = \"kws-dynapy-v1\" ; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // formulate the frame scoring task request Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"KWS\" , Scenic . TraitType . REGION_SCORER , pluginList ); // Perform KWS frame scoring task MyRegionScoresRequest ( server , pd , audioFileName ); } public static boolean MyRegionScoresRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle KWS results from the server Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > rc = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; return ClientUtils . requestRegionScores ( server , pp , filename , 0 , rc , true , false , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } } SDD Enrollment And Scoring Request The following code shows a full implementation of a speaker enrollment request, followed by a scoring request made to a SDD plugin. The scoring request in this client is a RegionScorerRequest because SDD plugins are region scorers. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MySDDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySDDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multicond-v1\" ; private static String pluginName = \"sdd-sbc-embed-v1-1\" ; private static String enrollmentFileName ; private static String speakerName = \"Mr.X\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain SDD plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform SDD enrollment task MySDDEnrollmentAndScoreRequest ( server , pd , speakerName , enrollmentFileName , scoreWaveFileName ); } public static boolean MySDDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String enrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // enrollment result received if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make a synchronized enrollment request, so we know the speaker is enrolled before we make the score request boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); if ( enrolled ){ // do the SDD scoring request // First create a callback to handle scoring result from server Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // scoring result received if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"SDD Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // SDD is a region scorer, so make a region score reqeust: return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } SAD Supervised Adaptation Request Below is a full implementation of a SAD request to adapt a new domain from an existing domain. The list of adaptation training files is passed into the client as a file. The code handles both supervised (speech regions specified) and unsupervised (speech regions not specified) SAD adaptations. However, some SAD plugins may not have the unsupervised adaptation capability, in which case the client will exit with a failure message. Java package com.sri.scenic.api.client ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Scenic.AnnotationRegion ; import com.sri.scenic.api.Scenic.AudioAnnotation ; import com.sri.scenic.api.Scenic.Domain ; import com.sri.scenic.api.Scenic.Plugin ; import com.sri.scenic.api.Scenic.TraitType ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.ClientUtils ; import com.sri.scenic.api.utils.LearningParser ; import com.sri.scenic.api.utils.LearningParser.LearningDataType ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Paths ; import java.util.ArrayList ; import java.util.Collection ; import java.util.HashMap ; import java.util.List ; import java.util.Map ; import java.util.UUID ; import javax.sound.sampled.UnsupportedAudioFileException ; import org.apache.commons.cli.CommandLine ; import org.apache.commons.cli.CommandLineParser ; import org.apache.commons.cli.DefaultParser ; import org.apache.commons.cli.HelpFormatter ; import org.apache.commons.cli.Option ; import org.apache.commons.cli.Options ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; public class MySADAdaptation { private static Logger log = LoggerFactory . getLogger ( MySADAdaptation . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"tel-v1\" ; private static String newDomainName = \"new-tel-v1\" ; private static String pluginName = \"sad-dnn-v6_rc5\" ; private static LearningParser learningParser = new LearningParser (); private static LearningDataType dataType ; /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file list is passed as an argument String audioFileList = args [ 0 ] ; learningParser . parse ( audioFileList ); if ( ! learningParser . isValid ()) { System . err . println ( \"Invalid input file: \" + audioFileList ); System . exit ( - 1 ); } if ( learningParser . hasRegions ()) { dataType = LearningDataType . SUPERVISED_WITH_REGIONS ; } else if ( learningParser . hasClasses ()) { dataType = LearningDataType . SUPERVISED ; } else { dataType = LearningDataType . UNSUPERVISED ; } // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // find plugin handle for adaptation task Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , learningParser . isUnsupervised () ? Scenic . TraitType . UNSUPERVISED_ADAPTER : Scenic . TraitType . SUPERVISED_ADAPTER , pluginList ); // Preproces audio - doesn't matter if supervised or unsupervised String adaptID = UUID . randomUUID (). toString (); Plugin p = pd . getFirst (); Domain d = pd . getSecond (); // optional annotations, generated if found in the parser (supervised adaptation) Map < String , List < AudioAnnotation >> annotations = new HashMap <> (); // annotations -> <classID> ->* <AudioAnnotations>, annotations will be empty for unsupervised adaptation int numPreprocessed = OliveLearn . preprocessAllAudio ( server , p , d , learningParser , adaptID , annotations ); if ( ! learningParser . isUnsupervised ()) { // supervised adaptation if ( numPreprocessed > 0 ) { OliveLearn . finalizeSupervisedAdaptation ( server , p , d , adaptID , newDomainName , annotations ); } else { System . err . println ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } else { // unsupervised adaptation if ( numPreprocessed > 0 ) { OliveLearn . finalizeUnsupervisedAdaptation ( server , p , d , adaptID , newDomainName ); } else { System . err . println ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } System . out . println ( \"\" ); System . out . println ( \"Learning finished. Exiting...\" ); System . exit ( 0 ); } } TPD Enrollment and Scoring Request This final example shows a full implementation of a TPD topic enrollment request using positive examples from selected regions of an audio, followed by a scoring request of the enrolled topic on another audio. Enrollment using negative audio samples are also included in this example as commented out lines. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.atomic.AtomicInteger ; public class MyTPDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MyTPDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"r-tdnn-tel-v1\" ; private static String pluginName = \"tpd-dynapy-v1\" ; private static String posEnrollmentFileName ; private static String negEnrollmentFileName ; private static String topicName = \"hello\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); //private static String outputDirName = \"./\";; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String posEnrollmentFileName = args [ 0 ] ; String negEnrollmentFileName = args [ 1 ] ; String scoreWaveFileName = args [ 2 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // set audio mode to read from buffer //Scenic.Audio audio = SimpleClient.packageAudioAsRawBuffer(enrollmentFileName, 0, null); // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain TPD plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform TPD enrollment and scoring tasks MyTPDEnrollmentAndScoreRequest ( server , pd , topicName , posEnrollmentFileName , negEnrollmentFileName , scoreWaveFileName ); } public static boolean MyTPDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String posEnrollmentFileName , String negEnrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } //enroll a positive example List < RegionWord > posRegions = new ArrayList <> (); posRegions . add ( new RegionWord ( 0.100 , 2.500 )); // NOTE to add regions in seconds posRegions . add ( new RegionWord ( 7.500 , 10.000 )); // NOTE to add regions in seconds /** //we can also enroll a negative example enrollmentOptions.add(new Pair<>(\"isNegative\", \"True\")); List<RegionWord> negRegions = new ArrayList<>(); negRegions.add(new RegionWord(1.000, 1.500)); // NOTE to add regions in seconds negRegions.add(new RegionWord(5.000, 7.000)); // NOTE to add regions in seconds **/ // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it a synchronized call, so we know enrollment is complete before we score boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , topicName , posEnrollmentFileName , 0 , enrollmentCallback , false , false , posRegions , enrollmentOptions ); /** //goes with negative example enrolled=ClientUtils.requestEnrollClass(server, pp, topicName, negEnrollmentFileName, 0, enrollmentCallback, false, false, posRegions, enrollmentOptions); **/ if ( enrolled ){ // Create a call back to handle the TPD scoring request Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} regions:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()){ log . info ( \"\\t{} = {}, From {} to {}\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // TPD is a region scorer, so make a region score reqeust return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } }","title":"Integrating (Java) Client API"},{"location":"apiCode.html#integrating-a-client-api-with-olive","text":"","title":"Integrating a Client API with OLIVE"},{"location":"apiCode.html#java-client-ide-setup-guide","text":"If you would like to import the provided OLIVE Java API sample code and reference implementation into an IDE to explore the code and/or get started with integrating this code, please refer over to the Java Client IDE Setup Guide page. If you've already done this step, plan on integrating this functionality just using the provided JAR file, or just wish to browse code samples, continue below.","title":"Java Client IDE Setup Guide"},{"location":"apiCode.html#the-olive-java-api","text":"While the OLIVE system allows client integration via Protobuf messages sent from a variety of languages such as Java, Python, C++ and C#, and machine types such as Windows, Linux, and MacOS, the currently available OLIVE reference API implementation is Java-based only, and is also referred to as the OLIVE Java Client API. Therefore all instructions and code examples presented in this section currently assume that client programs are also in Java. Pseudocode and Python versions of the included code will be rolled into this page as they are available. Fundamentally any OLIVE API implementation is based on exchanging Protobuf messages between the client and server. These messages are defined in the API Message Reference Documentation. If you would like to create a reference implementation of the OLIVE API in another language, please refer to the information in the Developing an OLIVE API Reference Implementation page, that should prove to be helpful. If you would like an API reference implementation in another language to be created, please reach out to olive-support@sri.com to discuss your needs with the team.","title":"The OLIVE Java API"},{"location":"apiCode.html#basic-recipe-for-using-the-olive-java-client-api","text":"This section covers the first steps towards building a client based on implementing SRI's OLIVE API Java Reference implementation in order to use the OLIVE Java API for speech processing. The sections immediately following cover setting up an OLIVE server instance to connect to, then move into a number of steps necessary to connect a client to this server, and finally cover how to build and submit scoring and enrollment requests from the client program. The first step towards integrating a client with the provided OLIVE Java Reference Implementation is to review the API Message Reference to understand the available request and response messages, then dive in by setting up your IDE to work with this code. We have created a step-by-step guide for this configuration and setup process. Note that these steps are not necessary for implementing the actual functionality from this software package, but could be very useful for exploring and learning the code. If only the functionality from this reference implementation is desired, SRI can provide an appropriate JAR file to include in your project.","title":"Basic Recipe for using the OLIVE Java Client API"},{"location":"apiCode.html#olive-server","text":"As mentioned above, the OLIVE Enterprise API operates on a client/server model, making the OLIVE server an essential part of the OLIVE system. You must run the OLIVE server and manage its lifecycle as part of your integration effort. For more information about the server's duties, how to interact with it, what options and functionality are available to it, and general setup and troubleshooting information, please refer to the OLIVE Server Information Page .","title":"OLIVE server"},{"location":"apiCode.html#establish-server-connection","text":"Before making any task request, a client must establish a connection with the server. By default, the OLIVE server listens on ports 5588 (request port) and 5589 (status port) for client connection and status requests. These ports are configurable, but if the server has not been instructed to change its listening ports, the code below should establish a connection. Using the OLIVE Java API, connection to the server can be made with a single call, as shown below. This call is available from the API Server class , included in the package com.sri.scenic.api.Server. Java Server server = new Server (); server . connect ( \"scenic-client\" , //client-id \"localhost\" , //address of server 5588 , //request-port 5589 , //status-port 10000 //timeout for failed connection request ); Python --- Coming Soon --- Psudocode connect ( server - host - name , 5588 , 5589 ) The request port (5588 by default) is used for call and response messages (Protobuf messages). Each request message sent to this port is guaranteed a response from the server (this is why the messages in the API Message Reference are often suffixed with 'Request' and 'Result'). There is no need to poll the server for information on a submitted request, as the result/response for the a request is returned to the client as soon as it is available. The status port (5589 by default) is used by the Server to publish health and status messages (Heartbeat) to client(s). Clients can not send requests on this port.","title":"Establish Server Connection"},{"location":"apiCode.html#request-available-plugins","text":"In order to submit most server requests, the client must specify the plugin and domain to handle the request. To obtain the handle of a targeted plugin, the client first requests a list of all currently available valid plugins from the server. From the returned plugins list, the client looks up the specific plugin handle by the plugin's unique name (id) and its associated trait for the task to be performed. This handle can then be used in a future request message sent to the server. In the Java API Reference Implementation, this is accomplished by the utilities requestPlugin() and findPluginDomainByTrait(), within the included ClientUtils package. The steps are shown below. Java // ask the server for a list of currently available plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // Look up a specific plugin from the plugin list using // the plugin's unique name (pluginName and domainName) and associated trait e.g. FRAME_SCORER Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Scenic . TraitType . FRAME_SCORER , pluginList ); Python def request_plugins ( self ): request = PluginDirectoryRequest () # Wrap message in an Envelop request = self . _wrap_message ( request , FRAME_SCORER_REQUEST ) # Now send the message logging . debug ( \"Sending a Plugin request message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message...\" ) envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): server_msg = envelope . message [ i ] if broker_msg . HasField ( \"error\" ): print ( server_msg . error ) else : plugin_dir_msg = PluginDirectoryResult () plugin_dir_msg . ParseFromString ( server_msg . message_data [ 0 ]) # There is likely to be multiple plugin/domains print ( \"Server has {} plugins \" . format ( len ( plugin_dir_msg . plugins ))) for p in plugin_dir_msg . plugins : print ( \"Plugin {} has {} domains\" . format ( p . id , len ( p . domain ))) Pseudocode plugin - list = getListOfAvailablePlugins ( server ); targeted - plugin = lookupPlugin ( plugin - list , pluginName , pluginTrait ); The targeted plugin handle, pd in the above example, can then be used with other utilities within the Client API to submit requests, or to otherwise interact with the plugin. For example, this code below shows how one might check what Traits that each plugin in the pluginList returned above support: Java for ( Pair < Scenic . Plugin , Scenic . Domain > pair : pluginList ){ for ( Scenic . Trait t : pair . getFirst (). getTraitList ()) { if ( t . getType () == Scenic . TraitType . FRAME_SCORER ){ log . info ( \"Plugin {} supports frame scoring\" , pair . getFirst (). getId ()); } if ( t . getType () == Scenic . TraitType . GLOBAL_SCORER ){ // Supports global scoring (i.e. SID or LID) log . info ( \"Plugin {} supports global scoring\" , pair . getFirst (). getId ()); } if ( t . getType () == Scenic . TraitType . REGION_SCORER ){ // Supports region scoring (i.e. KWS) log . info ( \"Plugin {} supports region scoring\" , pair . getFirst (). getId ()); } } } Python -- coming soon -- Pseudocode --coming soon--","title":"Request Available Plugins"},{"location":"apiCode.html#audio-submission-guidelines","text":"One of the core client activities is submitting Audio with a request. In the OLIVE API, three ways are provided for a client to package audio data to send to the OLIVE server: file path buffer of raw audio sample data serialized file buffer object The enum AudioTransferType is used to specify they type of audio transfer to use. When the client and the OLIVE server share the same file system, the easiest way for the client to send audio data to the server is by specifying the audio's file path on disk. The OLIVE Client API provides the utility below to package audio files which are accessible to the server locally: Java packageAudioAsPath () // AudioTransferType.SEND_AS_PATH When the client and the server don't share the same file system, as in the case of a client making a remote connection to the OLIVE server, it is necessary to send the client's local audio files as a file buffer. To help package the client's audio data in a raw buffer, the OLIVE Client API provides the utility below: Java packageAudioAsRawBuffer () // AudioTransferType.SEND_SAMPLES_BUFFER When submitting audio to the server as a buffer of raw samples, it is important to include information characterizing the audio, such as the bit depth, audio encoding, sample rate, and number of channels, to ensure the server knows how to properly treat the buffer it receives. A third utility, shown below, also packages the client's audio data in a buffer. This utility passes the original file to the server in its entirety in one contiguous buffer, leaving the audio file header intact. Java packageAudioAsSerializedBuffer () // AudioTransferType.SEND_SERIALIZED_BUFFER Sending audio data as a serialized file buffer ensures that all audio header information is provided intact to the server. This allows the server to properly decode and process the audio once its received, since it can directly access the bit depth, encoding type, sample rate and other necessary information from the header itself. The tradeoff with serialized files is that there may be additional overhead needed to process the audio into a consumable form. If the client and server reside on the same hardware and file system, it is advisable to simply pass filepaths when possible. This saves the memory overhead burden of both the client and server loading audio into memory. If using common audio types, like 16-bit PCM .wav files, it may also be possible to simply pass a non-serialized file buffer. The OLIVE Java Client API provides utilities such as requestFrameScore(), requestEnrollClass(), etc. to handle various client message requests that were covered above. In these utilities, the enum argument transferType is used to select in what way the audio data is to be sent. For example, when transferType is set to AudioTransferType.SEND_SERIALIZED_BUFFER , audio data will be interpreted as if it were sent as a serialized buffer.","title":"Audio Submission Guidelines"},{"location":"apiCode.html#synchronous-vs-asynchronous-message-submission","text":"The OLIVE Client API allows the client to choose between processing a task request synchronously or asynchronously. Processing a task request synchronously means the client will block and wait for the task result to return before proceeding to other task requests. On the other hand, asynchronous processing means the client will not wait for the result to come back before moving on, allowing several jobs to be submitted in parallel. The examples below generally show submitting requests asynchronously. The argument async in the API utilities requestFrameRequest(), requestEnrollClass(), etc., can be used to select if the client intends to wait for the task result to return. When async is set to true , the client will not block when a request is sent to the server, so other task requests can be made before the results are received asynchronously and handled by the callback.","title":"Synchronous vs. Asynchronous Message Submission"},{"location":"apiCode.html#construct-request-message","text":"Information contained in a task request message may be different depending on the type of task to be performed. For a scoring task, the request message usually contains specific plugin and domain names, and the audio to be scored. For an adaptation or enrollment task, it also contains class ID information. To make a task request, the client program must first assemble the necessary information into a request message, and then send the message to the server. These 2 steps are shown in pseudo code below. Pseudocode request = packageRequest ( pluginName , pluginDomain , requestType , any other information specific to requestType ) sendRequest ( server , request , audio )","title":"Construct Request Message"},{"location":"apiCode.html#client-api-code-samples","text":"The OLIVE Reference API includes functionality pre-coded to accommodate many of the available request messages. Utilities such as requestFrameScore(), requestEnrollClass(), etc. not only do the packaging of request messages, but also take care of sending the request messages to the server, all in one call. They are available from the SRI Java API package sri.com.scenic.api.ClientUtils. The required parameters to send requests using these scoring utilities include: server handle (server - see here ) the plugin handle (pd - see here ) the name of the audio file to submit to the server (filename) channel number of the audio to be processed when audio has more than 1 channel (channelNumber) a callback function for handling results returned either asynchronously or synchronously (rc) whether the client will block for task result to return (async - see here ) an enum of how to submit audio to the server (transferType) optional lists of annotations of the submitted audio (regions) optional list of parameters for customizing plugin behavior (options) optional list of class IDs for filtering the results (classIDs) Performing an enrollment request adds an additional parameter: the ID of the class to be enrolled The primary requests covered below are: Frame score requests - used to make most SAD requests (note that some SAD plugins return regions, not frame scores) Global score requests - used to make LID, SID, or Gender score requests Region score requests - used to make QbE, KWS, Diarization, and sometimes SAD score requests Enrollment requests - used to enroll speakers or other class types for plugins that support the ClassEnroller trait","title":"Client API Code Samples"},{"location":"apiCode.html#frame-score-request","text":"The example below provides sample code for a function MyFrameScoreRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Frame scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. Note that only a plugin that support the FrameScorer trait, cna handle this request. All SAD plugins support this train, while some also support the RegionScorer trait. For an example of how to call this code with a specific plugin, refer to the SAD Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyFrameScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pp , filename , 1 , rc , true , true , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } Python --- Coming Soon --- Pseudocode -- Coming Soon --","title":"Frame Score Request"},{"location":"apiCode.html#global-score-request","text":"The example below provides sample code for a function MyGlobalScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Global scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. Note also that the code required to submit a GlobalScorerRequest message doesn't care what type of plugin is going to be doing the scoring, as long as the plugin implements the GlobalScorer Trait. This means that the exact same code can be used for submitting audio to global scoring SID plugins, LID plugins, or any other global scoring plugin. For an example of how to call this code with a specific plugin, refer to the SID Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyGlobalScorerRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a call back to handle the global scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ System . out . println ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { System . out . println ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // make the global scoring reqeust return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , true , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } Python def request_lid ( self , plugin , domain , filename , classes ): ''' Request a LID analysis of 'filename' :param plugin: the name of the LID plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param languages: optional list of languages trigraphs used to filter (restrict) results returned from the server. This list should only include languages supported by the specified plugin :return: the LID analysis as a list of (global) scores ''' request = GlobalScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio # send the name of the file to the server: audio . path = filename # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) request = self . _wrap_message ( request , GLOBAL_SCORER_REQUEST ) # Now send the message logging . debug ( \"Sending a LID (global score request) message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): broker_msg = envelope . message [ i ] if broker_msg . HasField ( \"error\" ): print broker_msg . error else : global_score_msg = GlobalScorerResult () global_score_msg . ParseFromString ( broker_msg . message_data [ 0 ]) # Assume there is only one result set (for 'speech'): frame_score_msg.result[0] print ( \"Received {} global scores\" . format ( len ( global_score_msg . result [ 0 ] . score ))) return global_score_msg . result [ 0 ] return None Pseudocode --- Coming Soon ---","title":"Global Score Request"},{"location":"apiCode.html#region-score-request","text":"The example below provides sample code for a function MyRegionScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Region scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. For an example of how to call this code with a specific plugin, refer to the KWS Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyRegionScorerRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a call back to handle the global scoring request Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // scoring result received if ( ! r . hasError ()){ log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"SDD Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // make the region scoring reqeust return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } Python --- Coming soon --- Pseudocode -- Coming soon --","title":"Region Score Request"},{"location":"apiCode.html#enrollment-request","text":"The example below provides sample code for a function MyEnrollmentRequest that takes a server connection, a plugin/domain handle, the name of the class (speaker) to enroll, and a path to an audio file as arguments, and uses this information to build and submit a Region Scoring request to the connected server. Examples below show an enrollment request with one file, followed by an enrollment with multiple files (for one class).","title":"Enrollment Request"},{"location":"apiCode.html#enrollment-request-with-a-single-audio-fileenrollment","text":"Java public static boolean MyEnrollmentRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String classID , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it synchronous so we know enrollment is complete boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , classID , enrollmentFileName , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); /** can do something else now, enrollment is complete **/ return true ; } Python --- Coming soon --- Pseudocode --- Coming soon ---","title":"Enrollment Request with a single audio file/enrollment."},{"location":"apiCode.html#batch-enrollment-request-with-multiple-files","text":"Java public static boolean MyEnrollmentUsingMultipleFilesRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String classID , ArrayList < String > audioFiles , int enrollmentFileCount ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****server processed 1 enrollment file*****\" ); responseCount ++ ; } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; for ( String audioFilename : audioFiles ) { // For simplicity, enrollments requests are done synchronously boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , classID , audioFilename , 0 , enrollmentCallback , false , AudioTransferType . SEND_AS_FILE , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out enrollment file processed } } // wait for all enrollment files to be processed while ( responseCount != enrollmentFileCount ) { try { TimeUnit . SECONDS . sleep ( 10 ); } catch ( InterruptedException e ) { System . out . println ( \"*****Timeout Exception*****\" ); } } return true ; // all enrollment files are processed, now we can proceed to do other things if necessary } Python --- Coming soon --- Pseudocode --- Coming soon --- The function call definitions for some of these scoring and enrollment request utilities follows: Java boolean requestFrameScore ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename , int channelNumber , Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a frame scoring request to the OLIVE server, e.g. SAD scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestGlobalScore ( Server server , Pair < Scenic . Plugin , Scenic . Domain > plugin , Scenic . TraitType trait , String filename , int channelNumber , Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a global scoring request to the OLIVE server, e.g. LID scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestRegionScores ( Server server , Pair < Scenic . Plugin , Scenic . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a region scoring request to the OLIVE server, e.g. KWS scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestEnrollClass ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String id , String wavePath , int channelNumber , Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options ) This call submits an enrollment request to the OLIVE server, e.g. enrolling speakers in a SID plugin. A class ID (id) is required for enrollment. If selected regions of an audio are used for enrollment, they are passed along in a list (regions). To see examples of how these utilities are used, consult sample client code provided in the next section.","title":"Batch enrollment request with multiple files."},{"location":"apiCode.html#plugin-specific-code-examples","text":"This section shows examples of using the functions just outlined to make calls to specific plugins, and demonstrate how the same code can be reused for several purposes - for example, requestGlobalScore is valid to request scoring from both SID and LID plugins. All client examples below can be found in the OLIVE-API-examples tree in the src/main/java/com/sri/scenic/api/client folder. SAD Scoring Example SID Enrollment and Scoring Example LID Enrollment and Scoring Example (synchronized approach) LID Enrollment and Scoring Example (asynchronized approach) KWS Scoring Example SDD Enrollment and Scoring Example SAD Supervised Adaptation Example TPD Enrollment and Scoring Example","title":"Plugin Specific Code Examples"},{"location":"apiCode.html#sad-scoring-request","text":"This shows a full implementation of a client program which sends a frame scoring request to a SAD plugin. Upon return of the result, it outputs the received frame scores. Included also is a second version where a threshold is used to filter out frame scores which are higher. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.ArrayBlockingQueue ; import java.util.concurrent.BlockingQueue ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MySADFrameScorer { private static Logger log = LoggerFactory . getLogger ( MySADFrameScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"tel-v1\" ; private static String pluginName = \"sad-dnn-v6_rc5\" ; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // from the list of plugins, find the targeted plugin for the task Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Scenic . TraitType . FRAME_SCORER , pluginList ); // formulate SAD frame scoring request and send to server MyFrameScoreRequest ( server , pd , audioFileName ); } public static boolean MyFrameScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pp , filename , 1 , rc , true , true , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } } The code above outputs all frame scores of the input audio, this can generate massive amount of output, especially when the audio is long. One good way to trim down the output is to filter out regions with frame scores higher than a preset threshold value. The following shows how this can be done in the @Override section of the callback routine, using a threshold of 0.0. Java // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); // filter to output speech regions with scores > 0.0 int speech_start = 0 ; int speech_end = 0 ; double sum_scores = 0.0 ; int num_frames = 0 ; for ( int i = 0 ; i < scores . length ; i ++ ) { if ( scores [ i ] > 0.0 ) { if ( speech_end == speech_start ) { speech_start = i ; } speech_end = i + 1 ; sum_scores = sum_scores + scores [ i ] ; num_frames = num_frames + 1 ; } else { if ( speech_end > speech_start ) { int start = ( int ) ( 100 * speech_start / ( double ) rate ); int end = ( int ) ( 100 * speech_end / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , sum_scores / num_frames )); } speech_start = i ; speech_end = i ; sum_scores = 0.0 ; num_frames = 0 ; } } } } System . exit ( 0 ); } };","title":"SAD Scoring Request"},{"location":"apiCode.html#sid-enrollment-and-scoring-request","text":"This example is a full implementation of a client program which sends an enrollment request to a SID plugin, followed by a scoring request to the same SID plugin. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.atomic.AtomicInteger ; public class MySIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multicond-v1\" ; private static String pluginName = \"sid-embed-v5b\" ; private static String enrollmentFileName ; private static String speakerName = \"Mr.X\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static String outputDirName = \"./\" ;; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain SID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform SID enrollment task MySIDEnrollmentAndScoreRequest ( server , pd , speakerName , enrollmentFileName , scoreWaveFileName ); } public static boolean MySIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String enrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it a synchronized call, so we know the speaker is enrolled before we make the score request boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); if ( enrolled ){ // Create a call back to handle the SID scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ System . out . println ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { System . out . println ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // SID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , AudioTransferType . SEND_SERIALIZED_BUFFER , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } }","title":"SID Enrollment and Scoring Request"},{"location":"apiCode.html#lid-enrollment-and-scoring-request","text":"","title":"LID Enrollment and Scoring Request"},{"location":"apiCode.html#synchronized-approach","text":"A LID enrollment of a new language may involve multiple enrollment files. The synchronized version of this client waits for the server to complete the enrollment request with each enrollment file sequentially, before requesting LID scores on an input file. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MysyncLIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MysyncLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v1-rc6\" ; private static String audioList ; private static ArrayList < String > audioFiles = new ArrayList < String > (); private static String audioFilename ; private static int enrollmentFileCount = 0 ; private static String languageName = \"JKL\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String audioList = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // read enrollment files from input list if ( ! Files . exists ( Paths . get ( audioList ). toAbsolutePath ())) { System . err . println ( \"ERROR: '\" + audioList + \"' does not exist\" ); } BufferedReader br = new BufferedReader ( new FileReader ( audioList )); while (( audioFilename = br . readLine ()) != null ){ audioFiles . add ( audioFilename ); } enrollmentFileCount = audioFiles . size (); // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain LID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform LID enrollment task MyLIDEnrollmentAndScoreRequest ( server , pd , languageName , audioFiles , enrollmentFileCount , scoreWaveFileName ); } public static boolean MyLIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String languageName , ArrayList < String > audioFiles , int enrollmentFileCount , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****Enrollment Succeeded*****\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment requests, with one enrollment file at a time // make a synchronized enrollment call, so we know the language is enrolled before we make the score request for ( String audioFilename : audioFiles ) { boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , languageName , audioFilename , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out name of enrollment file used } } // do the score request // First create a call back to handle the LID scoring result Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ log . info ( \"\\t{} = {}\" , gs . getClassId (), gs . getScore ()); } } else { log . error ( \"Global scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } }","title":"Synchronized Approach"},{"location":"apiCode.html#asynchronized-approach","text":"This example provides an alternate LID client example that performs asynchronized enrollment and does not wait for the last enrollment request to complete before making another. Instead, it keeps count on server responses to enrollment requests made, and makes sure that all enrollment requests are completed before making a LID scoring request on an input file. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MyasyncLIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MyasyncLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v1-rc6\" ; private static String audioList ; private static ArrayList < String > audioFiles = new ArrayList < String > (); private static String audioFilename ; private static int enrollmentFileCount = 0 ; private static int responseCount = 0 ; private static String languageName = \"MNO\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static String outputDirName = \"./\" ;; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment list is passed in as an argument String audioList = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // read enrollment files from input list and keep count of enrollment files to be processed if ( ! Files . exists ( Paths . get ( audioList ). toAbsolutePath ())) { System . err . println ( \"ERROR: '\" + audioList + \"' does not exist\" ); } BufferedReader br = new BufferedReader ( new FileReader ( audioList )); while (( audioFilename = br . readLine ()) != null ){ audioFiles . add ( audioFilename ); } enrollmentFileCount = audioFiles . size (); // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain LID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform LID enrollment task MyLIDEnrollmentAndScoreRequest ( server , pd , languageName , audioFiles , enrollmentFileCount , scoreWaveFileName ); } public static boolean MyLIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String languageName , ArrayList < String > audioFiles , int enrollmentFileCount , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****server processed 1 enrollment file*****\" ); responseCount ++ ; } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; for ( String audioFilename : audioFiles ) { boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , languageName , audioFilename , 0 , enrollmentCallback , true , true , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out enrollment file processed } } // wait for all enrollments to complete while ( responseCount != enrollmentFileCount ) { try { TimeUnit . SECONDS . sleep ( 10 ); } catch ( InterruptedException e ) { System . out . println ( \"*****Timeout Exception*****\" ); } } // We next do the score request // First create a call back to handle the LID scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ log . info ( \"\\t{} = {}\" , gs . getClassId (), gs . getScore ()); } } else { log . error ( \"Global scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , true , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } }","title":"Asynchronized Approach"},{"location":"apiCode.html#kws-scoring-request","text":"This portrays a full implementation of a KWS scoring request, against the list of default keywords already trained. Note that unlike the previous examples that requested GlobalScores, this client issues a RegionScoreRequest because KWS is a \"RegionScorer\". Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MyKWSRegionScorer { private static Logger log = LoggerFactory . getLogger ( MyKWSRegionScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"r-tdnn-tel-v1\" ; private static String pluginName = \"kws-dynapy-v1\" ; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // formulate the frame scoring task request Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"KWS\" , Scenic . TraitType . REGION_SCORER , pluginList ); // Perform KWS frame scoring task MyRegionScoresRequest ( server , pd , audioFileName ); } public static boolean MyRegionScoresRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle KWS results from the server Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > rc = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; return ClientUtils . requestRegionScores ( server , pp , filename , 0 , rc , true , false , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } }","title":"KWS Scoring Request"},{"location":"apiCode.html#sdd-enrollment-and-scoring-request","text":"The following code shows a full implementation of a speaker enrollment request, followed by a scoring request made to a SDD plugin. The scoring request in this client is a RegionScorerRequest because SDD plugins are region scorers. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MySDDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySDDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multicond-v1\" ; private static String pluginName = \"sdd-sbc-embed-v1-1\" ; private static String enrollmentFileName ; private static String speakerName = \"Mr.X\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain SDD plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform SDD enrollment task MySDDEnrollmentAndScoreRequest ( server , pd , speakerName , enrollmentFileName , scoreWaveFileName ); } public static boolean MySDDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String enrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // enrollment result received if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make a synchronized enrollment request, so we know the speaker is enrolled before we make the score request boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); if ( enrolled ){ // do the SDD scoring request // First create a callback to handle scoring result from server Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // scoring result received if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"SDD Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // SDD is a region scorer, so make a region score reqeust: return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; }","title":"SDD Enrollment And Scoring Request"},{"location":"apiCode.html#sad-supervised-adaptation-request","text":"Below is a full implementation of a SAD request to adapt a new domain from an existing domain. The list of adaptation training files is passed into the client as a file. The code handles both supervised (speech regions specified) and unsupervised (speech regions not specified) SAD adaptations. However, some SAD plugins may not have the unsupervised adaptation capability, in which case the client will exit with a failure message. Java package com.sri.scenic.api.client ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Scenic.AnnotationRegion ; import com.sri.scenic.api.Scenic.AudioAnnotation ; import com.sri.scenic.api.Scenic.Domain ; import com.sri.scenic.api.Scenic.Plugin ; import com.sri.scenic.api.Scenic.TraitType ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.ClientUtils ; import com.sri.scenic.api.utils.LearningParser ; import com.sri.scenic.api.utils.LearningParser.LearningDataType ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Paths ; import java.util.ArrayList ; import java.util.Collection ; import java.util.HashMap ; import java.util.List ; import java.util.Map ; import java.util.UUID ; import javax.sound.sampled.UnsupportedAudioFileException ; import org.apache.commons.cli.CommandLine ; import org.apache.commons.cli.CommandLineParser ; import org.apache.commons.cli.DefaultParser ; import org.apache.commons.cli.HelpFormatter ; import org.apache.commons.cli.Option ; import org.apache.commons.cli.Options ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; public class MySADAdaptation { private static Logger log = LoggerFactory . getLogger ( MySADAdaptation . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"tel-v1\" ; private static String newDomainName = \"new-tel-v1\" ; private static String pluginName = \"sad-dnn-v6_rc5\" ; private static LearningParser learningParser = new LearningParser (); private static LearningDataType dataType ; /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file list is passed as an argument String audioFileList = args [ 0 ] ; learningParser . parse ( audioFileList ); if ( ! learningParser . isValid ()) { System . err . println ( \"Invalid input file: \" + audioFileList ); System . exit ( - 1 ); } if ( learningParser . hasRegions ()) { dataType = LearningDataType . SUPERVISED_WITH_REGIONS ; } else if ( learningParser . hasClasses ()) { dataType = LearningDataType . SUPERVISED ; } else { dataType = LearningDataType . UNSUPERVISED ; } // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // find plugin handle for adaptation task Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , learningParser . isUnsupervised () ? Scenic . TraitType . UNSUPERVISED_ADAPTER : Scenic . TraitType . SUPERVISED_ADAPTER , pluginList ); // Preproces audio - doesn't matter if supervised or unsupervised String adaptID = UUID . randomUUID (). toString (); Plugin p = pd . getFirst (); Domain d = pd . getSecond (); // optional annotations, generated if found in the parser (supervised adaptation) Map < String , List < AudioAnnotation >> annotations = new HashMap <> (); // annotations -> <classID> ->* <AudioAnnotations>, annotations will be empty for unsupervised adaptation int numPreprocessed = OliveLearn . preprocessAllAudio ( server , p , d , learningParser , adaptID , annotations ); if ( ! learningParser . isUnsupervised ()) { // supervised adaptation if ( numPreprocessed > 0 ) { OliveLearn . finalizeSupervisedAdaptation ( server , p , d , adaptID , newDomainName , annotations ); } else { System . err . println ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } else { // unsupervised adaptation if ( numPreprocessed > 0 ) { OliveLearn . finalizeUnsupervisedAdaptation ( server , p , d , adaptID , newDomainName ); } else { System . err . println ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } System . out . println ( \"\" ); System . out . println ( \"Learning finished. Exiting...\" ); System . exit ( 0 ); } }","title":"SAD Supervised Adaptation Request"},{"location":"apiCode.html#tpd-enrollment-and-scoring-request","text":"This final example shows a full implementation of a TPD topic enrollment request using positive examples from selected regions of an audio, followed by a scoring request of the enrolled topic on another audio. Enrollment using negative audio samples are also included in this example as commented out lines. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.atomic.AtomicInteger ; public class MyTPDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MyTPDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"r-tdnn-tel-v1\" ; private static String pluginName = \"tpd-dynapy-v1\" ; private static String posEnrollmentFileName ; private static String negEnrollmentFileName ; private static String topicName = \"hello\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); //private static String outputDirName = \"./\";; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String posEnrollmentFileName = args [ 0 ] ; String negEnrollmentFileName = args [ 1 ] ; String scoreWaveFileName = args [ 2 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // set audio mode to read from buffer //Scenic.Audio audio = SimpleClient.packageAudioAsRawBuffer(enrollmentFileName, 0, null); // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain TPD plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform TPD enrollment and scoring tasks MyTPDEnrollmentAndScoreRequest ( server , pd , topicName , posEnrollmentFileName , negEnrollmentFileName , scoreWaveFileName ); } public static boolean MyTPDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String posEnrollmentFileName , String negEnrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } //enroll a positive example List < RegionWord > posRegions = new ArrayList <> (); posRegions . add ( new RegionWord ( 0.100 , 2.500 )); // NOTE to add regions in seconds posRegions . add ( new RegionWord ( 7.500 , 10.000 )); // NOTE to add regions in seconds /** //we can also enroll a negative example enrollmentOptions.add(new Pair<>(\"isNegative\", \"True\")); List<RegionWord> negRegions = new ArrayList<>(); negRegions.add(new RegionWord(1.000, 1.500)); // NOTE to add regions in seconds negRegions.add(new RegionWord(5.000, 7.000)); // NOTE to add regions in seconds **/ // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it a synchronized call, so we know enrollment is complete before we score boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , topicName , posEnrollmentFileName , 0 , enrollmentCallback , false , false , posRegions , enrollmentOptions ); /** //goes with negative example enrolled=ClientUtils.requestEnrollClass(server, pp, topicName, negEnrollmentFileName, 0, enrollmentCallback, false, false, posRegions, enrollmentOptions); **/ if ( enrolled ){ // Create a call back to handle the TPD scoring request Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} regions:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()){ log . info ( \"\\t{} = {}, From {} to {}\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // TPD is a region scorer, so make a region score reqeust return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } }","title":"TPD Enrollment and Scoring Request"},{"location":"apiInfo.html","text":"OLIVE Enterprise API Primer Introduction The OLIVE Enterprise API enables third-party tools and existing workflows to interface with the OLIVE backend system. This page provides an introduction to the concepts and information needed to begin implementing OLIVE\u2019s speech processing tools and capabilities within a client application by integrating through the OLIVE Enterprise API (previously known as the SCENIC Enterprise API). In general, the three main components of an OLIVE-based audio processing system are: The OLIVE Server The OLIVE Plugins An OLIVE-enabled Client Links are provided above and throughout this page for more information about the OLIVE Server and the Plugins. The main focus of this page is covering how to help implement or create an OLIVE-enabled Client. SRI offers a Java-based Reference API implementation for OLIVE to allow integrators relying on Java to quickly start folding OLIVE functionality into their new or current project. This code is also available as example code for integrators using different programming languages to use as a general reference for accomplishing certain tasks. This guide covers both implementing the existing Java-based API reference implementation, as well as some information that you will need to build your own reference implentation, if desired. If you're interested in having a reference implementation built and provided in a language other than Java, please refer to this section below. The next section starts by introducing general operating concepts of the OLIVE system, with links to resources that expand on this information if you'd like to dig a bit deeper. Following that is a section that describes the steps a client uses to connect to the OLIVE server, submit requests for processing, and unpack results returned from the server. If you are interested in full Java examples of clients requesting various tasks to be performed using this API, continue on to the Java API Client Integration Guide . We also provide a page that describes in details how to set up your Java IDE , using IntelliJ as an example, to help you get started with building your own custom Java clients using our example code. Although the current reference OLIVE API is Java based, this message based API can be implemented in different language. Should you have a need for a non-Java OLIVE API, this page of the documentation contains useful information for building an entirely new OLIVE API in a language other than Java. Useful Concepts to Know Before you start using the OLIVE API to put together client programs, it is useful to have some background information about how the OLIVE system operates. The OLIVE system is client/server based, using Google Protocol Buffers (Protobuf) transported over ZMQ sockets to communicate between the client and server. Therefore any API implementation that allows client access to the backend OLIVE system must also follow the same client/server model of exchanging Google Protocol Buffer based messages over ZMQ sockets. The OLIVE server accepts client connections at pre-selected but configurable ports. Once a connection is established between a client and the server, task requests and results (implemented as Protobuf messages) can be sent and received over this connection. Via the server, client requests are directed to and performed by OLIVE speech processing agents (plugins). Also via the server, results of completed requests are returned to the client. Task requests and results are sent in the form of OLIVE messages which contain information pertinent to the type of request and the type of plugin performing the request. The contemts and details of all of the possible messages, both requests and responses, are detailed in the API Message Reference page, but more details about which of these will be important to you and how to actually use these messages follow below. If you are new to the OLIVE Enterprise API then you will quickly discover that the API does not offer explicit API calls for making \u201cSAD\u201d, \u201cSID\u201d, or \u201cLID\u201d requests. The reason for this is the API is based on the OLIVE Plugin Framework used by the OLIVE server, which abstracts speech systems such as SAD, SID, and LID into \u201ctraits\u201d. It is these \u201ctraits\u201d that are expressed in this API, so score and class modification requests are based on the plugins traits and not the type of speech system. The OLIVE server identifies plugins by the task types (SAD, SID, LID, etc.) they claim they can perform, and the details of how these plugins can go about those tasks are defined by the Traits that they implement. For example, a plugin with a Speaker Identification task often implements the GlobalScorer Trait. From the table below or from the Traits Info Page , you can see the messages that accompany implementing this Trait, which define the functionality associated with being a GlobalScorer, and let us see the types of output we can expect to retrieve from such a plugin. The following subsections describe each of these basic concepts in more details. OLIVE Plugin Tasks and Traits The OLIVE server identifies plugins by the type of tasks they can perform. For example, a plugin designed to recognize and identify the voice of a pre-enrolled speaker carries the Task of Speaker Identification , or SID. Likewise, a plugin that is designed to label speech regions within an audio clip has the Task of Speech Activity Detection , or SAD. In order to perform these Tasks, plugins require some abilities - for example, a SID task plugin requires the ability to perform enrollment of speakers and scoring of audio against those enrollments; a SAD type plugin must be able to perform a scoring task to calculate the likelihood of speech throughout an audio clip, and sometimes adaptation tasks; etc. These abilities are defined by what OLIVE calls Traits . More detailed information on the available OLIVE Traits can be found on the respective info page, but a quick primer is also included below. Continuing with the previous example, a SID type plugin needs to be able to perform the scoring task. Historically, most SID plugins assume that the audio being passed to it is homogenous, and consists of a single speaker, and it is therefore possible to score all of the audio and return a single score for each enrolled speaker model representing whether or not the candidate speech is likely from this speaker. This type of score is called a global score, and the Trait that the plugin implements to gain this functionality is called GlobalScorer . \"Implementing\" this Trait means that the plugin contains routine definitions that allow it to receive and appropriately respond to the the OLIVE API Messages associated with that Trait. In order to have speaker models to score against, a SID plugin also needs the ability to enroll speaker models as classes. The ability to modify enrolled classes comes from the ClassModifier Trait and its associated API messages. For a complete list of the available Traits and their associated request messages, including the appropriate reply message to each one, refer to the Plugin Traits info page and/or the API Message Reference page. For a high level overview of what this means in terms of available plugins, continue below. The following table shows the likely traits for the scoring functionality of selected OLIVE plugin task types: Plugin Type Scoring Trait Speech Activity Detection (SAD) FrameScorer and/or RegionScorer Language Identification (LID) GlobalScorer Speaker Identification (SID) GlobalScorer Speaker Diarization and Detection (SDD) RegionScorer Keyword Spotting (KWS) RegionScorer Query By Example KWS (QBE) RegionScorer Topic Detection (TPD) RegionScorer Speech Enhancement (ENH) AudioConverter Voice Type Discrimination (VTD) FrameScorer and/or RegionScorer For plugins that allow or require enrollment functionality, the associated Trait is ClassModifier . The following plugin types may currently have this Trait for the enrollment task: Plugin Type Enrollment Trait Language Identification (LID) * ClassModifier Speaker Identification (SID) ClassModifier Speaker Diarization and Detection (SDD) ClassModifier Query By Example KWS (QBE) ClassModifier Topic Detection (TPD) ClassModifier *Note that not all LID plugins allow or support language/class enrollment. When in doubt, refer to individual plugin documentation, or check the plugin's implemented Traits. Please remember, these tables may not be true for all plugins and some Plugins may support additional Traits. This mapping is only intended to help introduce the OLIVE Enterprise API and its underlying Plugin Framework to new developers. Some SAD plugins also allow the end user to perform domain adaptation to improve plugin performance in certain audio conditions. The Trait listed below is associated with this task. Plugin Type Adaptation Trait Speech Activity Detection (SAD) SupervisedAdapter OLIVE Message Requests / Results By Plugin Traits Now that you know a bit about the available Plugin Tasks and the Traits they're likely to implement, we will discuss the Messages that actually allow for requests to be made to the plugins, and for information to be passed back from the plugins to the client. A client connected to the OLIVE server can submit message to the server to request information from plugins. The table below shows what requests are generally available for selected plugin types. Note that it is possible to create plugins that may stray from this list and may implement a different Trait than what is shown below. Scoring Traits Plugin Trait Task Request Message Result Message Global Scorer LID , SID GlobalScorerRequest GlobalScorerStereoRequest GlobalScorerResult GlobalScorerStereoResult RegionScorer SDD , SAD *, KWS , QBE , TPD RegionScorerRequest RegionScorerStereoRequest RegionScorerResult RegionScorerStereoResult FrameScorer SAD * FrameScorerRequest FrameScorerStereoRequest FrameScorerResult FrameScorerStereoResult *Note that not all SAD plugins support FrameScorer and/or RegionScorer. Please refer to specific plugin documentation or consult with SRI if unsure. As you can see from this table, the same few API messages are reused for most scoring requests, meaning the actual code implementation for these tasks can be kept simple. Other Traits Plugin Trait Task Functionality Request Message Result Message AudioConverter ENH Audio Modification, Speech Enhancement AudioModificationRequest AudioModificationResult GlobalComparer FOR Forensic Audio Comparison GlobalComparerRequest GlobalComparerResult LearningTrait / SupervisedAdapter SAD Audio Condition Domain Adaptation SupervisedAdaptationRequest PreprocessAudioAdaptRequest SupervisedAdaptationResult PreprocessAudioAdaptResult Other Useful OLIVE Message Types Besides the messages related to plugin tasking and interaction mentioned in the two sections above, there are several additional messages which are useful to know for server management and other non-plugin-specific tasks. PluginDirectoryRequest and PluginDirectoryResult The client sends a PluginDirectoryRequest to the server to request a list of available plugins, and the server sends back to the client a PluginDirectoryResult . The PluginDirectoryResult response includes supported plugins, their traits, and optional parameters they support. A comprehensive list of OLIVE API Messages is available in OLIVE API Message Reference . Information Persistence As of OLIVE 4.0 the backend OLIVE server and API no longer support persistence. It is the responsibility of the client to store, manage, and reference results from the OLIVE server. The OLIVE server does persist enrolled class models and some collected adaptation information. Dependencies The OLIVE Enterprise API utilizes the following dependencies: Google Protocol Buffers 3.4: Used to define the messages that comprise the OLIVE API. Most messages are in the form of request/reply. ZeroMQ 3.2.3: Provides inter-process communication over several possible mechanisms including TCP. Protobuf-net : Optional - needed if you wish to integrate from a .NET/Mono application. You will need versions of these software dependencies appropriate for your system architecture/operating system in order to communicate with the OLIVE server. Supported Languages Given the dependencies described in the previous section it is possible to utilize the OLIVE API from the following programming languages/runtimes: Java (or other JVM languages that provide Java interoperability) C# (or other .NET language, via the protobuf.net library. This is an extra dependency) Python C++ Note that because the Java-based OLIVE UI utilizes the API, SRI has already developed a Java client library to facilitate use of the API from Java. For more information see the \u201cJava Client Library\u201d section of this document.","title":"Enterprise API Primer"},{"location":"apiInfo.html#olive-enterprise-api-primer","text":"","title":"OLIVE Enterprise API Primer"},{"location":"apiInfo.html#introduction","text":"The OLIVE Enterprise API enables third-party tools and existing workflows to interface with the OLIVE backend system. This page provides an introduction to the concepts and information needed to begin implementing OLIVE\u2019s speech processing tools and capabilities within a client application by integrating through the OLIVE Enterprise API (previously known as the SCENIC Enterprise API). In general, the three main components of an OLIVE-based audio processing system are: The OLIVE Server The OLIVE Plugins An OLIVE-enabled Client Links are provided above and throughout this page for more information about the OLIVE Server and the Plugins. The main focus of this page is covering how to help implement or create an OLIVE-enabled Client. SRI offers a Java-based Reference API implementation for OLIVE to allow integrators relying on Java to quickly start folding OLIVE functionality into their new or current project. This code is also available as example code for integrators using different programming languages to use as a general reference for accomplishing certain tasks. This guide covers both implementing the existing Java-based API reference implementation, as well as some information that you will need to build your own reference implentation, if desired. If you're interested in having a reference implementation built and provided in a language other than Java, please refer to this section below. The next section starts by introducing general operating concepts of the OLIVE system, with links to resources that expand on this information if you'd like to dig a bit deeper. Following that is a section that describes the steps a client uses to connect to the OLIVE server, submit requests for processing, and unpack results returned from the server. If you are interested in full Java examples of clients requesting various tasks to be performed using this API, continue on to the Java API Client Integration Guide . We also provide a page that describes in details how to set up your Java IDE , using IntelliJ as an example, to help you get started with building your own custom Java clients using our example code. Although the current reference OLIVE API is Java based, this message based API can be implemented in different language. Should you have a need for a non-Java OLIVE API, this page of the documentation contains useful information for building an entirely new OLIVE API in a language other than Java.","title":"Introduction"},{"location":"apiInfo.html#useful-concepts-to-know","text":"Before you start using the OLIVE API to put together client programs, it is useful to have some background information about how the OLIVE system operates. The OLIVE system is client/server based, using Google Protocol Buffers (Protobuf) transported over ZMQ sockets to communicate between the client and server. Therefore any API implementation that allows client access to the backend OLIVE system must also follow the same client/server model of exchanging Google Protocol Buffer based messages over ZMQ sockets. The OLIVE server accepts client connections at pre-selected but configurable ports. Once a connection is established between a client and the server, task requests and results (implemented as Protobuf messages) can be sent and received over this connection. Via the server, client requests are directed to and performed by OLIVE speech processing agents (plugins). Also via the server, results of completed requests are returned to the client. Task requests and results are sent in the form of OLIVE messages which contain information pertinent to the type of request and the type of plugin performing the request. The contemts and details of all of the possible messages, both requests and responses, are detailed in the API Message Reference page, but more details about which of these will be important to you and how to actually use these messages follow below. If you are new to the OLIVE Enterprise API then you will quickly discover that the API does not offer explicit API calls for making \u201cSAD\u201d, \u201cSID\u201d, or \u201cLID\u201d requests. The reason for this is the API is based on the OLIVE Plugin Framework used by the OLIVE server, which abstracts speech systems such as SAD, SID, and LID into \u201ctraits\u201d. It is these \u201ctraits\u201d that are expressed in this API, so score and class modification requests are based on the plugins traits and not the type of speech system. The OLIVE server identifies plugins by the task types (SAD, SID, LID, etc.) they claim they can perform, and the details of how these plugins can go about those tasks are defined by the Traits that they implement. For example, a plugin with a Speaker Identification task often implements the GlobalScorer Trait. From the table below or from the Traits Info Page , you can see the messages that accompany implementing this Trait, which define the functionality associated with being a GlobalScorer, and let us see the types of output we can expect to retrieve from such a plugin. The following subsections describe each of these basic concepts in more details.","title":"Useful Concepts to Know"},{"location":"apiInfo.html#olive-plugin-tasks-and-traits","text":"The OLIVE server identifies plugins by the type of tasks they can perform. For example, a plugin designed to recognize and identify the voice of a pre-enrolled speaker carries the Task of Speaker Identification , or SID. Likewise, a plugin that is designed to label speech regions within an audio clip has the Task of Speech Activity Detection , or SAD. In order to perform these Tasks, plugins require some abilities - for example, a SID task plugin requires the ability to perform enrollment of speakers and scoring of audio against those enrollments; a SAD type plugin must be able to perform a scoring task to calculate the likelihood of speech throughout an audio clip, and sometimes adaptation tasks; etc. These abilities are defined by what OLIVE calls Traits . More detailed information on the available OLIVE Traits can be found on the respective info page, but a quick primer is also included below. Continuing with the previous example, a SID type plugin needs to be able to perform the scoring task. Historically, most SID plugins assume that the audio being passed to it is homogenous, and consists of a single speaker, and it is therefore possible to score all of the audio and return a single score for each enrolled speaker model representing whether or not the candidate speech is likely from this speaker. This type of score is called a global score, and the Trait that the plugin implements to gain this functionality is called GlobalScorer . \"Implementing\" this Trait means that the plugin contains routine definitions that allow it to receive and appropriately respond to the the OLIVE API Messages associated with that Trait. In order to have speaker models to score against, a SID plugin also needs the ability to enroll speaker models as classes. The ability to modify enrolled classes comes from the ClassModifier Trait and its associated API messages. For a complete list of the available Traits and their associated request messages, including the appropriate reply message to each one, refer to the Plugin Traits info page and/or the API Message Reference page. For a high level overview of what this means in terms of available plugins, continue below. The following table shows the likely traits for the scoring functionality of selected OLIVE plugin task types: Plugin Type Scoring Trait Speech Activity Detection (SAD) FrameScorer and/or RegionScorer Language Identification (LID) GlobalScorer Speaker Identification (SID) GlobalScorer Speaker Diarization and Detection (SDD) RegionScorer Keyword Spotting (KWS) RegionScorer Query By Example KWS (QBE) RegionScorer Topic Detection (TPD) RegionScorer Speech Enhancement (ENH) AudioConverter Voice Type Discrimination (VTD) FrameScorer and/or RegionScorer For plugins that allow or require enrollment functionality, the associated Trait is ClassModifier . The following plugin types may currently have this Trait for the enrollment task: Plugin Type Enrollment Trait Language Identification (LID) * ClassModifier Speaker Identification (SID) ClassModifier Speaker Diarization and Detection (SDD) ClassModifier Query By Example KWS (QBE) ClassModifier Topic Detection (TPD) ClassModifier *Note that not all LID plugins allow or support language/class enrollment. When in doubt, refer to individual plugin documentation, or check the plugin's implemented Traits. Please remember, these tables may not be true for all plugins and some Plugins may support additional Traits. This mapping is only intended to help introduce the OLIVE Enterprise API and its underlying Plugin Framework to new developers. Some SAD plugins also allow the end user to perform domain adaptation to improve plugin performance in certain audio conditions. The Trait listed below is associated with this task. Plugin Type Adaptation Trait Speech Activity Detection (SAD) SupervisedAdapter","title":"OLIVE Plugin Tasks and Traits"},{"location":"apiInfo.html#olive-message-requests-results-by-plugin-traits","text":"Now that you know a bit about the available Plugin Tasks and the Traits they're likely to implement, we will discuss the Messages that actually allow for requests to be made to the plugins, and for information to be passed back from the plugins to the client. A client connected to the OLIVE server can submit message to the server to request information from plugins. The table below shows what requests are generally available for selected plugin types. Note that it is possible to create plugins that may stray from this list and may implement a different Trait than what is shown below.","title":"OLIVE Message Requests / Results By Plugin Traits"},{"location":"apiInfo.html#scoring-traits","text":"Plugin Trait Task Request Message Result Message Global Scorer LID , SID GlobalScorerRequest GlobalScorerStereoRequest GlobalScorerResult GlobalScorerStereoResult RegionScorer SDD , SAD *, KWS , QBE , TPD RegionScorerRequest RegionScorerStereoRequest RegionScorerResult RegionScorerStereoResult FrameScorer SAD * FrameScorerRequest FrameScorerStereoRequest FrameScorerResult FrameScorerStereoResult *Note that not all SAD plugins support FrameScorer and/or RegionScorer. Please refer to specific plugin documentation or consult with SRI if unsure. As you can see from this table, the same few API messages are reused for most scoring requests, meaning the actual code implementation for these tasks can be kept simple.","title":"Scoring Traits"},{"location":"apiInfo.html#other-traits","text":"Plugin Trait Task Functionality Request Message Result Message AudioConverter ENH Audio Modification, Speech Enhancement AudioModificationRequest AudioModificationResult GlobalComparer FOR Forensic Audio Comparison GlobalComparerRequest GlobalComparerResult LearningTrait / SupervisedAdapter SAD Audio Condition Domain Adaptation SupervisedAdaptationRequest PreprocessAudioAdaptRequest SupervisedAdaptationResult PreprocessAudioAdaptResult","title":"Other Traits"},{"location":"apiInfo.html#other-useful-olive-message-types","text":"Besides the messages related to plugin tasking and interaction mentioned in the two sections above, there are several additional messages which are useful to know for server management and other non-plugin-specific tasks. PluginDirectoryRequest and PluginDirectoryResult The client sends a PluginDirectoryRequest to the server to request a list of available plugins, and the server sends back to the client a PluginDirectoryResult . The PluginDirectoryResult response includes supported plugins, their traits, and optional parameters they support. A comprehensive list of OLIVE API Messages is available in OLIVE API Message Reference .","title":"Other Useful OLIVE Message Types"},{"location":"apiInfo.html#information-persistence","text":"As of OLIVE 4.0 the backend OLIVE server and API no longer support persistence. It is the responsibility of the client to store, manage, and reference results from the OLIVE server. The OLIVE server does persist enrolled class models and some collected adaptation information.","title":"Information Persistence"},{"location":"apiInfo.html#dependencies","text":"The OLIVE Enterprise API utilizes the following dependencies: Google Protocol Buffers 3.4: Used to define the messages that comprise the OLIVE API. Most messages are in the form of request/reply. ZeroMQ 3.2.3: Provides inter-process communication over several possible mechanisms including TCP. Protobuf-net : Optional - needed if you wish to integrate from a .NET/Mono application. You will need versions of these software dependencies appropriate for your system architecture/operating system in order to communicate with the OLIVE server.","title":"Dependencies"},{"location":"apiInfo.html#supported-languages","text":"Given the dependencies described in the previous section it is possible to utilize the OLIVE API from the following programming languages/runtimes: Java (or other JVM languages that provide Java interoperability) C# (or other .NET language, via the protobuf.net library. This is an extra dependency) Python C++ Note that because the Java-based OLIVE UI utilizes the API, SRI has already developed a Java client library to facilitate use of the API from Java. For more information see the \u201cJava Client Library\u201d section of this document.","title":"Supported Languages"},{"location":"apiLegacy.html","text":"","title":"apiLegacy"},{"location":"apiMessage.html","text":"OLIVE API Message Protocol Documentation olive.proto Protocol Buffer Definitions The messages defined on this page are what define the OLIVE Enterprise API. These messages are how a client application will interact with and provide tasks to an OLIVE Server. The format used by the OLIVE API is based on the Google Protocol Buffer . For more information on how to integrate the ability to send and receive these messages into a client application using a provided Client API implementation from SRI, refer to the Integrating the (Java) Client API Guide. For more information on creating your own reference implementation with the functionality of these messages, head over to the Creating an API Reference Implementation page that offers guidelines and information. Server Management Messages GetActiveRequest Message to request the list of ScenicMessages that are still active GetActiveResult Response to GetActiveRequest containing the ScenicMessages that are still active Field Type Label Description message_id string repeated List containing the IDs of each message still being processed on the server total_num string required Total number of messages still being processed GetStatusRequest Request a simple server status message, similar to a heartbeat except the request reply is sent on the status port and is requested by the client GetStatusResult The result of a GetStatusRequest Field Type Label Description num_pending uint32 required The number of pending jobs num_busy uint32 required the number of active jobs num_finished uint32 required The number of finished jobs Heartbeat A heartbeat Message, acknowledging that the server is running, this message is continuously broadcast by the server on it's status port (this is the only message sent on the status port) Field Type Label Description stats ServerStats optional The current status of the server, optional since status is not available when the server first starts, but a heartbeat is still sent logdir string required The location where the server writes it log files ServerStats Current status of the OLIVE server, sent as part of a Heatbeat message. Field Type Label Description cpu_percent float required The current percentage of CPU used cpu_average float required The average CPU percentage used since the server was started mem_percent float required The percentage of memory used max_mem_percent float required The most memory used since the server was started swap_percent float required The current swap used max_swap_percent float required The max (most) swap space used since the server was started pool_busy uint32 required The number of jobs currently running on the server pool_pending uint32 required The number of jobs queued on the server pool_finished uint32 required The number of jobs completed pool_reinit bool required The number of jobs that need to be re-ran max_num_jobs uint32 optional The max number of concurrent jobs server_version string optional The current version of the server LoadPluginDomainRequest Request a plugin be pre-loaded to optimize later score request(s) Field Type Label Description plugin string required The plugin domain string required The domain LoadPluginDomainResult Acknowledgment that a plugin is being loaded Field Type Label Description successful bool required True if the request is being loaded (but receipt of this message does not guarantee the plugin has finished loaded) RemovePluginDomainRequest Unload (remove from memory) a previously loaded plugin. Use to free resources on the server or force reloading of a plugin/domain Field Type Label Description plugin string required The plugin to remove domain string optional The domain to remove, if omitted all domains removed for this plugin. RemovePluginDomainResult Acknowledgment that a plugin/domain has been removed (unloaded) Field Type Label Description successful bool required True if the plugin was been removed PluginDirectoryRequest Use a PluginDirectoryRequest message to receive the list of plugins available on the server. A Plugin performs tasks such as SAD, LID, SID, or KWS. There may be multiple plugins registered for a given task. A plugin typically has one or more Domains. Plugins contain the code of the recognizer, while Domains correspond to a particular training or adaptation sessions. Domains therefore represent the data/conditions. A plugin and domain together are necessary to perform scoring. PluginDirectoryResult The collection of plugins available on the server, response to PluginDirectoryRequest Field Type Label Description plugins Plugin repeated The available plugins Global Scorer Messages GlobalScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a GlobalScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a GlobalScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide. GlobalScore The global score for a class Field Type Label Description class_id string required The class score float required The score associated with the class confidence float optional An optional confidence value when part of a calibration report comment string optional An optional suggested action when part of a calibration report GlobalScorerRequest Request global scoring using the specified plugin. The plugin must implement the GlobalScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of global scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio optional The audio to analyze/score. Either audio or vector must be set. vector AudioVector optional The preprocessed audio vector to analyze/score. Either audio or vector must be set. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored GlobalScorerResult The result from a GlobalScorerRequest, having zero or more GlobalScore elements Field Type Label Description score GlobalScore repeated The class scores Region Scorer Messages RegionScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a RegionScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a RegionScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide. RegionScore The basic unit of a region score. There may be multiple RegionScore values in a RegionScorerResult Field Type Label Description start_t int32 required Begin-time of the region (in seconds) end_t int32 required End-time of the region (in seconds) class_id string required Class ID associated with region score float optional Optional score associated with the class_id label RegionScorerRequest Request region scoring for the specified plugin/domain. The plugin must implement the RegionScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of region scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored RegionScorerResult The set of region score results, response to RegionScorerRequest Field Type Label Description region RegionScore repeated The scored regions Frame Scorer Messages FrameScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a FrameScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a FrameScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide. FrameScorerRequest Request frame scoring using the specified plugin and audio. The plugin must implement the FrameScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of frame scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored FrameScorerResult The results from a FrameScorerRequest Field Type Label Description result FrameScores repeated List of frame scores by class_id FrameScores The basic unit of a frame score, returned in a FrameScorerRequest Field Type Label Description class_id string required The class ID to which the frame scores pertain frame_rate int32 required The number of frames per second frame_offset double required The offset to the center of the frame 'window' score double repeated The frame-level scores for the class_id Text Transformation Messages TextTransformer is an OLIVE Plugin Trait for scoring using text (instead of audio). For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page. TextTransformationRequest Request the transformation of a text/string using MT Field Type Label Description plugin string required The plugin to invoke domain string required The domain text string optional The string text to analyze/score, Optional as of OLIVE 5.0 since data input(s) can be specified as part of a workflow option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored TextTransformationResult The response to a TextTransformationRequest Field Type Label Description transformation TextTransformation repeated TextTransformation The text value returned in a TextTransformationResult Field Type Label Description class_id string required A classifier for this result, usually just 'text' transformed_text string required The text result Audio Alignment Messages AudioAlignmentScorer is an OLIVE Plugin Trait for alignment of two or more audio inputs. For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page. AudioAlignmentScoreRequest Request the alignment of two or more audio inputs Field Type Label Description plugin string required The plugin to invoke domain string required The domain audios Audio repeated The audio to analyze/score, Optional as of OLIVE 5.0 since Audio can be specified option OptionValue repeated as part of a workflow. If specified there should be two or more audio inputs Any options specified | | class_id | string | repeated | Optionally specify the classes to be scored | AudioAlignmentScoreResult The result of a AudioAlignmentScoreRequest Field Type Label Description scores AudioAlignmentScore repeated AudioAlignmentScore A score in an AudioAlignmentScoreResult Field Type Label Description reference_audio_label string required The source or reference audio name (file 1) other_audio_label string required The name of the audio input in comparison (file 2) shift_offset float required shift offset between the audion in the reference and confidence float required The confidence of this score Global Comparer Messages GlobalComparerReport The visual representation of a global comparison Field Type Label Description type ReportType required The type of report (normally a PDF) report_data bytes required The serialized report GlobalComparerRequest Request the comparison of two audio submission. The plugin must implement the GlobalComparer trait to handle this request Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio_one Audio required One of two audio submissions to analyze/score. audio_two Audio required One of two audio submissions to analyze/score. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored // todo remove! GlobalComparerResult The result of a GlobalComparerRequest Field Type Label Description results Metadata repeated The metadata/scores returned from a global compare analysis report GlobalComparerReport repeated A comparison report generated by the plugin ReportType Possible report formats Name Number Description PDF 1 PNG 2 GIF 3 JPEG 4 TIFF 5 Class Modifier Messages ClassModificationRequest Request a modification of a class for the specified plugin. The plugin must implement the ClassModifier Trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class being enrolled/modified addition Audio repeated List of Audio, action pairs to apply to the class removal Audio repeated List of Audio, action pairs to apply to the class addition_vector AudioVector repeated List of preprocessed audio vector to apply to the class finalize bool optional Whether or not to finalize the class. You can send multiple ClassModificationRequests and only finalize on the last request for efficiency. Default: true option OptionValue repeated Any modification options ClassModificationResult Response to ClassModificationRequest . Field Type Label Description addition_result AudioResult repeated Provides feedback about the success/failure of individual audio additions removal_result AudioResult repeated Provides feedback about the success/failure of individual audio removals vector_addition_result AudioResult repeated Provides feedback about the success/failure of individual audio vector additions ClassRemovalRequest Request removal of a class in the specified plugin/domain Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The id of the class to be removed ClassRemovalResult Acknowledgment that a ClassRemovalRequest was received AudioResult The feedback/description of class modification for a result in a ClassModificationResult message Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred on this audio Audio Converter Messages AudioModification The contents of an AudioModificationResult. Field Type Label Description audio AudioBuffer required The transformed audio message string required Description of how this audio was transformed, or an error description. Not sure if needed? scores Metadata repeated Zero or more scores (metadata) about the modifed audio. Metadata is a list of name/value pairs. AudioModificationRequest Request enhancement (modification) of the submitted audio. The plugin must support the AudioConverter trait to support this request Field Type Label Description plugin string required The plugin domain string required The domain requested_channels uint32 required Convert audio to have this number of channels requested_rate uint32 required Convert audio to this sample rate modifications Audio repeated List of Audio, action pairs to apply to the class - may have to limit to one audio submission per request, not sure how to handle multiple results option OptionValue repeated Any options specified AudioModificationResult The result of an AudioModificationRequest Field Type Label Description successful bool required Whether or not the individual audio modification succeeded modification_result AudioModification repeated Provides feedback about the success/failure of individual audio additions. Audio Vectorizer Messages AudioVector Represents audio preprocessed by a plugin/domain. Field Type Label Description plugin string required The origin plugin domain string required The origin domain data bytes required The audio vector data, varies by plugin params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing PluginAudioVectorRequest Request one or more audio submissions be vectorized (preprocesssed) by the specified plugin. The resulting vectorized audio can only be processed by the same plugin. A plugin must support the AudioVectorizer Trait to support this request. Field Type Label Description plugin string required The plugin domain string required The domain addition Audio repeated List of Audio to process PluginAudioVectorResult The result of a PluginAudioVectorRequest, containing a set of VectorResults Field Type Label Description vector_result VectorResult repeated The results of processing the submitted audio. One result per audio addition. VectorResult The status of the vector request, and if successful includes an AudioVector Field Type Label Description successful bool required Whether or not the audio was successfully processed message string optional Description of what occurred to cause an error audio_vector AudioVector optional If successful, the vectorized audio ClassExportRequest Exports an existing class enrollment (i.e. speaker enrollment) from the server for the specified class_id. The plugin must support the ClassExporter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class model to export ClassExportResult The result of an enrollment export Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred to cause an error enrollment EnrollmentModel optional If successful, then this is the exported model for the specified class. ClassImportRequest Used to import an enrollment model (exported via a ClassExportRequest). Only plugins that support the ClassExporter trait can handle this request. Only import an enrollment into the same plugin AND domain as previously exported. Field Type Label Description plugin string required The plugin domain string required The domain class_id string optional Import the model using this class name, instead of the original name enrollment EnrollmentModel required the enrollment to import ClassImportResult The status of a ClassImportRequest. Field Type Label Description successful bool required Whether or not the import succeeded message string optional Description of what occurred to cause an error EnrollmentModel An enrollment model for a specific plugin and domain. This is used to save a current enrollment or restore a class enrollment via a ClassImportRequest. This model is not used as an AudioVector in a scoring requests. Field Type Label Description plugin string required The origin plugin domain string required The origin domain class_id string required the class_id of the enrollment data bytes required The enrollment model data params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing Updater Messages ApplyUpdateRequest Used to request an update of a Plugin that supports the Update trait. Use GetUpdateStatusRequest to check if a plugin is ready for an update, otherwise this request is ignored by the server/plugin Field Type Label Description plugin string required The plugin to apply the update domain string required The domain params Metadata repeated Name/value options, plugin dependent ApplyUpdateResult This message is immediately returned after an ApplyUpdateRequest, as the updating process can take an extended time range to complete. Field Type Label Description successful bool required True if the plugin is being updated GetUpdateStatusRequest Used to request the status for a Plugin that supports the Update trait Field Type Label Description plugin string required The plugin domain string required The domain GetUpdateStatusResult The result of a GetUpdateStatusRequest message. Field Type Label Description update_ready bool required True if the plugin has determiend it is ready for an update last_update DateTime optional The date of the last update, if any params Metadata repeated Zero or Metadata values describing the update status of the plugin. Metadata is a list of name/value pairs. DateTime Date and time info Field Type Label Description year uint32 required Year month uint32 required Month day uint32 required Day hour uint32 required Hour min uint32 required Minute sec uint32 required Seconds Learning Trait Messages These messages are used by plugins that support adaptation and/or training. PreprocessAudioAdaptRequest Request preprocessing of this audio submission, which may be part of an adaptation set. Adaptation can be unsupervised (neither class_id, start_t, and end_t set) or supervised by setting class_id or class_id, start_t, and end_t. Adaptation should be finalized by calling either SupervisedAdaptationRequest or UnsupervisedAdaptationRequest. Plugins must support either the SupervisedAdapter or UnsupervisedAdapter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain audio Audio required The submitted audio adapt_space string required A unique name for the client where pre-processed data is stored class_id string optional The id of the class annotation being preprocessed (supervised training) start_t uint32 optional Begin-time of the region (in seconds) end_t uint32 optional End-time of the region (in seconds) PreprocessAudioAdaptResult The result of a PreprocessAudioAdaptRequest Field Type Label Description audio_id string required The ID of the preprocessed audio duration double required The duration of the audio Supervised Adapter Messages SupervisedAdaptationRequest Finalize adaptation of the specified plugin/domain using audio preprocessed using calls to PreprocessAudioAdaptRequest. The plugin must implement the SupervisedAdapter trait to handle this request. When you adapt or train, you are creating a new domain for the target plugin, that is based on the domain passed in to the 'domain' field of this call. This new domain is specific to a plugin, so it is created within the plugin, and will be named with the string passed to SupervisedAdaptationRequest as 'new_domain'. To actually use this new domain, future scoring or enrollment requests must specify this new domain name, instead of the original, using the value specified during adaptation as the 'new_domain' field. To have access to this new domain, either restart the server, or send a RemovePluginDomainRequest message to the server, which will force a reload of that plugin. Upon successful completion of SupervisedAdaptationRequest, a SupervisedAdaptationResult message should be received, containing the path to the newly created domain on the server's file system. The file sizes of the actual models will not change as a result of adaptation. Rather, the values stored inside these files will. Field Type Label Description plugin string required The plugin to invoke domain string required The domain to adapt new_domain string required the new domain name class_annotations ClassAnnotation repeated The annotations to use for adaptation, audio annotations are created via PreprocessAudioAdaptRequest calls adapt_space string required A unique name for the client where pre-processed data is stored SupervisedAdaptationResult Acknowledgment message that adaptation successfully completed. Informs the client of the full path of the new domain created by the SupervisedAdaptationRequest . Field Type Label Description new_domain string required Confirmation of the new domain name ClassAnnotation Set of annotations for a class Field Type Label Description class_id string required The class ID (such as speaker name or language name) annotations AudioAnnotation repeated the set of all audio annotations for this class. AudioAnnotation A set of audio annotations for a specific audio submission Field Type Label Description audio_id string required The audio ID returned in a PreprocessAudioAdaptResult or PreprocessAudioTrainResult message regions AnnotationRegion repeated Set of annotations Basic Types Trait A Trait implemented by a plugin Field Type Label Description type TraitType optional The trait type options OptionDefinition repeated Any options specific to this plugin's implementation of the trait Plugin The description of a plugin Field Type Label Description id string optional The id of the plugin task string optional e.g. LID, SID, SAD, KWS, AED, etc. label string optional Display label for plugin desc string optional A brief description of how the plugin works/technologies it employs. vendor string optional A brief description of how the plugin works/technologies it employs. domain Domain repeated The domains owned by this plugin trait Trait repeated The traits (capabilities) of this plugin Domain A description of a domain. Field Type Label Description id string optional The ID of the domain label string optional Display label for the domain desc string optional A brief description of the domain conditions class_id string repeated The list of classes known to this domain Envelope Every message passed between the server and client is an instance of Envelope. An Envelope can contain multiple ScenicMessage instances, so it's important to iterate through them all when you receive an Envelope. Field Type Label Description message ScenicMessage repeated The messages to be sent sender_id string required string description of the message sender ScenicMessage A ScenicMessage represents a single logical message between a client and server. It is placed within an Envelope. It contains nested messages in serialized form. The message_type field is used to determine the type of the nested data. Not all ScenicMessage instance will have message_data, and some may have multiple, but they will all be of the same type. It depends on the value of message_type. Field Type Label Description message_id string required id issued by client (and unique to that client) used to track a request. Any reply for that request will have the same id. message_type MessageType required type of message message_data bytes repeated nested message data that can be deserialized according to message_type. Some messages do not have nested data, some have multiple records error string optional error message; if present an error has occurred on the server info string optional informational message, typically used to explain why message_data is empty but no error is reported MessageType The MessageType enum provides a value for each top-level SCENIC message. It is used within a ScenicMessage to indicate the type of the serialized message contained therein. Name Number Description PLUGIN_DIRECTORY_REQUEST 1 PLUGIN_DIRECTORY_RESULT 2 GLOBAL_SCORER_REQUEST 3 GLOBAL_SCORER_RESULT 4 REGION_SCORER_REQUEST 5 REGION_SCORER_RESULT 6 FRAME_SCORER_REQUEST 7 FRAME_SCORER_RESULT 8 CLASS_MODIFICATION_REQUEST 9 CLASS_MODIFICATION_RESULT 10 CLASS_REMOVAL_REQUEST 11 CLASS_REMOVAL_RESULT 12 GET_ACTIVE_REQUEST 13 GET_ACTIVE_RESULT 14 LOAD_PLUGIN_REQUEST 15 LOAD_PLUGIN_RESULT 16 GET_STATUS_REQUEST 17 GET_STATUS_RESULT 18 HEARTBEAT 19 PREPROCESS_AUDIO_TRAIN_REQUEST 20 PREPROCESS_AUDIO_TRAIN_RESULT 21 PREPROCESS_AUDIO_ADAPT_REQUEST 22 PREPROCESS_AUDIO_ADAPT_RESULT 23 SUPERVISED_TRAINING_REQUEST 24 SUPERVISED_TRAINING_RESULT 25 SUPERVISED_ADAPTATION_REQUEST 26 SUPERVISED_ADAPTATION_RESULT 27 UNSUPERVISED_ADAPTATION_REQUEST 28 UNSUPERVISED_ADAPTATION_RESULT 29 CLASS_ANNOTATION 30 AUDIO_ANNOTATION 31 ANNOTATION_REGION 32 REMOVE_PLUGIN_REQUEST 33 REMOVE_PLUGIN_RESULT 34 AUDIO_MODIFICATION_REQUEST 35 AUDIO_MODIFICATION_RESULT 36 PLUGIN_AUDIO_VECTOR_REQUEST 37 PLUGIN_AUDIO_VECTOR_RESULT 38 CLASS_EXPORT_REQUEST 39 CLASS_EXPORT_RESULT 40 CLASS_IMPORT_REQUEST 41 CLASS_IMPORT_RESULT 42 APPLY_UPDATE_REQUEST 43 APPLY_UPDATE_RESULT 44 GET_UPDATE_STATUS_REQUEST 45 GET_UPDATE_STATUS_RESULT 46 GLOBAL_COMPARER_REQUEST 47 GLOBAL_COMPARER_RESULT 48 | AUDIO_ALIGN_REQUEST | 68 | | | AUDIO_ALIGN_RESULT | 69 | | | TEXT_TRANSFORM_REQUEST | 70 | | | TEXT_TRANSFORM_RESULT | 71 | | | PREPROCESSED_AUDIO_RESULT | 72 | | | INVALID_MESSAGE | 73 | | OptionType Classifies how a OptionDefinition (TraitOption) should be represented in a UI widget Name Number Description BOOLEAN 1 CHOICE 2 TraitType The list of possible traits that a plugin can implement Name Number Description GLOBAL_SCORER 1 REGION_SCORER 2 FRAME_SCORER 3 CLASS_ENROLLER 4 CLASS_MODIFIER 5 SUPERVISED_TRAINER 6 SUPERVISED_ADAPTER 7 UNSUPERVISED_ADAPTER 8 AUDIO_CONVERTER 9 AUDIO_VECTORIZER 10 CLASS_EXPORTER 11 UPDATER 12 LEARNING_TRAIT 13 GLOBAL_COMPARER 14 TEXT_TRANSFORMER 15 AUDIO_ALIGNMENT_SCORER 16 TaskType Name Number Description SAD 1 Speech Activity Detection SID 2 Speaker ID SDD 3 Speaker ID, but output in regions LID 4 Language ID LDD 5 Language ID, but output in regions KWS 6 Keyword Spotting TPD 7 Topic Detection VTD 8 Voice Type Discrimination GID 9 Gender ID GDD 10 Gender ID, but output in regions ASR 11 Automatic Speech Recognition ENH 12 Audio Enhancement CMP 13 Audio Comparison SDD 14 Speaker Detection DIA 15 Diarization QBE 16 Query by Example SHL 17 Speaker Highlighing FID 18 Face ID TMT 19 Text Machine Translation ALN 20 Audio Alignment OptionDefinition A plugin TraitOption, describing how a plugin trait is used Field Type Label Description name string required The name/ID of the option label string required Display label for the option desc string optional A description of the option type OptionType required The type of the option (boolean, choice/drop-down, etc) choice string repeated Optional list of choices used by CHOICE type options default string optional The default option in the list of Options OptionValue A name/value property pair Field Type Label Description name string required The name/ID of the option value string required The option value as a string InputDataType Workflow(?) Input Data Types Name Number Description AUDIO 1 VIDEO 2 TEXT 3 IMAGE 4 InputType Name Number Description FRAME 1 REGION 2 JobClass Field Type Label Description job_name string required The parent job name in a Workflow JobDefinition task TaskClass repeated TaskClass Field Type Label Description task_name string required The ID from the associated WorkflowTask (consumer_result_label) class_id string repeated Zero or more class IDs available to this task. Some tasks do not support classes class_label string optional An optional label/name to describe the classes used by this task such a 'speaker' or 'language' classes_label string optional The speaker label when refering to plural classes, such as speakers, or languages Shared Types Audio Represents an object. Can either refer to a local file or embed an audio buffer directly. The path and audioSamples fields should be treated as mutually exclusive, with one and only one of these fields implemented. For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description path string optional Path to the audio file represented by this record (if not specified then audio is input as a buffer) audioSamples AudioBuffer optional Audio included as a buffer (if not specified, then path must be set) selected_channel uint32 optional Optional - if using multi-channel audio and 'mode' is SELECTED, then this channel is provided to the plugin(s). regions AnnotationRegion repeated Optional annotated regions for this audio label string optional Optional - label used to identify this audio input AnnotationRegion A single pair of timestamps (start and end) that make up an annotated region. Timestamps are in seconds. Field Type Label Description start_t uint32 required Begin-time of the region (in seconds) end_t uint32 required End-time of the region (in seconds) AudioBuffer Audio is contained in a buffer (and the path filed is NOT set in Audio) - by default the audio in the buffer should be PCM encoded, unless the buffer contains a serialized file (unencoded) in which case the serialized_file must be set to true. If the data has been decoded and is not PCM encoded data, then the encoding field must be specified For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description channels uint32 optional The number of channels contained in data, ignored for serialized buffers samples uint32 optional The number of samples (in each channel), ignored for serialized buffers rate uint32 optional The sample rate, ignored for serialized buffers bit_depth AudioBitDepth optional The number of bits in each sample, ignored for serialized buffers data bytes required Should be channels * samples long, striped by channels serialized_file bool optional Optional - true if data contains a serialized buffer encoding AudioEncodingType optional Optional - Not Yet supported - the audio encoding type. Assumed to be PCM if not specified. Ignored for serialized buffer AudioEncodingType Audio encoding types Name Number Description PCMU8 1 PCMS8 2 PCM16 3 PCM24 4 PCM32 5 FLOAT32 6 FLOAT64 7 ULAW 8 ALAW 9 IMA_ADPCM 10 MS_ADPCM 11 GSM610 12 G723_24 13 G721_32 14 DWW12 15 DWW16 16 DWW24 17 VORBIS 18 VOX_ADPCM 19 DPCM16 20 DPCM8 21 AudioBitDepth Number of bits in each audio sample Name Number Description BIT_DEPTH_8 1 BIT_DEPTH_16 2 BIT_DEPTH_24 3 BIT_DEPTH_32 4 Metadata The parent container for Metadata so that typed name/value properties can be transported in a generalized way Field Type Label Description type MetadataType required Indicates the type of this metadata, so it can be deserialized to the appropriate type name string required The name (key) for this metadata value bytes required The value is one of MetadataType, must be deserialized by the client into the type specified by type MetadataType Data types supported in an AudioModificationResult's metadata: Name Number Description STRING_META 1 INTEGER_META 2 DOUBLE_META 3 BOOLEAN_META 4 LIST_META 5 BooleanMetadata Value as boolean Field Type Label Description value bool required DoubleMetadata Value as a double Field Type Label Description value double required IntegerMetadata Value as an integer Field Type Label Description value int32 required ListMetadata Value as list of Metadata values Field Type Label Description type MetadataType repeated The type for the corresponding element value bytes repeated The value is one or more MetadataType elements, each element must be deserialized by the client into the type specified by type. For example, for the type, STRING_META, deserialize data as StringMetadata StringMetadata Value as a string Field Type Label Description value string required Scalar Value Types .proto Type Notes C++ Type Java Type Python Type double double double float float float float float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long uint32 Uses variable-length encoding. uint32 int int/long uint64 Uses variable-length encoding. uint64 long int/long sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long sfixed32 Always four bytes. int32 int int sfixed64 Always eight bytes. int64 long int/long bool bool boolean boolean string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode bytes May contain any arbitrary sequence of bytes. string ByteString str","title":"Enterprise API Message Reference"},{"location":"apiMessage.html#olive-api-message-protocol-documentation","text":"","title":"OLIVE API Message Protocol Documentation"},{"location":"apiMessage.html#oliveproto-protocol-buffer-definitions","text":"The messages defined on this page are what define the OLIVE Enterprise API. These messages are how a client application will interact with and provide tasks to an OLIVE Server. The format used by the OLIVE API is based on the Google Protocol Buffer . For more information on how to integrate the ability to send and receive these messages into a client application using a provided Client API implementation from SRI, refer to the Integrating the (Java) Client API Guide. For more information on creating your own reference implementation with the functionality of these messages, head over to the Creating an API Reference Implementation page that offers guidelines and information.","title":"olive.proto Protocol Buffer Definitions"},{"location":"apiMessage.html#server-management-messages","text":"","title":"Server Management Messages"},{"location":"apiMessage.html#getactiverequest","text":"Message to request the list of ScenicMessages that are still active","title":"GetActiveRequest"},{"location":"apiMessage.html#getactiveresult","text":"Response to GetActiveRequest containing the ScenicMessages that are still active Field Type Label Description message_id string repeated List containing the IDs of each message still being processed on the server total_num string required Total number of messages still being processed","title":"GetActiveResult"},{"location":"apiMessage.html#getstatusrequest","text":"Request a simple server status message, similar to a heartbeat except the request reply is sent on the status port and is requested by the client","title":"GetStatusRequest"},{"location":"apiMessage.html#getstatusresult","text":"The result of a GetStatusRequest Field Type Label Description num_pending uint32 required The number of pending jobs num_busy uint32 required the number of active jobs num_finished uint32 required The number of finished jobs","title":"GetStatusResult"},{"location":"apiMessage.html#heartbeat","text":"A heartbeat Message, acknowledging that the server is running, this message is continuously broadcast by the server on it's status port (this is the only message sent on the status port) Field Type Label Description stats ServerStats optional The current status of the server, optional since status is not available when the server first starts, but a heartbeat is still sent logdir string required The location where the server writes it log files","title":"Heartbeat"},{"location":"apiMessage.html#serverstats","text":"Current status of the OLIVE server, sent as part of a Heatbeat message. Field Type Label Description cpu_percent float required The current percentage of CPU used cpu_average float required The average CPU percentage used since the server was started mem_percent float required The percentage of memory used max_mem_percent float required The most memory used since the server was started swap_percent float required The current swap used max_swap_percent float required The max (most) swap space used since the server was started pool_busy uint32 required The number of jobs currently running on the server pool_pending uint32 required The number of jobs queued on the server pool_finished uint32 required The number of jobs completed pool_reinit bool required The number of jobs that need to be re-ran max_num_jobs uint32 optional The max number of concurrent jobs server_version string optional The current version of the server","title":"ServerStats"},{"location":"apiMessage.html#loadplugindomainrequest","text":"Request a plugin be pre-loaded to optimize later score request(s) Field Type Label Description plugin string required The plugin domain string required The domain","title":"LoadPluginDomainRequest"},{"location":"apiMessage.html#loadplugindomainresult","text":"Acknowledgment that a plugin is being loaded Field Type Label Description successful bool required True if the request is being loaded (but receipt of this message does not guarantee the plugin has finished loaded)","title":"LoadPluginDomainResult"},{"location":"apiMessage.html#removeplugindomainrequest","text":"Unload (remove from memory) a previously loaded plugin. Use to free resources on the server or force reloading of a plugin/domain Field Type Label Description plugin string required The plugin to remove domain string optional The domain to remove, if omitted all domains removed for this plugin.","title":"RemovePluginDomainRequest"},{"location":"apiMessage.html#removeplugindomainresult","text":"Acknowledgment that a plugin/domain has been removed (unloaded) Field Type Label Description successful bool required True if the plugin was been removed","title":"RemovePluginDomainResult"},{"location":"apiMessage.html#plugindirectoryrequest","text":"Use a PluginDirectoryRequest message to receive the list of plugins available on the server. A Plugin performs tasks such as SAD, LID, SID, or KWS. There may be multiple plugins registered for a given task. A plugin typically has one or more Domains. Plugins contain the code of the recognizer, while Domains correspond to a particular training or adaptation sessions. Domains therefore represent the data/conditions. A plugin and domain together are necessary to perform scoring.","title":"PluginDirectoryRequest"},{"location":"apiMessage.html#plugindirectoryresult","text":"The collection of plugins available on the server, response to PluginDirectoryRequest Field Type Label Description plugins Plugin repeated The available plugins","title":"PluginDirectoryResult"},{"location":"apiMessage.html#global-scorer-messages","text":"GlobalScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a GlobalScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a GlobalScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide.","title":"Global Scorer Messages"},{"location":"apiMessage.html#globalscore","text":"The global score for a class Field Type Label Description class_id string required The class score float required The score associated with the class confidence float optional An optional confidence value when part of a calibration report comment string optional An optional suggested action when part of a calibration report","title":"GlobalScore"},{"location":"apiMessage.html#globalscorerrequest","text":"Request global scoring using the specified plugin. The plugin must implement the GlobalScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of global scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio optional The audio to analyze/score. Either audio or vector must be set. vector AudioVector optional The preprocessed audio vector to analyze/score. Either audio or vector must be set. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"GlobalScorerRequest"},{"location":"apiMessage.html#globalscorerresult","text":"The result from a GlobalScorerRequest, having zero or more GlobalScore elements Field Type Label Description score GlobalScore repeated The class scores","title":"GlobalScorerResult"},{"location":"apiMessage.html#region-scorer-messages","text":"RegionScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a RegionScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a RegionScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide.","title":"Region Scorer Messages"},{"location":"apiMessage.html#regionscore","text":"The basic unit of a region score. There may be multiple RegionScore values in a RegionScorerResult Field Type Label Description start_t int32 required Begin-time of the region (in seconds) end_t int32 required End-time of the region (in seconds) class_id string required Class ID associated with region score float optional Optional score associated with the class_id label","title":"RegionScore"},{"location":"apiMessage.html#regionscorerrequest","text":"Request region scoring for the specified plugin/domain. The plugin must implement the RegionScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of region scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"RegionScorerRequest"},{"location":"apiMessage.html#regionscorerresult","text":"The set of region score results, response to RegionScorerRequest Field Type Label Description region RegionScore repeated The scored regions","title":"RegionScorerResult"},{"location":"apiMessage.html#frame-scorer-messages","text":"FrameScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a FrameScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a FrameScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide.","title":"Frame Scorer Messages"},{"location":"apiMessage.html#framescorerrequest","text":"Request frame scoring using the specified plugin and audio. The plugin must implement the FrameScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of frame scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"FrameScorerRequest"},{"location":"apiMessage.html#framescorerresult","text":"The results from a FrameScorerRequest Field Type Label Description result FrameScores repeated List of frame scores by class_id","title":"FrameScorerResult"},{"location":"apiMessage.html#framescores","text":"The basic unit of a frame score, returned in a FrameScorerRequest Field Type Label Description class_id string required The class ID to which the frame scores pertain frame_rate int32 required The number of frames per second frame_offset double required The offset to the center of the frame 'window' score double repeated The frame-level scores for the class_id","title":"FrameScores"},{"location":"apiMessage.html#text-transformation-messages","text":"TextTransformer is an OLIVE Plugin Trait for scoring using text (instead of audio). For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page.","title":"Text Transformation Messages"},{"location":"apiMessage.html#texttransformationrequest","text":"Request the transformation of a text/string using MT Field Type Label Description plugin string required The plugin to invoke domain string required The domain text string optional The string text to analyze/score, Optional as of OLIVE 5.0 since data input(s) can be specified as part of a workflow option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"TextTransformationRequest"},{"location":"apiMessage.html#texttransformationresult","text":"The response to a TextTransformationRequest Field Type Label Description transformation TextTransformation repeated","title":"TextTransformationResult"},{"location":"apiMessage.html#texttransformation","text":"The text value returned in a TextTransformationResult Field Type Label Description class_id string required A classifier for this result, usually just 'text' transformed_text string required The text result","title":"TextTransformation"},{"location":"apiMessage.html#audio-alignment-messages","text":"AudioAlignmentScorer is an OLIVE Plugin Trait for alignment of two or more audio inputs. For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page.","title":"Audio Alignment  Messages"},{"location":"apiMessage.html#audioalignmentscorerequest","text":"Request the alignment of two or more audio inputs Field Type Label Description plugin string required The plugin to invoke domain string required The domain audios Audio repeated The audio to analyze/score, Optional as of OLIVE 5.0 since Audio can be specified option OptionValue repeated as part of a workflow. If specified there should be two or more audio inputs Any options specified | | class_id | string | repeated | Optionally specify the classes to be scored |","title":"AudioAlignmentScoreRequest"},{"location":"apiMessage.html#audioalignmentscoreresult","text":"The result of a AudioAlignmentScoreRequest Field Type Label Description scores AudioAlignmentScore repeated","title":"AudioAlignmentScoreResult"},{"location":"apiMessage.html#audioalignmentscore","text":"A score in an AudioAlignmentScoreResult Field Type Label Description reference_audio_label string required The source or reference audio name (file 1) other_audio_label string required The name of the audio input in comparison (file 2) shift_offset float required shift offset between the audion in the reference and confidence float required The confidence of this score","title":"AudioAlignmentScore"},{"location":"apiMessage.html#global-comparer-messages","text":"","title":"Global Comparer Messages"},{"location":"apiMessage.html#globalcomparerreport","text":"The visual representation of a global comparison Field Type Label Description type ReportType required The type of report (normally a PDF) report_data bytes required The serialized report","title":"GlobalComparerReport"},{"location":"apiMessage.html#globalcomparerrequest","text":"Request the comparison of two audio submission. The plugin must implement the GlobalComparer trait to handle this request Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio_one Audio required One of two audio submissions to analyze/score. audio_two Audio required One of two audio submissions to analyze/score. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored // todo remove!","title":"GlobalComparerRequest"},{"location":"apiMessage.html#globalcomparerresult","text":"The result of a GlobalComparerRequest Field Type Label Description results Metadata repeated The metadata/scores returned from a global compare analysis report GlobalComparerReport repeated A comparison report generated by the plugin","title":"GlobalComparerResult"},{"location":"apiMessage.html#reporttype","text":"Possible report formats Name Number Description PDF 1 PNG 2 GIF 3 JPEG 4 TIFF 5","title":"ReportType"},{"location":"apiMessage.html#class-modifier-messages","text":"","title":"Class Modifier Messages"},{"location":"apiMessage.html#classmodificationrequest","text":"Request a modification of a class for the specified plugin. The plugin must implement the ClassModifier Trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class being enrolled/modified addition Audio repeated List of Audio, action pairs to apply to the class removal Audio repeated List of Audio, action pairs to apply to the class addition_vector AudioVector repeated List of preprocessed audio vector to apply to the class finalize bool optional Whether or not to finalize the class. You can send multiple ClassModificationRequests and only finalize on the last request for efficiency. Default: true option OptionValue repeated Any modification options","title":"ClassModificationRequest"},{"location":"apiMessage.html#classmodificationresult","text":"Response to ClassModificationRequest . Field Type Label Description addition_result AudioResult repeated Provides feedback about the success/failure of individual audio additions removal_result AudioResult repeated Provides feedback about the success/failure of individual audio removals vector_addition_result AudioResult repeated Provides feedback about the success/failure of individual audio vector additions","title":"ClassModificationResult"},{"location":"apiMessage.html#classremovalrequest","text":"Request removal of a class in the specified plugin/domain Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The id of the class to be removed","title":"ClassRemovalRequest"},{"location":"apiMessage.html#classremovalresult","text":"Acknowledgment that a ClassRemovalRequest was received","title":"ClassRemovalResult"},{"location":"apiMessage.html#audioresult","text":"The feedback/description of class modification for a result in a ClassModificationResult message Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred on this audio","title":"AudioResult"},{"location":"apiMessage.html#audio-converter-messages","text":"","title":"Audio Converter Messages"},{"location":"apiMessage.html#audiomodification","text":"The contents of an AudioModificationResult. Field Type Label Description audio AudioBuffer required The transformed audio message string required Description of how this audio was transformed, or an error description. Not sure if needed? scores Metadata repeated Zero or more scores (metadata) about the modifed audio. Metadata is a list of name/value pairs.","title":"AudioModification"},{"location":"apiMessage.html#audiomodificationrequest","text":"Request enhancement (modification) of the submitted audio. The plugin must support the AudioConverter trait to support this request Field Type Label Description plugin string required The plugin domain string required The domain requested_channels uint32 required Convert audio to have this number of channels requested_rate uint32 required Convert audio to this sample rate modifications Audio repeated List of Audio, action pairs to apply to the class - may have to limit to one audio submission per request, not sure how to handle multiple results option OptionValue repeated Any options specified","title":"AudioModificationRequest"},{"location":"apiMessage.html#audiomodificationresult","text":"The result of an AudioModificationRequest Field Type Label Description successful bool required Whether or not the individual audio modification succeeded modification_result AudioModification repeated Provides feedback about the success/failure of individual audio additions.","title":"AudioModificationResult"},{"location":"apiMessage.html#audio-vectorizer-messages","text":"","title":"Audio Vectorizer Messages"},{"location":"apiMessage.html#audiovector","text":"Represents audio preprocessed by a plugin/domain. Field Type Label Description plugin string required The origin plugin domain string required The origin domain data bytes required The audio vector data, varies by plugin params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing","title":"AudioVector"},{"location":"apiMessage.html#pluginaudiovectorrequest","text":"Request one or more audio submissions be vectorized (preprocesssed) by the specified plugin. The resulting vectorized audio can only be processed by the same plugin. A plugin must support the AudioVectorizer Trait to support this request. Field Type Label Description plugin string required The plugin domain string required The domain addition Audio repeated List of Audio to process","title":"PluginAudioVectorRequest"},{"location":"apiMessage.html#pluginaudiovectorresult","text":"The result of a PluginAudioVectorRequest, containing a set of VectorResults Field Type Label Description vector_result VectorResult repeated The results of processing the submitted audio. One result per audio addition.","title":"PluginAudioVectorResult"},{"location":"apiMessage.html#vectorresult","text":"The status of the vector request, and if successful includes an AudioVector Field Type Label Description successful bool required Whether or not the audio was successfully processed message string optional Description of what occurred to cause an error audio_vector AudioVector optional If successful, the vectorized audio","title":"VectorResult"},{"location":"apiMessage.html#classexportrequest","text":"Exports an existing class enrollment (i.e. speaker enrollment) from the server for the specified class_id. The plugin must support the ClassExporter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class model to export","title":"ClassExportRequest"},{"location":"apiMessage.html#classexportresult","text":"The result of an enrollment export Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred to cause an error enrollment EnrollmentModel optional If successful, then this is the exported model for the specified class.","title":"ClassExportResult"},{"location":"apiMessage.html#classimportrequest","text":"Used to import an enrollment model (exported via a ClassExportRequest). Only plugins that support the ClassExporter trait can handle this request. Only import an enrollment into the same plugin AND domain as previously exported. Field Type Label Description plugin string required The plugin domain string required The domain class_id string optional Import the model using this class name, instead of the original name enrollment EnrollmentModel required the enrollment to import","title":"ClassImportRequest"},{"location":"apiMessage.html#classimportresult","text":"The status of a ClassImportRequest. Field Type Label Description successful bool required Whether or not the import succeeded message string optional Description of what occurred to cause an error","title":"ClassImportResult"},{"location":"apiMessage.html#enrollmentmodel","text":"An enrollment model for a specific plugin and domain. This is used to save a current enrollment or restore a class enrollment via a ClassImportRequest. This model is not used as an AudioVector in a scoring requests. Field Type Label Description plugin string required The origin plugin domain string required The origin domain class_id string required the class_id of the enrollment data bytes required The enrollment model data params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing","title":"EnrollmentModel"},{"location":"apiMessage.html#updater-messages","text":"","title":"Updater Messages"},{"location":"apiMessage.html#applyupdaterequest","text":"Used to request an update of a Plugin that supports the Update trait. Use GetUpdateStatusRequest to check if a plugin is ready for an update, otherwise this request is ignored by the server/plugin Field Type Label Description plugin string required The plugin to apply the update domain string required The domain params Metadata repeated Name/value options, plugin dependent","title":"ApplyUpdateRequest"},{"location":"apiMessage.html#applyupdateresult","text":"This message is immediately returned after an ApplyUpdateRequest, as the updating process can take an extended time range to complete. Field Type Label Description successful bool required True if the plugin is being updated","title":"ApplyUpdateResult"},{"location":"apiMessage.html#getupdatestatusrequest","text":"Used to request the status for a Plugin that supports the Update trait Field Type Label Description plugin string required The plugin domain string required The domain","title":"GetUpdateStatusRequest"},{"location":"apiMessage.html#getupdatestatusresult","text":"The result of a GetUpdateStatusRequest message. Field Type Label Description update_ready bool required True if the plugin has determiend it is ready for an update last_update DateTime optional The date of the last update, if any params Metadata repeated Zero or Metadata values describing the update status of the plugin. Metadata is a list of name/value pairs.","title":"GetUpdateStatusResult"},{"location":"apiMessage.html#datetime","text":"Date and time info Field Type Label Description year uint32 required Year month uint32 required Month day uint32 required Day hour uint32 required Hour min uint32 required Minute sec uint32 required Seconds","title":"DateTime"},{"location":"apiMessage.html#learning-trait-messages","text":"These messages are used by plugins that support adaptation and/or training.","title":"Learning Trait Messages"},{"location":"apiMessage.html#preprocessaudioadaptrequest","text":"Request preprocessing of this audio submission, which may be part of an adaptation set. Adaptation can be unsupervised (neither class_id, start_t, and end_t set) or supervised by setting class_id or class_id, start_t, and end_t. Adaptation should be finalized by calling either SupervisedAdaptationRequest or UnsupervisedAdaptationRequest. Plugins must support either the SupervisedAdapter or UnsupervisedAdapter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain audio Audio required The submitted audio adapt_space string required A unique name for the client where pre-processed data is stored class_id string optional The id of the class annotation being preprocessed (supervised training) start_t uint32 optional Begin-time of the region (in seconds) end_t uint32 optional End-time of the region (in seconds)","title":"PreprocessAudioAdaptRequest"},{"location":"apiMessage.html#preprocessaudioadaptresult","text":"The result of a PreprocessAudioAdaptRequest Field Type Label Description audio_id string required The ID of the preprocessed audio duration double required The duration of the audio","title":"PreprocessAudioAdaptResult"},{"location":"apiMessage.html#supervised-adapter-messages","text":"","title":"Supervised Adapter Messages"},{"location":"apiMessage.html#supervisedadaptationrequest","text":"Finalize adaptation of the specified plugin/domain using audio preprocessed using calls to PreprocessAudioAdaptRequest. The plugin must implement the SupervisedAdapter trait to handle this request. When you adapt or train, you are creating a new domain for the target plugin, that is based on the domain passed in to the 'domain' field of this call. This new domain is specific to a plugin, so it is created within the plugin, and will be named with the string passed to SupervisedAdaptationRequest as 'new_domain'. To actually use this new domain, future scoring or enrollment requests must specify this new domain name, instead of the original, using the value specified during adaptation as the 'new_domain' field. To have access to this new domain, either restart the server, or send a RemovePluginDomainRequest message to the server, which will force a reload of that plugin. Upon successful completion of SupervisedAdaptationRequest, a SupervisedAdaptationResult message should be received, containing the path to the newly created domain on the server's file system. The file sizes of the actual models will not change as a result of adaptation. Rather, the values stored inside these files will. Field Type Label Description plugin string required The plugin to invoke domain string required The domain to adapt new_domain string required the new domain name class_annotations ClassAnnotation repeated The annotations to use for adaptation, audio annotations are created via PreprocessAudioAdaptRequest calls adapt_space string required A unique name for the client where pre-processed data is stored","title":"SupervisedAdaptationRequest"},{"location":"apiMessage.html#supervisedadaptationresult","text":"Acknowledgment message that adaptation successfully completed. Informs the client of the full path of the new domain created by the SupervisedAdaptationRequest . Field Type Label Description new_domain string required Confirmation of the new domain name","title":"SupervisedAdaptationResult"},{"location":"apiMessage.html#classannotation","text":"Set of annotations for a class Field Type Label Description class_id string required The class ID (such as speaker name or language name) annotations AudioAnnotation repeated the set of all audio annotations for this class.","title":"ClassAnnotation"},{"location":"apiMessage.html#audioannotation","text":"A set of audio annotations for a specific audio submission Field Type Label Description audio_id string required The audio ID returned in a PreprocessAudioAdaptResult or PreprocessAudioTrainResult message regions AnnotationRegion repeated Set of annotations","title":"AudioAnnotation"},{"location":"apiMessage.html#basic-types","text":"","title":"Basic Types"},{"location":"apiMessage.html#trait","text":"A Trait implemented by a plugin Field Type Label Description type TraitType optional The trait type options OptionDefinition repeated Any options specific to this plugin's implementation of the trait","title":"Trait"},{"location":"apiMessage.html#plugin","text":"The description of a plugin Field Type Label Description id string optional The id of the plugin task string optional e.g. LID, SID, SAD, KWS, AED, etc. label string optional Display label for plugin desc string optional A brief description of how the plugin works/technologies it employs. vendor string optional A brief description of how the plugin works/technologies it employs. domain Domain repeated The domains owned by this plugin trait Trait repeated The traits (capabilities) of this plugin","title":"Plugin"},{"location":"apiMessage.html#domain","text":"A description of a domain. Field Type Label Description id string optional The ID of the domain label string optional Display label for the domain desc string optional A brief description of the domain conditions class_id string repeated The list of classes known to this domain","title":"Domain"},{"location":"apiMessage.html#envelope","text":"Every message passed between the server and client is an instance of Envelope. An Envelope can contain multiple ScenicMessage instances, so it's important to iterate through them all when you receive an Envelope. Field Type Label Description message ScenicMessage repeated The messages to be sent sender_id string required string description of the message sender","title":"Envelope"},{"location":"apiMessage.html#scenicmessage","text":"A ScenicMessage represents a single logical message between a client and server. It is placed within an Envelope. It contains nested messages in serialized form. The message_type field is used to determine the type of the nested data. Not all ScenicMessage instance will have message_data, and some may have multiple, but they will all be of the same type. It depends on the value of message_type. Field Type Label Description message_id string required id issued by client (and unique to that client) used to track a request. Any reply for that request will have the same id. message_type MessageType required type of message message_data bytes repeated nested message data that can be deserialized according to message_type. Some messages do not have nested data, some have multiple records error string optional error message; if present an error has occurred on the server info string optional informational message, typically used to explain why message_data is empty but no error is reported","title":"ScenicMessage"},{"location":"apiMessage.html#messagetype","text":"The MessageType enum provides a value for each top-level SCENIC message. It is used within a ScenicMessage to indicate the type of the serialized message contained therein. Name Number Description PLUGIN_DIRECTORY_REQUEST 1 PLUGIN_DIRECTORY_RESULT 2 GLOBAL_SCORER_REQUEST 3 GLOBAL_SCORER_RESULT 4 REGION_SCORER_REQUEST 5 REGION_SCORER_RESULT 6 FRAME_SCORER_REQUEST 7 FRAME_SCORER_RESULT 8 CLASS_MODIFICATION_REQUEST 9 CLASS_MODIFICATION_RESULT 10 CLASS_REMOVAL_REQUEST 11 CLASS_REMOVAL_RESULT 12 GET_ACTIVE_REQUEST 13 GET_ACTIVE_RESULT 14 LOAD_PLUGIN_REQUEST 15 LOAD_PLUGIN_RESULT 16 GET_STATUS_REQUEST 17 GET_STATUS_RESULT 18 HEARTBEAT 19 PREPROCESS_AUDIO_TRAIN_REQUEST 20 PREPROCESS_AUDIO_TRAIN_RESULT 21 PREPROCESS_AUDIO_ADAPT_REQUEST 22 PREPROCESS_AUDIO_ADAPT_RESULT 23 SUPERVISED_TRAINING_REQUEST 24 SUPERVISED_TRAINING_RESULT 25 SUPERVISED_ADAPTATION_REQUEST 26 SUPERVISED_ADAPTATION_RESULT 27 UNSUPERVISED_ADAPTATION_REQUEST 28 UNSUPERVISED_ADAPTATION_RESULT 29 CLASS_ANNOTATION 30 AUDIO_ANNOTATION 31 ANNOTATION_REGION 32 REMOVE_PLUGIN_REQUEST 33 REMOVE_PLUGIN_RESULT 34 AUDIO_MODIFICATION_REQUEST 35 AUDIO_MODIFICATION_RESULT 36 PLUGIN_AUDIO_VECTOR_REQUEST 37 PLUGIN_AUDIO_VECTOR_RESULT 38 CLASS_EXPORT_REQUEST 39 CLASS_EXPORT_RESULT 40 CLASS_IMPORT_REQUEST 41 CLASS_IMPORT_RESULT 42 APPLY_UPDATE_REQUEST 43 APPLY_UPDATE_RESULT 44 GET_UPDATE_STATUS_REQUEST 45 GET_UPDATE_STATUS_RESULT 46 GLOBAL_COMPARER_REQUEST 47 GLOBAL_COMPARER_RESULT 48 | AUDIO_ALIGN_REQUEST | 68 | | | AUDIO_ALIGN_RESULT | 69 | | | TEXT_TRANSFORM_REQUEST | 70 | | | TEXT_TRANSFORM_RESULT | 71 | | | PREPROCESSED_AUDIO_RESULT | 72 | | | INVALID_MESSAGE | 73 | |","title":"MessageType"},{"location":"apiMessage.html#optiontype","text":"Classifies how a OptionDefinition (TraitOption) should be represented in a UI widget Name Number Description BOOLEAN 1 CHOICE 2","title":"OptionType"},{"location":"apiMessage.html#traittype","text":"The list of possible traits that a plugin can implement Name Number Description GLOBAL_SCORER 1 REGION_SCORER 2 FRAME_SCORER 3 CLASS_ENROLLER 4 CLASS_MODIFIER 5 SUPERVISED_TRAINER 6 SUPERVISED_ADAPTER 7 UNSUPERVISED_ADAPTER 8 AUDIO_CONVERTER 9 AUDIO_VECTORIZER 10 CLASS_EXPORTER 11 UPDATER 12 LEARNING_TRAIT 13 GLOBAL_COMPARER 14 TEXT_TRANSFORMER 15 AUDIO_ALIGNMENT_SCORER 16","title":"TraitType"},{"location":"apiMessage.html#tasktype","text":"Name Number Description SAD 1 Speech Activity Detection SID 2 Speaker ID SDD 3 Speaker ID, but output in regions LID 4 Language ID LDD 5 Language ID, but output in regions KWS 6 Keyword Spotting TPD 7 Topic Detection VTD 8 Voice Type Discrimination GID 9 Gender ID GDD 10 Gender ID, but output in regions ASR 11 Automatic Speech Recognition ENH 12 Audio Enhancement CMP 13 Audio Comparison SDD 14 Speaker Detection DIA 15 Diarization QBE 16 Query by Example SHL 17 Speaker Highlighing FID 18 Face ID TMT 19 Text Machine Translation ALN 20 Audio Alignment","title":"TaskType"},{"location":"apiMessage.html#optiondefinition","text":"A plugin TraitOption, describing how a plugin trait is used Field Type Label Description name string required The name/ID of the option label string required Display label for the option desc string optional A description of the option type OptionType required The type of the option (boolean, choice/drop-down, etc) choice string repeated Optional list of choices used by CHOICE type options default string optional The default option in the list of Options","title":"OptionDefinition"},{"location":"apiMessage.html#optionvalue","text":"A name/value property pair Field Type Label Description name string required The name/ID of the option value string required The option value as a string","title":"OptionValue"},{"location":"apiMessage.html#inputdatatype","text":"Workflow(?) Input Data Types Name Number Description AUDIO 1 VIDEO 2 TEXT 3 IMAGE 4","title":"InputDataType"},{"location":"apiMessage.html#inputtype","text":"Name Number Description FRAME 1 REGION 2","title":"InputType"},{"location":"apiMessage.html#jobclass","text":"Field Type Label Description job_name string required The parent job name in a Workflow JobDefinition task TaskClass repeated","title":"JobClass"},{"location":"apiMessage.html#taskclass","text":"Field Type Label Description task_name string required The ID from the associated WorkflowTask (consumer_result_label) class_id string repeated Zero or more class IDs available to this task. Some tasks do not support classes class_label string optional An optional label/name to describe the classes used by this task such a 'speaker' or 'language' classes_label string optional The speaker label when refering to plural classes, such as speakers, or languages","title":"TaskClass"},{"location":"apiMessage.html#shared-types","text":"","title":"Shared Types"},{"location":"apiMessage.html#audio","text":"Represents an object. Can either refer to a local file or embed an audio buffer directly. The path and audioSamples fields should be treated as mutually exclusive, with one and only one of these fields implemented. For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description path string optional Path to the audio file represented by this record (if not specified then audio is input as a buffer) audioSamples AudioBuffer optional Audio included as a buffer (if not specified, then path must be set) selected_channel uint32 optional Optional - if using multi-channel audio and 'mode' is SELECTED, then this channel is provided to the plugin(s). regions AnnotationRegion repeated Optional annotated regions for this audio label string optional Optional - label used to identify this audio input","title":"Audio"},{"location":"apiMessage.html#annotationregion","text":"A single pair of timestamps (start and end) that make up an annotated region. Timestamps are in seconds. Field Type Label Description start_t uint32 required Begin-time of the region (in seconds) end_t uint32 required End-time of the region (in seconds)","title":"AnnotationRegion"},{"location":"apiMessage.html#audiobuffer","text":"Audio is contained in a buffer (and the path filed is NOT set in Audio) - by default the audio in the buffer should be PCM encoded, unless the buffer contains a serialized file (unencoded) in which case the serialized_file must be set to true. If the data has been decoded and is not PCM encoded data, then the encoding field must be specified For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description channels uint32 optional The number of channels contained in data, ignored for serialized buffers samples uint32 optional The number of samples (in each channel), ignored for serialized buffers rate uint32 optional The sample rate, ignored for serialized buffers bit_depth AudioBitDepth optional The number of bits in each sample, ignored for serialized buffers data bytes required Should be channels * samples long, striped by channels serialized_file bool optional Optional - true if data contains a serialized buffer encoding AudioEncodingType optional Optional - Not Yet supported - the audio encoding type. Assumed to be PCM if not specified. Ignored for serialized buffer","title":"AudioBuffer"},{"location":"apiMessage.html#audioencodingtype","text":"Audio encoding types Name Number Description PCMU8 1 PCMS8 2 PCM16 3 PCM24 4 PCM32 5 FLOAT32 6 FLOAT64 7 ULAW 8 ALAW 9 IMA_ADPCM 10 MS_ADPCM 11 GSM610 12 G723_24 13 G721_32 14 DWW12 15 DWW16 16 DWW24 17 VORBIS 18 VOX_ADPCM 19 DPCM16 20 DPCM8 21","title":"AudioEncodingType"},{"location":"apiMessage.html#audiobitdepth","text":"Number of bits in each audio sample Name Number Description BIT_DEPTH_8 1 BIT_DEPTH_16 2 BIT_DEPTH_24 3 BIT_DEPTH_32 4","title":"AudioBitDepth"},{"location":"apiMessage.html#metadata","text":"The parent container for Metadata so that typed name/value properties can be transported in a generalized way Field Type Label Description type MetadataType required Indicates the type of this metadata, so it can be deserialized to the appropriate type name string required The name (key) for this metadata value bytes required The value is one of MetadataType, must be deserialized by the client into the type specified by type","title":"Metadata"},{"location":"apiMessage.html#metadatatype","text":"Data types supported in an AudioModificationResult's metadata: Name Number Description STRING_META 1 INTEGER_META 2 DOUBLE_META 3 BOOLEAN_META 4 LIST_META 5","title":"MetadataType"},{"location":"apiMessage.html#booleanmetadata","text":"Value as boolean Field Type Label Description value bool required","title":"BooleanMetadata"},{"location":"apiMessage.html#doublemetadata","text":"Value as a double Field Type Label Description value double required","title":"DoubleMetadata"},{"location":"apiMessage.html#integermetadata","text":"Value as an integer Field Type Label Description value int32 required","title":"IntegerMetadata"},{"location":"apiMessage.html#listmetadata","text":"Value as list of Metadata values Field Type Label Description type MetadataType repeated The type for the corresponding element value bytes repeated The value is one or more MetadataType elements, each element must be deserialized by the client into the type specified by type. For example, for the type, STRING_META, deserialize data as StringMetadata","title":"ListMetadata"},{"location":"apiMessage.html#stringmetadata","text":"Value as a string Field Type Label Description value string required","title":"StringMetadata"},{"location":"apiMessage.html#scalar-value-types","text":".proto Type Notes C++ Type Java Type Python Type double double double float float float float float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long uint32 Uses variable-length encoding. uint32 int int/long uint64 Uses variable-length encoding. uint64 long int/long sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long sfixed32 Always four bytes. int32 int int sfixed64 Always eight bytes. int64 long int/long bool bool boolean boolean string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode bytes May contain any arbitrary sequence of bytes. string ByteString str","title":"Scalar Value Types"},{"location":"audioFormats.html","text":"OLIVE Supported Audio Formats Overview There are five main methods of interacting with the OLIVE system that carry different implications with respect to the support audio formats. They are as follows: NIGHTINGALE (Forensic) GUI \u2013 Submitting audio files using the Nightingale GUI, also known as the Forensic GUI. OLIVE (Batch) GUI \u2013 Submitting audio files using the OLIVE Batch GUI, also known as the Batch GUI or SCENIC Batch GUI, for submitting large file-based audio processing jobs. COMMAND LINE INTERFACE (CLI) TOOLS \u2013 submitting audio through the localenroll, localanalyze, localtrain command line tools. OLIVE API (BUFFERED) \u2013 sending pre-loaded memory buffers of audio samples to the server through the OLIVE API. OLIVE API (SERIALIZED) \u2013 sending a serialized object to the server that consists of an entire audio file with its header intact. These interaction methods can be combined into three groups that share limitations: Local file-based processing by server. Dictates compatibility for 2. OLIVE Batch GUI , 3. Command Line Interface Tools , and 5. OLIVE API (Serialized) . Audio files being opened and processed by Java. Dictates compatibility for 1. Nightingale GUI . Audio samples buffered into memory. Dictates compatibility for 4. OLIVE API (Buffered) . The limitations of each group are defined below. Local file-based processing by server The audio file compatibility for this group of OLIVE interactions is dictated by the libsndfile for reading and writing audio files. All files submitted to the OLIVE Batch GUI, through the localenroll, localanalyze, and localtrain command line tools, or as Serialized files through the OLIVE API can be of any audio file format and type supported by the libsndfile package. Stereo files are supported, but are merged into a single channel before scoring when submitting to the OLIVE Batch GUI and Command Line Interface Tools. When submitting files as serialized objects through the API, there is flexibility regarding how the channels are processed \u2013 please refer to the API Documentation for more details. Supported audio formats include: Microsoft WAV SGI/Apple AIFF/AIFC Sun AU/Snd Raw (headerless) Paris Audio File (PAF) Commodore IFF/SVX Sphere/NIST WAV IRCAM SF Creative VOC SoundForge W64 GNU Octave MAT4.4 Portable Voice Format Fasttracker 2 XI HMM Tool Kit HTK Apple CAF Sound Designer II SD2 Free Lossless Audio Codec (FLAC) Supported encodings vary by the format used (see the link below for a comprehensive compatibility table), but samples of several supported encodings are as follows: Unsigned and signed 8, 16, 24 and 32 bit PCM IEEE 32 and 64 floating point U-LAW A-LAW IMA ADPCM MS ADPCM GSM 6.10 G721/723 ADPCM 12/16/24 bit DWVW OK Dialogic ADPCM 8/16 DPCM More information on libsndfile supported audio formats can be found here: http://www.mega-nerd.com/libsndfile/#Features Audio files being opened and processed by Java Any files being opened in the Nightingale GUI for close analysis work must be able to be opened and read by the underlying Java code libraries: Java Media Framework JavaX.Sound FLAC Only mono files are currently supported. Supported sample rates include: 8 kHz multiples to 48 kHz 11.025 kHz multiples to 44.1 kHz Supported container formats: FLAC RIFF (.wav) AIFF AIFC AU Supported encoding formats: - Compressed: * FLAC - PCM: * 16 bit signed int, big or little endian * 8 bit signed or unsigned int * 32 bit float, little endian only (i.e. RIFF, not RIFX) * 8 bit mulaw or alaw - ADPCM: * Microsoft or IMA ADPCM Audio samples buffered into memory Buffered audio samples being sent to the OLIVE server for enrollment or scoring are read and processed under the assumption that they are raw 16-bit Linear PCM sampes at an 8 kHz sampling rate.","title":"Supported Audio Formats"},{"location":"audioFormats.html#olive-supported-audio-formats","text":"","title":"OLIVE Supported Audio Formats"},{"location":"audioFormats.html#overview","text":"There are five main methods of interacting with the OLIVE system that carry different implications with respect to the support audio formats. They are as follows: NIGHTINGALE (Forensic) GUI \u2013 Submitting audio files using the Nightingale GUI, also known as the Forensic GUI. OLIVE (Batch) GUI \u2013 Submitting audio files using the OLIVE Batch GUI, also known as the Batch GUI or SCENIC Batch GUI, for submitting large file-based audio processing jobs. COMMAND LINE INTERFACE (CLI) TOOLS \u2013 submitting audio through the localenroll, localanalyze, localtrain command line tools. OLIVE API (BUFFERED) \u2013 sending pre-loaded memory buffers of audio samples to the server through the OLIVE API. OLIVE API (SERIALIZED) \u2013 sending a serialized object to the server that consists of an entire audio file with its header intact. These interaction methods can be combined into three groups that share limitations: Local file-based processing by server. Dictates compatibility for 2. OLIVE Batch GUI , 3. Command Line Interface Tools , and 5. OLIVE API (Serialized) . Audio files being opened and processed by Java. Dictates compatibility for 1. Nightingale GUI . Audio samples buffered into memory. Dictates compatibility for 4. OLIVE API (Buffered) . The limitations of each group are defined below.","title":"Overview"},{"location":"audioFormats.html#local-file-based-processing-by-server","text":"The audio file compatibility for this group of OLIVE interactions is dictated by the libsndfile for reading and writing audio files. All files submitted to the OLIVE Batch GUI, through the localenroll, localanalyze, and localtrain command line tools, or as Serialized files through the OLIVE API can be of any audio file format and type supported by the libsndfile package. Stereo files are supported, but are merged into a single channel before scoring when submitting to the OLIVE Batch GUI and Command Line Interface Tools. When submitting files as serialized objects through the API, there is flexibility regarding how the channels are processed \u2013 please refer to the API Documentation for more details. Supported audio formats include: Microsoft WAV SGI/Apple AIFF/AIFC Sun AU/Snd Raw (headerless) Paris Audio File (PAF) Commodore IFF/SVX Sphere/NIST WAV IRCAM SF Creative VOC SoundForge W64 GNU Octave MAT4.4 Portable Voice Format Fasttracker 2 XI HMM Tool Kit HTK Apple CAF Sound Designer II SD2 Free Lossless Audio Codec (FLAC) Supported encodings vary by the format used (see the link below for a comprehensive compatibility table), but samples of several supported encodings are as follows: Unsigned and signed 8, 16, 24 and 32 bit PCM IEEE 32 and 64 floating point U-LAW A-LAW IMA ADPCM MS ADPCM GSM 6.10 G721/723 ADPCM 12/16/24 bit DWVW OK Dialogic ADPCM 8/16 DPCM More information on libsndfile supported audio formats can be found here: http://www.mega-nerd.com/libsndfile/#Features","title":"Local file-based processing by server"},{"location":"audioFormats.html#audio-files-being-opened-and-processed-by-java","text":"Any files being opened in the Nightingale GUI for close analysis work must be able to be opened and read by the underlying Java code libraries: Java Media Framework JavaX.Sound FLAC Only mono files are currently supported. Supported sample rates include: 8 kHz multiples to 48 kHz 11.025 kHz multiples to 44.1 kHz Supported container formats: FLAC RIFF (.wav) AIFF AIFC AU Supported encoding formats: - Compressed: * FLAC - PCM: * 16 bit signed int, big or little endian * 8 bit signed or unsigned int * 32 bit float, little endian only (i.e. RIFF, not RIFX) * 8 bit mulaw or alaw - ADPCM: * Microsoft or IMA ADPCM","title":"Audio files being opened and processed by Java"},{"location":"audioFormats.html#audio-samples-buffered-into-memory","text":"Buffered audio samples being sent to the OLIVE server for enrollment or scoring are read and processed under the assumption that they are raw 16-bit Linear PCM sampes at an 8 kHz sampling rate.","title":"Audio samples buffered into memory"},{"location":"buildaworkflow.html","text":"Test Diagram graph LR A[Input Audio] --> B[SAD]; B --> |frame scores| C[SID]; B --> |frame scores| D[LID]; D --> |language filtering| E[ASR]; E --> |text output| F[TMT]; F --> |translated transcript| G[Consolidated Workflow Results]; B --> |speech activity| G; C --> |speaker labels| G; D --> |language labels| G; E --> G; So as y\u2019all may know, if you want to \u201cprint\u201d the workflow definition using the Python client you can use the \u2013print_actualized argument like below, but this output didn\u2019t include any info about the \u201cdata\u201d used by the workflow and how this data is\u201cpreprocessed\u201d. I was concerned this information was kind of an implementation detail, but as I was working on some workflow documentation it seemed like this info could be helpful for some clients. Here is what the print output looks like if I use the raw protobuf info: Olivepyworkflow sad-lid-sid_abstract.workflow --print_actualized Analysis Task Info: [ { \"data\": { \"minNumberInputs\": 1, \"maxNumberInputs\": 1, \"type\": \"AUDIO\", \"preprocessingRequired\": true, \"resampleRate\": 8000, \"mode\": \"MONO\" }, \"SAD\": { \"messageType\": \"REGION_SCORER_REQUEST\", \"traitOutput\": \"REGION_SCORER\", \"task\": \"SAD\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"SAD\", \"returnResult\": true, \"job_name\": \"job_sad_lid_sid\", \"analysis\": { \"plugin\": \"sad-dnn\", \"domain\": \"multi-v1\" } }, \"LID\": { \"messageType\": \"GLOBAL_SCORER_REQUEST\", \"traitOutput\": \"GLOBAL_SCORER\", \"task\": \"LID\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"LID\", \"returnResult\": true, \"job_name\": \"job_sad_lid_sid\", \"analysis\": { \"plugin\": \"lid-embedplda\", \"domain\": \"multi-v1\" } }, \"SID\": { \"messageType\": \"GLOBAL_SCORER_REQUEST\", \"traitOutput\": \"GLOBAL_SCORER\", \"task\": \"SID\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"SID\", \"returnResult\": true, \"job_name\": \"job_sad_lid_sid\", \"analysis\": { \"plugin\": \"sid-dplda\", \"domain\": \"multi-v1\" } } } ] But some of the fields in the data section are important: \"data\": { \"minNumberInputs\": 1, \"maxNumberInputs\": 1, \"type\": \"AUDIO\", \"preprocessingRequired\": true, \"resampleRate\": 8000, \"mode\": \"MONO\" }, Specifically the field \u2018mode\u2019 is important. I\u2019ve only release workflows where the mode is \u2018MONO\u2019, which means treat multi-channel audio as MONO. But if the mode is \u2018SPLIT\u2019 then each channel is processed as its own \u201cjob\u201d so there would be a set of SAD/LID/SID scores for each channel (this is like the stereo messages in Olive 4.x). I\u2019m not sure the best way to show this, as I think this is basically just a mode for Probity/Francis. Y\u2019all have any thoughts on how to present this info so it would make more sense to users? For starters, maybe \u2018data\u2019 should be \u2018preprocess_data\u2019 instead?","title":"Test Diagram"},{"location":"buildaworkflow.html#test-diagram","text":"graph LR A[Input Audio] --> B[SAD]; B --> |frame scores| C[SID]; B --> |frame scores| D[LID]; D --> |language filtering| E[ASR]; E --> |text output| F[TMT]; F --> |translated transcript| G[Consolidated Workflow Results]; B --> |speech activity| G; C --> |speaker labels| G; D --> |language labels| G; E --> G; So as y\u2019all may know, if you want to \u201cprint\u201d the workflow definition using the Python client you can use the \u2013print_actualized argument like below, but this output didn\u2019t include any info about the \u201cdata\u201d used by the workflow and how this data is\u201cpreprocessed\u201d. I was concerned this information was kind of an implementation detail, but as I was working on some workflow documentation it seemed like this info could be helpful for some clients. Here is what the print output looks like if I use the raw protobuf info: Olivepyworkflow sad-lid-sid_abstract.workflow --print_actualized Analysis Task Info: [ { \"data\": { \"minNumberInputs\": 1, \"maxNumberInputs\": 1, \"type\": \"AUDIO\", \"preprocessingRequired\": true, \"resampleRate\": 8000, \"mode\": \"MONO\" }, \"SAD\": { \"messageType\": \"REGION_SCORER_REQUEST\", \"traitOutput\": \"REGION_SCORER\", \"task\": \"SAD\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"SAD\", \"returnResult\": true, \"job_name\": \"job_sad_lid_sid\", \"analysis\": { \"plugin\": \"sad-dnn\", \"domain\": \"multi-v1\" } }, \"LID\": { \"messageType\": \"GLOBAL_SCORER_REQUEST\", \"traitOutput\": \"GLOBAL_SCORER\", \"task\": \"LID\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"LID\", \"returnResult\": true, \"job_name\": \"job_sad_lid_sid\", \"analysis\": { \"plugin\": \"lid-embedplda\", \"domain\": \"multi-v1\" } }, \"SID\": { \"messageType\": \"GLOBAL_SCORER_REQUEST\", \"traitOutput\": \"GLOBAL_SCORER\", \"task\": \"SID\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"SID\", \"returnResult\": true, \"job_name\": \"job_sad_lid_sid\", \"analysis\": { \"plugin\": \"sid-dplda\", \"domain\": \"multi-v1\" } } } ] But some of the fields in the data section are important: \"data\": { \"minNumberInputs\": 1, \"maxNumberInputs\": 1, \"type\": \"AUDIO\", \"preprocessingRequired\": true, \"resampleRate\": 8000, \"mode\": \"MONO\" }, Specifically the field \u2018mode\u2019 is important. I\u2019ve only release workflows where the mode is \u2018MONO\u2019, which means treat multi-channel audio as MONO. But if the mode is \u2018SPLIT\u2019 then each channel is processed as its own \u201cjob\u201d so there would be a set of SAD/LID/SID scores for each channel (this is like the stereo messages in Olive 4.x). I\u2019m not sure the best way to show this, as I think this is basically just a mode for Probity/Francis. Y\u2019all have any thoughts on how to present this info so it would make more sense to users? For starters, maybe \u2018data\u2019 should be \u2018preprocess_data\u2019 instead?","title":"Test Diagram"},{"location":"cli.html","text":"OLIVE Command Line Interface Guide Introduction This document describes running the OLIVE (formerly SCENIC) system from a command line. Our command line applications are geared toward a variety of specialized users such as researchers, system evaluators (i.e. Leidos for the DARPA RATS program), and testers. Casual users should consider using our graphical application. However, our command line applications can function as general-purpose tools, but may require specially formatted files such the RATS XML files for audio analysis and LDC-format TSV files for training annotations. 1: Overview OLIVE command line interface (CLI) tools include: localenroll \u2013 Used to enroll \u2018targets\u2019 into the system, such as a target speaker for speaker identification (SID), a topic of interest for topic identification (TID), or a keyword or phrase of interest for query-by-example keyword spotting (QBE). localanalyze \u2013 Used to query the OLIVE server to score audio to find speech with a speech activity detection (SAD) plugin, report scores for potential speakers or languages of interest for SID or language identification (LID) plugins, report likelihood and location(s) of conversation topics or keywords of interest (TID, QBE, KWS). localtrain \u2013 Used to train or adapt plugins that support the LearningTrait (SupervisedAdapter, SupervisedTrainer, or UnsupervisedAdapter) with examples of new audio conditions to improve performance in such conditions. Also used to add new language recognition capabilities to a LID plugin, and to retrain the background models of a SID plugin to prepare it for new audio conditions. Training and adaptation are not available in all plugins, please refer to individual plugin documentation or plugin capabilities matrix to verify availability of training or adaptation. 2: Command Line Testing and Analysis A. Enrollment with localenroll The localenroll command is used to enroll audio for SID and TID. It can be invoked from a BASH or C-shell terminal. It takes a simply formatted text file as input and does not produce an output file. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TID, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in the Plugin Appendix. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7 The basic syntax for calling localenroll (more details and options below) is: $ ./localenroll <path_to_plugin_domain> <path_to_enrollment_file> Where an example of that may be: $ ./localenroll plugins/sid-embed-v1/domains/multi-v1/ /data/sid/smoke_enroll.lst The numerous options available in localenroll can be seen by executing localenroll --help, the output of which is shown below: usage : localenroll [- h ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path enrollment_file positional arguments : plugin_domain_path path to plugin domain used for analysis enrollment_file List of enrollments of the form < audio_path > < class_id > OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have a configuration section named enrollment . Only values from the enrollment section are read -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting B. Scoring and processing with localanalyze I. Invoking localanalyze The localanalyze utility is used to perform OLIVE scoring and analysis with most plugins (SAD, SID, SDD, LID, KWS, QBE, GID, TID), or processing with an ENH plugin, all on list-based input files. It can be invoked from a BASH or C-shell terminal. A path to a valid OLIVE plugin and domain as well as an audio paths input file are required for all tasks. For some plugins, like LID and SID, an optional IDs input file can be specified via the --class_ids argument to limit which languages or speakers are scored. This IDs input file is also how a keyword spotting plugin is informed what the keywords of interest are for a given analysis. The exact details for invoking localanalyze will depend upon the plugin technology being used, and may vary slightly depending upon the options available to each individual plugin, but the general format for running this utility is: $ localanalyze <path_to_plugin_domain> <list_of_files_to_analyze_or_process> With an example (SID): $ localanalyze plugins/sid-embed-v1/domains/multi-v1/ /data/sid/test_data.lst The format of the audio input file is simply a list of one or more newline-separated lines containing a path to an audio file: <audio_path> Example audio input file: /data/sid/test/unknownSpkr1.wav /data/sid/test/unknownSpkr27.wav As mentioned above, if you would only like to score a subset of the enrolled speakers or languages, you can optionally pass a list of these identifiers as a newline-separated list text file, with the --class_ids command line argument. This same argument is how you select keywords to search for when running localanalyze with a keyword spotting plugin (see KWS section in the Plugin Appendix for more information). IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Speaker Identification IDs example: Chris Billy Spkr3 A Keyword Spotting IDs example: turn left torpedo watermelon Example (KWS) of a localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/test-audio.lst Note that re-running localanalyze will overwrite the contents of the output.txt file or OUTPUT directory, depending on what type of plugin is being run. The OLIVE usage/help statement for localanalyze : usage : localanalyze [- h ] [-- output OUTPUT_PATH ] [-- thresholds THRESHOLDS ] [-- class_ids ID_LIST_PATH ] [-- options OPTIONS_PATH ] [-- regions REGION_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path audio_paths_file positional arguments : plugin_domain_path path to plugin domain used for analysis audio_paths_file List of audio files to analyze OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- output OUTPUT_PATH , - o OUTPUT_PATH path to output file or directory -- thresholds THRESHOLDS Optional comma - separated threshold values to apply to frame - level scores , e . g . 0.0 , 1.5 . Use syntax '-- thresholds=' for negative values , e . g -- thresholds =- 2.0 ,- 1.0 -- class_ids ID_LIST_PATH , - i ID_LIST_PATH Optional file that specifies class ids to be scored . E . g . limit the speakers that scored . -- options OPTIONS_PATH Optional file containing plugin specific name / value pairs . The option file may have more or more section headings s for each plugin type . Common section names are 'frame scoring' , , 'global scoring' or 'region scoring' -- regions REGION_PATH , - r REGION_PATH Optional flag indicating that the audio paths file should be supplemented with regions from a PEM formated file , it is up to the plugin to utilize these regions to supplement its scoring . This flag is ignored if the audio input list ( audio_paths_file ) is a PEM formatted file . -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting II. Output Plugin Scoring Types In general, the output format and location of a call to localanalyze will depend on the type of \u2018scorer\u2019 the plugin being used is. There are currently four types of plugins in OLIVE: Global scorer Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Currently SID, LID, and GID are the only global scoring plugins. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Example \u2013 sid-embed-v1, lid-embed-v1 Region scorer Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. TID, SDD, QBE, and KWS are all region scorers. Example \u2013 sdd-embed-v1, qbe-tdnn-v4, kws-batch-v9 Frame scorer A frame scoring plugin provides a score for every \u2018frame\u2019 of audio within every test file passed to localanalyze . This allows OLIVE to find distinct regions of speech with high precision in recordings with noise and/or silence. SAD is a frame scoring plugin. It is also possible to apply a threshold to a frame scoring plugin at run-time to report regions of detection instead of frame scores. For a plugin like SAD, this allows OLIVE to provide output in the form of speech regions. A frame is a short segment of audio that typically consists of 10 milliseconds of audio (100 frames per second). Example \u2013 sad-dnn-v4 Audio to audio This plugin takes an audio file as input, and also returns an audio file as output. Currently the only plugins that fall into this category are speech/audio enhancement plugins, where the goal is removing noise and distortion from an audio file to improve the human listening experience and intelligibility. Example \u2013 enh-mmse-v1 Global Scorer Output In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> The name and location of the output file can be overridden by passing it as the argument to the -o or --output argument when calling localanalyze . To see specific examples for each plugin type, please refer to the appropriate section of the Plugin Appendix. Region Scorer Output Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: <audio_path> <region_start_timestamp> <region_end_timestamp> <class_id> <score> Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. Specific examples can be found in the Plugin Appendix at the end of this document. Frame Scorer Output In the case of frame scorers like SAD, an output file is generated for each audio input file, where each audio output file contains a score for each frame in the audio input. There is one frame score per line. Alternatively, an option exists to produce segmentation scores from SAD results by using the --threshold argument. When using the --threshold argument, the output file adheres to standard 5-column PEM format. Without supplying a threshold to localanalyze , the frame scorer output looks like this: <frame_1_score> <frame_2_score> \u2026 <frame_N_score> When a threshold is provided, the output file will resemble the following: <filename>, <channel>, <label (\u201cspeech\u201d)>, <speech region start time (seconds)>, <end time (seconds)> Audio to Audio Output An audio-to-audio plugin takes an audio file as input and returns a corresponding audio file as output. Currently, this plugin type is used to supply enhancement capabilities to OLIVE, to allow OLIVE to improve the quality, intelligibility, or just general human listening experience for an audio file. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav 3: Command Line Field Adaptation A. Command Line Field Adaptation Overview In general, training and adaptation are very resource and time intensive operations. Very large amounts of RAM are used at certain steps in training. When attempting to train or adapt, the machine should be dedicated to that operation. If the plugin path contains a domain then adaptation is implied, otherwise training is implied. The high-level difference between training and adaptation is that adaptation will use the new data supplied during adaptation in addition to the data already used to train the model used by the plugin/domain. Training, on the other hand, ignores the data originally used for training a model and retrains from scratch using only the new data provided. When performing training, none of the data in the base plugin will be used, but the feature configs will. Check the plugin\u2019s traits to determine if full training and/or adaptation are supported. B. Invoking localtrain Not to be confused with enrollment, the localtrain command line application is used to perform field adaptations for SAD, LID & SID. localtrain takes a plugin or plugin_domain path, and one or more data input files formatted for: Unsupervised data - a newline separated list of audio file paths Supervised data with file level annotations - a newline separated list of audio files paths with a class Id (i.e. \u201caudio_file1.flac fas\\n\u201d) Supervised data with region level annotations - a newline separated list of audio file paths, start time (seconds), end time (seconds), and class ID (i.e. \u201caudio_file1.flac 1.25 3.5 fas\\n\u201d) If multiple data files are specified then they must all use the same annotation format. The localtrain utility outputs a new domain in the plugin path. The details of the localtrain executable are below: usage : localtrain [- h ] -- domain - id DOMAIN_ID [-- overwrite ] [-- preprocess ] [-- finalize ] [-- unique ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_or_domain_path data [ data ...] Train or adapt OLIVE audio recognition systems positional arguments : plugin_or_domain_path Path to the plugin or domain . A plugin path implies full training . A domain path implies adaptation of the specified domain . data paths to data files for training / adapation . The files can have one of three forms . 1 : < audio_path >\\ n 2 : < audio_path > < class_id >\\ n 3 : < audio_path > < class_id > < start > < end > \\ n . The first form has no annotations and implies unsupervised . The second form provides for file - level annotations while the third form supports region - level annotations . Start and end times should be in seconds . If multiple files are specified , they must have the same form . optional arguments : - h , -- help show this help message and exit -- domain - id DOMAIN_ID The id of the new domain you 're creating through training or adaptation. Should be a string that is somewhat descriptive of the conditions --overwrite Forcefully overwite an existing domain --preprocess Pre-process audio only, do not finalize training/adaptation --finalize Pre-process audio only, do not finalize training/adaptation --unique gurantees log files are written to unique directoires/files. Helpful when running in SGE mode --options OPTIONS_PATH Optional file containing plugin specific name/value pairs. The option file must have one or more sections for each plugin type. Common section names are ' supervised trainer ', ,' supervised adapter ', ' unsupervised trainer ' or ' unsupervised adapter ' --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- version show program ' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting In order to use an adapted system plugin, simply pass the full path of the domain generated by localtrain to localenroll or localanalyze as the plugin_domain_path argument. For training, do not include the domain in the plugin path. When running on a SGE, you may split the audio processing from the finalization step by using the --preprocess flag to first pre-process audio files, then use invoke localtrain with the --finalize argument to finalize training. Guidelines for the minimum amount of audio data required to successfully execute localtrain are listed in the table below. Task Operation Speech Duration SAD Adapt to new channel 1h LID Adapt to new channel 20m LID Train a new language 3h SID Adapt to new channel 1h i. Examples SAD Adaptation Example: $ localtrain ./plugins/sad-dnn-v1/domains/ptt-v1/ adaptation-data.lst Where each line of adaptation-data.lst has the following format: /path/to/audio.wav label C. LID Training/Adaptation When training new channel conditions, it is recommended to train all supported languages in the LID model to produce the best results. The out of set language is labeled as \u2018xxx\u2019. Use this language ID when training to add languages that you do not want to target in the LID task but are known to be in the test dataset. 4: Log Files a. OLIVE Command Line Logging When executing localtrain , localenroll , and localanalyze , here are three named log files that may be of interest should something go awry. The top-level log file: This log file corresponds to the -l option to the localtrain , localenroll , and localanalyze utilities. By default, it is named the same as the utility being used with \u201c.log\u201d appended (i.e. localanalyze.log when running localanalyze ) and will be written to the directory from which you executed the utility. The pool executor log file: This file will be written to [work_directory]/logs/pool_executor.log, where work_directory corresponds to the -w option to localtrain / localenroll / localanalyze and defaults to your current directory/WORK. The pool executor log file is the best log file to look at if unexpected errors occur. It corresponds to our internal job scheduler also known as the pool executor. The pool monitor log file: This file will be written to [work_directory]/logs/pool_monitor.log, where work_directory corresponds to the -w option to the localtrain , localenroll , localanalyze utilities and defaults to your current directory/WORK/. This log contains stats about memory and CPU utilization. All three of these log files will exhibit log rotation behavior. In the event of errors, [work_directory]/logs may also contain log files named [order_id].failed , where order_id generally corresponds to the file names of the audio files being used for adaptation/training, enrollment, or analysis. The id can be used to tie errors in the pool executor log file to the \u201c.failed\u201d log files. If you run the OLIVE CLI utilities in debug mode (-d), all log files will be maintained, even if they were successful. b. Rotating Log Files OLIVE employs rotating log files in many places. In this context, rotating refers to a log file that is rewritten each time the application is run. The old log file, if any, is renamed with an integer suffix denoting how many invocations in the past it corresponds to. For instance, if you run localanalyze and don\u2019t specify a -l option, you\u2019ll get the default localanalyze.log file. If localanalyze.log already exists, it is moved to localanalyze.log.1 . The system will keep the 10 most recent log files. A file named localanalyze.log.8 means that the file corresponds to eight invocations ago. 5: Plugin Appendix Plugin Types and Acronyms Currently, OLIVE supports the plugin technologies listed in the following list. For operating instructions that apply to only a specific technology, refer to that section within this appendix. SAD \u2013 Speech activity detection. SID \u2013 Speaker identification. LID \u2013 Language identification. KWS \u2013 Keyword spotting. QBE \u2013 Query by example based keyword spotting. TID \u2013 Topic identification. SDD \u2013 Speaker diarization and detection. GID \u2013 Gender identification. ENH \u2013 Speech and audio enhancement. Speech Activity Detection (SAD) SAD plugins are frame scorers that take an audio list file and annotate the presence and location of speech in each audio file in that list. In standard operation, SAD plugins produce a single output file for each input file, by default in a directory called OUTPUT in the location localanalyze was called from. Output files carry the name of the original input file, but with a new extension \u201c.scores\u201d \u2013 for example, audioFile1.wav will become audioFile1.wav.scores, saved inside OUTPUT/. The format of these results files is a newline separated list of numerical values representing the likelihood that each 10ms frame of the audio file contains speech. Typically, a score above 0 represents speech detection, and a score below 0 represents no speech. SAD analysis example: $ localanalyze /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst Output files: OUTPUT/audioFile1.wav.scores OUTPUT/audioFile2.wav.scores Example audioFile1.wav.scores contents: -0.22624 -0.10081 0.00925 0.12365 Alternatively, SAD plugins can be run with the --thresholds flag to have localanalyze automatically convert the frame scores to regions of speech, by applying the provided threshold. SAD analysis example using thresholds: $ localanalyze --thresholds = 0 .0 /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst This will provide a single output file in OUTPUT directory corresponding to the provided threshold: 0.0.pem. If more than one threshold is provided, there will be a PEM file placed into OUTPUT corresponding to each provided threshold. Example PEM output: /data/sad/test/audioFile1.wav 1 speech 63.110 66.060 /data/sad/test/audioFile1.wav 1 speech 66.510 69.230 /data/sad/test/audioFile1.wav 1 speech 93.480 96.090 /data/sad/test/audioFile1.wav 1 speech 96.570 100.760 Note that if negative thresholds are to be used, it is very important to specify the thresholds using an \u2018=\u2019 character. For example, this threshold specification is valid: --thresholds=-2.0,4.0 And this is not valid: --thresholds -2.0,4.0 If only thresholds of 0 or above are going to be used, it is acceptable to omit the equals sign. Speaker Identification (SID) SID plugins are global scorers that take an audio list file and return a score for each enrolled speaker model scored against the audio in each input audio file. Generally, a score above 0 for an enrolled speaker model represents that speaker being detected in the respective audio file. In order to perform analysis on a file with a SID plugin you must first enroll one or more target speakers. The enrollment list file for a SID plugin follows this format for each line: <audio_file_path> <speaker_id> An example enroll.lst: /data/spkr_example_audio_5760.wav UIM1 /data/spkr_example_audio_5761.wav UIM1 /data/spkr_example_audio_5762.wav John /data/spkr_example_audio_5763.wav John Enrolling these speakers with localenroll : $ localenroll /path/to/plugins/sid-embed-v2/domains/multi-v1/ enroll.lst Example localanalyze call: $ localanalyze ./plugins/sid-embed-v2/domains/multi-v1/ ./data/sid/test/testAudio.lst By default, the output of this call is written to output.txt in the directory the command was run. The format of output.txt contains one line for each enrolled speaker model, for each input audio file, and the corresponding score: <audio_file_path> <speaker_id> <score> Example output.txt: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Trial-based Calibration Speaker Identification (SID TBC) Trial-based Calibration SID plugins are identified by \u2018tbc\u2019 in the plugin name. They are used very similarly to a basic SID plugin, using localenroll and localanalyze just like the examples shown in the previous example. The benefit of TBC plugins is that they allow OLIVE to perform calibration at test-time, based on the actual data conditions being encountered, rather than being forced to use a single, global calibration model that has been trained a priori. The basics of TBC enrollment and testing follow the previous SID examples, but the additional options and outputs available to TBC are detailed below. The standard approach to calibration uses a \u201cone size fits all\u201d calibration model based on the developer\u2019s best understanding of potential operating conditions. This is problematic when the user either doesn\u2019t know ahead of time what likely conditions are, or when operating conditions may vary widely. Trial-based calibration was developed as a means of providing calibration that is responsive to the particular conditions of a trial, and adapts its calibration model based on the conditions encountered. There are two ways we have developed to do this. The first draws from a pool of available data (either provided by the developer or augmented with user-provided data) and uses measures of the conditions found within this data and the trial conditions to build an ideal calibration set on the fly. This is useful in that this approach can also determine when a trial CANNOT be calibrated, and to measure the success of calibration when it is possible. The clear downside of this approach is that it is quite slow. A second approach to TBC is to use a model that has used a DNN to learn to predict both calibration parameters and confidence from large sets of trials and available calibration data. This approach is very fast (about 5000 times faster than the first approach) but has the downside that expanding the calibration set by the user\u2019s data isn\u2019t possible. This plug-in provides both approaches in the TBC plug-in, as two separate domains. In addition to the output score file detailed in the SID section, TBC plugins have additional possible outputs. Speech Detection Output Segmentation files are used to label time regions in the speech signal. We use this format for voice activity detection (VAD). If an output_ivs_dump_path is provided as an option to localenroll or localanalyze , the system will produce this file in a folder corresponding to the wav_id for all registered waveforms. The format is the following: md5sum start end (in seconds) Example: b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.060 0.060 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.090 0.090 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.110 0.170 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.200 0.200 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.560 3.550 Persistent I-Vectors For both localenroll and localanalyze , if the output_ivs_dump_path is defined via an options file with the --options flag, an i-vector from each audio file is saved in this directory for re-use. This reduces unnecessary computation when there exists overlap between lists of wave files to be processed. For instance, if the enroll and test wave file lists are identical (i.e., the case of an exhaustive comparison of a set of files), i-vector persistence will reduce overall computation by almost a factor of 2 since i-vector extraction consumes more of the computation required for an evaluation. I-vectors will be saved in a sub-directory of output_ivs_dump_path based on the base name of the wave file. In addition to this optional feature, the enrollment vectors are loaded in memory prior to verification and if the md5sum of a test audio file matches one used in the enrollment process, the corresponding vector will be used instead of re-processing the audio. This is because vector extraction is identical between enrollment and verification. Trial-based Calibration Trial-based calibration (TBC) does not change the way calibration works but changes the way calibration is used. It relaxes the constraint on the system developers to train a calibration model that is ideally matched to the end use conditions. Rather than train a calibration model a priori, the system postpones this training until the conditions of the particular verification trial are known to the system; a trial consists of comparing test audio to an enrolled speaker model. The goal of trial-based calibration is to use information about the trial to generate an ideal calibration set for the trial conditions using the reservoir of possible calibration audio files available. Using this set, a calibration model tailored to the conditions of the trial can be trained and used to effectively calibrate the verification score. The TBC operation and output differs from traditional SID plugins; it may choose to reject a trial and NOT output a score if insufficient data is available for calibrating for those conditions. For instance, the output may look similar to the following: waves/T6_ACK2.sph T6 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00) waves/T6_ACK3.sph T1 0.0 -inf Unable to calibrate with only 12 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T2 0.0 -inf Unable to calibrate with only 3 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T3 0.0 -inf Unable to calibrate with only 0 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T4 0.0 -inf Unable to calibrate with only 2 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T6 0.0 -inf Unable to calibrate with only 9 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T4_ACK3.sph T5 0.96153318882 4.04821968079 Used 95 target trials in calibration waves/T4_RC.sph T4 3.46785068512 4.07499170303 Used 95 target trials in calibration waves/T5_Tip2.sph T5 8.90770149231 4.07352733612 Used 98 target trials in calibration waves/T5_RC.sph T5 10.2386112213 4.03855705261 Used 47 target trials in calibration waves/T4_Tip2.sph T4 10.8663234711 4.07404613495 Used 218 target trials in calibration waves/T4_Tip1.sph T4 11.793006897 3.98730397224 Used 164 target trials in calibration waves/T4_ACK2.sph T4 11.8091144562 3.90610170364 Used 119 target trials in calibration waves/T4_ACK1.sph T4 12.2115001678 4.16342687607 Used 208 target trials in calibration waves/T5_ACK1.sph T5 13.8099250793 3.99625587463 Used 99 target trials in calibration waves/T5_Tip1.sph T5 14.9411458969 3.96994686127 Used 83 target trials in calibration waves/T4_ACK3.sph T4 16.003446579 4.05554199219 Used 146 target trials in calibration The output follows the structure: <testwave> <modelid> <score> <confidence> <info> In the instance of insufficient calibration segments being located, a score of 0.0 and a calibration confidence of -inf is given. In contrast, when sufficient data is found for calibration, the number of segments used in calibration is reported. There exist two options for applying calibration with the current plugin: DNN-assisted TBC, normal TBC, or global calibration. Each of these options use duration information to reduce the impact of duration variation impacting calibration performance. Note that changing calibration domains does NOT require re-enrollment of models as these are done in a domain-independent way for any TBC-enabled plugin. DNN-assisted Trial-based Calibration DNN-assisted TBC is invoked by passing the tbcdnn-v1 domain to localanalyze . This is a very fast and newly pioneered effort by SRI to reduce the computation needed to apply dynamic calibration methods to speaker recognition and operates with very low overhead compared to global calibration, and yet significantly benefits calibration performance in varying conditions or conditions that differ from the development conditions. localanalyze ... <plugin>/domains/tbcdnn-v1 test.lst Normal Trial-based Calibration TBC is applied by default with the 'sid-embedDnnTbc-v1' plugin. The data within the domain (such as 'tbc-v1') is used as candidate calibration data. localanalyze ... <plugin>/domains/tbc-v1 test.lst TBC is applied to verification scores on a trial-by-trial basis. As such, verification using TBC will operate at a speed much slower than global or DNN-assisted TBC depending on the size and make-up of the TBC data. This should be considered when using TBC in a cluster environment where it is the number of trials (model vs test comparisons) that determine the running time instead of the number of test files. Global Calibration Each domain can be used to invoke global calibration. This is particularly useful for user-defined data as it provides a rapid means of improving calibration performance without a dramatic increase in computation time. In this case, verification will operate at a much faster pace since TBC is essentially disabled and the global calibration model parameters are applied to all scores. In order to invoke global calibration, and optional parameter must be passed to localanalyze via an options file: echo \"[global scoring] global_calibration = True \" > options.lst localanalyze -- options options . lst ... < plugin >/ domains / tbc - v1 test . lst Optional Parameters The TBC-based plugins offer several tunable parameters via the options parameter to localenroll or localanalyze . These can be passed to the enrollment phase or verification phase by preceding the options in an ascii text file as such: $ cat options . lst [ enrollment ] ... enrollment options per line ... [ global scoring ] ... verification options per line ... The optional parameters and their purpose are provided below. tbc_confidence_threshold = 3.0, # Similarity threshold for processing a trial with TBC score_threshold = 0.0, # Score offset subtracted from output LLRs to assist in making 0 threshold output tgt_max = 300, # The maxmimum number of target trials used for TBC of a trial imp_max = 3000, # The maxmimum number of impostor trials used for TBC of a trial tgt_imp_min = 20, # The mimum number of relevant target and impostor calibration trials needed to use TBC (rejected otherwise) global_calibration = False, # Apply global calibration instead of TBC ivs_dump_path = None, # Output path for dumping vectors and meta information sad_threshold = 0.5, # Threshold for speech activity detection (higher results in less speech) sad_filter = 1, # Smoothing of LLRs from SAD DNN prior to thresholding sad_interpolate = 1, # If > 1, a speed up of SAD by interpolating values between frames (4 works well) Utilizing these parameters in an options file may look like this: echo \"[enrollment] sad_threshold = 1.0 ivs_dump_path = . / embeddings [ global scoring ] sad_threshold = 1.0 ivs_dump_path = . / embeddings tgt_max = 100 tgt_imp_min = 50 \" > options.lst localenroll -- options options . lst ... < plugin >/ domains /< domain > enroll . lst localanalyze -- options options . lst ... < plugin >/ domains /< domain > test . lst Verification Trial Output The format for the verification trial is the following. Note that for global calibration, the optional parameters (calibration_confidence and calibration_remarks) are not output. Output format: wav_id speaker_id score [ calibration_confidence calibration_remarks ] Here is an example of score executed with Global Calibration: waves/T1_ACK1.sph T6 5.19274568558 waves/T1_ACK1.sph T4 1.204241395 waves/T1_ACK1.sph T5 1.69025540352 Here is an example of scores executed with DNN-assisted TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used DNN-assisted TBC with confidence 5.751 waves/T1_ACK1.sph T4 1.204241395 5.12 Used DNN-assisted TBC with confidence 3.122 waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with confidence above threshold (3.00) Here is an example of scores executed with normal TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used 67 target trials in calibration waves/T1_ACK1.sph T4 1.204241395 5.12 Used 73 target trials in calibration waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00) Speaker Diarization and Detection (SDD) The overall goal of SDD plugins is to detect regions of speech within an audio recording that are associated with different speakers, and then identify those speakers if possible. SDD plugins have three different modes of operation, as outlined below. Changing the execution mode for SDD is done by passing an options file to localanalyze as an argument to the --options flag. The main behavior and premise of the technology and plugin remain the same, but each mode changes the format and information contained in the output file. Running the SDD plugin is very similar to running SID plugins, with the same syntax for enrollment and testing. Currently, training or adaptation through localtrain is not supported, but enrolling new speakers and testing against enrolled speakers is as simple as: $ localenroll /path/to/plugins/sdd-embed-v1/domains/multi-v1/ enrollmentAudio.lst $ localanalyze /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst The enrollment and test audio file lists in this example follow the same format as the lists used by SID plugins, described above. By default, if run as above with no options, the plugin will run in Speaker Detection mode, and provide the output described above. In order to run in SID or SID Exhaustive mode, you will need to provide an options file to specify that behavior: $ localanalyze --options options.lst /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst Where options.lst is a text file with contents similar to: [region scoring] mode: SID_EXHAUSTIVE sad_threshold: 0.0 diarization_max_num_speakers: 2 The [region scoring] header is alerting the plugin that the options are being passed for scoring, and all of the parameters shown above (sad_threshold, diarization_max_num_speakers, mode) are optional parameters. The mode option controls the output behavior as described above, and the possible options are SID, SID_EXHAUSTIVE, and SPEAKER_DETECTION, and described directly below. The sad_threshold defaults to 2.0, and is used to fine tune the threshold for the internal speech activity detection plugin if necessary. The parameter diarization_max_num_speakers defaults to 4, and is the largest number of speakers the plugin will attempt to consider when clustering segments within the file. SDD Execution Modes To facilitate the understanding of each mode\u2019s output, consider that speech from an audio file is made up of clusters of speakers, and each cluster will have one or more contiguous segment of speech. SPEAKER_DETECTION The goal of Speaker Detection is to show the most probable speaker model for each segment of the input audio file. As output, this mode gives one line per segment within the file, along with the top scoring enrolled model for the cluster that segment belongs to, and that cluster's score for the given model. Note that many scores will be repeated in the output file, since each segment in the cluster shares the same score for a given speaker model. This mode is performed by default if no options file with a mode override is given. SID This mode is meant for triaging large amounts of audio files when the main goal is just finding which of these files may contain speech from one of the enrolled speakers. The output is the maximum score for each enrolled speaker within the audio file after scoring against each cluster in the file, as well as the timestamps for the beginning and end of the longest segment within the cluster that scored the highest for that model. This gives a specific segment to spot check and evaluate the plugin's decision if needed. SID_EXHAUSTIVE When using SID Exhaustive, each diarized cluster is scored against each enrolled model. The output is a complete listing for every speech segment of the input audio file, the score from testing every enrolled model against the cluster that the segment belongs to. Many scores will be repeated in the output file, since each segment in the cluster shares the same score. In this example, Chris and Jimmy are the only enrolled models, and 5 total segments were identified within the file. Language Identification (LID) LID plugins are global scorers that act very similar to SID with respect to scoring, except that each score corresponds to a language model rather than a speaker model. In most cases, LID plugins will be delivered from SRI with a set of languages already enrolled. Languages can be added to some plugins by the user if enough appropriate data is available, through the localtrain CLI call. Details on this will be added to this document in a later revision. Example localanalyze call: $ localanalyze /path/to/plugins/lid-embed-v2/domains/multi-v1/ testAudio.lst Output format: <audio_file_path> <language_id> <score> Output example: /data/lid/audio/file1.wav fre -0.5348 /data/lid/audio/file1.wav eng 3.2122 /data/lid/audio/file1.wav spa -5.5340 /data/lid/audio/file1.wav rus 0.5333 /data/lid/audio/file1.wav ara -4.9444 /data/lid/audio/file2.wav fre -2.6564 Keyword Spotting (KWS) KWS is an automatic speech recognition (ASR) based approach to detecting spoken keywords in audio. Rather than enrolling target keywords from audio, as you would with query-by-example, telling the plugin what keywords to search for is done by passing an IDs file to localanalyze . The format of the IDs file is: IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Keyword Spotting IDs example, search_list.lst: remote torpedo voice recognition Example KWS localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst /path/to/plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/testAudio.lst The output format for KWS plugins is identical to that of QBE. It is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/kws/testFile1.wav 7.170 7.570 remote 1.0 /data/kws/testFile1.wav 10.390 10.930 remote 0.693357 /data/kws/testFile1.wav 1.639 2.549 voice recognition 1.0 Automatic Speech Recognition (ASR) Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. ASR plugins do not require any enrollment of words (like QBE) or specification of words of interest (like traditional KWS), but instead rely on the vocabulary model built into the domain to define the list of available words. All that is necessary for scoring an audio file for ASR is a list of input files to be scored, which follows the format below. Generic input audio list format: <audioFile_1> <audioFile_2 (opt)> ... <audioFile_N (opt)> A specific example of this, called testAudio.lst, might look like: /data/asr/testFile1.wav /data/asr/testFile2.wav /data/asr/testFile3.wav Note that if the files are not contained within the directory that localanalyze is being run from, or if a relative path from that location is not provided, the full file path to each file is necessary. An example ASR localanalyze call: $ localanalyze /home/user/oliveAppData/plugins/asr-dynapy-v1/domains/eng-tdnnChain-tel-v1/ /data/asr/test/testAudio.lst The output format for KWS plugins is identical to that of QBE and other region-scoring OLIVE plugins. ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. Output is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <word> <score> An example in English: /data/asr/testEnglish1.wav 0.000 0.190 and 43.00000000 /data/asr/testEnglish1.wav 0.210 0.340 we're 44.00000000 /data/asr/testEnglish1.wav 0.330 0.460 going 97.00000000 /data/asr/testEnglish1.wav 0.450 0.520 to 97.00000000 /data/asr/testEnglish1.wav 0.510 0.940 fly 66.00000000 /data/asr/testEnglish1.wav 1.080 1.300 was 31.00000000 /data/asr/testEnglish1.wav 1.290 1.390 that 24.00000000 /data/asr/testEnglish1.wav 1.290 1.390 it 22.00000000 /data/asr/testEnglish1.wav 1.380 1.510 we're 27.00000000 /data/asr/testEnglish1.wav 1.500 1.660 going 97.00000000 /data/asr/testEnglish1.wav 1.650 1.720 to 98.00000000 /data/asr/testEnglish1.wav 1.710 1.930 fly 94.00000000 /data/asr/testEnglish1.wav 1.920 2.110 over 79.00000000 /data/asr/testEnglish1.wav 2.100 2.380 saint 93.00000000 /data/asr/testEnglish1.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: /data/asr/testMandarin1.wav 0.280 0.610 \u6218\u6597 99.00000000 /data/asr/testMandarin1.wav 0.600 0.880 \u7206\u53d1 98.00000000 /data/asr/testMandarin1.wav 0.870 0.970 \u7684 99.00000000 /data/asr/testMandarin1.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 /data/asr/testMandarin1.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 /data/asr/testMandarin1.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 /data/asr/testMandarin1.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 /data/asr/testMandarin1.wav 3.130 3.340 \u7684 100.00000000 /data/asr/testMandarin1.wav 3.330 3.720 \u6b66\u88c5 55.00000000 /data/asr/testMandarin1.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: /data/asr/testFarsi1.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 /data/asr/testFarsi1.wav 0.470 0.740 51.00000000 \u0627\u06cc /data/asr/testFarsi1.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 /data/asr/testFarsi1.wav 0.310 0.460 99.00000000 \u0645\u06cc /data/asr/testFarsi1.wav 0.450 0.680 99.00000000 \u06af\u0645 /data/asr/testFarsi1.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f /data/asr/testFarsi1.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Query by Example Keyword Spotting (QBE) Query by example is a specific type of keyword spotting plugin that searches for keywords matching a spoken word or phrase example, rather than from a text example like traditional KWS. This means that it is necessary to enroll keywords into the system from audio examples with localenroll before using QBE to search audio for these keywords or phrases. Enrollment follows the same format as enrolling speakers into a SID plugin, with the enrollment audio list following this format: <audio_file_path> <keyword_id> Example: /data/qbe/enroll/watermelon_example1.wav Watermelon /data/qbe/enroll/watermelon_example2.wav Watermelon /data/qbe/enroll/airplane_example.wav Airplane /data/qbe/enroll/keyword_example.wav Keyword Note that currently each enrollment audio file must contain ONLY the keyword that is desired to be enrolled. Also note that the text label in the second column of the enrollment file is only for user readability and is not used by the system when determining what to search the audio file for. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006 Topic Identification (TID) Topic Identification plugins attempt to detect and categorize the topic being discussed within an audio recording from a known and pre-enrolled set of available topics, and report this topic (if any are detected) to the user. Each domain of a TID plugin is language-dependent, and should display the target language as the first string of the domain\u2019s name. Some TID plugins may be delivered with pre-enrolled topics \u2013 please consult the documentation that accompanied your delivery if you are unsure if this is the case. If no topics are enrolled, however, or if you wish to enroll new topics, the format is very similar to enrolling new speakers into a SID plugin, and follows the same CLI call structure, with one caveat. $ localenroll --local $domain $enroll_list Due to limitations with the current TID technology, enrollment must be performed with the --local flag set. This limits OLIVE to serialized processing, which will process the enrollment slightly slower, but avoid resource-competition issues that may cause the system to become unstable and crash. Enrollment audio lists for TID follow the same format as SID, but substitute a topic name for a speaker label. Note: in the current version, we require the user to provide audio examples that are primarily about the topic of interest as enrollment examples. If there are significant portions of an audio file that are off-topic, we suggest the file be cut and fed as separate examples. Enroll list format: <audio_file_path> <topic_id> Example: /tid/enroll/topic_example_audio_5760.wav Travel /tid/enroll/topic_example_audio_5761.wav Travel /tid/enroll/topic_example_audio_5762.wav Travel /tid/enroll/topic_example_audio_5763.wav Travel To run TID, once target topics have been enrolled, the call to localanalyze is very similar to other plugin types, with the current plugin again requiring the --local flag. $ localanalyze --local /path/to/plugins/tid-svm-v2/domains/r-tel-v1/ testAudio.lst As with the SID and LID plugins, by default the plugin\u2019s output will be written to the file \u201coutput.txt\u201d in the directory localanalyze was called from. This can be overridden by passing localanalyze the -o flag, as well as an alternate file to save the results to. The TID results structure is very similar to KWS, with the following format for each line: <audio_file_path> <start_time_s> <end_time_s> <topic_id> <confidence_score> Example: /data/tid-example_wavs/20110511_GET-TOGETHER.wav 18.790 55.300 transportation 0.0222 /data/tid-example_wavs/20110511_TRANSPORTATION.wav 4.010 19.140 transportation 0.4532 The start and end timestamps above are provided in seconds. The will be displayed as it was supplied in the second column of the enrollment list. The will be between 0 and 1, and marks the confidence of the system in the decision of this topic. Please note that output.txt will be overwritten by each successive experiment. Please back it up or use the -o option to localanalyze if you would like to save the results. Also note that the start and end times for each topic refer to the chunk in each audio that has the highest probability of being about that topic. Currently, we report only ONE such segment per file in order to help the user locate the most useful part. The score associated with that segment is global, in that it represents the likelihood that this topic is present anywhere in the document. Important Background Example Information In order to train a Topic detector, we currently use an SVM classifier. This model uses \"positive\" examples of the topic as provided by the user during the enrollment phase, as well as \"negative\" examples to model what is not the topic. Those negative examples can be crucial to the performance of the final system. Currently, those \"negative\" examples come pre-processed as a python numpy archive and cannot be modified by the user explicitly. We do provide two different numpy archives that can be tried by a user: BG_RUS001-train-acc+neg-no-travel.npz (default) BG_RUS001-train-test-random-all-chunk-plugin.npz Archive 2) includes only data from the RUS001 conversational corpus, which didn't have very topic-specific prompts. Archive 1) includes a mix of RUS001 data as well as a subset of the RU_CTS conversational corpus which was topic annotated. We excluded examples pertaining to 'TRAVEL' in this archive, but this archive contains conversations about the following (loosely defined) topics: ACTIVITIES BIRTHDAY_WISHES CHILDREN ECONOMY EDUCATION ENTERTAINMENT FOOD_DRINK FRIENDS_RELATIVES GET-TOGETHER HEALTH HOME HOME_MAINTENANCE IMMIGRATION LANGUAGE_COMMUNICATION LEISURE LIFE_PHILOSOPHY_RELATIONSHIPS LOCATION_DESCRIPTION MARRIAGE MOOD_PHYSICAL MOVING_HOMES MUSIC PERFORMANCE_REHEARSAL PETS POLITICS PROJECT READING_WRITING RELIGION_HOLIDAY SPEECH_COLLECTION_PROJECT TECHNOLOGY TRANSPORTATION TV_MOVIES WEATHER_CLIMATE WORK If the topic you are training for is very similar to a topic listed above, it might be worth it trying archive (2) as well. In the future, we will provide the opportunity for the user to feed his own negative examples. Gender Identification (GID) Gender ID plugins allow for triage of audio files to identify only those containing speakers of a certain gender. For scoring files, gender identification plugins operate in the same manner as SID and LID plugins. GID plugins are delivered with two pre-enrolled classes; \u2018m\u2019 and \u2018f\u2019, for male and female, respectively, so user-side enrollment is not necessary. To score files with a GID plugin using localanalyze , use the following syntax: $ localanalyze /path/to/plugins/gid-gb-v1/domains/clean-v1/ testAudio.lst Where the output follows this format: <audio file 1> m <male likelihood score> <audio file 1> f <female likelihood score> \u2026 <audio file N> m <male likelihood score> <audio file N> f <female likelihood score> Example: /data/gender/m-testFile1.wav m 0.999999777927 /data/gender/m-testFile1.wav f -0.22073142865 Enhancement (ENH) Enhancement or AudioConverter plugins are audio-to-audio plugins that take an audio file as input and provide a second audio file as output. Currently they are used to enhance the input audio file, for listening comfort and/or intelligibility. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav Running speech enhancement is as simple as: $ localanalyze /path/to/plugins/enh-mmse-v1/domains/multi-v1/ inputAudio.lst In addition, you can pass an optional PEM file that specifies optional regions to the plugin \u2013 the current enhancement plugin uses this file to pass the \u2018noise regions\u2019 to OLIVE to allow the plugin to build a noise profile for more accurate characterization and removal of the noise present in that audio file. 6: Testing In this section you will find a description of benchmarking that is performed with each release of OLIVE to measure the performance of each plugin with respect to speed and memory usage. The hardware and software details used for these results are provided in the next section, with each of the current plugins\u2019 memory and speed results following. a. Benchmarking Setup (Hardware/Software) Each data point below was obtained by running the OLIVE 4.9.1 software with a 4.8.0 runtime that has been patched to 4.9.1 (contains libraries needed by the new Topic Identification and Enhancement plugins). Tests were performed on the CentOS 7 operating system, on a Gigabyte BRIX GB-BXI7-5500. This machine has 16GB RAM available, and an Intel i7-5500U processor, which is a dual core (quad-thread) processor that runs at 2.4 GHz base (3.0 GHz turbo). b. Plugin Memory Usage Results These results were generated by running several files through each plugin via the OLIVE CLI using localanalyze , one at a time, while measuring the memory used by the localanalyze utility. The files vary in length from 10 seconds through 4 hours, and this allows us to see how each plugin handles scaling the audio file length up, and also compare overall resource utilization between individual plugins. The values reported for each plugin and audio file are the peak memory usage, in MB \u2013 lower values are better. Note that this report is for processing a single file at a time and is representative of memory utilization that can be expected for serialized processing, or processing on a machine with a single processing core. Parallel processing can cause memory usage to rise. TODO: Need to put the charts/resuts in here! SAD SAD Memory Usage (MB) Plugin sad-dnn-v4 sad-dnn-v4 sad-dnn-v4 Domain digPtt-v1 ptt-v1 tel-v1 10s 142 142 142 1 min 158 158 158 10 min 221 221 221 30 min 408 408 408 2 hr 1,262 1,262 1,262 4 hr 2,377 2,377 2,377 c. Plugin Speed Analysis Results The following charts show the speed performance of the current release of each OLIVE plugin. Values are reported as the speed of the plugin in \u2018times faster than real time\u2019 and represent how fast the plugin is able to process the input audio data, with respect to the length of that data \u2013 higher is better. Each plugin is fed 10 hours of total data consisting of roughly 4-minute audio cuts to achieve this measurement. For this test, OLIVE has been limited to using a single core for processing, in order to keep measurements and results consistent. Note that enabling parallel processing if multiple CPU cores are available will improve performance. Plugin Speed Statistics Reported in Times Faster than Real Time Plugin Domain Speed (x RT) sad-dnn-v4 digPtt-v1 104.8 sad-dnn-v4 ptt-v1 111.5 sad-dnn-v4 tel-v1 117.0 sid-embed-v2 multi-v1 51.6 sid-embedDnnTbc-v1 tbc-v1 43.9 sid-embedDnnTbc-v1 tbcdnn-v1 64.5 sid-embedTbc-v1 tbc-v1 40.2 lid-embed-v2 multi-v1 39.7 kws-batch-v9 eng-tel-v1 1.11 kws-batch-v9 eng-tel-v2 1.09 kws-batch-v9 f-tel-v1 1.23 kws-batch-v9 r-tel-v1 1.61 kws-batch-v9 r-tel-v2 1.61 qbe-tdnn-v4 digPtt-v1 15.2 qbe-tdnn-v4 multi-v1 16.1","title":"Command Line Tools"},{"location":"cli.html#olive-command-line-interface-guide","text":"","title":"OLIVE Command Line Interface Guide"},{"location":"cli.html#introduction","text":"This document describes running the OLIVE (formerly SCENIC) system from a command line. Our command line applications are geared toward a variety of specialized users such as researchers, system evaluators (i.e. Leidos for the DARPA RATS program), and testers. Casual users should consider using our graphical application. However, our command line applications can function as general-purpose tools, but may require specially formatted files such the RATS XML files for audio analysis and LDC-format TSV files for training annotations.","title":"Introduction"},{"location":"cli.html#1-overview","text":"OLIVE command line interface (CLI) tools include: localenroll \u2013 Used to enroll \u2018targets\u2019 into the system, such as a target speaker for speaker identification (SID), a topic of interest for topic identification (TID), or a keyword or phrase of interest for query-by-example keyword spotting (QBE). localanalyze \u2013 Used to query the OLIVE server to score audio to find speech with a speech activity detection (SAD) plugin, report scores for potential speakers or languages of interest for SID or language identification (LID) plugins, report likelihood and location(s) of conversation topics or keywords of interest (TID, QBE, KWS). localtrain \u2013 Used to train or adapt plugins that support the LearningTrait (SupervisedAdapter, SupervisedTrainer, or UnsupervisedAdapter) with examples of new audio conditions to improve performance in such conditions. Also used to add new language recognition capabilities to a LID plugin, and to retrain the background models of a SID plugin to prepare it for new audio conditions. Training and adaptation are not available in all plugins, please refer to individual plugin documentation or plugin capabilities matrix to verify availability of training or adaptation.","title":"1: Overview"},{"location":"cli.html#2-command-line-testing-and-analysis","text":"","title":"2: Command Line Testing and Analysis"},{"location":"cli.html#a-enrollment-with-localenroll","text":"The localenroll command is used to enroll audio for SID and TID. It can be invoked from a BASH or C-shell terminal. It takes a simply formatted text file as input and does not produce an output file. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TID, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in the Plugin Appendix. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7 The basic syntax for calling localenroll (more details and options below) is: $ ./localenroll <path_to_plugin_domain> <path_to_enrollment_file> Where an example of that may be: $ ./localenroll plugins/sid-embed-v1/domains/multi-v1/ /data/sid/smoke_enroll.lst The numerous options available in localenroll can be seen by executing localenroll --help, the output of which is shown below: usage : localenroll [- h ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path enrollment_file positional arguments : plugin_domain_path path to plugin domain used for analysis enrollment_file List of enrollments of the form < audio_path > < class_id > OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have a configuration section named enrollment . Only values from the enrollment section are read -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting","title":"A.  Enrollment with localenroll"},{"location":"cli.html#b-scoring-and-processing-with-localanalyze","text":"","title":"B.  Scoring and processing with localanalyze"},{"location":"cli.html#i-invoking-localanalyze","text":"The localanalyze utility is used to perform OLIVE scoring and analysis with most plugins (SAD, SID, SDD, LID, KWS, QBE, GID, TID), or processing with an ENH plugin, all on list-based input files. It can be invoked from a BASH or C-shell terminal. A path to a valid OLIVE plugin and domain as well as an audio paths input file are required for all tasks. For some plugins, like LID and SID, an optional IDs input file can be specified via the --class_ids argument to limit which languages or speakers are scored. This IDs input file is also how a keyword spotting plugin is informed what the keywords of interest are for a given analysis. The exact details for invoking localanalyze will depend upon the plugin technology being used, and may vary slightly depending upon the options available to each individual plugin, but the general format for running this utility is: $ localanalyze <path_to_plugin_domain> <list_of_files_to_analyze_or_process> With an example (SID): $ localanalyze plugins/sid-embed-v1/domains/multi-v1/ /data/sid/test_data.lst The format of the audio input file is simply a list of one or more newline-separated lines containing a path to an audio file: <audio_path> Example audio input file: /data/sid/test/unknownSpkr1.wav /data/sid/test/unknownSpkr27.wav As mentioned above, if you would only like to score a subset of the enrolled speakers or languages, you can optionally pass a list of these identifiers as a newline-separated list text file, with the --class_ids command line argument. This same argument is how you select keywords to search for when running localanalyze with a keyword spotting plugin (see KWS section in the Plugin Appendix for more information). IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Speaker Identification IDs example: Chris Billy Spkr3 A Keyword Spotting IDs example: turn left torpedo watermelon Example (KWS) of a localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/test-audio.lst Note that re-running localanalyze will overwrite the contents of the output.txt file or OUTPUT directory, depending on what type of plugin is being run. The OLIVE usage/help statement for localanalyze : usage : localanalyze [- h ] [-- output OUTPUT_PATH ] [-- thresholds THRESHOLDS ] [-- class_ids ID_LIST_PATH ] [-- options OPTIONS_PATH ] [-- regions REGION_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path audio_paths_file positional arguments : plugin_domain_path path to plugin domain used for analysis audio_paths_file List of audio files to analyze OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- output OUTPUT_PATH , - o OUTPUT_PATH path to output file or directory -- thresholds THRESHOLDS Optional comma - separated threshold values to apply to frame - level scores , e . g . 0.0 , 1.5 . Use syntax '-- thresholds=' for negative values , e . g -- thresholds =- 2.0 ,- 1.0 -- class_ids ID_LIST_PATH , - i ID_LIST_PATH Optional file that specifies class ids to be scored . E . g . limit the speakers that scored . -- options OPTIONS_PATH Optional file containing plugin specific name / value pairs . The option file may have more or more section headings s for each plugin type . Common section names are 'frame scoring' , , 'global scoring' or 'region scoring' -- regions REGION_PATH , - r REGION_PATH Optional flag indicating that the audio paths file should be supplemented with regions from a PEM formated file , it is up to the plugin to utilize these regions to supplement its scoring . This flag is ignored if the audio input list ( audio_paths_file ) is a PEM formatted file . -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting","title":"I. Invoking localanalyze"},{"location":"cli.html#ii-output","text":"","title":"II.    Output"},{"location":"cli.html#plugin-scoring-types","text":"In general, the output format and location of a call to localanalyze will depend on the type of \u2018scorer\u2019 the plugin being used is. There are currently four types of plugins in OLIVE: Global scorer Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Currently SID, LID, and GID are the only global scoring plugins. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Example \u2013 sid-embed-v1, lid-embed-v1 Region scorer Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. TID, SDD, QBE, and KWS are all region scorers. Example \u2013 sdd-embed-v1, qbe-tdnn-v4, kws-batch-v9 Frame scorer A frame scoring plugin provides a score for every \u2018frame\u2019 of audio within every test file passed to localanalyze . This allows OLIVE to find distinct regions of speech with high precision in recordings with noise and/or silence. SAD is a frame scoring plugin. It is also possible to apply a threshold to a frame scoring plugin at run-time to report regions of detection instead of frame scores. For a plugin like SAD, this allows OLIVE to provide output in the form of speech regions. A frame is a short segment of audio that typically consists of 10 milliseconds of audio (100 frames per second). Example \u2013 sad-dnn-v4 Audio to audio This plugin takes an audio file as input, and also returns an audio file as output. Currently the only plugins that fall into this category are speech/audio enhancement plugins, where the goal is removing noise and distortion from an audio file to improve the human listening experience and intelligibility. Example \u2013 enh-mmse-v1","title":"Plugin Scoring Types"},{"location":"cli.html#global-scorer-output","text":"In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> The name and location of the output file can be overridden by passing it as the argument to the -o or --output argument when calling localanalyze . To see specific examples for each plugin type, please refer to the appropriate section of the Plugin Appendix.","title":"Global Scorer Output"},{"location":"cli.html#region-scorer-output","text":"Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: <audio_path> <region_start_timestamp> <region_end_timestamp> <class_id> <score> Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. Specific examples can be found in the Plugin Appendix at the end of this document.","title":"Region Scorer Output"},{"location":"cli.html#frame-scorer-output","text":"In the case of frame scorers like SAD, an output file is generated for each audio input file, where each audio output file contains a score for each frame in the audio input. There is one frame score per line. Alternatively, an option exists to produce segmentation scores from SAD results by using the --threshold argument. When using the --threshold argument, the output file adheres to standard 5-column PEM format. Without supplying a threshold to localanalyze , the frame scorer output looks like this: <frame_1_score> <frame_2_score> \u2026 <frame_N_score> When a threshold is provided, the output file will resemble the following: <filename>, <channel>, <label (\u201cspeech\u201d)>, <speech region start time (seconds)>, <end time (seconds)>","title":"Frame Scorer Output"},{"location":"cli.html#audio-to-audio-output","text":"An audio-to-audio plugin takes an audio file as input and returns a corresponding audio file as output. Currently, this plugin type is used to supply enhancement capabilities to OLIVE, to allow OLIVE to improve the quality, intelligibility, or just general human listening experience for an audio file. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav","title":"Audio to Audio Output"},{"location":"cli.html#3-command-line-field-adaptation","text":"","title":"3: Command Line Field Adaptation"},{"location":"cli.html#a-command-line-field-adaptation-overview","text":"In general, training and adaptation are very resource and time intensive operations. Very large amounts of RAM are used at certain steps in training. When attempting to train or adapt, the machine should be dedicated to that operation. If the plugin path contains a domain then adaptation is implied, otherwise training is implied. The high-level difference between training and adaptation is that adaptation will use the new data supplied during adaptation in addition to the data already used to train the model used by the plugin/domain. Training, on the other hand, ignores the data originally used for training a model and retrains from scratch using only the new data provided. When performing training, none of the data in the base plugin will be used, but the feature configs will. Check the plugin\u2019s traits to determine if full training and/or adaptation are supported.","title":"A.  Command Line Field Adaptation Overview"},{"location":"cli.html#b-invoking-localtrain","text":"Not to be confused with enrollment, the localtrain command line application is used to perform field adaptations for SAD, LID & SID. localtrain takes a plugin or plugin_domain path, and one or more data input files formatted for: Unsupervised data - a newline separated list of audio file paths Supervised data with file level annotations - a newline separated list of audio files paths with a class Id (i.e. \u201caudio_file1.flac fas\\n\u201d) Supervised data with region level annotations - a newline separated list of audio file paths, start time (seconds), end time (seconds), and class ID (i.e. \u201caudio_file1.flac 1.25 3.5 fas\\n\u201d) If multiple data files are specified then they must all use the same annotation format. The localtrain utility outputs a new domain in the plugin path. The details of the localtrain executable are below: usage : localtrain [- h ] -- domain - id DOMAIN_ID [-- overwrite ] [-- preprocess ] [-- finalize ] [-- unique ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_or_domain_path data [ data ...] Train or adapt OLIVE audio recognition systems positional arguments : plugin_or_domain_path Path to the plugin or domain . A plugin path implies full training . A domain path implies adaptation of the specified domain . data paths to data files for training / adapation . The files can have one of three forms . 1 : < audio_path >\\ n 2 : < audio_path > < class_id >\\ n 3 : < audio_path > < class_id > < start > < end > \\ n . The first form has no annotations and implies unsupervised . The second form provides for file - level annotations while the third form supports region - level annotations . Start and end times should be in seconds . If multiple files are specified , they must have the same form . optional arguments : - h , -- help show this help message and exit -- domain - id DOMAIN_ID The id of the new domain you 're creating through training or adaptation. Should be a string that is somewhat descriptive of the conditions --overwrite Forcefully overwite an existing domain --preprocess Pre-process audio only, do not finalize training/adaptation --finalize Pre-process audio only, do not finalize training/adaptation --unique gurantees log files are written to unique directoires/files. Helpful when running in SGE mode --options OPTIONS_PATH Optional file containing plugin specific name/value pairs. The option file must have one or more sections for each plugin type. Common section names are ' supervised trainer ', ,' supervised adapter ', ' unsupervised trainer ' or ' unsupervised adapter ' --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- version show program ' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting In order to use an adapted system plugin, simply pass the full path of the domain generated by localtrain to localenroll or localanalyze as the plugin_domain_path argument. For training, do not include the domain in the plugin path. When running on a SGE, you may split the audio processing from the finalization step by using the --preprocess flag to first pre-process audio files, then use invoke localtrain with the --finalize argument to finalize training. Guidelines for the minimum amount of audio data required to successfully execute localtrain are listed in the table below. Task Operation Speech Duration SAD Adapt to new channel 1h LID Adapt to new channel 20m LID Train a new language 3h SID Adapt to new channel 1h","title":"B.  Invoking localtrain"},{"location":"cli.html#i-examples","text":"","title":"i. Examples"},{"location":"cli.html#sad-adaptation-example","text":"$ localtrain ./plugins/sad-dnn-v1/domains/ptt-v1/ adaptation-data.lst Where each line of adaptation-data.lst has the following format: /path/to/audio.wav label","title":"SAD Adaptation Example:"},{"location":"cli.html#c-lid-trainingadaptation","text":"When training new channel conditions, it is recommended to train all supported languages in the LID model to produce the best results. The out of set language is labeled as \u2018xxx\u2019. Use this language ID when training to add languages that you do not want to target in the LID task but are known to be in the test dataset.","title":"C.  LID Training/Adaptation"},{"location":"cli.html#4-log-files","text":"","title":"4: Log Files"},{"location":"cli.html#a-olive-command-line-logging","text":"When executing localtrain , localenroll , and localanalyze , here are three named log files that may be of interest should something go awry. The top-level log file: This log file corresponds to the -l option to the localtrain , localenroll , and localanalyze utilities. By default, it is named the same as the utility being used with \u201c.log\u201d appended (i.e. localanalyze.log when running localanalyze ) and will be written to the directory from which you executed the utility. The pool executor log file: This file will be written to [work_directory]/logs/pool_executor.log, where work_directory corresponds to the -w option to localtrain / localenroll / localanalyze and defaults to your current directory/WORK. The pool executor log file is the best log file to look at if unexpected errors occur. It corresponds to our internal job scheduler also known as the pool executor. The pool monitor log file: This file will be written to [work_directory]/logs/pool_monitor.log, where work_directory corresponds to the -w option to the localtrain , localenroll , localanalyze utilities and defaults to your current directory/WORK/. This log contains stats about memory and CPU utilization. All three of these log files will exhibit log rotation behavior. In the event of errors, [work_directory]/logs may also contain log files named [order_id].failed , where order_id generally corresponds to the file names of the audio files being used for adaptation/training, enrollment, or analysis. The id can be used to tie errors in the pool executor log file to the \u201c.failed\u201d log files. If you run the OLIVE CLI utilities in debug mode (-d), all log files will be maintained, even if they were successful.","title":"a.  OLIVE Command Line Logging"},{"location":"cli.html#b-rotating-log-files","text":"OLIVE employs rotating log files in many places. In this context, rotating refers to a log file that is rewritten each time the application is run. The old log file, if any, is renamed with an integer suffix denoting how many invocations in the past it corresponds to. For instance, if you run localanalyze and don\u2019t specify a -l option, you\u2019ll get the default localanalyze.log file. If localanalyze.log already exists, it is moved to localanalyze.log.1 . The system will keep the 10 most recent log files. A file named localanalyze.log.8 means that the file corresponds to eight invocations ago.","title":"b.  Rotating Log Files"},{"location":"cli.html#5-plugin-appendix","text":"","title":"5: Plugin Appendix"},{"location":"cli.html#plugin-types-and-acronyms","text":"Currently, OLIVE supports the plugin technologies listed in the following list. For operating instructions that apply to only a specific technology, refer to that section within this appendix. SAD \u2013 Speech activity detection. SID \u2013 Speaker identification. LID \u2013 Language identification. KWS \u2013 Keyword spotting. QBE \u2013 Query by example based keyword spotting. TID \u2013 Topic identification. SDD \u2013 Speaker diarization and detection. GID \u2013 Gender identification. ENH \u2013 Speech and audio enhancement.","title":"Plugin Types and Acronyms"},{"location":"cli.html#speech-activity-detection-sad","text":"SAD plugins are frame scorers that take an audio list file and annotate the presence and location of speech in each audio file in that list. In standard operation, SAD plugins produce a single output file for each input file, by default in a directory called OUTPUT in the location localanalyze was called from. Output files carry the name of the original input file, but with a new extension \u201c.scores\u201d \u2013 for example, audioFile1.wav will become audioFile1.wav.scores, saved inside OUTPUT/. The format of these results files is a newline separated list of numerical values representing the likelihood that each 10ms frame of the audio file contains speech. Typically, a score above 0 represents speech detection, and a score below 0 represents no speech. SAD analysis example: $ localanalyze /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst Output files: OUTPUT/audioFile1.wav.scores OUTPUT/audioFile2.wav.scores Example audioFile1.wav.scores contents: -0.22624 -0.10081 0.00925 0.12365 Alternatively, SAD plugins can be run with the --thresholds flag to have localanalyze automatically convert the frame scores to regions of speech, by applying the provided threshold. SAD analysis example using thresholds: $ localanalyze --thresholds = 0 .0 /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst This will provide a single output file in OUTPUT directory corresponding to the provided threshold: 0.0.pem. If more than one threshold is provided, there will be a PEM file placed into OUTPUT corresponding to each provided threshold. Example PEM output: /data/sad/test/audioFile1.wav 1 speech 63.110 66.060 /data/sad/test/audioFile1.wav 1 speech 66.510 69.230 /data/sad/test/audioFile1.wav 1 speech 93.480 96.090 /data/sad/test/audioFile1.wav 1 speech 96.570 100.760 Note that if negative thresholds are to be used, it is very important to specify the thresholds using an \u2018=\u2019 character. For example, this threshold specification is valid: --thresholds=-2.0,4.0 And this is not valid: --thresholds -2.0,4.0 If only thresholds of 0 or above are going to be used, it is acceptable to omit the equals sign.","title":"Speech Activity Detection (SAD)"},{"location":"cli.html#speaker-identification-sid","text":"SID plugins are global scorers that take an audio list file and return a score for each enrolled speaker model scored against the audio in each input audio file. Generally, a score above 0 for an enrolled speaker model represents that speaker being detected in the respective audio file. In order to perform analysis on a file with a SID plugin you must first enroll one or more target speakers. The enrollment list file for a SID plugin follows this format for each line: <audio_file_path> <speaker_id> An example enroll.lst: /data/spkr_example_audio_5760.wav UIM1 /data/spkr_example_audio_5761.wav UIM1 /data/spkr_example_audio_5762.wav John /data/spkr_example_audio_5763.wav John Enrolling these speakers with localenroll : $ localenroll /path/to/plugins/sid-embed-v2/domains/multi-v1/ enroll.lst Example localanalyze call: $ localanalyze ./plugins/sid-embed-v2/domains/multi-v1/ ./data/sid/test/testAudio.lst By default, the output of this call is written to output.txt in the directory the command was run. The format of output.txt contains one line for each enrolled speaker model, for each input audio file, and the corresponding score: <audio_file_path> <speaker_id> <score> Example output.txt: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Speaker Identification (SID)"},{"location":"cli.html#trial-based-calibration-speaker-identification-sid-tbc","text":"Trial-based Calibration SID plugins are identified by \u2018tbc\u2019 in the plugin name. They are used very similarly to a basic SID plugin, using localenroll and localanalyze just like the examples shown in the previous example. The benefit of TBC plugins is that they allow OLIVE to perform calibration at test-time, based on the actual data conditions being encountered, rather than being forced to use a single, global calibration model that has been trained a priori. The basics of TBC enrollment and testing follow the previous SID examples, but the additional options and outputs available to TBC are detailed below. The standard approach to calibration uses a \u201cone size fits all\u201d calibration model based on the developer\u2019s best understanding of potential operating conditions. This is problematic when the user either doesn\u2019t know ahead of time what likely conditions are, or when operating conditions may vary widely. Trial-based calibration was developed as a means of providing calibration that is responsive to the particular conditions of a trial, and adapts its calibration model based on the conditions encountered. There are two ways we have developed to do this. The first draws from a pool of available data (either provided by the developer or augmented with user-provided data) and uses measures of the conditions found within this data and the trial conditions to build an ideal calibration set on the fly. This is useful in that this approach can also determine when a trial CANNOT be calibrated, and to measure the success of calibration when it is possible. The clear downside of this approach is that it is quite slow. A second approach to TBC is to use a model that has used a DNN to learn to predict both calibration parameters and confidence from large sets of trials and available calibration data. This approach is very fast (about 5000 times faster than the first approach) but has the downside that expanding the calibration set by the user\u2019s data isn\u2019t possible. This plug-in provides both approaches in the TBC plug-in, as two separate domains. In addition to the output score file detailed in the SID section, TBC plugins have additional possible outputs.","title":"Trial-based Calibration Speaker Identification (SID TBC)"},{"location":"cli.html#speech-detection-output","text":"Segmentation files are used to label time regions in the speech signal. We use this format for voice activity detection (VAD). If an output_ivs_dump_path is provided as an option to localenroll or localanalyze , the system will produce this file in a folder corresponding to the wav_id for all registered waveforms. The format is the following: md5sum start end (in seconds) Example: b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.060 0.060 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.090 0.090 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.110 0.170 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.200 0.200 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.560 3.550","title":"Speech Detection Output"},{"location":"cli.html#persistent-i-vectors","text":"For both localenroll and localanalyze , if the output_ivs_dump_path is defined via an options file with the --options flag, an i-vector from each audio file is saved in this directory for re-use. This reduces unnecessary computation when there exists overlap between lists of wave files to be processed. For instance, if the enroll and test wave file lists are identical (i.e., the case of an exhaustive comparison of a set of files), i-vector persistence will reduce overall computation by almost a factor of 2 since i-vector extraction consumes more of the computation required for an evaluation. I-vectors will be saved in a sub-directory of output_ivs_dump_path based on the base name of the wave file. In addition to this optional feature, the enrollment vectors are loaded in memory prior to verification and if the md5sum of a test audio file matches one used in the enrollment process, the corresponding vector will be used instead of re-processing the audio. This is because vector extraction is identical between enrollment and verification.","title":"Persistent I-Vectors"},{"location":"cli.html#trial-based-calibration","text":"Trial-based calibration (TBC) does not change the way calibration works but changes the way calibration is used. It relaxes the constraint on the system developers to train a calibration model that is ideally matched to the end use conditions. Rather than train a calibration model a priori, the system postpones this training until the conditions of the particular verification trial are known to the system; a trial consists of comparing test audio to an enrolled speaker model. The goal of trial-based calibration is to use information about the trial to generate an ideal calibration set for the trial conditions using the reservoir of possible calibration audio files available. Using this set, a calibration model tailored to the conditions of the trial can be trained and used to effectively calibrate the verification score. The TBC operation and output differs from traditional SID plugins; it may choose to reject a trial and NOT output a score if insufficient data is available for calibrating for those conditions. For instance, the output may look similar to the following: waves/T6_ACK2.sph T6 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00) waves/T6_ACK3.sph T1 0.0 -inf Unable to calibrate with only 12 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T2 0.0 -inf Unable to calibrate with only 3 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T3 0.0 -inf Unable to calibrate with only 0 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T4 0.0 -inf Unable to calibrate with only 2 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T6 0.0 -inf Unable to calibrate with only 9 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T4_ACK3.sph T5 0.96153318882 4.04821968079 Used 95 target trials in calibration waves/T4_RC.sph T4 3.46785068512 4.07499170303 Used 95 target trials in calibration waves/T5_Tip2.sph T5 8.90770149231 4.07352733612 Used 98 target trials in calibration waves/T5_RC.sph T5 10.2386112213 4.03855705261 Used 47 target trials in calibration waves/T4_Tip2.sph T4 10.8663234711 4.07404613495 Used 218 target trials in calibration waves/T4_Tip1.sph T4 11.793006897 3.98730397224 Used 164 target trials in calibration waves/T4_ACK2.sph T4 11.8091144562 3.90610170364 Used 119 target trials in calibration waves/T4_ACK1.sph T4 12.2115001678 4.16342687607 Used 208 target trials in calibration waves/T5_ACK1.sph T5 13.8099250793 3.99625587463 Used 99 target trials in calibration waves/T5_Tip1.sph T5 14.9411458969 3.96994686127 Used 83 target trials in calibration waves/T4_ACK3.sph T4 16.003446579 4.05554199219 Used 146 target trials in calibration The output follows the structure: <testwave> <modelid> <score> <confidence> <info> In the instance of insufficient calibration segments being located, a score of 0.0 and a calibration confidence of -inf is given. In contrast, when sufficient data is found for calibration, the number of segments used in calibration is reported. There exist two options for applying calibration with the current plugin: DNN-assisted TBC, normal TBC, or global calibration. Each of these options use duration information to reduce the impact of duration variation impacting calibration performance. Note that changing calibration domains does NOT require re-enrollment of models as these are done in a domain-independent way for any TBC-enabled plugin.","title":"Trial-based Calibration"},{"location":"cli.html#dnn-assisted-trial-based-calibration","text":"DNN-assisted TBC is invoked by passing the tbcdnn-v1 domain to localanalyze . This is a very fast and newly pioneered effort by SRI to reduce the computation needed to apply dynamic calibration methods to speaker recognition and operates with very low overhead compared to global calibration, and yet significantly benefits calibration performance in varying conditions or conditions that differ from the development conditions. localanalyze ... <plugin>/domains/tbcdnn-v1 test.lst","title":"DNN-assisted Trial-based Calibration"},{"location":"cli.html#normal-trial-based-calibration","text":"TBC is applied by default with the 'sid-embedDnnTbc-v1' plugin. The data within the domain (such as 'tbc-v1') is used as candidate calibration data. localanalyze ... <plugin>/domains/tbc-v1 test.lst TBC is applied to verification scores on a trial-by-trial basis. As such, verification using TBC will operate at a speed much slower than global or DNN-assisted TBC depending on the size and make-up of the TBC data. This should be considered when using TBC in a cluster environment where it is the number of trials (model vs test comparisons) that determine the running time instead of the number of test files.","title":"Normal Trial-based Calibration"},{"location":"cli.html#global-calibration","text":"Each domain can be used to invoke global calibration. This is particularly useful for user-defined data as it provides a rapid means of improving calibration performance without a dramatic increase in computation time. In this case, verification will operate at a much faster pace since TBC is essentially disabled and the global calibration model parameters are applied to all scores. In order to invoke global calibration, and optional parameter must be passed to localanalyze via an options file: echo \"[global scoring] global_calibration = True \" > options.lst localanalyze -- options options . lst ... < plugin >/ domains / tbc - v1 test . lst","title":"Global Calibration"},{"location":"cli.html#optional-parameters","text":"The TBC-based plugins offer several tunable parameters via the options parameter to localenroll or localanalyze . These can be passed to the enrollment phase or verification phase by preceding the options in an ascii text file as such: $ cat options . lst [ enrollment ] ... enrollment options per line ... [ global scoring ] ... verification options per line ... The optional parameters and their purpose are provided below. tbc_confidence_threshold = 3.0, # Similarity threshold for processing a trial with TBC score_threshold = 0.0, # Score offset subtracted from output LLRs to assist in making 0 threshold output tgt_max = 300, # The maxmimum number of target trials used for TBC of a trial imp_max = 3000, # The maxmimum number of impostor trials used for TBC of a trial tgt_imp_min = 20, # The mimum number of relevant target and impostor calibration trials needed to use TBC (rejected otherwise) global_calibration = False, # Apply global calibration instead of TBC ivs_dump_path = None, # Output path for dumping vectors and meta information sad_threshold = 0.5, # Threshold for speech activity detection (higher results in less speech) sad_filter = 1, # Smoothing of LLRs from SAD DNN prior to thresholding sad_interpolate = 1, # If > 1, a speed up of SAD by interpolating values between frames (4 works well) Utilizing these parameters in an options file may look like this: echo \"[enrollment] sad_threshold = 1.0 ivs_dump_path = . / embeddings [ global scoring ] sad_threshold = 1.0 ivs_dump_path = . / embeddings tgt_max = 100 tgt_imp_min = 50 \" > options.lst localenroll -- options options . lst ... < plugin >/ domains /< domain > enroll . lst localanalyze -- options options . lst ... < plugin >/ domains /< domain > test . lst","title":"Optional Parameters"},{"location":"cli.html#verification-trial-output","text":"The format for the verification trial is the following. Note that for global calibration, the optional parameters (calibration_confidence and calibration_remarks) are not output. Output format: wav_id speaker_id score [ calibration_confidence calibration_remarks ] Here is an example of score executed with Global Calibration: waves/T1_ACK1.sph T6 5.19274568558 waves/T1_ACK1.sph T4 1.204241395 waves/T1_ACK1.sph T5 1.69025540352 Here is an example of scores executed with DNN-assisted TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used DNN-assisted TBC with confidence 5.751 waves/T1_ACK1.sph T4 1.204241395 5.12 Used DNN-assisted TBC with confidence 3.122 waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with confidence above threshold (3.00) Here is an example of scores executed with normal TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used 67 target trials in calibration waves/T1_ACK1.sph T4 1.204241395 5.12 Used 73 target trials in calibration waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00)","title":"Verification Trial Output"},{"location":"cli.html#speaker-diarization-and-detection-sdd","text":"The overall goal of SDD plugins is to detect regions of speech within an audio recording that are associated with different speakers, and then identify those speakers if possible. SDD plugins have three different modes of operation, as outlined below. Changing the execution mode for SDD is done by passing an options file to localanalyze as an argument to the --options flag. The main behavior and premise of the technology and plugin remain the same, but each mode changes the format and information contained in the output file. Running the SDD plugin is very similar to running SID plugins, with the same syntax for enrollment and testing. Currently, training or adaptation through localtrain is not supported, but enrolling new speakers and testing against enrolled speakers is as simple as: $ localenroll /path/to/plugins/sdd-embed-v1/domains/multi-v1/ enrollmentAudio.lst $ localanalyze /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst The enrollment and test audio file lists in this example follow the same format as the lists used by SID plugins, described above. By default, if run as above with no options, the plugin will run in Speaker Detection mode, and provide the output described above. In order to run in SID or SID Exhaustive mode, you will need to provide an options file to specify that behavior: $ localanalyze --options options.lst /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst Where options.lst is a text file with contents similar to: [region scoring] mode: SID_EXHAUSTIVE sad_threshold: 0.0 diarization_max_num_speakers: 2 The [region scoring] header is alerting the plugin that the options are being passed for scoring, and all of the parameters shown above (sad_threshold, diarization_max_num_speakers, mode) are optional parameters. The mode option controls the output behavior as described above, and the possible options are SID, SID_EXHAUSTIVE, and SPEAKER_DETECTION, and described directly below. The sad_threshold defaults to 2.0, and is used to fine tune the threshold for the internal speech activity detection plugin if necessary. The parameter diarization_max_num_speakers defaults to 4, and is the largest number of speakers the plugin will attempt to consider when clustering segments within the file.","title":"Speaker Diarization and Detection (SDD)"},{"location":"cli.html#sdd-execution-modes","text":"To facilitate the understanding of each mode\u2019s output, consider that speech from an audio file is made up of clusters of speakers, and each cluster will have one or more contiguous segment of speech. SPEAKER_DETECTION The goal of Speaker Detection is to show the most probable speaker model for each segment of the input audio file. As output, this mode gives one line per segment within the file, along with the top scoring enrolled model for the cluster that segment belongs to, and that cluster's score for the given model. Note that many scores will be repeated in the output file, since each segment in the cluster shares the same score for a given speaker model. This mode is performed by default if no options file with a mode override is given. SID This mode is meant for triaging large amounts of audio files when the main goal is just finding which of these files may contain speech from one of the enrolled speakers. The output is the maximum score for each enrolled speaker within the audio file after scoring against each cluster in the file, as well as the timestamps for the beginning and end of the longest segment within the cluster that scored the highest for that model. This gives a specific segment to spot check and evaluate the plugin's decision if needed. SID_EXHAUSTIVE When using SID Exhaustive, each diarized cluster is scored against each enrolled model. The output is a complete listing for every speech segment of the input audio file, the score from testing every enrolled model against the cluster that the segment belongs to. Many scores will be repeated in the output file, since each segment in the cluster shares the same score. In this example, Chris and Jimmy are the only enrolled models, and 5 total segments were identified within the file.","title":"SDD Execution Modes"},{"location":"cli.html#language-identification-lid","text":"LID plugins are global scorers that act very similar to SID with respect to scoring, except that each score corresponds to a language model rather than a speaker model. In most cases, LID plugins will be delivered from SRI with a set of languages already enrolled. Languages can be added to some plugins by the user if enough appropriate data is available, through the localtrain CLI call. Details on this will be added to this document in a later revision. Example localanalyze call: $ localanalyze /path/to/plugins/lid-embed-v2/domains/multi-v1/ testAudio.lst Output format: <audio_file_path> <language_id> <score> Output example: /data/lid/audio/file1.wav fre -0.5348 /data/lid/audio/file1.wav eng 3.2122 /data/lid/audio/file1.wav spa -5.5340 /data/lid/audio/file1.wav rus 0.5333 /data/lid/audio/file1.wav ara -4.9444 /data/lid/audio/file2.wav fre -2.6564","title":"Language Identification (LID)"},{"location":"cli.html#keyword-spotting-kws","text":"KWS is an automatic speech recognition (ASR) based approach to detecting spoken keywords in audio. Rather than enrolling target keywords from audio, as you would with query-by-example, telling the plugin what keywords to search for is done by passing an IDs file to localanalyze . The format of the IDs file is: IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Keyword Spotting IDs example, search_list.lst: remote torpedo voice recognition Example KWS localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst /path/to/plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/testAudio.lst The output format for KWS plugins is identical to that of QBE. It is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/kws/testFile1.wav 7.170 7.570 remote 1.0 /data/kws/testFile1.wav 10.390 10.930 remote 0.693357 /data/kws/testFile1.wav 1.639 2.549 voice recognition 1.0","title":"Keyword Spotting (KWS)"},{"location":"cli.html#automatic-speech-recognition-asr","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. ASR plugins do not require any enrollment of words (like QBE) or specification of words of interest (like traditional KWS), but instead rely on the vocabulary model built into the domain to define the list of available words. All that is necessary for scoring an audio file for ASR is a list of input files to be scored, which follows the format below. Generic input audio list format: <audioFile_1> <audioFile_2 (opt)> ... <audioFile_N (opt)> A specific example of this, called testAudio.lst, might look like: /data/asr/testFile1.wav /data/asr/testFile2.wav /data/asr/testFile3.wav Note that if the files are not contained within the directory that localanalyze is being run from, or if a relative path from that location is not provided, the full file path to each file is necessary. An example ASR localanalyze call: $ localanalyze /home/user/oliveAppData/plugins/asr-dynapy-v1/domains/eng-tdnnChain-tel-v1/ /data/asr/test/testAudio.lst The output format for KWS plugins is identical to that of QBE and other region-scoring OLIVE plugins. ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. Output is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <word> <score> An example in English: /data/asr/testEnglish1.wav 0.000 0.190 and 43.00000000 /data/asr/testEnglish1.wav 0.210 0.340 we're 44.00000000 /data/asr/testEnglish1.wav 0.330 0.460 going 97.00000000 /data/asr/testEnglish1.wav 0.450 0.520 to 97.00000000 /data/asr/testEnglish1.wav 0.510 0.940 fly 66.00000000 /data/asr/testEnglish1.wav 1.080 1.300 was 31.00000000 /data/asr/testEnglish1.wav 1.290 1.390 that 24.00000000 /data/asr/testEnglish1.wav 1.290 1.390 it 22.00000000 /data/asr/testEnglish1.wav 1.380 1.510 we're 27.00000000 /data/asr/testEnglish1.wav 1.500 1.660 going 97.00000000 /data/asr/testEnglish1.wav 1.650 1.720 to 98.00000000 /data/asr/testEnglish1.wav 1.710 1.930 fly 94.00000000 /data/asr/testEnglish1.wav 1.920 2.110 over 79.00000000 /data/asr/testEnglish1.wav 2.100 2.380 saint 93.00000000 /data/asr/testEnglish1.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: /data/asr/testMandarin1.wav 0.280 0.610 \u6218\u6597 99.00000000 /data/asr/testMandarin1.wav 0.600 0.880 \u7206\u53d1 98.00000000 /data/asr/testMandarin1.wav 0.870 0.970 \u7684 99.00000000 /data/asr/testMandarin1.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 /data/asr/testMandarin1.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 /data/asr/testMandarin1.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 /data/asr/testMandarin1.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 /data/asr/testMandarin1.wav 3.130 3.340 \u7684 100.00000000 /data/asr/testMandarin1.wav 3.330 3.720 \u6b66\u88c5 55.00000000 /data/asr/testMandarin1.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: /data/asr/testFarsi1.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 /data/asr/testFarsi1.wav 0.470 0.740 51.00000000 \u0627\u06cc /data/asr/testFarsi1.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 /data/asr/testFarsi1.wav 0.310 0.460 99.00000000 \u0645\u06cc /data/asr/testFarsi1.wav 0.450 0.680 99.00000000 \u06af\u0645 /data/asr/testFarsi1.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f /data/asr/testFarsi1.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Automatic Speech Recognition (ASR)"},{"location":"cli.html#query-by-example-keyword-spotting-qbe","text":"Query by example is a specific type of keyword spotting plugin that searches for keywords matching a spoken word or phrase example, rather than from a text example like traditional KWS. This means that it is necessary to enroll keywords into the system from audio examples with localenroll before using QBE to search audio for these keywords or phrases. Enrollment follows the same format as enrolling speakers into a SID plugin, with the enrollment audio list following this format: <audio_file_path> <keyword_id> Example: /data/qbe/enroll/watermelon_example1.wav Watermelon /data/qbe/enroll/watermelon_example2.wav Watermelon /data/qbe/enroll/airplane_example.wav Airplane /data/qbe/enroll/keyword_example.wav Keyword Note that currently each enrollment audio file must contain ONLY the keyword that is desired to be enrolled. Also note that the text label in the second column of the enrollment file is only for user readability and is not used by the system when determining what to search the audio file for. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006","title":"Query by Example Keyword Spotting (QBE)"},{"location":"cli.html#topic-identification-tid","text":"Topic Identification plugins attempt to detect and categorize the topic being discussed within an audio recording from a known and pre-enrolled set of available topics, and report this topic (if any are detected) to the user. Each domain of a TID plugin is language-dependent, and should display the target language as the first string of the domain\u2019s name. Some TID plugins may be delivered with pre-enrolled topics \u2013 please consult the documentation that accompanied your delivery if you are unsure if this is the case. If no topics are enrolled, however, or if you wish to enroll new topics, the format is very similar to enrolling new speakers into a SID plugin, and follows the same CLI call structure, with one caveat. $ localenroll --local $domain $enroll_list Due to limitations with the current TID technology, enrollment must be performed with the --local flag set. This limits OLIVE to serialized processing, which will process the enrollment slightly slower, but avoid resource-competition issues that may cause the system to become unstable and crash. Enrollment audio lists for TID follow the same format as SID, but substitute a topic name for a speaker label. Note: in the current version, we require the user to provide audio examples that are primarily about the topic of interest as enrollment examples. If there are significant portions of an audio file that are off-topic, we suggest the file be cut and fed as separate examples. Enroll list format: <audio_file_path> <topic_id> Example: /tid/enroll/topic_example_audio_5760.wav Travel /tid/enroll/topic_example_audio_5761.wav Travel /tid/enroll/topic_example_audio_5762.wav Travel /tid/enroll/topic_example_audio_5763.wav Travel To run TID, once target topics have been enrolled, the call to localanalyze is very similar to other plugin types, with the current plugin again requiring the --local flag. $ localanalyze --local /path/to/plugins/tid-svm-v2/domains/r-tel-v1/ testAudio.lst As with the SID and LID plugins, by default the plugin\u2019s output will be written to the file \u201coutput.txt\u201d in the directory localanalyze was called from. This can be overridden by passing localanalyze the -o flag, as well as an alternate file to save the results to. The TID results structure is very similar to KWS, with the following format for each line: <audio_file_path> <start_time_s> <end_time_s> <topic_id> <confidence_score> Example: /data/tid-example_wavs/20110511_GET-TOGETHER.wav 18.790 55.300 transportation 0.0222 /data/tid-example_wavs/20110511_TRANSPORTATION.wav 4.010 19.140 transportation 0.4532 The start and end timestamps above are provided in seconds. The will be displayed as it was supplied in the second column of the enrollment list. The will be between 0 and 1, and marks the confidence of the system in the decision of this topic. Please note that output.txt will be overwritten by each successive experiment. Please back it up or use the -o option to localanalyze if you would like to save the results. Also note that the start and end times for each topic refer to the chunk in each audio that has the highest probability of being about that topic. Currently, we report only ONE such segment per file in order to help the user locate the most useful part. The score associated with that segment is global, in that it represents the likelihood that this topic is present anywhere in the document.","title":"Topic Identification (TID)"},{"location":"cli.html#important-background-example-information","text":"In order to train a Topic detector, we currently use an SVM classifier. This model uses \"positive\" examples of the topic as provided by the user during the enrollment phase, as well as \"negative\" examples to model what is not the topic. Those negative examples can be crucial to the performance of the final system. Currently, those \"negative\" examples come pre-processed as a python numpy archive and cannot be modified by the user explicitly. We do provide two different numpy archives that can be tried by a user: BG_RUS001-train-acc+neg-no-travel.npz (default) BG_RUS001-train-test-random-all-chunk-plugin.npz Archive 2) includes only data from the RUS001 conversational corpus, which didn't have very topic-specific prompts. Archive 1) includes a mix of RUS001 data as well as a subset of the RU_CTS conversational corpus which was topic annotated. We excluded examples pertaining to 'TRAVEL' in this archive, but this archive contains conversations about the following (loosely defined) topics: ACTIVITIES BIRTHDAY_WISHES CHILDREN ECONOMY EDUCATION ENTERTAINMENT FOOD_DRINK FRIENDS_RELATIVES GET-TOGETHER HEALTH HOME HOME_MAINTENANCE IMMIGRATION LANGUAGE_COMMUNICATION LEISURE LIFE_PHILOSOPHY_RELATIONSHIPS LOCATION_DESCRIPTION MARRIAGE MOOD_PHYSICAL MOVING_HOMES MUSIC PERFORMANCE_REHEARSAL PETS POLITICS PROJECT READING_WRITING RELIGION_HOLIDAY SPEECH_COLLECTION_PROJECT TECHNOLOGY TRANSPORTATION TV_MOVIES WEATHER_CLIMATE WORK If the topic you are training for is very similar to a topic listed above, it might be worth it trying archive (2) as well. In the future, we will provide the opportunity for the user to feed his own negative examples.","title":"Important Background Example Information"},{"location":"cli.html#gender-identification-gid","text":"Gender ID plugins allow for triage of audio files to identify only those containing speakers of a certain gender. For scoring files, gender identification plugins operate in the same manner as SID and LID plugins. GID plugins are delivered with two pre-enrolled classes; \u2018m\u2019 and \u2018f\u2019, for male and female, respectively, so user-side enrollment is not necessary. To score files with a GID plugin using localanalyze , use the following syntax: $ localanalyze /path/to/plugins/gid-gb-v1/domains/clean-v1/ testAudio.lst Where the output follows this format: <audio file 1> m <male likelihood score> <audio file 1> f <female likelihood score> \u2026 <audio file N> m <male likelihood score> <audio file N> f <female likelihood score> Example: /data/gender/m-testFile1.wav m 0.999999777927 /data/gender/m-testFile1.wav f -0.22073142865","title":"Gender Identification (GID)"},{"location":"cli.html#enhancement-enh","text":"Enhancement or AudioConverter plugins are audio-to-audio plugins that take an audio file as input and provide a second audio file as output. Currently they are used to enhance the input audio file, for listening comfort and/or intelligibility. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav Running speech enhancement is as simple as: $ localanalyze /path/to/plugins/enh-mmse-v1/domains/multi-v1/ inputAudio.lst In addition, you can pass an optional PEM file that specifies optional regions to the plugin \u2013 the current enhancement plugin uses this file to pass the \u2018noise regions\u2019 to OLIVE to allow the plugin to build a noise profile for more accurate characterization and removal of the noise present in that audio file.","title":"Enhancement (ENH)"},{"location":"cli.html#6-testing","text":"In this section you will find a description of benchmarking that is performed with each release of OLIVE to measure the performance of each plugin with respect to speed and memory usage. The hardware and software details used for these results are provided in the next section, with each of the current plugins\u2019 memory and speed results following.","title":"6: Testing"},{"location":"cli.html#a-benchmarking-setup-hardwaresoftware","text":"Each data point below was obtained by running the OLIVE 4.9.1 software with a 4.8.0 runtime that has been patched to 4.9.1 (contains libraries needed by the new Topic Identification and Enhancement plugins). Tests were performed on the CentOS 7 operating system, on a Gigabyte BRIX GB-BXI7-5500. This machine has 16GB RAM available, and an Intel i7-5500U processor, which is a dual core (quad-thread) processor that runs at 2.4 GHz base (3.0 GHz turbo).","title":"a.  Benchmarking Setup (Hardware/Software)"},{"location":"cli.html#b-plugin-memory-usage-results","text":"These results were generated by running several files through each plugin via the OLIVE CLI using localanalyze , one at a time, while measuring the memory used by the localanalyze utility. The files vary in length from 10 seconds through 4 hours, and this allows us to see how each plugin handles scaling the audio file length up, and also compare overall resource utilization between individual plugins. The values reported for each plugin and audio file are the peak memory usage, in MB \u2013 lower values are better. Note that this report is for processing a single file at a time and is representative of memory utilization that can be expected for serialized processing, or processing on a machine with a single processing core. Parallel processing can cause memory usage to rise. TODO: Need to put the charts/resuts in here!","title":"b.  Plugin Memory Usage Results"},{"location":"cli.html#sad","text":"","title":"SAD"},{"location":"cli.html#sad-memory-usage-mb","text":"Plugin sad-dnn-v4 sad-dnn-v4 sad-dnn-v4 Domain digPtt-v1 ptt-v1 tel-v1 10s 142 142 142 1 min 158 158 158 10 min 221 221 221 30 min 408 408 408 2 hr 1,262 1,262 1,262 4 hr 2,377 2,377 2,377","title":"SAD Memory Usage (MB)"},{"location":"cli.html#c-plugin-speed-analysis-results","text":"The following charts show the speed performance of the current release of each OLIVE plugin. Values are reported as the speed of the plugin in \u2018times faster than real time\u2019 and represent how fast the plugin is able to process the input audio data, with respect to the length of that data \u2013 higher is better. Each plugin is fed 10 hours of total data consisting of roughly 4-minute audio cuts to achieve this measurement. For this test, OLIVE has been limited to using a single core for processing, in order to keep measurements and results consistent. Note that enabling parallel processing if multiple CPU cores are available will improve performance.","title":"c.  Plugin Speed Analysis Results"},{"location":"cli.html#plugin-speed-statistics-reported-in-times-faster-than-real-time","text":"Plugin Domain Speed (x RT) sad-dnn-v4 digPtt-v1 104.8 sad-dnn-v4 ptt-v1 111.5 sad-dnn-v4 tel-v1 117.0 sid-embed-v2 multi-v1 51.6 sid-embedDnnTbc-v1 tbc-v1 43.9 sid-embedDnnTbc-v1 tbcdnn-v1 64.5 sid-embedTbc-v1 tbc-v1 40.2 lid-embed-v2 multi-v1 39.7 kws-batch-v9 eng-tel-v1 1.11 kws-batch-v9 eng-tel-v2 1.09 kws-batch-v9 f-tel-v1 1.23 kws-batch-v9 r-tel-v1 1.61 kws-batch-v9 r-tel-v2 1.61 qbe-tdnn-v4 digPtt-v1 15.2 qbe-tdnn-v4 multi-v1 16.1","title":"Plugin Speed Statistics Reported in Times Faster than Real Time"},{"location":"contact.html","text":"The Team The OLIVE Software and Plugins are developed and maintained by the engineers, scientists, and linguists of the Speech Technology and Research (STAR) Laboratory at SRI International. OLIVE Page at SRI.com For more information about STAR Lab and current research or development, please visit the STAR Lab SRI International page, or email olive-support@sri.com .","title":"Contact"},{"location":"contact.html#the-team","text":"The OLIVE Software and Plugins are developed and maintained by the engineers, scientists, and linguists of the Speech Technology and Research (STAR) Laboratory at SRI International. OLIVE Page at SRI.com For more information about STAR Lab and current research or development, please visit the STAR Lab SRI International page, or email olive-support@sri.com .","title":"The Team"},{"location":"docker.html","text":"OLIVE Installation for container-based Deliveries These instructions cover the traditional Docker container based OLIVE delivery where the container only includes the OLIVE server alone. For the new, multi-server delivery packaging that includes the Raven Web GUI , please refer to the OLIVE Martini Setup Guide. Deploying OLIVE in an Existing Multi-Container Application If deploying the OLIVE Docker image as a single container in a conventional system then skip to the next section . Otherwise, for users deploying OLIVE in an existing multi-container application managed by Docker Compose or Kubernetes, be sure to create OLIVE containers that meet these requirements: The oliveAppData directory, distributed with an OLIVE release, must be mounted as /home/olive/olive inside the docker container. The oliveAppData is the directory that contains the 'plugins' folder with one or more OLIVE plugins. OLIVE will write logs and persistent enrollments to the oliveAppData directory. The plugins folder mounted in /home/olive/olive can vary by container if specific tasks need to be supported by a container. For example, an olive data directory can be mounted that contains only one plugin. The OLIVE server is executed as 'oliveserver' The OLIVE server is accessible via ports 5588 and 5589 (and internally uses port 5590). External clients must have access to these ports on your OLIVE container(s). You can change the default ports used by the OLIVE server via the --port argument. For example, this command will start the OLIVE server using client accessible ports 5591 and 5592: oliveserver --port 5591 Starting the server with different ports is only necessary if the default OLIVE ports are already used by an existing application, or multiple OLIVE containers are addressable at the same hostname. The number of concurrent jobs supported by an OLIVE server can be restricted using the -j argument. For example, to limit the server to one job at a time: oliveserver -j 1 Limiting the number jobs to one in necessary for the TextTransformer plugin, tmt-statistical-v1 If audio needs to be submitted to OLIVE as a path instead of sent as buffer then please contact SRI for some advanced configuration options. Download, Install, and Launch Docker First you will need to obtain and install Docker - this can be done in a number of ways, the easiest of which is likely through the Docker website. The Community Edition is free (though you now need to create an account with Docker to obtain it), should fulfill most or all OLIVE needs, and is available for several operating systems, including Windows 10. Docker is available here - https://www.docker.com/community-edition and up-to-date installation instructions can be found from Docker. Once Docker is installed, launch it to proceed. When running Docker for the first time, if you are using a Windows OS, you may be prompted to enable some Windows features that are not enabled by default, but are required by Docker to run; Hyper-V and Containers. Select \u201cOk\u201d and wait for the computer to restart to enable these features. Docker should automatically start up once the computer has restarted. Adjust Docker settings (RAM, Cores) If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS Download OLIVE Docker Package Now that Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: olive5.1.0-docker-19Sep2020.tar.gz You should find similar content to below unless told otherwise: olive5.1.0 docs/ - Directory containing the OLIVE documentation oliveDocker/ olive-5.1.0-docker.tar \u2013 Docker image containing OLIVE run.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and oliveserver. run.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and oliveserver. run-shell.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. run-shell.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ sad-dnn-v7.0.0 \u2013 Speech Activity Detection plugin Actual plugins included will depend on the customer, mission, and delivery --optional-- oliveAppDataMT/ plugins/ tmt-statistical-v1.0.0 You may optionally be provided with Machine Translation capabilities, as shown at the end of the list - this plugin has some special runtime constraints, and will be contained in a separate oliveAppData directory as a result. Load the OLIVE Docker Image The first setup step is to load the OLIVE Docker image. Loading the image is as simple as opening a command prompt like PowerShell in Windows or Terminal in MacOS, navigating to the directory containing the OLIVE Docker image, and following the examples below. Loading the docker image (only necessary once). Windows $ cd C : \\ Users \\ < username > \\ olive5 . 1.0 \\ oliveDocker $ docker load - i olive - 5.1 . 0 - docker . tar macOS / linux $ cd / home /< username >/ olive5 . 1.0 / oliveDocker $ docker load - i olive - 5.1 . 0 - docker . tar This operation can take some time; quite a few minutes, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed. Set up run and run-shell scripts Note that we include two (optionally three, if translation is provided) 'run' scripts in this release. run-shell.sh will open a shell within the container, with the OLIVE environment properly set up, allowing a user to run CLI tasks using localenroll and localanalyze. This script is mainly used for troubleshooting and debugging. The other script(s) will launch an OLIVE server using the olive-5.1.0 container, and vary only slightly. run.sh is a general-purpose script currently configured to launch an OLIVE server that will only restrict the number of jobs/workers based on the number of cores available on the host machine, and will allow parallel processing to scale based on this. This should be used for most purposes. (optional) run_mt.sh is a specialized script that is meant to be used to launch an OLIVE server that only points to a Machine Translation plugin. It is configured to limit the number of jobs/workers to one and does not allow parallel processing. It is also configured to have this server listen on different ports than a standard OLIVE server would, in case you need both OLIVE servers to run on the same host. Note that this distinction is the reason that the plugins for deliveries that include translation are split between oliveAppData and oliveAppDataMT - you should be able to support running two containers; one for all plugins that support parallel processing and one for MT - without having to shuffle plugin directories around. The run.cmd / run.sh and run-shell.cmd / run-shell.sh scripts do most of the heavy-lifting for starting the OLIVE docker and associated processes, but before you can run them, you will need to either set an environment variable, or for a more permanent change, open the desired script with a text editor and edit one or two lines to allow them to establish a shared file location that both the host file system (Windows, Linux, or MacOS) and the OLIVE Docker container can access. Both processes are shown below. Environment Variable The run and run-shell scripts are shipped configured to pull the location of the plugins from an environment variable, so that the scripts don't need to be edited out of the box. This does require these variables to be set each time you will need to run If you would like to permanently edit the scripts instead, refer to the next section, direct script editing . To set this location, you will need to set the OLIVE_APP_DATA environment variable, that will adjust what the OLIVE_DATA variable inside the script is set to. This variable needs to point to the path on the host machine that contains the 'plugins' directory you wish for it to use - for typical OLIVE deliveries, this will be [REPLACE-WITH-LOCAL-PATH]/olive5.1.0/oliveAppData/. An example of this setup step for the non-translation server: $ export OLIVE_APP_DATA =[ REPLACE-WITH-LOCAL-PATH ] /olive5.1.0/oliveAppData/ $ ./run.sh Docker will get access to directory: /Users/allen/oliveAppData/ as /home/olive/olive and as /olive-data. Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- ASR asr-dynapy-v2.0.2 2 .0.2 [ 'cmn-tdnnChain-tel-v1' , 'eng-tdnnChain-tel-v1' , 'fas-tdnn-tel-v1' , 'rus-tdnnChain-tel-v1' , 'spa-tdnnChain-tel-v1' ] LDD ldd-sbcEmbed-v1.0.1 1 .0.1 [ 'multi-v1' ] LID lid-embedplda-v2.0.1 2 .0.1 [ 'multi-v1' ] ... etc. --------- Server ready Tue Feb 23 04 :03:13 2021 --------- -- Optional step for Machine Translation Only -- This same step described above is necessary for the run-mt.sh script that will launch the job-limited server that is only running Machine Translation. The only difference is that the environment variable is instead $OLIVE_APP_DATA_MT , and the default location for most OLIVE deliveries will likely be [REPLACE-WITH-LOCAL-PATH]/olive5.1.0/oliveAppDataMT/. $ export OLIVE_APP_DATA_MT= [ REPLACE - WITH - LOCAL - PATH ] /olive5 . 1 . 0/oliveAppDataMT/ $ . /run - mt . sh Docker will get access to directory: /Users/allen/oliveAppDataMT/ as /home/olive/olive and as /olive - data . Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- TMT tmt - statistical - v1 . 0 . 1 1 . 0 . 1 [ 'spa - eng - generic - v2' , 'fre - eng - generic - v1' ] --------- Server ready Tue Feb 23 04:03:13 2021 --------- NOTE that it is still necessary to set the LOCAL_OLIVE_DATA variable within the run-shell script if you wish to have access to shared audio files. On top of setting OLIVE_APP_DATA as above (which will mount plugins inside the container /home/olive/olive/plugins ), you will also need to set the LOCAL_OLIVE_DATA variable within the script. LOCAL_OLIVE_DATA must be set to a location that ends with a directory called olive-data , and can contain scripts, text files, audio files, and any other utilities you might wish to use with the shell while inside your container. To set this variable, you will need to edit this line in side run-shell.sh : LOCAL_OLIVE_DATA=`cd \"$THISDIR/../../../../../../olive-data\" && pwd` So that the path matches where your audio and scripts are stored, for example: LOCAL_OLIVE_DATA=/home/user1/audio/olive-data/ Direct Script Editing The first of these shared file locations, stored as the variable OLIVE_DATA within the run/run-shell scripts, is where the container will write information such as model enrollments and log files, as well as where the server will find plugins. Plugins must be contained in a directory called plugins, located directly within the directory that LOCAL_OLIVE is assigned to. In the examples below, C:\\Users\\<username>\\olive5.1.0\\oliveAppData for the Windows example, and /home/<username>/olive5.1.0/oliveAppData for the macOS/linux example has a child directory called plugins: C:\\Users\\<username>\\olive5.1.0\\oliveAppData\\plugins or /home/<username>/olive5.1.0/oliveAppData/plugins respectively. The second location is saved into the LOCAL_OLIVE_DATA variable, and should be assigned to a folder that contains audio you with to process with OLIVE, or where you plan on placing this audio. It can be any directory on the host file system (Windows or MacOS) as long as the current user has access to it and the last directory in the path is a directory named \u2018olive-data\u2019. The files and folders contained within the directory assigned to LOCAL_OLIVE_DATA will be mounted to /olive-data/ within the OLIVE Docker container, and accessible at that location when operating within the container using the run or run-shell scripts. run-shell.sh and run.sh (macOS and linux) # MUST set OLIVE_DATA - For convenience we set it to the value of $OLIVE_APP_DATA , but one could also manually edit if # you don't normally set OLIVE_APP_DATA. The path at OLIVE_DATA must include your OLIVE 'plugins' folder : \" ${ OLIVE_APP_DATA : ? \"ERROR: OLIVE_APP_DATA is not set. Set this to the location of your OLIVE plugins folder\" } \" OLIVE_DATA= $OLIVE_APP_DATA # Optionally set LOCAL_OLIVE_DATA to a path on the host that ends with a directory named 'olive-data' LOCAL_OLIVE_DATA=\"/home/ <username> /olive-data\" run-shell.cmd and run.cmd (Windows) REM You must set LOCAL_OLIVE below to the directory on your local host that contains the OLIVE 'plugins' directory set OLIVE_DATA=C:\\Users\\<username>\\olive5.1.0\\oliveAppData set LOCAL_OLIVE_DATA=C:\\Users\\<username>\\olive-data Note that for macOS and linux hosts, the OLIVE_DATA variable is automatically assigned to the value of the $OLIVE_APP_DATA environment variable, if it is preferable to set this before calling the run scripts, instead of editing the scripts themselves. Your delivered run.sh and/or run-shell.sh scripts may have had the line that sets OLIVE_DATA modified to match your specific software installation package more closely and make the necessary changes clearer, especially if there are special cases to consider for your delivery, and may look more similar to this: OLIVE_DATA=[REPLACE-WITH-LOCAL-PATH]/olive5.1.0/oliveAppDataMT/ An example of what this might look like after adapting it to your local environment: OLIVE_DATA=/home/users/allen/olive5.1.0/oliveAppDataMT/ or OLIVE_DATA=/Users/allen/olive5.1.0/oliveAppDataMT/ Note that if your run.sh or run-shell.sh scripts look like this, OLIVE_DATA will not be set automatically based on your OLIVE_APP_DATA environment variable, but must be manually changed within the script before running. Run OLIVE scripts Now that all of the prep work has been done, you are ready to launch OLIVE and start running jobs. Windows hosts Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd C:\\Users\\<username>\\olive5.1.0\\oliveDocker $ .\\run.cmd -or- $ .\\run-shell.cmd macOS and linux hosts Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd / home /< username >/ olive5 .1.0 / oliveDocker $ export OLIVE_APP_DATA =/ home /< username >/ olive5 .1.0 / oliveAppData / ( Optional step if you haven 't edited the specified line in the `run` scripts to point to your plugins) $ ./run.sh -or- $ ./run-shell.sh Which script you run will depend on what task you are hoping to complete. The run.cmd or run.sh script will fire up the OLIVE Docker container and immediately load the oliveserver process. This is to be used alongside the OLIVE GUI or another tool that interacts with oliveserver through the OLIVE API. For command line processing, the run-shell.cmd or run-shell.sh scripts will launch the OLIVE Docker container and open a command line prompt with the OLIVE environment properly set up for running localenroll , localanalyze , and other command-line based tasks. Unload OLIVE Docker Container Image To remove/unload a docker image, whether to upgrade OLIVE containers or for any other reason, first, check that the container is actually still loaded. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE olive-5.1.0-docker latest d00396687de1 5 days ago 3.56GB And then unload it. $ docker rmi olive-5.1.0-docker [Optional] Install, set up, and launch OLIVE GUI An installation of OpenJDK 11 is required for running the OLIVE GUI. Instructions for obtaining and installing OpenJDK 11 will depend on your host's OS and can be found here Once you have installed Open JDK 11, you may need to set the JAVA_HOME environment variable, so that the host operating system knows where to find the appropriate JDK. On macOS and linux, this should be done automatically, or can be done as simply as exporting the JAVA_HOME environment variable to the appropriate location for your machine and OS, and/or with the correct settings: $ export JAVA_HOME =/ usr / libexec / java_home - v 11 You should then be free to launch the OLIVE GUI by simply executing the launcher script included in On Windows this process is slightly more complicated, but can be done by right-clicking This PC and selecting Properties. Before continuing, please refer to any additional instructions you may have been given when provided with access to the OLIVE software delivery. This process outlined below is currently being revised to be simpler, and may not be necessary for you to perform, especially for macOS or Windows 10 hosts. From there, select Advanced system settings in the left-side navigation menu. Navigate to the Advanced tab and select the Environment Variables button on the lower right portion of the dialog. From the Environment Variables window, select New from the lower System variables section, and create a new variable named JAVA_HOME that points to the full path of the JDK installation path. Typically, this is C:\\Program Files\\Java\\jdk11\\bin. Finally, you are ready to launch the GUI, either by navigating in File Explorer to the location where the package was uncompressed, then OliveGui/olive-ui/bin/, and double-clicking Nightingale.bat. You can create a shortcut to this file that you can then place on your desktop for more convenient access. Alternatively, it can be launched from the Windows PowerShell by navigating to the same location, then invoking Nightingale.bat. When launching this way, it is possible to provide a config file if desired. $ cd C:\\Users\\<username>\\olive5.1.0\\oliveGui $ .\\Nightingale.bat -c nightingale_config.xml [Optional] If you wish to use the Batch mode of the OLIVE GUI, an additional step is required to allow the Batch GUI to access audio files stored on the host operating system. Once you\u2019ve installed and run the GUI at least once, open and edit the .scenic-properties file that was created in the user\u2019s home folder in the host OS. For example, for a user named \u2018olive\u2019, this file will be created at the location C:\\Users\\olive.scenic-properties. Once you\u2019ve located and opened this file (it is a text file), add the following lines: use_scenic_data=true olive_data_directory=C:\\\\Users\\\\<username>\\\\olive-data Note that it is very important that the directory assigned to olive_data_directory matches the directory assigned to LOCAL_OLIVE_DATA in the step above. Note also that it is necessary to include double backslashes. If you don\u2019t, the Java properties reader strips them from the variable, and the path will not be valid.","title":"OLIVE Docker Setup"},{"location":"docker.html#olive-installation-for-container-based-deliveries","text":"These instructions cover the traditional Docker container based OLIVE delivery where the container only includes the OLIVE server alone. For the new, multi-server delivery packaging that includes the Raven Web GUI , please refer to the OLIVE Martini Setup Guide.","title":"OLIVE Installation for container-based Deliveries"},{"location":"docker.html#deploying-olive-in-an-existing-multi-container-application","text":"If deploying the OLIVE Docker image as a single container in a conventional system then skip to the next section . Otherwise, for users deploying OLIVE in an existing multi-container application managed by Docker Compose or Kubernetes, be sure to create OLIVE containers that meet these requirements: The oliveAppData directory, distributed with an OLIVE release, must be mounted as /home/olive/olive inside the docker container. The oliveAppData is the directory that contains the 'plugins' folder with one or more OLIVE plugins. OLIVE will write logs and persistent enrollments to the oliveAppData directory. The plugins folder mounted in /home/olive/olive can vary by container if specific tasks need to be supported by a container. For example, an olive data directory can be mounted that contains only one plugin. The OLIVE server is executed as 'oliveserver' The OLIVE server is accessible via ports 5588 and 5589 (and internally uses port 5590). External clients must have access to these ports on your OLIVE container(s). You can change the default ports used by the OLIVE server via the --port argument. For example, this command will start the OLIVE server using client accessible ports 5591 and 5592: oliveserver --port 5591 Starting the server with different ports is only necessary if the default OLIVE ports are already used by an existing application, or multiple OLIVE containers are addressable at the same hostname. The number of concurrent jobs supported by an OLIVE server can be restricted using the -j argument. For example, to limit the server to one job at a time: oliveserver -j 1 Limiting the number jobs to one in necessary for the TextTransformer plugin, tmt-statistical-v1 If audio needs to be submitted to OLIVE as a path instead of sent as buffer then please contact SRI for some advanced configuration options.","title":"Deploying OLIVE in an Existing Multi-Container Application"},{"location":"docker.html#download-install-and-launch-docker","text":"First you will need to obtain and install Docker - this can be done in a number of ways, the easiest of which is likely through the Docker website. The Community Edition is free (though you now need to create an account with Docker to obtain it), should fulfill most or all OLIVE needs, and is available for several operating systems, including Windows 10. Docker is available here - https://www.docker.com/community-edition and up-to-date installation instructions can be found from Docker. Once Docker is installed, launch it to proceed. When running Docker for the first time, if you are using a Windows OS, you may be prompted to enable some Windows features that are not enabled by default, but are required by Docker to run; Hyper-V and Containers. Select \u201cOk\u201d and wait for the computer to restart to enable these features. Docker should automatically start up once the computer has restarted.","title":"Download, Install, and Launch Docker"},{"location":"docker.html#adjust-docker-settings-ram-cores","text":"If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS","title":"Adjust Docker settings (RAM, Cores)"},{"location":"docker.html#download-olive-docker-package","text":"Now that Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: olive5.1.0-docker-19Sep2020.tar.gz You should find similar content to below unless told otherwise: olive5.1.0 docs/ - Directory containing the OLIVE documentation oliveDocker/ olive-5.1.0-docker.tar \u2013 Docker image containing OLIVE run.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and oliveserver. run.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and oliveserver. run-shell.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. run-shell.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ sad-dnn-v7.0.0 \u2013 Speech Activity Detection plugin Actual plugins included will depend on the customer, mission, and delivery --optional-- oliveAppDataMT/ plugins/ tmt-statistical-v1.0.0 You may optionally be provided with Machine Translation capabilities, as shown at the end of the list - this plugin has some special runtime constraints, and will be contained in a separate oliveAppData directory as a result.","title":"Download OLIVE Docker Package"},{"location":"docker.html#load-the-olive-docker-image","text":"The first setup step is to load the OLIVE Docker image. Loading the image is as simple as opening a command prompt like PowerShell in Windows or Terminal in MacOS, navigating to the directory containing the OLIVE Docker image, and following the examples below. Loading the docker image (only necessary once). Windows $ cd C : \\ Users \\ < username > \\ olive5 . 1.0 \\ oliveDocker $ docker load - i olive - 5.1 . 0 - docker . tar macOS / linux $ cd / home /< username >/ olive5 . 1.0 / oliveDocker $ docker load - i olive - 5.1 . 0 - docker . tar This operation can take some time; quite a few minutes, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed.","title":"Load the OLIVE Docker Image"},{"location":"docker.html#set-up-run-and-run-shell-scripts","text":"Note that we include two (optionally three, if translation is provided) 'run' scripts in this release. run-shell.sh will open a shell within the container, with the OLIVE environment properly set up, allowing a user to run CLI tasks using localenroll and localanalyze. This script is mainly used for troubleshooting and debugging. The other script(s) will launch an OLIVE server using the olive-5.1.0 container, and vary only slightly. run.sh is a general-purpose script currently configured to launch an OLIVE server that will only restrict the number of jobs/workers based on the number of cores available on the host machine, and will allow parallel processing to scale based on this. This should be used for most purposes. (optional) run_mt.sh is a specialized script that is meant to be used to launch an OLIVE server that only points to a Machine Translation plugin. It is configured to limit the number of jobs/workers to one and does not allow parallel processing. It is also configured to have this server listen on different ports than a standard OLIVE server would, in case you need both OLIVE servers to run on the same host. Note that this distinction is the reason that the plugins for deliveries that include translation are split between oliveAppData and oliveAppDataMT - you should be able to support running two containers; one for all plugins that support parallel processing and one for MT - without having to shuffle plugin directories around. The run.cmd / run.sh and run-shell.cmd / run-shell.sh scripts do most of the heavy-lifting for starting the OLIVE docker and associated processes, but before you can run them, you will need to either set an environment variable, or for a more permanent change, open the desired script with a text editor and edit one or two lines to allow them to establish a shared file location that both the host file system (Windows, Linux, or MacOS) and the OLIVE Docker container can access. Both processes are shown below.","title":"Set up run and run-shell scripts"},{"location":"docker.html#environment-variable","text":"The run and run-shell scripts are shipped configured to pull the location of the plugins from an environment variable, so that the scripts don't need to be edited out of the box. This does require these variables to be set each time you will need to run If you would like to permanently edit the scripts instead, refer to the next section, direct script editing . To set this location, you will need to set the OLIVE_APP_DATA environment variable, that will adjust what the OLIVE_DATA variable inside the script is set to. This variable needs to point to the path on the host machine that contains the 'plugins' directory you wish for it to use - for typical OLIVE deliveries, this will be [REPLACE-WITH-LOCAL-PATH]/olive5.1.0/oliveAppData/. An example of this setup step for the non-translation server: $ export OLIVE_APP_DATA =[ REPLACE-WITH-LOCAL-PATH ] /olive5.1.0/oliveAppData/ $ ./run.sh Docker will get access to directory: /Users/allen/oliveAppData/ as /home/olive/olive and as /olive-data. Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- ASR asr-dynapy-v2.0.2 2 .0.2 [ 'cmn-tdnnChain-tel-v1' , 'eng-tdnnChain-tel-v1' , 'fas-tdnn-tel-v1' , 'rus-tdnnChain-tel-v1' , 'spa-tdnnChain-tel-v1' ] LDD ldd-sbcEmbed-v1.0.1 1 .0.1 [ 'multi-v1' ] LID lid-embedplda-v2.0.1 2 .0.1 [ 'multi-v1' ] ... etc. --------- Server ready Tue Feb 23 04 :03:13 2021 --------- -- Optional step for Machine Translation Only -- This same step described above is necessary for the run-mt.sh script that will launch the job-limited server that is only running Machine Translation. The only difference is that the environment variable is instead $OLIVE_APP_DATA_MT , and the default location for most OLIVE deliveries will likely be [REPLACE-WITH-LOCAL-PATH]/olive5.1.0/oliveAppDataMT/. $ export OLIVE_APP_DATA_MT= [ REPLACE - WITH - LOCAL - PATH ] /olive5 . 1 . 0/oliveAppDataMT/ $ . /run - mt . sh Docker will get access to directory: /Users/allen/oliveAppDataMT/ as /home/olive/olive and as /olive - data . Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- TMT tmt - statistical - v1 . 0 . 1 1 . 0 . 1 [ 'spa - eng - generic - v2' , 'fre - eng - generic - v1' ] --------- Server ready Tue Feb 23 04:03:13 2021 --------- NOTE that it is still necessary to set the LOCAL_OLIVE_DATA variable within the run-shell script if you wish to have access to shared audio files. On top of setting OLIVE_APP_DATA as above (which will mount plugins inside the container /home/olive/olive/plugins ), you will also need to set the LOCAL_OLIVE_DATA variable within the script. LOCAL_OLIVE_DATA must be set to a location that ends with a directory called olive-data , and can contain scripts, text files, audio files, and any other utilities you might wish to use with the shell while inside your container. To set this variable, you will need to edit this line in side run-shell.sh : LOCAL_OLIVE_DATA=`cd \"$THISDIR/../../../../../../olive-data\" && pwd` So that the path matches where your audio and scripts are stored, for example: LOCAL_OLIVE_DATA=/home/user1/audio/olive-data/","title":"Environment Variable"},{"location":"docker.html#direct-script-editing","text":"The first of these shared file locations, stored as the variable OLIVE_DATA within the run/run-shell scripts, is where the container will write information such as model enrollments and log files, as well as where the server will find plugins. Plugins must be contained in a directory called plugins, located directly within the directory that LOCAL_OLIVE is assigned to. In the examples below, C:\\Users\\<username>\\olive5.1.0\\oliveAppData for the Windows example, and /home/<username>/olive5.1.0/oliveAppData for the macOS/linux example has a child directory called plugins: C:\\Users\\<username>\\olive5.1.0\\oliveAppData\\plugins or /home/<username>/olive5.1.0/oliveAppData/plugins respectively. The second location is saved into the LOCAL_OLIVE_DATA variable, and should be assigned to a folder that contains audio you with to process with OLIVE, or where you plan on placing this audio. It can be any directory on the host file system (Windows or MacOS) as long as the current user has access to it and the last directory in the path is a directory named \u2018olive-data\u2019. The files and folders contained within the directory assigned to LOCAL_OLIVE_DATA will be mounted to /olive-data/ within the OLIVE Docker container, and accessible at that location when operating within the container using the run or run-shell scripts. run-shell.sh and run.sh (macOS and linux) # MUST set OLIVE_DATA - For convenience we set it to the value of $OLIVE_APP_DATA , but one could also manually edit if # you don't normally set OLIVE_APP_DATA. The path at OLIVE_DATA must include your OLIVE 'plugins' folder : \" ${ OLIVE_APP_DATA : ? \"ERROR: OLIVE_APP_DATA is not set. Set this to the location of your OLIVE plugins folder\" } \" OLIVE_DATA= $OLIVE_APP_DATA # Optionally set LOCAL_OLIVE_DATA to a path on the host that ends with a directory named 'olive-data' LOCAL_OLIVE_DATA=\"/home/ <username> /olive-data\" run-shell.cmd and run.cmd (Windows) REM You must set LOCAL_OLIVE below to the directory on your local host that contains the OLIVE 'plugins' directory set OLIVE_DATA=C:\\Users\\<username>\\olive5.1.0\\oliveAppData set LOCAL_OLIVE_DATA=C:\\Users\\<username>\\olive-data Note that for macOS and linux hosts, the OLIVE_DATA variable is automatically assigned to the value of the $OLIVE_APP_DATA environment variable, if it is preferable to set this before calling the run scripts, instead of editing the scripts themselves. Your delivered run.sh and/or run-shell.sh scripts may have had the line that sets OLIVE_DATA modified to match your specific software installation package more closely and make the necessary changes clearer, especially if there are special cases to consider for your delivery, and may look more similar to this: OLIVE_DATA=[REPLACE-WITH-LOCAL-PATH]/olive5.1.0/oliveAppDataMT/ An example of what this might look like after adapting it to your local environment: OLIVE_DATA=/home/users/allen/olive5.1.0/oliveAppDataMT/ or OLIVE_DATA=/Users/allen/olive5.1.0/oliveAppDataMT/ Note that if your run.sh or run-shell.sh scripts look like this, OLIVE_DATA will not be set automatically based on your OLIVE_APP_DATA environment variable, but must be manually changed within the script before running.","title":"Direct Script Editing"},{"location":"docker.html#run-olive-scripts","text":"Now that all of the prep work has been done, you are ready to launch OLIVE and start running jobs.","title":"Run OLIVE scripts"},{"location":"docker.html#windows-hosts","text":"Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd C:\\Users\\<username>\\olive5.1.0\\oliveDocker $ .\\run.cmd -or- $ .\\run-shell.cmd","title":"Windows hosts"},{"location":"docker.html#macos-and-linux-hosts","text":"Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd / home /< username >/ olive5 .1.0 / oliveDocker $ export OLIVE_APP_DATA =/ home /< username >/ olive5 .1.0 / oliveAppData / ( Optional step if you haven 't edited the specified line in the `run` scripts to point to your plugins) $ ./run.sh -or- $ ./run-shell.sh Which script you run will depend on what task you are hoping to complete. The run.cmd or run.sh script will fire up the OLIVE Docker container and immediately load the oliveserver process. This is to be used alongside the OLIVE GUI or another tool that interacts with oliveserver through the OLIVE API. For command line processing, the run-shell.cmd or run-shell.sh scripts will launch the OLIVE Docker container and open a command line prompt with the OLIVE environment properly set up for running localenroll , localanalyze , and other command-line based tasks.","title":"macOS and linux hosts"},{"location":"docker.html#unload-olive-docker-container-image","text":"To remove/unload a docker image, whether to upgrade OLIVE containers or for any other reason, first, check that the container is actually still loaded. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE olive-5.1.0-docker latest d00396687de1 5 days ago 3.56GB And then unload it. $ docker rmi olive-5.1.0-docker","title":"Unload OLIVE Docker Container Image"},{"location":"docker.html#optional-install-set-up-and-launch-olive-gui","text":"An installation of OpenJDK 11 is required for running the OLIVE GUI. Instructions for obtaining and installing OpenJDK 11 will depend on your host's OS and can be found here Once you have installed Open JDK 11, you may need to set the JAVA_HOME environment variable, so that the host operating system knows where to find the appropriate JDK. On macOS and linux, this should be done automatically, or can be done as simply as exporting the JAVA_HOME environment variable to the appropriate location for your machine and OS, and/or with the correct settings: $ export JAVA_HOME =/ usr / libexec / java_home - v 11 You should then be free to launch the OLIVE GUI by simply executing the launcher script included in On Windows this process is slightly more complicated, but can be done by right-clicking This PC and selecting Properties. Before continuing, please refer to any additional instructions you may have been given when provided with access to the OLIVE software delivery. This process outlined below is currently being revised to be simpler, and may not be necessary for you to perform, especially for macOS or Windows 10 hosts. From there, select Advanced system settings in the left-side navigation menu. Navigate to the Advanced tab and select the Environment Variables button on the lower right portion of the dialog. From the Environment Variables window, select New from the lower System variables section, and create a new variable named JAVA_HOME that points to the full path of the JDK installation path. Typically, this is C:\\Program Files\\Java\\jdk11\\bin. Finally, you are ready to launch the GUI, either by navigating in File Explorer to the location where the package was uncompressed, then OliveGui/olive-ui/bin/, and double-clicking Nightingale.bat. You can create a shortcut to this file that you can then place on your desktop for more convenient access. Alternatively, it can be launched from the Windows PowerShell by navigating to the same location, then invoking Nightingale.bat. When launching this way, it is possible to provide a config file if desired. $ cd C:\\Users\\<username>\\olive5.1.0\\oliveGui $ .\\Nightingale.bat -c nightingale_config.xml [Optional] If you wish to use the Batch mode of the OLIVE GUI, an additional step is required to allow the Batch GUI to access audio files stored on the host operating system. Once you\u2019ve installed and run the GUI at least once, open and edit the .scenic-properties file that was created in the user\u2019s home folder in the host OS. For example, for a user named \u2018olive\u2019, this file will be created at the location C:\\Users\\olive.scenic-properties. Once you\u2019ve located and opened this file (it is a text file), add the following lines: use_scenic_data=true olive_data_directory=C:\\\\Users\\\\<username>\\\\olive-data Note that it is very important that the directory assigned to olive_data_directory matches the directory assigned to LOCAL_OLIVE_DATA in the step above. Note also that it is necessary to include double backslashes. If you don\u2019t, the Java properties reader strips them from the variable, and the path will not be valid.","title":"[Optional] Install, set up, and launch OLIVE GUI"},{"location":"glossary.html","text":"Glossary / Appendix Below you will find definitions of terms commonly used throughout this documentation. If anything is unclear, please reach out for clarification . General Terms Plugin A module that encapsulates a process designed to perform a specific task (detect speakers, identify languages, find keywords) in a specific way (using a deep neural network embeddings, i-vectors+DNN bottleneck, etc.). A plugin thus contains the plan that links together a series of components (acoustic front-end, representation, classifier, fusion, calibration) into a pipeline. This generally captures a specific approach (algorithm) or process that uses a data model in the domain to perform the task, though the algorithm is generally independent of the data or audio condition specialization in a plugin's domain(s). For more information on the different types and capabilities, see the Plugins information page and domain , below. Domain A domain always resides within a plugin. The domain contains specific information (trained models, parameters, etc.) needed to prepare the plugin for specific operating conditions. Every plugin must have at least one domain, but may have many. Examples include telephone (tel), analog and/or digital push-to-talk (ptt), or distant microphone. Some plugins have a general domain trained on many data types, commonly-called \"multi-condition\". Task In the context of OLIVE, Task typically refers to the goal of a plugin, or the problem it is designed to address. For example, the plugin sad-dnn-v6 has the Task of SAD, or speech activity detection. For more information on the range of Tasks we currently have plugins for, please refer to the Plugins documentation. Class A specific target category of interest to be identified or detected by the system. A class can refer to a variety of things, depending on the respective plugin type it belongs to. For example, a 'class' in the context of a speaker identification plugin is an individual speaker of interest; it is a language or dialect in a LID plugin, a keyword in a KWS or QBE plugin, or a topic when referring to a TPD plugin. Classes can be pre-enrolled within a plugin, as is often the case with language identification plugins, but it is often necessary for end users to enroll their own classes of interest, as in the case of speaker identification plugins. Frame A frame of audio is a very short, typically 10ms slice of audio. FrameScoring plugins will report a score for each frame of audio submitted. Plugin Traits (Common API Processes) A plugin's functionality is defined by the Traits that it implements. Each plugin trait is associated with the set of messages that it is allowed to send, and that must be implemented for proper functionality. Below, these traits and their associated messages are defined. FrameScorer A frame scorer provides a score output for every X ms of an audio file or buffer, generally 10 ms. SAD and VTD are currently the only frame scorers in OLIVE. RegionScorer A region scorer provides scores for audio sub-segments detected within an audio file or buffer. For example, a KWS plugin would provide a keyword detection, its boundaries in time and score. GlobalScorer A global scorer assumes that an audio file or buffer is all of the same class and scores it as a unit. Examples include language identification and speaker verification. Common API Processes Adaptable / Adaptation Adaptation typically uses in-domain data from a specific operational environment to alter the core behavior of the system such that it functions more effectively. Unlike adding data to a class, adaptation is altering the system as a whole and thus produces a new domain. Plugins that are adaptable support either supervised or unsupervised adaptation. Unsupervised adaptation improves performance without human input, using audio examples provided by users or accrued from use in a mission's audio conditions. This type of adaptation is \"triggered\" either when a user-specific amount of data is accrued or explicitly called by the end user application and applied to the plugin. Unsupervised adaptation does not create a new domain, it alters an existing domain, but it is reversible. Supervised adaptation, however, requires human input. Generally data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech/non-speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. Supervised adaptation creates a new domain in most cases. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate. Supervised Adaptation Human assisted improvement of the plugin, generally with feedback to the system in the form of annotations of target phenomena, or error corrections. Unsupervised Adaptation Autonomous adaptation using unlabelled data; requires no human labelling or feedback. This is not currently supported by any OLIVE plugins in the traditional sense, but some plugins do support the ability to perform an Update , which is a form of unsupervised adaptation. Enrollable / Enrollment Enrollment is the mechanism by which target classes are added to a plugin domain. An enroll able plugin allows users to add new classes and augment existing classes. Examples include speaker detection, language recognition and keyword spotting. Augmentable / Augmentation Plugins that support enrollment also support augmentation. Augmentation is simply the process of adding additional data to an existing class. Updateable / Update Updating occurs when a user invokes unsupervised adaptation on a plugin/domain by requesting that the plugin use the operational data examples it has accrued throughout normal usage to update the plugin's parameters and models to better fit the usage environment. Diarization Diarization is the process of automatically segmenting an audio file or stream based on a set of target phenomena. SAD is diarization based on speech and non-speech segments. Speaker diarization segments a files based on speaker changes. Audio Vector An audio vector is a representation of an audio file in a form pre-processed for a specific task. For example, an audio files stored as a speaker vector representation for a speaker detection plugin. This is useful since it is a very compact form of the file that is very small and quick to read into memory and very fast to score against versus reading in a file from disk.","title":"Glossary"},{"location":"glossary.html#glossary-appendix","text":"Below you will find definitions of terms commonly used throughout this documentation. If anything is unclear, please reach out for clarification .","title":"Glossary / Appendix"},{"location":"glossary.html#general-terms","text":"","title":"General Terms"},{"location":"glossary.html#plugin","text":"A module that encapsulates a process designed to perform a specific task (detect speakers, identify languages, find keywords) in a specific way (using a deep neural network embeddings, i-vectors+DNN bottleneck, etc.). A plugin thus contains the plan that links together a series of components (acoustic front-end, representation, classifier, fusion, calibration) into a pipeline. This generally captures a specific approach (algorithm) or process that uses a data model in the domain to perform the task, though the algorithm is generally independent of the data or audio condition specialization in a plugin's domain(s). For more information on the different types and capabilities, see the Plugins information page and domain , below.","title":"Plugin"},{"location":"glossary.html#domain","text":"A domain always resides within a plugin. The domain contains specific information (trained models, parameters, etc.) needed to prepare the plugin for specific operating conditions. Every plugin must have at least one domain, but may have many. Examples include telephone (tel), analog and/or digital push-to-talk (ptt), or distant microphone. Some plugins have a general domain trained on many data types, commonly-called \"multi-condition\".","title":"Domain"},{"location":"glossary.html#task","text":"In the context of OLIVE, Task typically refers to the goal of a plugin, or the problem it is designed to address. For example, the plugin sad-dnn-v6 has the Task of SAD, or speech activity detection. For more information on the range of Tasks we currently have plugins for, please refer to the Plugins documentation.","title":"Task"},{"location":"glossary.html#class","text":"A specific target category of interest to be identified or detected by the system. A class can refer to a variety of things, depending on the respective plugin type it belongs to. For example, a 'class' in the context of a speaker identification plugin is an individual speaker of interest; it is a language or dialect in a LID plugin, a keyword in a KWS or QBE plugin, or a topic when referring to a TPD plugin. Classes can be pre-enrolled within a plugin, as is often the case with language identification plugins, but it is often necessary for end users to enroll their own classes of interest, as in the case of speaker identification plugins.","title":"Class"},{"location":"glossary.html#frame","text":"A frame of audio is a very short, typically 10ms slice of audio. FrameScoring plugins will report a score for each frame of audio submitted.","title":"Frame"},{"location":"glossary.html#plugin-traits-common-api-processes","text":"A plugin's functionality is defined by the Traits that it implements. Each plugin trait is associated with the set of messages that it is allowed to send, and that must be implemented for proper functionality. Below, these traits and their associated messages are defined.","title":"Plugin Traits (Common API Processes)"},{"location":"glossary.html#framescorer","text":"A frame scorer provides a score output for every X ms of an audio file or buffer, generally 10 ms. SAD and VTD are currently the only frame scorers in OLIVE.","title":"FrameScorer"},{"location":"glossary.html#regionscorer","text":"A region scorer provides scores for audio sub-segments detected within an audio file or buffer. For example, a KWS plugin would provide a keyword detection, its boundaries in time and score.","title":"RegionScorer"},{"location":"glossary.html#globalscorer","text":"A global scorer assumes that an audio file or buffer is all of the same class and scores it as a unit. Examples include language identification and speaker verification.","title":"GlobalScorer"},{"location":"glossary.html#common-api-processes","text":"","title":"Common API Processes"},{"location":"glossary.html#adaptable-adaptation","text":"Adaptation typically uses in-domain data from a specific operational environment to alter the core behavior of the system such that it functions more effectively. Unlike adding data to a class, adaptation is altering the system as a whole and thus produces a new domain. Plugins that are adaptable support either supervised or unsupervised adaptation. Unsupervised adaptation improves performance without human input, using audio examples provided by users or accrued from use in a mission's audio conditions. This type of adaptation is \"triggered\" either when a user-specific amount of data is accrued or explicitly called by the end user application and applied to the plugin. Unsupervised adaptation does not create a new domain, it alters an existing domain, but it is reversible. Supervised adaptation, however, requires human input. Generally data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech/non-speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. Supervised adaptation creates a new domain in most cases. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate.","title":"Adaptable / Adaptation"},{"location":"glossary.html#supervised-adaptation","text":"Human assisted improvement of the plugin, generally with feedback to the system in the form of annotations of target phenomena, or error corrections.","title":"Supervised Adaptation"},{"location":"glossary.html#unsupervised-adaptation","text":"Autonomous adaptation using unlabelled data; requires no human labelling or feedback. This is not currently supported by any OLIVE plugins in the traditional sense, but some plugins do support the ability to perform an Update , which is a form of unsupervised adaptation.","title":"Unsupervised Adaptation"},{"location":"glossary.html#enrollable-enrollment","text":"Enrollment is the mechanism by which target classes are added to a plugin domain. An enroll able plugin allows users to add new classes and augment existing classes. Examples include speaker detection, language recognition and keyword spotting.","title":"Enrollable / Enrollment"},{"location":"glossary.html#augmentable-augmentation","text":"Plugins that support enrollment also support augmentation. Augmentation is simply the process of adding additional data to an existing class.","title":"Augmentable / Augmentation"},{"location":"glossary.html#updateable-update","text":"Updating occurs when a user invokes unsupervised adaptation on a plugin/domain by requesting that the plugin use the operational data examples it has accrued throughout normal usage to update the plugin's parameters and models to better fit the usage environment.","title":"Updateable / Update"},{"location":"glossary.html#diarization","text":"Diarization is the process of automatically segmenting an audio file or stream based on a set of target phenomena. SAD is diarization based on speech and non-speech segments. Speaker diarization segments a files based on speaker changes.","title":"Diarization"},{"location":"glossary.html#audio-vector","text":"An audio vector is a representation of an audio file in a form pre-processed for a specific task. For example, an audio files stored as a speaker vector representation for a speaker detection plugin. This is useful since it is a very compact form of the file that is very small and quick to read into memory and very fast to score against versus reading in a file from disk.","title":"Audio Vector"},{"location":"hardware.html","text":"Speed and Memory Requirements Notes and Disclaimer for the Resource Requirement Estimates Provided A few performance-related things that might be important to note: These estimates provided below were recorded on a native linux installation of OLIVE 5.1.0 which may be slightly different than running the equivalent job on Windows or in a Docker based environment. Any speed estimates we give are going to be hardware dependent. The numbers reported below should be pessimistic, as they are limited to running on a single core of a low-power computer. Stronger cores will be faster than what's reported, weaker CPU cores will run a bit slower. If you have more than 1 processor core(s) available, which is likely, OLIVE is able to parallelize and run more jobs simultaneously, so the speed should scale accordingly - but we're reporting single-core jobs just to keep everything on the same relative scale so that you can compare plugins to each other. Just as speed will increase/scale as the number of processor cores being used increases (i.e. number of simultaneous jobs), so will memory usage. The provided stats are for a process limited to one job at a time. Memory usage scales sometimes significantly depending on how large the input audio files are. For a plugin like SAD, that has a very small base memory footprint, which only barely increases even if processing many, many small files, can see a much larger memory utilization if you start running 1GB+ audio files through. ASR performance is largely domain-dependent - for example, the Russian domain currently has a much larger language model than other plugins because of how the language is structured, and priorities of the project that funded its development, so its memory usage is quite significant, \\~9+GB per processor core. Another thing to note for ASR speed performance is that because of how sizable the models can be, the overhead of the loading time of this model into memory can really come into play. It may take some time to get an initial response back from the server due to this overhead, but subsequent responses should be much faster as this 'heavy lifting' is already done. Note that if you're just running the CLI tools like localanalyze, this loading must be performed every time, so you won't realize this speedup unless you're running with the OLIVE server. If you are using the OLIVE server, it's possible to send a 'preload' request to load a plugin's models before any audio is submitted for scoring and avoid this initial delay. The models for each ASR/TPD domain/language are disjoint, and take up separate memory footprints. So if you would like to run data through both the Russian and English ASR domains when running the OLIVE server, the models, once loaded, are retained in memory for later processing and you may quickly run out of memory. For example, if you run a Russian job, a minimum of ~9GB of memory will be used. If you run an English job shortly after, this will load another ~6GB or so worth of models into memory. If you have insufficient memory, you will need to either explicity unload plugins/domains using API calls (not a feature currently offered by our GUI), or will need to restart the server to clear out the loaded models. QBE performance will depend on how many queries/keywords are currently enrolled - as more queries are enrolled and need to be considered during the search, the speed of the plugin will decrease. Some of the statistics below may be extra pessimistic because some of these readings will depend on how much of the input audio actually contains speech. If you have a 3 hour file, but only 5 minutes of it is speech, many of these plugins can be much, much faster and use less memory, because the task-specific processing (LID, for example) will only process audio that is identified as speech, and so will be operating on a much smaller piece of the audio than the whole file. The audio used to generate these numbers is pretty packed with speech, so should be close to a 'worst case.' Plugin Resource Requirement Estimates With that out of the way, here is a summary for most of the plugins: plugin / domain speed mem (1 min) mem (2 hr) sad-dnn-v7.0.1 / fast-multi-v1 214.1 105 MB 766 MB sad-dnn-v7.0.1 / multi-v1 90.6 127 MB 775 MB gid-gb-v2.0.0 / clean-v1 354.9 161 MB 1.56 GB ldd-sbcEmbed-v1.0.1 / multi-v1 18.5 582 MB 5.04 GB lid-embedplda-v2.0.1 / multi-v1 29.2 660 MB 3.15 GB qbe-tdnn-v5.0.0 / multi-v1* 28.3 198 MB 2.55 GB sdd-sbcEmbed-v2.0.2 / telClosetalk-v1 42.8 232 MB 1.12 GB sid-dplda-v2.0.1 / multi-v1 42.2 296 MB 2.38 GB asr-dynapy-v2.0.2 / rus-tdnnChain-tel-v1** 10.6 8.94 GB 10.19 GB tpd-dynapy-v3.0.0 / rus-cts-v1** 6.3 7.46 GB 8.66 GB * QBE Note: with 3 enrolled keywords ** ASR/TPD Note - the mem (2 hr) and speed statistics are generated from different data than the other plugins. That is because these plugins are language-dependent, and the data I used for the rest of the tests does not match the language of the domains I was running here. Feeding mismatched data into these plugins can cause both runtime and memory usage to balloon, as the plugin tries very hard to make sense of something that it's never seen before. Instead, 100 files adding to ~2.5 hrs were used for the speed test and one 2-hr file was used for the mem (2 hr) test, but a different one than the rest of the plugins. Note also that the Russian models are by far the largest delivered - this will change depending on which language/domain you are using, but these should represent a 'worst case' for ASR/TPD for planning purposes. Speed numbers are reported in terms of \"times faster than real time\", and the numbers were reached by scoring 90 files adding up to approximately ~5 hrs of data on a single core of a ~2016 Gigabyte BRIX Compact PC (Intel i7-5500U 2.40 GHz processor). Higher is better, so for the slower SAD domain, which scores ~90 here, that means it can process a 90 second input file in 1 second. Two memory points are provided for each plugin - the memory used to score a single 1 minute file, which should show roughly the baseline usage of the plugin, as well as memory used to score a single 2 hour file, to give a sense of how the usage scales as files grow. Lower is better.","title":"Speed and Memory Requirements"},{"location":"hardware.html#speed-and-memory-requirements","text":"","title":"Speed and Memory Requirements"},{"location":"hardware.html#notes-and-disclaimer-for-the-resource-requirement-estimates-provided","text":"A few performance-related things that might be important to note: These estimates provided below were recorded on a native linux installation of OLIVE 5.1.0 which may be slightly different than running the equivalent job on Windows or in a Docker based environment. Any speed estimates we give are going to be hardware dependent. The numbers reported below should be pessimistic, as they are limited to running on a single core of a low-power computer. Stronger cores will be faster than what's reported, weaker CPU cores will run a bit slower. If you have more than 1 processor core(s) available, which is likely, OLIVE is able to parallelize and run more jobs simultaneously, so the speed should scale accordingly - but we're reporting single-core jobs just to keep everything on the same relative scale so that you can compare plugins to each other. Just as speed will increase/scale as the number of processor cores being used increases (i.e. number of simultaneous jobs), so will memory usage. The provided stats are for a process limited to one job at a time. Memory usage scales sometimes significantly depending on how large the input audio files are. For a plugin like SAD, that has a very small base memory footprint, which only barely increases even if processing many, many small files, can see a much larger memory utilization if you start running 1GB+ audio files through. ASR performance is largely domain-dependent - for example, the Russian domain currently has a much larger language model than other plugins because of how the language is structured, and priorities of the project that funded its development, so its memory usage is quite significant, \\~9+GB per processor core. Another thing to note for ASR speed performance is that because of how sizable the models can be, the overhead of the loading time of this model into memory can really come into play. It may take some time to get an initial response back from the server due to this overhead, but subsequent responses should be much faster as this 'heavy lifting' is already done. Note that if you're just running the CLI tools like localanalyze, this loading must be performed every time, so you won't realize this speedup unless you're running with the OLIVE server. If you are using the OLIVE server, it's possible to send a 'preload' request to load a plugin's models before any audio is submitted for scoring and avoid this initial delay. The models for each ASR/TPD domain/language are disjoint, and take up separate memory footprints. So if you would like to run data through both the Russian and English ASR domains when running the OLIVE server, the models, once loaded, are retained in memory for later processing and you may quickly run out of memory. For example, if you run a Russian job, a minimum of ~9GB of memory will be used. If you run an English job shortly after, this will load another ~6GB or so worth of models into memory. If you have insufficient memory, you will need to either explicity unload plugins/domains using API calls (not a feature currently offered by our GUI), or will need to restart the server to clear out the loaded models. QBE performance will depend on how many queries/keywords are currently enrolled - as more queries are enrolled and need to be considered during the search, the speed of the plugin will decrease. Some of the statistics below may be extra pessimistic because some of these readings will depend on how much of the input audio actually contains speech. If you have a 3 hour file, but only 5 minutes of it is speech, many of these plugins can be much, much faster and use less memory, because the task-specific processing (LID, for example) will only process audio that is identified as speech, and so will be operating on a much smaller piece of the audio than the whole file. The audio used to generate these numbers is pretty packed with speech, so should be close to a 'worst case.'","title":"Notes and Disclaimer for the Resource Requirement Estimates Provided"},{"location":"hardware.html#plugin-resource-requirement-estimates","text":"With that out of the way, here is a summary for most of the plugins: plugin / domain speed mem (1 min) mem (2 hr) sad-dnn-v7.0.1 / fast-multi-v1 214.1 105 MB 766 MB sad-dnn-v7.0.1 / multi-v1 90.6 127 MB 775 MB gid-gb-v2.0.0 / clean-v1 354.9 161 MB 1.56 GB ldd-sbcEmbed-v1.0.1 / multi-v1 18.5 582 MB 5.04 GB lid-embedplda-v2.0.1 / multi-v1 29.2 660 MB 3.15 GB qbe-tdnn-v5.0.0 / multi-v1* 28.3 198 MB 2.55 GB sdd-sbcEmbed-v2.0.2 / telClosetalk-v1 42.8 232 MB 1.12 GB sid-dplda-v2.0.1 / multi-v1 42.2 296 MB 2.38 GB asr-dynapy-v2.0.2 / rus-tdnnChain-tel-v1** 10.6 8.94 GB 10.19 GB tpd-dynapy-v3.0.0 / rus-cts-v1** 6.3 7.46 GB 8.66 GB * QBE Note: with 3 enrolled keywords ** ASR/TPD Note - the mem (2 hr) and speed statistics are generated from different data than the other plugins. That is because these plugins are language-dependent, and the data I used for the rest of the tests does not match the language of the domains I was running here. Feeding mismatched data into these plugins can cause both runtime and memory usage to balloon, as the plugin tries very hard to make sense of something that it's never seen before. Instead, 100 files adding to ~2.5 hrs were used for the speed test and one 2-hr file was used for the mem (2 hr) test, but a different one than the rest of the plugins. Note also that the Russian models are by far the largest delivered - this will change depending on which language/domain you are using, but these should represent a 'worst case' for ASR/TPD for planning purposes. Speed numbers are reported in terms of \"times faster than real time\", and the numbers were reached by scoring 90 files adding up to approximately ~5 hrs of data on a single core of a ~2016 Gigabyte BRIX Compact PC (Intel i7-5500U 2.40 GHz processor). Higher is better, so for the slower SAD domain, which scores ~90 here, that means it can process a 90 second input file in 1 second. Two memory points are provided for each plugin - the memory used to score a single 1 minute file, which should show roughly the baseline usage of the plugin, as well as memory used to score a single 2 hour file, to give a sense of how the usage scales as files grow. Lower is better.","title":"Plugin Resource Requirement Estimates"},{"location":"install.html","text":"OLIVE Installation OLIVE Folder Structure Overview Typical OLIVE installations for native, linux-based operating systems follow the general formats shown below. If your delivery is based on a docker container, please refer to the OLIVE Docker Container Setup Guide . We recommend installing the OLIVE Runtime and OLIVE itself within a directory in an easy-to-find location. Typically olive5.x.y/ within $HOME is a reasonable starting place. Plugins we generally store outside of this location, in a directory we name oliveAppData to allow for retention of plugins and audio enrollments if/when a new version of OLIVE is installed. $HOME is again often a reasonable location for this folder. The configuration or relationship between the runtime and the OLIVE package itself generally matches one of the following two examples. In either case, you will need to properly set up the OLIVE environment before performing any operations with OLIVE, and SRI provides a script to make this as straightforward as possible. olive_env.sh setup script This script is included with every release and is generally the recommended and easiest way to properly set up the OLIVE environment, as shown above. It should be possible to use this script to cover environmental setup in even advanced OLIVE deployments. This script will properly set a number of necessary environment variables and paths essential for OLIVE operation all in one go. Depending on how you've chosen to lay out your OLIVE package on disk, you may need to source two of these scripts. As a quick primer, this is the README included at the top of the olive_env.sh script, that will be explained in more detail below: # Sets the Olive/Scenic environment when running from a distribution. # Assumes this will be sourced in the Olive directory, as: # # % source olive_env.sh # # If this script is sourced in the runtime distribution, it will only set # runtime-related values. # # If this script is sourced in the Olive distribution, it will set both # OLIVE (SCENIC) and runtime-related values if: # (1) the Olive directory is a sub-directory of the runtime directory # or (2) the Olive and runtime directories have been combined. # # If the Olive directory is a subdirectory of the runtime, then you # should only source olive_env.sh from the Olive directory. # # If the runtime and Olive directories are completely separate, then # do the following (in this order, so that $OLIVE paths come before # $OLIVE_RUNTIME paths): # # (1) cd /path/to/runtime; source olive_env.sh # (2) cd /path/to/olive; source olive_env.sh One important thing to note is that the olive_env.sh scripts do not cover setting the OLIVE_APP_DATA environment variable, which informs the server of the location of the OLIVE plugins, among other things. More details available here . This is a very important step to make sure the Server can find and utilize the appropriate plugins. Setting this variable is included in each of the examples below. OLIVE/OLIVE-Runtime Layouts Often, the OLIVE Runtime and OLIVE itself are placed side-by-side in the same parent directory, for easier readability. If this configuration is used, it is necessary to source two separate olive_env.sh files to properly establish the OLIVE environment, as outlined in the olive_env.sh README shown above. You should always start by sourcing the olive_env.sh file for the runtime. An example file structure of side-by-side OLIVE and OLIVE runtime directories: $HOME olive5.1.0/ runtime-5.1.0-centos-7.3.1611-x86_64/ olive-5.1.0-centos-7.3.1611-x86_64/ documentation/ oliveAppData/ plugins/ sad-dnn-v7.0.0 sid-dplda-v2.0.0/ lid-embedplda-v2.0.0/ Example environment setup with this configuration: $ cd $ HOME / olive5 . 1.0 / runtime - 5.1 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ cd $ HOME / olive5 . 1.0 / olive - 5.1 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / Alternately, the OLIVE package can be nested within the runtime directory, a structure shown in the example below. If this arrangement is used, it is only necessary to source the olive_env.sh script within the OLIVE package, as it is aware that its parent directory is the OLIVE runtime, and appropriately establishes the environment with this information. $HOME olive5.1.0/ runtime-5.1.0-centos-7.3.1611-x86_64/ olive-5.1.0-centos-7.3.1611-x86_64/ documentation/ oliveAppData/ plugins/ lid-embed-v2/ sad-dnn-v4a/ sid-embed-v2/ Example environment setup with this configuration: $ cd $ HOME / olive5 . 1.0 / runtime - 5.1 . 0 - centos - 7.3 . 1611 - x86_64 / olive - 5.1 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / After performing these installation and setup steps, you should be ready to get started and run your own instance of the OLIVE Server For more information on what variables are being established by this environment setup script and what is needed by OLIVE to run, continue on to the Environment Variables section of the OLIVE Server Page . For more information on getting started with running the OLIVE server, continue on to the Running The Server section. Or skip to the OLIVE Runtime for more details about the purpose and contents of the OLIVE Runtime.","title":"Installation"},{"location":"install.html#olive-installation","text":"","title":"OLIVE Installation"},{"location":"install.html#olive-folder-structure-overview","text":"Typical OLIVE installations for native, linux-based operating systems follow the general formats shown below. If your delivery is based on a docker container, please refer to the OLIVE Docker Container Setup Guide . We recommend installing the OLIVE Runtime and OLIVE itself within a directory in an easy-to-find location. Typically olive5.x.y/ within $HOME is a reasonable starting place. Plugins we generally store outside of this location, in a directory we name oliveAppData to allow for retention of plugins and audio enrollments if/when a new version of OLIVE is installed. $HOME is again often a reasonable location for this folder. The configuration or relationship between the runtime and the OLIVE package itself generally matches one of the following two examples. In either case, you will need to properly set up the OLIVE environment before performing any operations with OLIVE, and SRI provides a script to make this as straightforward as possible.","title":"OLIVE Folder Structure Overview"},{"location":"install.html#olive_envsh-setup-script","text":"This script is included with every release and is generally the recommended and easiest way to properly set up the OLIVE environment, as shown above. It should be possible to use this script to cover environmental setup in even advanced OLIVE deployments. This script will properly set a number of necessary environment variables and paths essential for OLIVE operation all in one go. Depending on how you've chosen to lay out your OLIVE package on disk, you may need to source two of these scripts. As a quick primer, this is the README included at the top of the olive_env.sh script, that will be explained in more detail below: # Sets the Olive/Scenic environment when running from a distribution. # Assumes this will be sourced in the Olive directory, as: # # % source olive_env.sh # # If this script is sourced in the runtime distribution, it will only set # runtime-related values. # # If this script is sourced in the Olive distribution, it will set both # OLIVE (SCENIC) and runtime-related values if: # (1) the Olive directory is a sub-directory of the runtime directory # or (2) the Olive and runtime directories have been combined. # # If the Olive directory is a subdirectory of the runtime, then you # should only source olive_env.sh from the Olive directory. # # If the runtime and Olive directories are completely separate, then # do the following (in this order, so that $OLIVE paths come before # $OLIVE_RUNTIME paths): # # (1) cd /path/to/runtime; source olive_env.sh # (2) cd /path/to/olive; source olive_env.sh One important thing to note is that the olive_env.sh scripts do not cover setting the OLIVE_APP_DATA environment variable, which informs the server of the location of the OLIVE plugins, among other things. More details available here . This is a very important step to make sure the Server can find and utilize the appropriate plugins. Setting this variable is included in each of the examples below.","title":"olive_env.sh setup script"},{"location":"install.html#oliveolive-runtime-layouts","text":"Often, the OLIVE Runtime and OLIVE itself are placed side-by-side in the same parent directory, for easier readability. If this configuration is used, it is necessary to source two separate olive_env.sh files to properly establish the OLIVE environment, as outlined in the olive_env.sh README shown above. You should always start by sourcing the olive_env.sh file for the runtime. An example file structure of side-by-side OLIVE and OLIVE runtime directories: $HOME olive5.1.0/ runtime-5.1.0-centos-7.3.1611-x86_64/ olive-5.1.0-centos-7.3.1611-x86_64/ documentation/ oliveAppData/ plugins/ sad-dnn-v7.0.0 sid-dplda-v2.0.0/ lid-embedplda-v2.0.0/ Example environment setup with this configuration: $ cd $ HOME / olive5 . 1.0 / runtime - 5.1 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ cd $ HOME / olive5 . 1.0 / olive - 5.1 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / Alternately, the OLIVE package can be nested within the runtime directory, a structure shown in the example below. If this arrangement is used, it is only necessary to source the olive_env.sh script within the OLIVE package, as it is aware that its parent directory is the OLIVE runtime, and appropriately establishes the environment with this information. $HOME olive5.1.0/ runtime-5.1.0-centos-7.3.1611-x86_64/ olive-5.1.0-centos-7.3.1611-x86_64/ documentation/ oliveAppData/ plugins/ lid-embed-v2/ sad-dnn-v4a/ sid-embed-v2/ Example environment setup with this configuration: $ cd $ HOME / olive5 . 1.0 / runtime - 5.1 . 0 - centos - 7.3 . 1611 - x86_64 / olive - 5.1 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / After performing these installation and setup steps, you should be ready to get started and run your own instance of the OLIVE Server For more information on what variables are being established by this environment setup script and what is needed by OLIVE to run, continue on to the Environment Variables section of the OLIVE Server Page . For more information on getting started with running the OLIVE server, continue on to the Running The Server section. Or skip to the OLIVE Runtime for more details about the purpose and contents of the OLIVE Runtime.","title":"OLIVE/OLIVE-Runtime Layouts"},{"location":"integrationOverview.html","text":"OLIVE Integration Guide OLIVE Workflow API Starting with OLIVE 5.1, OLIVE Workflows are now supported, to make integration with the OLIVE infrastructure even easier, and to allow for more advanced processing sequences. This is now the recommended method of integrating with OLIVE and will be the most-supported future option. If you need For details on the capabilities of Workflows and how to get started integrating with the OLIVE Workflows feature set, refer to the: OLIVE Workflow Integration Guide OLIVE Enterprise API The legacy, low-level single-message integration with the OLIVE API is still supported and possible if you need features or control not yet supported by Workflows. Note that integration using this method will be more complex and in many cases will be less than optimal, especially when results are required from multiple plugins, since a new message request will need to be crafted and submitted for each individual plugin, even if the audio to be analyzed is the same. Details on integration using the OLIVE Legacy Enterprise API can be found on the pages linked below: Enterprise API Information - If interested in details about the mechanisms that allow integrating the OLIVE Server and backend audio processing capabilities to an existing system or GUI. Integrating the (Java) Client API - Provides code examples (currently only in Java) for how to perform several OLIVE tasks and integrate the OLIVE Java Reference Implementation to create a new client or augment a client with OLIVE functionality. Creating an API Reference Implementation - Contains information to help guide the creation of a new API Reference Implementation if Java is not the target language for client integration. Enterprise API Message Definitions - Provides more of the low-level details on the available API messages, their structures and contents.","title":"OLIVE Integration Guide"},{"location":"integrationOverview.html#olive-integration-guide","text":"","title":"OLIVE Integration Guide"},{"location":"integrationOverview.html#olive-workflow-api","text":"Starting with OLIVE 5.1, OLIVE Workflows are now supported, to make integration with the OLIVE infrastructure even easier, and to allow for more advanced processing sequences. This is now the recommended method of integrating with OLIVE and will be the most-supported future option. If you need For details on the capabilities of Workflows and how to get started integrating with the OLIVE Workflows feature set, refer to the: OLIVE Workflow Integration Guide","title":"OLIVE Workflow API"},{"location":"integrationOverview.html#olive-enterprise-api","text":"The legacy, low-level single-message integration with the OLIVE API is still supported and possible if you need features or control not yet supported by Workflows. Note that integration using this method will be more complex and in many cases will be less than optimal, especially when results are required from multiple plugins, since a new message request will need to be crafted and submitted for each individual plugin, even if the audio to be analyzed is the same. Details on integration using the OLIVE Legacy Enterprise API can be found on the pages linked below: Enterprise API Information - If interested in details about the mechanisms that allow integrating the OLIVE Server and backend audio processing capabilities to an existing system or GUI. Integrating the (Java) Client API - Provides code examples (currently only in Java) for how to perform several OLIVE tasks and integrate the OLIVE Java Reference Implementation to create a new client or augment a client with OLIVE functionality. Creating an API Reference Implementation - Contains information to help guide the creation of a new API Reference Implementation if Java is not the target language for client integration. Enterprise API Message Definitions - Provides more of the low-level details on the available API messages, their structures and contents.","title":"OLIVE Enterprise API"},{"location":"javaClientSetup.html","text":"Setting up IDE for Java Client Development with OLIVE Reference Implementation Before you do integration with the OLIVE Java API, make sure you have the OLIVE server running. For more details, see Run OLIVE Server . Also, make sure you have Java installed, since you will be writing Java client programs. Source code and examples of the OLIVE Java API are included in your OLIVE delivery package, in a folder named OLIVE-API-example. At the top level, you should see the following items: Expand the src folder a few levels down and you should see a folder named client. This folder contains examples of Java client programs which make use of the OLIVE Java API. All examples shown in Example Client Code Using the OLIVE API are included in the folder. The easiest way to set up your environment to write Java client programs to use the API to access OLIVE services is to use a Java IDE, for example, IntelliJ, which you can download and install a free version of the community edition from here . There are other Java IDE available, we will use IntelliJ to illustrate the integration steps. Next is bring the scenic-example Java project into your IDE. At the top level of scenic-example, there is a gradlew script. Use it to create a Java project compatible with IntelliJ as follows: ./gradlew idea This creates a file named OLIVE-API-examples.ipr. Double click the file to launch the IntelliJ IDE window. Then click the \"Import Gradle Project\" link at the bottom right of the IntelliJ window to import the project into IntelliJ. If the IntelliJ system gives you a message to upgrade your gradle build to 2.6, as shown below, click the upgrade link to do the upgrade. Furthermore, if the IntelliJ system gives you a message to upgrade the gradle wrapper to be compatible with the version of Java you have on your local system, click the upgrade link to do the gradle wrapper upgrade. When the upgrades are done, you should see an IntelliJ window with a \"CONFIGURE SUCCESSFUL\" message. Close the gradle-wrapper.properties file. Click the \"Project\" folder icon all the way to the top left. This should bring the OLIVE-API-examples project tree into your IDE. Right click on \"OLIVE-API-examples\" and select \"BUILD Module OLIVE-API-examples\". This should compile the OLIVE API package and make its resources available. Expand OLIVE-API-examples and you should see this: Expand OLIVE-API-examples and navigate a few levels down into src, until you see the Java programs in the client folder. If there are incompatibilites between any source file and the version of gradle being used, the source file will have a little orange icon to its bottom left. To fix the incompatibilities, re-import the project by clicking the \"2 arrow\" icon in the gradle window. Once the reimport is done, the orange incompatible icons should change into blue compatible icons, as shown below. To read or edit MySADFrameScorer.java, double click the file to bring it into the edit window. Before you run the program, add a run-time configuration parameter by using the \"Add Configuration...\" pull-down menu. Click \"+\" to add a new configuration. Name this new configuration \"MySADFrameScorer\". Choose \"MySADFrameScorer (com.sri.scenic.api.client)\" for \"Main Class\", and \"OLIVE-API-examples.main\" for \"Use classpath of module\". Enter the location of an audio file on your local system in \"Program arguments\", as MySADFrameScorer.java takes one argument which is the path to the file for SAD scoring. Click \"OK\" to save the configuration. To run the program, right click on the file and choose \"Run MySADFrameScorer\". The SAD scores returned from the OLIVE server are shown in the bottom right window of the IDE, as seen below. Once you have brought the OLIVE API project into your Java development IDE, a convenient location to place and run your custom built client programs is the same client folder where you find the sample client programs such as SAD scoring request . You can start with any of the example programs in the client folder, modify it to suit your needs, and run it the same way as you would run the SAD scoring request program .","title":"javaClientSetup"},{"location":"javaClientSetup.html#setting-up-ide-for-java-client-development-with-olive-reference-implementation","text":"Before you do integration with the OLIVE Java API, make sure you have the OLIVE server running. For more details, see Run OLIVE Server . Also, make sure you have Java installed, since you will be writing Java client programs. Source code and examples of the OLIVE Java API are included in your OLIVE delivery package, in a folder named OLIVE-API-example. At the top level, you should see the following items: Expand the src folder a few levels down and you should see a folder named client. This folder contains examples of Java client programs which make use of the OLIVE Java API. All examples shown in Example Client Code Using the OLIVE API are included in the folder. The easiest way to set up your environment to write Java client programs to use the API to access OLIVE services is to use a Java IDE, for example, IntelliJ, which you can download and install a free version of the community edition from here . There are other Java IDE available, we will use IntelliJ to illustrate the integration steps. Next is bring the scenic-example Java project into your IDE. At the top level of scenic-example, there is a gradlew script. Use it to create a Java project compatible with IntelliJ as follows: ./gradlew idea This creates a file named OLIVE-API-examples.ipr. Double click the file to launch the IntelliJ IDE window. Then click the \"Import Gradle Project\" link at the bottom right of the IntelliJ window to import the project into IntelliJ. If the IntelliJ system gives you a message to upgrade your gradle build to 2.6, as shown below, click the upgrade link to do the upgrade. Furthermore, if the IntelliJ system gives you a message to upgrade the gradle wrapper to be compatible with the version of Java you have on your local system, click the upgrade link to do the gradle wrapper upgrade. When the upgrades are done, you should see an IntelliJ window with a \"CONFIGURE SUCCESSFUL\" message. Close the gradle-wrapper.properties file. Click the \"Project\" folder icon all the way to the top left. This should bring the OLIVE-API-examples project tree into your IDE. Right click on \"OLIVE-API-examples\" and select \"BUILD Module OLIVE-API-examples\". This should compile the OLIVE API package and make its resources available. Expand OLIVE-API-examples and you should see this: Expand OLIVE-API-examples and navigate a few levels down into src, until you see the Java programs in the client folder. If there are incompatibilites between any source file and the version of gradle being used, the source file will have a little orange icon to its bottom left. To fix the incompatibilities, re-import the project by clicking the \"2 arrow\" icon in the gradle window. Once the reimport is done, the orange incompatible icons should change into blue compatible icons, as shown below. To read or edit MySADFrameScorer.java, double click the file to bring it into the edit window. Before you run the program, add a run-time configuration parameter by using the \"Add Configuration...\" pull-down menu. Click \"+\" to add a new configuration. Name this new configuration \"MySADFrameScorer\". Choose \"MySADFrameScorer (com.sri.scenic.api.client)\" for \"Main Class\", and \"OLIVE-API-examples.main\" for \"Use classpath of module\". Enter the location of an audio file on your local system in \"Program arguments\", as MySADFrameScorer.java takes one argument which is the path to the file for SAD scoring. Click \"OK\" to save the configuration. To run the program, right click on the file and choose \"Run MySADFrameScorer\". The SAD scores returned from the OLIVE server are shown in the bottom right window of the IDE, as seen below. Once you have brought the OLIVE API project into your Java development IDE, a convenient location to place and run your custom built client programs is the same client folder where you find the sample client programs such as SAD scoring request . You can start with any of the example programs in the client folder, modify it to suit your needs, and run it the same way as you would run the SAD scoring request program .","title":"Setting up IDE for Java Client Development with OLIVE Reference Implementation"},{"location":"martini.html","text":"OLIVE Martini Docker Container Setup Introduction This page guides through the installation, setup, and launching of a docker-container based OLIVE software package featuring the new \"OLIVE Martini\" container setup. This new delivery method includes the usual OLIVE server and plugin capabilities delivered in the past, but includes a new addition - Batch GUI capabilities powered by the new Raven GUI that can be accessed through your web browser. For instructions on how to use this new GUI once you have the software properly installed, check out the Raven Web GUI page. The overall installation process is very similar to previous Docker container based OLIVE deliveries in the past, with the main changes affecting how the OLIVE container is managed for startup, shutdown, etc. Download, Install, and Launch Docker First you will need to obtain and install Docker - this can be done in a number of ways, the easiest of which is likely through the Docker website. The Community Edition is free (though you now need to create an account with Docker to obtain it), should fulfill most or all OLIVE needs, and is available for several operating systems, including Windows 10. Docker is available here - https://www.docker.com/community-edition and up-to-date installation instructions can be found from Docker. Once Docker is installed, launch it to proceed. When running Docker for the first time, if you are using a Windows OS, you may be prompted to enable some Windows features that are not enabled by default, but are required by Docker to run; Hyper-V and Containers. Select \u201cOk\u201d and wait for the computer to restart to enable these features. Docker should automatically start up once the computer has restarted. Adjust Docker settings (RAM, Cores) If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS Download OLIVE Docker Package Now that Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: olive5.1.0-martini-11June2021.tar.gz You should find similar content to below unless told otherwise: olive5.1.0/ api/ - (Optional) Directory containing the Python and/or Java OLIVE Client utilities docs/ - Directory containing the OLIVE documentation martini/ martini-docker.tar - Docker container including OLIVE , the Raven Web GUI , the OLIVE Web Broker, and various other utilities, including an html server to host this documentation martini.sh - Multi-purpose utility and management script for the OLIVE Martini Docker container martini.ps1 - Same as above, but for Windows PowerShell martini.bat - Wrapper script to allow the above script to execute on certain Windows systems. Use this first. OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ sad-dnn-v7.0.0 \u2013 Speech Activity Detection plugin Actual plugins included will depend on the customer, mission, and delivery workflows/ - Directory containing OLIVE workflows; actual included worfklows will depend on the customer, mission, and delivery Load the OLIVE Docker Image The first setup step is to load the OLIVE Docker image. Loading the image is as simple as opening a command prompt like PowerShell in Windows or Terminal in MacOS, navigating to the directory containing the OLIVE Docker image, and following the examples below. Loading the docker image (only necessary once). Windows $ cd C : \\ Users \\ < username > \\ olive5 . 1.0 \\ martini $ docker load - i martini - container . tar macOS / linux $ cd / home /< username >/ olive5 . 1.0 / martini $ docker load - i martini - container . tar This operation can take some time; quite a few minutes, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed. Other Required Software This container has been tested using docker version 19.03.9 API version 1.40, but should work with any recent version. The Olive Web UI should work with any recent, common web browser, but has been tested mostly with Chrome (90.0.4430.212), but also Edge (89.0.774.77), and Firefox (87.0). The Python scripts included should work with any 3.x version of Python, but have been tested with Python 3.7.9 Operating Systems Note There are three provided martini management scripts: martini.sh martini.bat martini.ps1 The functionality of each is intended to be identical, and these are provided as different options for different operating systems for convenience. The martini.sh script should be used for linux and for macOS. martini.ps1 should be used when managing the OLIVE Martini container from Windows PowerShell, and martini.bat , which calls martini.ps1 internally, should be used if martini.ps1 cannot execute for permission reasons in PowerShell, and when managing OLIVE Martini from Windows Command Prompt ( cmd ). The examples below attempt to show all three being used, but in the text, they are refered to as simply martini.sh for brevity. Their features and functionality are the same. Configuring Ports (Optional) By default the container exposes seven ports on the host machine running the container: 9900 9901 9904 9905 9970 9980 9988 If you want to change them, there are two ways to do this: You can directly edit the martini.sh or martini.ps1 file to change these seven variables near the top of the file: OLIVE_HOST_PORT_OLIVESERVER OLIVE_HOST_PORT_OLIVESECOND OLIVE_HOST_PORT_TEST OLIVE_HOST_PORT_BROKER OLIVE_HOST_PORT_DOCSERVER OLIVE_HOST_PORT_WEBSERVER OLIVE_HOST_PORT_REVERSE_PROXY Note that if you are using Windows Command Prompt (cmd) or Windows PowerShell via martini.bat , edit the martini.ps1 file, as it is called by martini.bat during operation. Doing this will use the new ports every time the container starts. Alternatively, you can set shell variables, with the same names as above, to the port numbers you want to use. This method will temporarily override the ports used to what you have set, but subsequent container starts will revert to the original, if the shell/environment variables are no longer set. Configuring Plugins, Workflows, and Documentation (Optional) If you are using the default installation, then no configuration is required. Your workflows must be in a directory called \"workflows\", your documentation in a directory called docs/ , and your plugins in a directory called oliveAppData/plugins/ , and the martini.sh script in an oliveDocker directory that is adjacent to docs/, workflows/, and oliveAppData/. If your data is not located in these default locations, then you have two ways to configure the OLIVE Martini container (similar to configuring ports described above): You can directly edit the martini.sh file to change these three variables near the top of the file: OLIVE_HOST_PLUGINS OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE Doing this will use the new locations every time the container starts. You can set shell variables to the full path names of the locations of your plugins, workflows, and documentation. Set these shell variables: OLIVE_HOST_PLUGINS OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE But remember, neither of these is required, if your plugins, workflows, and documentation are all in the default locations. You can also (optionally) edit martini.sh to change the DELAY count. It can take a few to 10 seconds or so for all the servers to start on the container, depending on the hardware performing the operation. Normally the martini.sh returns immediately, even if the servers have not had time to start. If you don't like that behavior, you can set the DELAY variable near the top of the file to the number of seconds it takes to start all the servers, usually 7 or 10 seconds. This would mean that the command will not return until the servers are actually up and running. Testing The Installation There are a few ways to quickly sanity check that the installation is properly configured, up and running. Use a web broser to see what workflows are available. Go to URL host :9905/api/workflows. You should see some json text. Use a web browser to test the Olive Web UI. Go to URL host :9980. You should see a page with \"SRI International\" in the upper right corner, and a section for \"Media\" and a section for \"Workflows\". (See below for getting started with the Olive Web UI.) Use a web browser to look at the documentation. Go to to URL host :9970. You should see a web page titled \"Open Language Interface for Voice Exploitation (OLIVE)\". Controlling the Container Controlling and managing the OLIVE Multi container is made much easier with the provided martini.sh management script. Using this script is as simple as running the script, and providing a command: macOS and linux $ ./martini.sh <command> Windows $ .\\martini.bat <command> The list of commands available to martini.sh are: help - Display the olivemulti.sh help statement (shown below) start - Start the container. stop - Stop the container. list - List the running container(s), if any. log - Display the OLIVE Server log in the terminal; ctrl + c to exit. Useful to expert users for troubleshooting. status - Show the status of the processes on the container. net - Show the ports on the host that the continer is linstening on. cli - Starts a shell on the container for debugging or operating the OLIVE CLI tools. More details for each command, how to use it, and the designed functionality can be found below. Important \"Host\" Notes All of these commands assume that your container is running on a machine called host . When performing the operations below, replace host with the actual name of the host machine. If all operations (client and server) are running on the same machine, this may be simply localhost . If they are on separate machines, you may have to use the actual hostname, or IP. martini.sh help Prints out the martini.sh help statement, reminding the user of the available commands: You must provide one argument which is a command. Supported commands include: cli, help, list, start, stop, status, net. olivemulti.sh start: Start the container. olivemulti.sh stop: Stop the container. olivemulti.sh list: List the running container. olivemulti.sh status: Shows status of the processes on the container. olivemulti.sh net: Shows the ports on the host that the container is listening on. olivemulti.sh cli: Starts a shell on the container for debugging. martini.sh start Starts up a previously built container. Note that it can take several seconds for all the servers to start on the container. You can edit the DELAY variable at the top of your script to a number (discussed above), in which case this command will wait that many seconds before returning, so you can be sure the servers are running when the command returns. martini.sh stop Stops a running container. olivemulti.sh status Prints out some basic information on the processes running, network ports, workflows and plugins which are active on the container. $ martini.sh status Message brokers running: Processes: 1 Plugins: aed-enrollable-v1.0.1 aln-waveformAlignment-v1.0.0 env-indoorOutdoor-v1.0.0 env-multiClass-v2.0.0 env-powerSupplyHum-v1.0.0 env-speakerCount-v1.0.0 lid-embedplda-v2.0.1 nsd-sadInverter-v1.0.0 sad-dnn-v7.0.1 sed-rmsEnergy-v1.0.0 sid-dplda-v2.0.1 voi-speakingStyle-v1.0.0 voi-vocalEffort-v1.0.1 vtd-dnn-v7.0.1 Workflows: Acoustic-Event-Detection.workflow Background-Noise-Detection.workflow SAD_SID_LID.workflow Speech-Analysis.workflow tcp 0 0 0.0.0.0:5589 0.0.0.0:* LISTEN 0 7905040 72/python tcp 0 0 0.0.0.0:5590 0.0.0.0:* LISTEN 0 7915534 73/python tcp 0 0 0.0.0.0:8070 0.0.0.0:* LISTEN 0 7910329 9/python tcp 0 0 0.0.0.0:5004 0.0.0.0:* LISTEN 0 7917573 138/nginx: master p tcp 0 0 0.0.0.0:5005 0.0.0.0:* LISTEN 0 7914186 15/java tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 0 7917572 138/nginx: master p tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 0 7897930 26/httpd tcp 0 0 0.0.0.0:5588 0.0.0.0:* LISTEN 0 7915524 16/python unix 2 [ ACC ] STREAM LISTENING 7915525 16/python /tmp/executor_714acd0b-0d0d-4259-84c2-127b84b8d26c.pipe unix 2 [ ACC ] STREAM LISTENING 7905043 80/python /tmp/pymp-vu6okza8/listener-4e5_70ll Httpd (web-ui) servers running: 6 Nginx (reverse proxy) server running: 2 Olive servers running: 12 Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:5589 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5590 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8070 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5004 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5005 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5588 0.0.0.0:* LISTEN martini.sh cli Starts up a shell within the OLIVE Martini container. The container must already be running. You can use this shell to run Olive CLI commands, such as: $ martini.sh cli # Show the plugins installed on the container: $ ls /home/olive/olive/plugins/ # Show the audio files on the container, which you can use as examples: $ ls /olive-data/media/shared/ # Run an analysis $ localanalyze /home/olive/olive/plugins/lid-embedplda-v2.0.1/domains/multi-v1 /olive-data/media/shared/Komodo_dragon_en.wav.lst # See the results $ more output.txt This can also be used for troubleshooting. When running with the cli command, your shell starts in /opt/olive . This is where the runtime is installed, and where things are run within the container. In /opt/olive : Running martini.sh status will print out what is running, and what network connections are active. And looking at this script will tell you what should be running. Do not run martini-process.sh. That is run when the container starts. The /home/olive/olive directory is the other important directory. Here: The martini-process.log file is the output of the script which starts all servers. The env.log file (if it exists) are the shell variables set. The other log files here were started by each server. The plugins/ directory contains all plugins. The server/ directory contains server logs. martini.sh net Lists the project network ports that are active on the host machine, the machine running the container. martini.sh list Lists the project containers that are running. martini.sh log Displays the OLIVE Server log; useful for checking on the status of the OLIVE server in case of a malfunction, or troubleshooting unexpected behavior. Using The Container Once you have started up the container using the martini.sh start command described above, you are ready to begin using it. Your container has four different applications within it, which you can use - the Raven Web UI, the OLIVE Server itself, the OLIVE Message Broker, and a web server hosting the OLIVE Documentation. In addition, most OLIVE deliveries typically ship with the interactive Nightingale Forensic GUI for performing close file analysis using OLIVE plugins and workflows, as well as both Java and Python OLIVE Client CLI example applications. Note that if you are attempting to connect from the same machine, you can access this feature at localhost:9980 . If you are accessing from a different machine, you must use the full hostname, or IP address. For convenience, the martini.sh script outputs the hostname and corresponding port for several activities: $ martini.sh start Started the container. From this machine: Run Nightingale (Olive GUI) using server localhost and port 9900. Use a web browser to localhost:9970 to see the documentation. Use a web browser to localhost:9980 to use the Web UI. If using the API point to the server running on localhost:9905. From any other machine: Run Nightingale (Olive GUI) using server OLIVE-MBP16 and port 9900. Use a web browser to OLIVE-MBP16:9970 to see the documentation. Use a web browser to OLIVE-MBP16:9980 to use the Web UI. If using the API point to the server running on OLIVE-MBP16:9905. Installed plugins (mounted from /Users/user1/olive/olive5.1.0/unicontainer/olive5.1.0/oliveAppData/plugins/) are: aed-enrollable-v1.0.1 env-speakerCount-v1.0.0 sid-dplda-v2.0.1 aln-waveformAlignment-v1.0.0 lid-embedplda-v2.0.1 voi-speakingStyle-v1.0.0 env-indoorOutdoor-v1.0.0 nsd-sadInverter-v1.0.0 voi-vocalEffort-v1.0.1 env-multiClass-v2.0.0 sad-dnn-v7.0.1 vtd-dnn-v7.0.1 env-powerSupplyHum-v1.0.0 sed-rmsEnergy-v1.0.0 Installed workflows (mounted from /Users/user1/olive/olive5.1.0/unicontainer/olive5.1.0/workflows/) are: Acoustic-Event-Detection.workflow SAD_SID_LID.workflow Background-Noise-Detection.workflow Speech-Analysis.workflow Please choose the appropriate hostname and port number for your desired activity and host situation. Raven Web UI To connect to the Raven Web/Batch GUI , open a web browser and navigate to URL host :9980. Once there, follow the on-screen prompts to drag-and-drop one or more audio files to the \"local media\" dropbox section, select one of the available workflows, and click on the action button in the lower right hand corner. More details for the Raven GUI can be found on its dedicated documentation page: Raven Web UI . The Documentation When the container is started, it launches a web server serving this documentation. To view the hosted version, navigate a web browser to URL host :9970. The OLIVE Server This is used internally by the Olive Web UI and the Olive Message Broker. You can also write Python programs which make calls directly to the server. See seperate documentation on olivepy for details. The OLIVE Message Broker This is used internally by the Olive Web UI. Nightingale Forensic UI This GUI provides access to the full suite of available OLIVE plugins and workflows, for performing close file analysis. It offers powerful audio visualization tools, live streaming capabilities, and a number of other useful features. For more information on what Nightingale offers and how to get started using it, please refer to the Nightingale GUI dedicated documentation page. As a quick primer, the Nightingale UI is typically provided in: <...>/olive5.1.0/OliveGUI Nightingale requires OpenJDK Java 11. Once this is installed, you can run it by navigating to: <...>/olive5.1.0/OliveGUI/bin/ And either double-clicking or running the Nightingale (macOS, linux) or Nightingale.bat (Windows) script appropriate for your OS. macOS / linux $ ./Nightingale Windows $ . \\N ightingale.bat NOTE that the Nightingale launcher scripts are designed to be launched from this bin/ directory in order to properly link to other folders that it uses within this package. NOTE that Nightingale will need to connect to the oliveMulti container using port 9900 by default. This should be set for you before the software ships if the GUI is accompanying an oliveMulti software package. OLIVE Example API Client Implementations OLIVE is also generally shipped with example API Client implementations in both Java and python. They are usually included in: <...>/olive5.1.0/api/[java, python] These can provide both a command-line-based method of interacting with the OLIVE server, and as integration examples providing templates or code bases for some integration teams. Final Notes / Troubleshooting There are a couple of known minor issues that may occur the first time you are starting the OLIVE Martini container on a new device. Since Docker will prompt asking for permission to share a few locations on the machine, in order to access the included plugins, workflows, and documentation, the container cannot initially read these and may report no plugins or workflows found at first. Once access is granted for Docker to share these locations, you should be able to check the plugins that are loaded by checking martini log , or in the extreme case, by stopping and restarting the container: macOS / linux $ ./martini.sh stop $ ./martini.sh start Windows $ . \\m artini.bat stop $ . \\m artini.bat start It can also take a moment for the Raven UI to populate the available Workflows on the bottom of the interface screen, especially the first time you are loading the web page. If this space is blank, please wait a moment (may take up to a minute on some systems) and/or refresh the page.","title":"OLIVE Martini Docker Setup"},{"location":"martini.html#olive-martini-docker-container-setup","text":"","title":"OLIVE Martini Docker Container Setup"},{"location":"martini.html#introduction","text":"This page guides through the installation, setup, and launching of a docker-container based OLIVE software package featuring the new \"OLIVE Martini\" container setup. This new delivery method includes the usual OLIVE server and plugin capabilities delivered in the past, but includes a new addition - Batch GUI capabilities powered by the new Raven GUI that can be accessed through your web browser. For instructions on how to use this new GUI once you have the software properly installed, check out the Raven Web GUI page. The overall installation process is very similar to previous Docker container based OLIVE deliveries in the past, with the main changes affecting how the OLIVE container is managed for startup, shutdown, etc.","title":"Introduction"},{"location":"martini.html#download-install-and-launch-docker","text":"First you will need to obtain and install Docker - this can be done in a number of ways, the easiest of which is likely through the Docker website. The Community Edition is free (though you now need to create an account with Docker to obtain it), should fulfill most or all OLIVE needs, and is available for several operating systems, including Windows 10. Docker is available here - https://www.docker.com/community-edition and up-to-date installation instructions can be found from Docker. Once Docker is installed, launch it to proceed. When running Docker for the first time, if you are using a Windows OS, you may be prompted to enable some Windows features that are not enabled by default, but are required by Docker to run; Hyper-V and Containers. Select \u201cOk\u201d and wait for the computer to restart to enable these features. Docker should automatically start up once the computer has restarted.","title":"Download, Install, and Launch Docker"},{"location":"martini.html#adjust-docker-settings-ram-cores","text":"If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS","title":"Adjust Docker settings (RAM, Cores)"},{"location":"martini.html#download-olive-docker-package","text":"Now that Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: olive5.1.0-martini-11June2021.tar.gz You should find similar content to below unless told otherwise: olive5.1.0/ api/ - (Optional) Directory containing the Python and/or Java OLIVE Client utilities docs/ - Directory containing the OLIVE documentation martini/ martini-docker.tar - Docker container including OLIVE , the Raven Web GUI , the OLIVE Web Broker, and various other utilities, including an html server to host this documentation martini.sh - Multi-purpose utility and management script for the OLIVE Martini Docker container martini.ps1 - Same as above, but for Windows PowerShell martini.bat - Wrapper script to allow the above script to execute on certain Windows systems. Use this first. OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ sad-dnn-v7.0.0 \u2013 Speech Activity Detection plugin Actual plugins included will depend on the customer, mission, and delivery workflows/ - Directory containing OLIVE workflows; actual included worfklows will depend on the customer, mission, and delivery","title":"Download OLIVE Docker Package"},{"location":"martini.html#load-the-olive-docker-image","text":"The first setup step is to load the OLIVE Docker image. Loading the image is as simple as opening a command prompt like PowerShell in Windows or Terminal in MacOS, navigating to the directory containing the OLIVE Docker image, and following the examples below. Loading the docker image (only necessary once). Windows $ cd C : \\ Users \\ < username > \\ olive5 . 1.0 \\ martini $ docker load - i martini - container . tar macOS / linux $ cd / home /< username >/ olive5 . 1.0 / martini $ docker load - i martini - container . tar This operation can take some time; quite a few minutes, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed.","title":"Load the OLIVE Docker Image"},{"location":"martini.html#other-required-software","text":"This container has been tested using docker version 19.03.9 API version 1.40, but should work with any recent version. The Olive Web UI should work with any recent, common web browser, but has been tested mostly with Chrome (90.0.4430.212), but also Edge (89.0.774.77), and Firefox (87.0). The Python scripts included should work with any 3.x version of Python, but have been tested with Python 3.7.9","title":"Other Required Software"},{"location":"martini.html#operating-systems-note","text":"There are three provided martini management scripts: martini.sh martini.bat martini.ps1 The functionality of each is intended to be identical, and these are provided as different options for different operating systems for convenience. The martini.sh script should be used for linux and for macOS. martini.ps1 should be used when managing the OLIVE Martini container from Windows PowerShell, and martini.bat , which calls martini.ps1 internally, should be used if martini.ps1 cannot execute for permission reasons in PowerShell, and when managing OLIVE Martini from Windows Command Prompt ( cmd ). The examples below attempt to show all three being used, but in the text, they are refered to as simply martini.sh for brevity. Their features and functionality are the same.","title":"Operating Systems Note"},{"location":"martini.html#configuring-ports-optional","text":"By default the container exposes seven ports on the host machine running the container: 9900 9901 9904 9905 9970 9980 9988 If you want to change them, there are two ways to do this: You can directly edit the martini.sh or martini.ps1 file to change these seven variables near the top of the file: OLIVE_HOST_PORT_OLIVESERVER OLIVE_HOST_PORT_OLIVESECOND OLIVE_HOST_PORT_TEST OLIVE_HOST_PORT_BROKER OLIVE_HOST_PORT_DOCSERVER OLIVE_HOST_PORT_WEBSERVER OLIVE_HOST_PORT_REVERSE_PROXY Note that if you are using Windows Command Prompt (cmd) or Windows PowerShell via martini.bat , edit the martini.ps1 file, as it is called by martini.bat during operation. Doing this will use the new ports every time the container starts. Alternatively, you can set shell variables, with the same names as above, to the port numbers you want to use. This method will temporarily override the ports used to what you have set, but subsequent container starts will revert to the original, if the shell/environment variables are no longer set.","title":"Configuring Ports (Optional)"},{"location":"martini.html#configuring-plugins-workflows-and-documentation-optional","text":"If you are using the default installation, then no configuration is required. Your workflows must be in a directory called \"workflows\", your documentation in a directory called docs/ , and your plugins in a directory called oliveAppData/plugins/ , and the martini.sh script in an oliveDocker directory that is adjacent to docs/, workflows/, and oliveAppData/. If your data is not located in these default locations, then you have two ways to configure the OLIVE Martini container (similar to configuring ports described above): You can directly edit the martini.sh file to change these three variables near the top of the file: OLIVE_HOST_PLUGINS OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE Doing this will use the new locations every time the container starts. You can set shell variables to the full path names of the locations of your plugins, workflows, and documentation. Set these shell variables: OLIVE_HOST_PLUGINS OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE But remember, neither of these is required, if your plugins, workflows, and documentation are all in the default locations. You can also (optionally) edit martini.sh to change the DELAY count. It can take a few to 10 seconds or so for all the servers to start on the container, depending on the hardware performing the operation. Normally the martini.sh returns immediately, even if the servers have not had time to start. If you don't like that behavior, you can set the DELAY variable near the top of the file to the number of seconds it takes to start all the servers, usually 7 or 10 seconds. This would mean that the command will not return until the servers are actually up and running.","title":"Configuring Plugins, Workflows, and Documentation (Optional)"},{"location":"martini.html#testing-the-installation","text":"There are a few ways to quickly sanity check that the installation is properly configured, up and running. Use a web broser to see what workflows are available. Go to URL host :9905/api/workflows. You should see some json text. Use a web browser to test the Olive Web UI. Go to URL host :9980. You should see a page with \"SRI International\" in the upper right corner, and a section for \"Media\" and a section for \"Workflows\". (See below for getting started with the Olive Web UI.) Use a web browser to look at the documentation. Go to to URL host :9970. You should see a web page titled \"Open Language Interface for Voice Exploitation (OLIVE)\".","title":"Testing The Installation"},{"location":"martini.html#controlling-the-container","text":"Controlling and managing the OLIVE Multi container is made much easier with the provided martini.sh management script. Using this script is as simple as running the script, and providing a command: macOS and linux $ ./martini.sh <command> Windows $ .\\martini.bat <command> The list of commands available to martini.sh are: help - Display the olivemulti.sh help statement (shown below) start - Start the container. stop - Stop the container. list - List the running container(s), if any. log - Display the OLIVE Server log in the terminal; ctrl + c to exit. Useful to expert users for troubleshooting. status - Show the status of the processes on the container. net - Show the ports on the host that the continer is linstening on. cli - Starts a shell on the container for debugging or operating the OLIVE CLI tools. More details for each command, how to use it, and the designed functionality can be found below.","title":"Controlling the Container"},{"location":"martini.html#important-host-notes","text":"All of these commands assume that your container is running on a machine called host . When performing the operations below, replace host with the actual name of the host machine. If all operations (client and server) are running on the same machine, this may be simply localhost . If they are on separate machines, you may have to use the actual hostname, or IP.","title":"Important \"Host\" Notes"},{"location":"martini.html#martinish-help","text":"Prints out the martini.sh help statement, reminding the user of the available commands: You must provide one argument which is a command. Supported commands include: cli, help, list, start, stop, status, net. olivemulti.sh start: Start the container. olivemulti.sh stop: Stop the container. olivemulti.sh list: List the running container. olivemulti.sh status: Shows status of the processes on the container. olivemulti.sh net: Shows the ports on the host that the container is listening on. olivemulti.sh cli: Starts a shell on the container for debugging.","title":"martini.sh help"},{"location":"martini.html#martinish-start","text":"Starts up a previously built container. Note that it can take several seconds for all the servers to start on the container. You can edit the DELAY variable at the top of your script to a number (discussed above), in which case this command will wait that many seconds before returning, so you can be sure the servers are running when the command returns.","title":"martini.sh start"},{"location":"martini.html#martinish-stop","text":"Stops a running container.","title":"martini.sh stop"},{"location":"martini.html#olivemultish-status","text":"Prints out some basic information on the processes running, network ports, workflows and plugins which are active on the container. $ martini.sh status Message brokers running: Processes: 1 Plugins: aed-enrollable-v1.0.1 aln-waveformAlignment-v1.0.0 env-indoorOutdoor-v1.0.0 env-multiClass-v2.0.0 env-powerSupplyHum-v1.0.0 env-speakerCount-v1.0.0 lid-embedplda-v2.0.1 nsd-sadInverter-v1.0.0 sad-dnn-v7.0.1 sed-rmsEnergy-v1.0.0 sid-dplda-v2.0.1 voi-speakingStyle-v1.0.0 voi-vocalEffort-v1.0.1 vtd-dnn-v7.0.1 Workflows: Acoustic-Event-Detection.workflow Background-Noise-Detection.workflow SAD_SID_LID.workflow Speech-Analysis.workflow tcp 0 0 0.0.0.0:5589 0.0.0.0:* LISTEN 0 7905040 72/python tcp 0 0 0.0.0.0:5590 0.0.0.0:* LISTEN 0 7915534 73/python tcp 0 0 0.0.0.0:8070 0.0.0.0:* LISTEN 0 7910329 9/python tcp 0 0 0.0.0.0:5004 0.0.0.0:* LISTEN 0 7917573 138/nginx: master p tcp 0 0 0.0.0.0:5005 0.0.0.0:* LISTEN 0 7914186 15/java tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 0 7917572 138/nginx: master p tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 0 7897930 26/httpd tcp 0 0 0.0.0.0:5588 0.0.0.0:* LISTEN 0 7915524 16/python unix 2 [ ACC ] STREAM LISTENING 7915525 16/python /tmp/executor_714acd0b-0d0d-4259-84c2-127b84b8d26c.pipe unix 2 [ ACC ] STREAM LISTENING 7905043 80/python /tmp/pymp-vu6okza8/listener-4e5_70ll Httpd (web-ui) servers running: 6 Nginx (reverse proxy) server running: 2 Olive servers running: 12 Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:5589 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5590 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8070 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5004 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5005 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5588 0.0.0.0:* LISTEN","title":"olivemulti.sh status"},{"location":"martini.html#martinish-cli","text":"Starts up a shell within the OLIVE Martini container. The container must already be running. You can use this shell to run Olive CLI commands, such as: $ martini.sh cli # Show the plugins installed on the container: $ ls /home/olive/olive/plugins/ # Show the audio files on the container, which you can use as examples: $ ls /olive-data/media/shared/ # Run an analysis $ localanalyze /home/olive/olive/plugins/lid-embedplda-v2.0.1/domains/multi-v1 /olive-data/media/shared/Komodo_dragon_en.wav.lst # See the results $ more output.txt This can also be used for troubleshooting. When running with the cli command, your shell starts in /opt/olive . This is where the runtime is installed, and where things are run within the container. In /opt/olive : Running martini.sh status will print out what is running, and what network connections are active. And looking at this script will tell you what should be running. Do not run martini-process.sh. That is run when the container starts. The /home/olive/olive directory is the other important directory. Here: The martini-process.log file is the output of the script which starts all servers. The env.log file (if it exists) are the shell variables set. The other log files here were started by each server. The plugins/ directory contains all plugins. The server/ directory contains server logs.","title":"martini.sh cli"},{"location":"martini.html#martinish-net","text":"Lists the project network ports that are active on the host machine, the machine running the container.","title":"martini.sh net"},{"location":"martini.html#martinish-list","text":"Lists the project containers that are running.","title":"martini.sh list"},{"location":"martini.html#martinish-log","text":"Displays the OLIVE Server log; useful for checking on the status of the OLIVE server in case of a malfunction, or troubleshooting unexpected behavior.","title":"martini.sh log"},{"location":"martini.html#using-the-container","text":"Once you have started up the container using the martini.sh start command described above, you are ready to begin using it. Your container has four different applications within it, which you can use - the Raven Web UI, the OLIVE Server itself, the OLIVE Message Broker, and a web server hosting the OLIVE Documentation. In addition, most OLIVE deliveries typically ship with the interactive Nightingale Forensic GUI for performing close file analysis using OLIVE plugins and workflows, as well as both Java and Python OLIVE Client CLI example applications. Note that if you are attempting to connect from the same machine, you can access this feature at localhost:9980 . If you are accessing from a different machine, you must use the full hostname, or IP address. For convenience, the martini.sh script outputs the hostname and corresponding port for several activities: $ martini.sh start Started the container. From this machine: Run Nightingale (Olive GUI) using server localhost and port 9900. Use a web browser to localhost:9970 to see the documentation. Use a web browser to localhost:9980 to use the Web UI. If using the API point to the server running on localhost:9905. From any other machine: Run Nightingale (Olive GUI) using server OLIVE-MBP16 and port 9900. Use a web browser to OLIVE-MBP16:9970 to see the documentation. Use a web browser to OLIVE-MBP16:9980 to use the Web UI. If using the API point to the server running on OLIVE-MBP16:9905. Installed plugins (mounted from /Users/user1/olive/olive5.1.0/unicontainer/olive5.1.0/oliveAppData/plugins/) are: aed-enrollable-v1.0.1 env-speakerCount-v1.0.0 sid-dplda-v2.0.1 aln-waveformAlignment-v1.0.0 lid-embedplda-v2.0.1 voi-speakingStyle-v1.0.0 env-indoorOutdoor-v1.0.0 nsd-sadInverter-v1.0.0 voi-vocalEffort-v1.0.1 env-multiClass-v2.0.0 sad-dnn-v7.0.1 vtd-dnn-v7.0.1 env-powerSupplyHum-v1.0.0 sed-rmsEnergy-v1.0.0 Installed workflows (mounted from /Users/user1/olive/olive5.1.0/unicontainer/olive5.1.0/workflows/) are: Acoustic-Event-Detection.workflow SAD_SID_LID.workflow Background-Noise-Detection.workflow Speech-Analysis.workflow Please choose the appropriate hostname and port number for your desired activity and host situation.","title":"Using The Container"},{"location":"martini.html#raven-web-ui","text":"To connect to the Raven Web/Batch GUI , open a web browser and navigate to URL host :9980. Once there, follow the on-screen prompts to drag-and-drop one or more audio files to the \"local media\" dropbox section, select one of the available workflows, and click on the action button in the lower right hand corner. More details for the Raven GUI can be found on its dedicated documentation page: Raven Web UI .","title":"Raven Web UI"},{"location":"martini.html#the-documentation","text":"When the container is started, it launches a web server serving this documentation. To view the hosted version, navigate a web browser to URL host :9970.","title":"The Documentation"},{"location":"martini.html#the-olive-server","text":"This is used internally by the Olive Web UI and the Olive Message Broker. You can also write Python programs which make calls directly to the server. See seperate documentation on olivepy for details.","title":"The OLIVE Server"},{"location":"martini.html#the-olive-message-broker","text":"This is used internally by the Olive Web UI.","title":"The OLIVE Message Broker"},{"location":"martini.html#nightingale-forensic-ui","text":"This GUI provides access to the full suite of available OLIVE plugins and workflows, for performing close file analysis. It offers powerful audio visualization tools, live streaming capabilities, and a number of other useful features. For more information on what Nightingale offers and how to get started using it, please refer to the Nightingale GUI dedicated documentation page. As a quick primer, the Nightingale UI is typically provided in: <...>/olive5.1.0/OliveGUI Nightingale requires OpenJDK Java 11. Once this is installed, you can run it by navigating to: <...>/olive5.1.0/OliveGUI/bin/ And either double-clicking or running the Nightingale (macOS, linux) or Nightingale.bat (Windows) script appropriate for your OS. macOS / linux $ ./Nightingale Windows $ . \\N ightingale.bat NOTE that the Nightingale launcher scripts are designed to be launched from this bin/ directory in order to properly link to other folders that it uses within this package. NOTE that Nightingale will need to connect to the oliveMulti container using port 9900 by default. This should be set for you before the software ships if the GUI is accompanying an oliveMulti software package.","title":"Nightingale Forensic UI"},{"location":"martini.html#olive-example-api-client-implementations","text":"OLIVE is also generally shipped with example API Client implementations in both Java and python. They are usually included in: <...>/olive5.1.0/api/[java, python] These can provide both a command-line-based method of interacting with the OLIVE server, and as integration examples providing templates or code bases for some integration teams.","title":"OLIVE Example API Client Implementations"},{"location":"martini.html#final-notes-troubleshooting","text":"There are a couple of known minor issues that may occur the first time you are starting the OLIVE Martini container on a new device. Since Docker will prompt asking for permission to share a few locations on the machine, in order to access the included plugins, workflows, and documentation, the container cannot initially read these and may report no plugins or workflows found at first. Once access is granted for Docker to share these locations, you should be able to check the plugins that are loaded by checking martini log , or in the extreme case, by stopping and restarting the container: macOS / linux $ ./martini.sh stop $ ./martini.sh start Windows $ . \\m artini.bat stop $ . \\m artini.bat start It can also take a moment for the Raven UI to populate the available Workflows on the bottom of the interface screen, especially the first time you are loading the web page. If this space is blank, please wait a moment (may take up to a minute on some systems) and/or refresh the page.","title":"Final Notes / Troubleshooting"},{"location":"nightingale.html","text":"OLIVE Nightingale GUI Installation and Startup Getting started with the OLIVE Nightingale GUI is a pretty straightforward process. The only prerequisite for running the GUI, apart from a functional and running OLIVE server, is openJDK Java 11, that can be sourced here: OpenJDK 11 The GUI is configured to automatically detect and connect to an OLIVE server instance running on the same machine as the GUI, as long as the default server ports are still being used. It is possible to connect this GUI to an OLIVE server running on a remote machine, so long as firewall and other settings permit. For details on how to do this if it's needed, get in touch with us for assistance. These instructions assume a typical OLIVE delivery layout as follows: olive5.1.0 docs/ oliveDocker or runtime-5.1.0*/olive-5.1.0* OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ workflows/ To launch the GUI, once you've installed the version of Java linked above, fire up the OLIVE server in your favorite manner, then you can launch the GUI by hopping into the appropriate directory, and starting the Nightingale launcher: $ cd olive5.1.0/OliveGUI/bin $ ./Nightingale This should start up the GUI that should automatically connect to the running server. The GUI should be pointing to your 'workflows' directory by default, so it should already know about the SAD_LID_SID.workflow that's included - but if for some reason this isn't showing up, you can add it using the '+' symbol in the GUI's Workflows panel. Nightingale GUI User Guide Overview Below is a basic diagram of Nightingale with all the major panels highlighted: Panels: Live Stream Playback Layout New Tier Server Status Workflows Plugins Channel List Spectogram Spectogram Control Global Score Results Region Score Results Enrollment What's an Enrollment? Enrollment is the process of adding a model to a plugin and is required for several types of plugins. In the case of Speaker Identification, and enrollment involves submitting a portion of audio that contains only the speaker that you want to look for in the future. The enrollment process turns this portion of audio into a model which the speaker identification plugin will use to score future audio against in order to detect additional instances of the speaker you enrolled. How to Enroll Enrollment in the Nightingale User Interface is fast and simple. You need to do the following steps: Select the audio you would like to enroll Select the plugin you would like to enroll it in Give the enrollment a name, or add it to an existing enrollment (this is called augmentation) Click enroll and then wait. Below are images that can be used as reference for each step. In these examples the enrollments are being done using the Speaker Identification plugin on the left side, and the Query By Example plugin on the right side. 1. Select Audio 2a. Select Plugin 2b. Select Plugin contd 3. Give Enrollment Name 4. Enroll Augmentation Augmentation is almost the exact same process as enrollment, but in augmentation instead of creating a new class (eg. speaker, keyword, etc) you'll be adding additional data to an existing class. This is useful for improving models, especially when the amount of audio, or the quality of audio for the initial enrollment is low. To augment a model simple follow the same steps for enrollment, but instead of giving the enrollment a name in Step 3, select an existing class from the following dropdown: Removing an Enrollment Removing an enrollment can be useful to remove clutter if you no longer need a class or classes. Removing an enrolled class removes the entirety of the class, if you had augmented the class with several audio clips, unenrolling will remove everything. There is no way through the Nightingale GUI to remove individual augmentation segments from an enrollment. To remove a class from a plugin follow the steps below: Select the 'Enroll' button to the right of the plugin you want to remove an enrolled model from. Select 'Remove ______'. Select which class you want to remove from the dropdown list. Click 'Submit' Analysis Analysis is the basic operation of most plugins, and mostly falls under 3 general types of scoring; Global Scoring and Region Scoring. Region scorers return score results for one or more classes, and associate them with a specific region (or regions) in time, whereas global scorers return a single score for each class for the entire portion of the audio that was submitted. Some examples of global scorers and region scorers are below; typically \"Identification\" plugins are global scorers, and \"Detection\" plugins are region scorers. Global Scorers: Speaker Identification (SID) Language Identification (LID) Gender Identification etc. Region Scorers: Language Detection (LDD) Speaker Detection (SDD) Query by Example Keyword Spotting (QBE) Acoustic Event Detection (AED) Automatic Speech Recognition (ASR) etc. The procedure for scoring audio is largely the same for both types of plugins, and entails following these steps: First select audio (if you are analyzing a specific region, otherwise just make sure the appropriate file is selected), then do the following steps to run analysis on that audio: Select plugin type from panel (eg. SID, LID, etc) Select from 'Entire File', 'Selection', and 'Tier' (If you are analyzing a specific Tier, you will also need to make sure the correct tier is selected from the dropdown) If you have multiple versions or domains of the plugin you will need to specify which version of the plugin to use from the '___ Plugin:' dropdown menu. Click 'Submit' Viewing Results Global Scoring After you've performed analysis with a global scoring plugin the results of the analysis will appear in the global scoring section in the top right of the Nightingale GUI. Depending on the plugin there will be a list of each of that plugins classes and a corresponding score displayed in this area. Scores that are above 0 are considered a detection and will be shown at the top of the list in large green text, scores that are below 0 are displayed below in grey text. The higher a positive score is the more confident the system is that that particular class was detected in the audio that was submitted, whereas conversely, the lower the score below zero the more confident the system is that that class was NOT detected in the submitted audio. Some plugins that have many classes enrolled will not be able to display all of the scores in the global score results region, instead they will display the top X number of results and will also display a small button that says \"more\" at the bottom of the list. Clicking \"more\" will display the rest of the class scores for that analysis. Global scoring results appear on the right side of the GUI in panel \"11.\" on the Nightingale Overview Region Scoring After you've performed analysis with a region scoring plugin the results will be displayed as a new tier beneath the audio file that was analyzed. If there were detections in the analysis regions with the name of the class that was detected will be displayed in this tier. Region Scoring results will populate below the file that they were run on in panel \"12.\" in the Nightingale Overview Areas in the region score tiers that are marked by a dot/hash pattern indicate that these areas were not included in the region scoring submission. This happens when only a selection or tier is submitted for analysis instead of the entire file. Workflows Workflows are prebuilt complex tasks that run a segment of audio through a predefined series of HLT algorithms. This can be very useful and timesaving when there is a well-defined use case or mission that requires the same tasks or procedure to be run on all audio. If you have been provided with one or multiple .workflow files you can load them into the Nightingale GUI by following the steps below: Once you've successfully loaded one or multiple workflows into the GUI you will see them appear as new buttons in the workflow panel. You can submit audio to a workflow in the same way as submitting audio for analysis, by highlighting a selection/tier/file and clicking the appropriate workflow button as shown below: Depending on how each workflow is configured and which plugins it consists of, the results will be displayed around the waveform as they are returned, and will be displayed either on the Region Scoring Results area below the waveform, or the Global Scoring Results area to the right of the waveform, as is appropriate for each plugin the workflow is running. Nightingale Basic Tools In addition to HLT capabilities Nightingale includes several built-in tools to help users navigate and manipulate audio files for analysis and submission to algorithms. Below are the descriptions of these basic tools and how to use them. Audio Playback Controls Nightingale has two 'Playback Control Panels' that can be used to play, stop, pause, or speed up audio. These panels will play the audio in the active panel, and will default to playing from the start of the active file if no audio is selected, otherwise they will cause playback to start at the beginning of the selected audio. The playback panels are the panels marked \"2.\" in the Nightingale Overview Layout Panel The layout panel, \"3.\" on the Nightingale Overview can be used to customize the number and layout of different channels the user would like to display for simultaneous viewing. This can be a usefull tool to scale Nightingale to the needs of the user. Typically a user focused on close analysis of a audio files might want to use 1-3 panels to display different files, a user focusing on monitoring many live audio streams may wish to use this panel to configure Nightingale to display up to 12 simultaneous audio files or streams. Tier Manipulation and Annotation The 'New Tier' panel marked \"4.\" in the Nightingale Overview allows users to create their own tiers. Additionally it allows for the creation of creating inverse tiers, and creating new tiers from the difference and sum of multiple existing tiers. This is a tool that can be used by advanced users to fine tune the portions of audio that they wish to run tasks on. The 'New Tier' panel also allows users to create an annotation tier, in which they can add comments and notes at different timestamps in the audio. Channel/File List The file list on the left side of the Nightingale GUI marked \"8.\" in the Nightingale Overview can be used to switch between any files (or channels if you are livestreaming) that have been loaded into Nightingale. If there are more files/streams loaded then there are Layout Panels allocated, then you can allocate one of the undisplayed files to one of the layout panels by hovering over it in the Channel/File list, and then clicking the panel on the grid where you would like the file to be displayed. Spectogram Nightingale's spectogram, displayed in panel \"9.\" and controlled from panel \"10.\" in the Nightingale Overview can be a powerful tool to analyze audio and pick out signal from the background noise. The spectogram in panel 9. is the main interface panel for users, and is where the majority of audio selection can be done. The spectogram control panel in panel 10. allows the user to customize the color schemes, sensitivity, and a host of more advance settings that can be accessed on panel 10. by selecting the \"more...\" button. Spectogram 1 Spectogram 2 Changes made here are temporary and will be reset when the GUI is closed.","title":"OLIVE Nightingale GUI"},{"location":"nightingale.html#olive-nightingale-gui","text":"","title":"OLIVE Nightingale GUI"},{"location":"nightingale.html#installation-and-startup","text":"Getting started with the OLIVE Nightingale GUI is a pretty straightforward process. The only prerequisite for running the GUI, apart from a functional and running OLIVE server, is openJDK Java 11, that can be sourced here: OpenJDK 11 The GUI is configured to automatically detect and connect to an OLIVE server instance running on the same machine as the GUI, as long as the default server ports are still being used. It is possible to connect this GUI to an OLIVE server running on a remote machine, so long as firewall and other settings permit. For details on how to do this if it's needed, get in touch with us for assistance. These instructions assume a typical OLIVE delivery layout as follows: olive5.1.0 docs/ oliveDocker or runtime-5.1.0*/olive-5.1.0* OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ workflows/ To launch the GUI, once you've installed the version of Java linked above, fire up the OLIVE server in your favorite manner, then you can launch the GUI by hopping into the appropriate directory, and starting the Nightingale launcher: $ cd olive5.1.0/OliveGUI/bin $ ./Nightingale This should start up the GUI that should automatically connect to the running server. The GUI should be pointing to your 'workflows' directory by default, so it should already know about the SAD_LID_SID.workflow that's included - but if for some reason this isn't showing up, you can add it using the '+' symbol in the GUI's Workflows panel.","title":"Installation and Startup"},{"location":"nightingale.html#nightingale-gui-user-guide","text":"","title":"Nightingale GUI User Guide"},{"location":"nightingale.html#overview","text":"Below is a basic diagram of Nightingale with all the major panels highlighted: Panels: Live Stream Playback Layout New Tier Server Status Workflows Plugins Channel List Spectogram Spectogram Control Global Score Results Region Score Results","title":"Overview"},{"location":"nightingale.html#enrollment","text":"","title":"Enrollment"},{"location":"nightingale.html#whats-an-enrollment","text":"Enrollment is the process of adding a model to a plugin and is required for several types of plugins. In the case of Speaker Identification, and enrollment involves submitting a portion of audio that contains only the speaker that you want to look for in the future. The enrollment process turns this portion of audio into a model which the speaker identification plugin will use to score future audio against in order to detect additional instances of the speaker you enrolled.","title":"What's an Enrollment?"},{"location":"nightingale.html#how-to-enroll","text":"Enrollment in the Nightingale User Interface is fast and simple. You need to do the following steps: Select the audio you would like to enroll Select the plugin you would like to enroll it in Give the enrollment a name, or add it to an existing enrollment (this is called augmentation) Click enroll and then wait. Below are images that can be used as reference for each step. In these examples the enrollments are being done using the Speaker Identification plugin on the left side, and the Query By Example plugin on the right side. 1. Select Audio 2a. Select Plugin 2b. Select Plugin contd 3. Give Enrollment Name 4. Enroll","title":"How to Enroll"},{"location":"nightingale.html#augmentation","text":"Augmentation is almost the exact same process as enrollment, but in augmentation instead of creating a new class (eg. speaker, keyword, etc) you'll be adding additional data to an existing class. This is useful for improving models, especially when the amount of audio, or the quality of audio for the initial enrollment is low. To augment a model simple follow the same steps for enrollment, but instead of giving the enrollment a name in Step 3, select an existing class from the following dropdown:","title":"Augmentation"},{"location":"nightingale.html#removing-an-enrollment","text":"Removing an enrollment can be useful to remove clutter if you no longer need a class or classes. Removing an enrolled class removes the entirety of the class, if you had augmented the class with several audio clips, unenrolling will remove everything. There is no way through the Nightingale GUI to remove individual augmentation segments from an enrollment. To remove a class from a plugin follow the steps below: Select the 'Enroll' button to the right of the plugin you want to remove an enrolled model from. Select 'Remove ______'. Select which class you want to remove from the dropdown list. Click 'Submit'","title":"Removing an Enrollment"},{"location":"nightingale.html#analysis","text":"Analysis is the basic operation of most plugins, and mostly falls under 3 general types of scoring; Global Scoring and Region Scoring. Region scorers return score results for one or more classes, and associate them with a specific region (or regions) in time, whereas global scorers return a single score for each class for the entire portion of the audio that was submitted. Some examples of global scorers and region scorers are below; typically \"Identification\" plugins are global scorers, and \"Detection\" plugins are region scorers. Global Scorers: Speaker Identification (SID) Language Identification (LID) Gender Identification etc. Region Scorers: Language Detection (LDD) Speaker Detection (SDD) Query by Example Keyword Spotting (QBE) Acoustic Event Detection (AED) Automatic Speech Recognition (ASR) etc. The procedure for scoring audio is largely the same for both types of plugins, and entails following these steps: First select audio (if you are analyzing a specific region, otherwise just make sure the appropriate file is selected), then do the following steps to run analysis on that audio: Select plugin type from panel (eg. SID, LID, etc) Select from 'Entire File', 'Selection', and 'Tier' (If you are analyzing a specific Tier, you will also need to make sure the correct tier is selected from the dropdown) If you have multiple versions or domains of the plugin you will need to specify which version of the plugin to use from the '___ Plugin:' dropdown menu. Click 'Submit'","title":"Analysis"},{"location":"nightingale.html#viewing-results","text":"","title":"Viewing Results"},{"location":"nightingale.html#global-scoring","text":"After you've performed analysis with a global scoring plugin the results of the analysis will appear in the global scoring section in the top right of the Nightingale GUI. Depending on the plugin there will be a list of each of that plugins classes and a corresponding score displayed in this area. Scores that are above 0 are considered a detection and will be shown at the top of the list in large green text, scores that are below 0 are displayed below in grey text. The higher a positive score is the more confident the system is that that particular class was detected in the audio that was submitted, whereas conversely, the lower the score below zero the more confident the system is that that class was NOT detected in the submitted audio. Some plugins that have many classes enrolled will not be able to display all of the scores in the global score results region, instead they will display the top X number of results and will also display a small button that says \"more\" at the bottom of the list. Clicking \"more\" will display the rest of the class scores for that analysis. Global scoring results appear on the right side of the GUI in panel \"11.\" on the Nightingale Overview","title":"Global Scoring"},{"location":"nightingale.html#region-scoring","text":"After you've performed analysis with a region scoring plugin the results will be displayed as a new tier beneath the audio file that was analyzed. If there were detections in the analysis regions with the name of the class that was detected will be displayed in this tier. Region Scoring results will populate below the file that they were run on in panel \"12.\" in the Nightingale Overview Areas in the region score tiers that are marked by a dot/hash pattern indicate that these areas were not included in the region scoring submission. This happens when only a selection or tier is submitted for analysis instead of the entire file.","title":"Region Scoring"},{"location":"nightingale.html#workflows","text":"Workflows are prebuilt complex tasks that run a segment of audio through a predefined series of HLT algorithms. This can be very useful and timesaving when there is a well-defined use case or mission that requires the same tasks or procedure to be run on all audio. If you have been provided with one or multiple .workflow files you can load them into the Nightingale GUI by following the steps below: Once you've successfully loaded one or multiple workflows into the GUI you will see them appear as new buttons in the workflow panel. You can submit audio to a workflow in the same way as submitting audio for analysis, by highlighting a selection/tier/file and clicking the appropriate workflow button as shown below: Depending on how each workflow is configured and which plugins it consists of, the results will be displayed around the waveform as they are returned, and will be displayed either on the Region Scoring Results area below the waveform, or the Global Scoring Results area to the right of the waveform, as is appropriate for each plugin the workflow is running.","title":"Workflows"},{"location":"nightingale.html#nightingale-basic-tools","text":"In addition to HLT capabilities Nightingale includes several built-in tools to help users navigate and manipulate audio files for analysis and submission to algorithms. Below are the descriptions of these basic tools and how to use them.","title":"Nightingale Basic Tools"},{"location":"nightingale.html#audio-playback-controls","text":"Nightingale has two 'Playback Control Panels' that can be used to play, stop, pause, or speed up audio. These panels will play the audio in the active panel, and will default to playing from the start of the active file if no audio is selected, otherwise they will cause playback to start at the beginning of the selected audio. The playback panels are the panels marked \"2.\" in the Nightingale Overview","title":"Audio Playback Controls"},{"location":"nightingale.html#layout-panel","text":"The layout panel, \"3.\" on the Nightingale Overview can be used to customize the number and layout of different channels the user would like to display for simultaneous viewing. This can be a usefull tool to scale Nightingale to the needs of the user. Typically a user focused on close analysis of a audio files might want to use 1-3 panels to display different files, a user focusing on monitoring many live audio streams may wish to use this panel to configure Nightingale to display up to 12 simultaneous audio files or streams.","title":"Layout Panel"},{"location":"nightingale.html#tier-manipulation-and-annotation","text":"The 'New Tier' panel marked \"4.\" in the Nightingale Overview allows users to create their own tiers. Additionally it allows for the creation of creating inverse tiers, and creating new tiers from the difference and sum of multiple existing tiers. This is a tool that can be used by advanced users to fine tune the portions of audio that they wish to run tasks on. The 'New Tier' panel also allows users to create an annotation tier, in which they can add comments and notes at different timestamps in the audio.","title":"Tier Manipulation and Annotation"},{"location":"nightingale.html#channelfile-list","text":"The file list on the left side of the Nightingale GUI marked \"8.\" in the Nightingale Overview can be used to switch between any files (or channels if you are livestreaming) that have been loaded into Nightingale. If there are more files/streams loaded then there are Layout Panels allocated, then you can allocate one of the undisplayed files to one of the layout panels by hovering over it in the Channel/File list, and then clicking the panel on the grid where you would like the file to be displayed.","title":"Channel/File List"},{"location":"nightingale.html#spectogram","text":"Nightingale's spectogram, displayed in panel \"9.\" and controlled from panel \"10.\" in the Nightingale Overview can be a powerful tool to analyze audio and pick out signal from the background noise. The spectogram in panel 9. is the main interface panel for users, and is where the majority of audio selection can be done. The spectogram control panel in panel 10. allows the user to customize the color schemes, sensitivity, and a host of more advance settings that can be accessed on panel 10. by selecting the \"more...\" button. Spectogram 1 Spectogram 2 Changes made here are temporary and will be reset when the GUI is closed.","title":"Spectogram"},{"location":"plugins.html","text":"Plugin Documentation This page contains high level information about OLIVE plugins and their associated concepts - for more detailed information about integrating with plugins, please refer to the more low-level focused Plugin API Integration Details pages. For more information about a specific plugin, please find its info page link in the list at the bottom of this page . Anatomy of a Plugin OLIVE Plugins encapsulate the actual audio processing technologies and capabilities of the OLIVE system into modular pieces that facilitate system upgrades, capability additions, incremental updates, and provide other benefits such as allowing for tuning models to improve processing in targeted audio conditions or allowing for multiple tools or options to choose from to accomplish a given task. Each plugin has a specific type, which defines the task that it is capable of performing (see Plugin Types below). They consist of two parts, the plugin proper, which contains the recipe or algorithm information on how to perform the task, and one or more domains, which contain the data models used to run the algorithm and perform the function. Since plugins are generally machine learning-based the same algorithm may have multiple strengths based on the specific data used to train it (e.g. telephone audio, push-to-talk audio, high effort vocalization, conversational speech). This separation between the algorithm and the data model allows us to deliver new functionality that is based on training data independently of the algorithm by delivering a new domain. Classes and enrollments (described below) are associated with a domain. Plugin Types Plugin function types refer to the core task the plugin performs and have standard abbreviations. Most plugins are designed to perform one specific function (for example, language identification, keyword spotting). We refer to plugins as working on audio segments, since OLIVE can process both audio files (on the file system) and audio buffers (sent as data through the API). For more information regarding each plugin type, but not a specific plugin or domain, including plugin type definitions, general output formats, and use cases, please click on the name of the plugin in the 'Function Type' column of the table below to visit that plugin type's information page: Table of Plugin Function Types Function type Abbreviation Scoring type Classes Description Speech Activity Detection SAD Frame / Region speech Identifies speech regions in an audio segment Speaker Identification SID Global Enrolled speakers Identifies whether a single-talker audio segment contains a target speaker Speaker Detection SDD Region Enrolled speakers Detects a target speaker in audio with multiple talkers Speaker Diarization DIA Region Detect each unlabeled speaker Segments audio into clusters of unique speakers Language Identification LID Global Languages in training data and/or enrolled languages in some plug-ins Language and dialect detection Keyword Detection KWS Region Enrolled text keywords Language-specific approach to keyword detection using speech recognition and text Query by Example Keyword Spotting QBE Region Enrolled audio keywords Language independent approach to word spotting using one or more audio examples Gender Identification GID Global male, female Determines whether the audio segment was spoken by a male or female voice Topic Detection TPD Region Enrolled topics Detects topic regions in an audio segment Speech Enhancement ENH N/A N/A Reduces noise in an audio segment Voice Type Discrimination VTD Frame live-speech Detects presence of live-produced human speech, differentiating from silence, noise, speech coming from electronic device Scoring Types Different function types score audio segments on different levels of granularity. Some plugin functionality differences are essentially differences is how an audio segment is treated -- as a single unit or potentially multiple units. For example, the main difference between speaker identification and speaker detection is how a segment is scored, in that speaker identification assumes that the audio segment sent to it for scoring is homogenous and comes from a single speaker, where speaker detection will instead allow for the possibility of the presence of multiple speakers in a given audio segment. There are three major scoring types: Frame - Assigns a score for each 10ms frame of the audio segment submitted for scoring. Region - Assigns and reports time boundaries defining region(s) within the audio segment, and for each region, an accompanying score for each detected class . Global - Assigns a single score for the entire audio segment for each of the plugin's classes . For more information on these scoring types, refer to the Plugin Traits page. Classes Certain plugin types have classes as an attribute. These can be common, cross-mission categories that are often pre-trained - like speech, languages, or dialects - or they can be ad-hoc mission-specific classes like speakers or topics. A plugin\u2019s classes may be completely fixed as in gender identification (male, female) or speech activity detection (speech) or an open set as in language identification (English, Spanish, Mandarin, etc.), topics, or speakers. Some plugins allow the user to add new classes or modify existing classes. Some class sets are inherently closed, like SAD and GID, where the plugin is complete and covers the world of possible classes. Others, like LID/SID/TPD plugin will probably never be complete in covering all classes and thus will always need to be able to treat a segment as though it may not be from among the classes the plugin recognizes (i.e. \u2018out of set\u2019). Enrollments Enrollments are a sub-set of classes that the user can create and/or modify. Both creation of a class and modification of an existing class are class modification requests, where the first class modification request for a given class also has the effect of creating the new class if it does not yet exist. Enrollments may be generated by end users with examples from their own data and can be learned from a single or small number of examples (SID, QBE) to a relatively large number of examples (LID, TID). Speakers are typically enrollments, as are query-based keywords and topics. Languages can also be enrolled and augmented with certain plugins. Since enrollments are dynamic, they may be incrementally updated \u201con the fly\u201d with new examples. For integration details regarding enrollments, refer to the Enrollments section of the API Integration page. To determine if a plugin supports enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page from the Specific Plugins list below. Online Updates Considerable improvements to system accuracy and calibration can be found by updating a plugin post-deployment to better align with the conditions observed in recent history. Several plugins are able to perform unsupervised updates to certain submodules of the plugin. The updates do not require labels or human input and are based on automatically collected information during normal system use. In most cases, a system update must be invoked by the user via the API, and an option to determine if an update is ready to be applied is also provided in the API. For integration details regarding the update functionality, refer to the Update section of the API Integration page. To check if a plugin supports online updates, refer to its detailed information page from the Specific Plugins list below. Adaptation Similarly to online updates, it can be possible to achieve even larger boosts in performance by updating a plugin by exposing it to the mission's audio conditions, or similarly representative audio conditions. Adaptation, however, requires human input, and in some cases, data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate. Naming Conventions Plugin Names Each plugin proper is given a three part name in the following form: function-attribute-version: function - three-letter abbreviation from column two of the Plugin Types Table above attribute - string that identifies the key attribute of the plugin, generally the algorithm name version - tracks the iteration of the plug-in in development in the form of v<digit> For example, sid-embed-v2 is a speaker identification plugin using speaker embed dings algorithm and is the second release or update of the approach. An additonal alphanumeric character may be appended to the version number if a plugin is re-released with bug fixes, but the performance is expected to be the same. For example, sad-dnn-v6a is a modified version of sad-dnn-v6, but the changes were meant to address errors or shortcomings in the plugin, not to change the algorithm or data used. Domain Names Domain names typically have two or three parts: condition-version or language-condition-version for plugins that have language-dependent domain components. Keyword spotting domains also contain the language for which the domain was trained. language - the language for which the domain was trained if language-dependent, or a representation of the set of languages contained within a LID plugin's domain condition - the specific audio environment for which the domain was trained, or \u201cmulti\u201d if the domain was developed to be condition independent version - tracks the iteration of the plug-in in development in the form of v<digit> Specific Plugins For additional information about specific plugins, their options, implementation details and other, please refer to the specific plugin pages, accessible from each of the individual Plugin Task pages.","title":"Overview"},{"location":"plugins.html#plugin-documentation","text":"This page contains high level information about OLIVE plugins and their associated concepts - for more detailed information about integrating with plugins, please refer to the more low-level focused Plugin API Integration Details pages. For more information about a specific plugin, please find its info page link in the list at the bottom of this page .","title":"Plugin Documentation"},{"location":"plugins.html#anatomy-of-a-plugin","text":"OLIVE Plugins encapsulate the actual audio processing technologies and capabilities of the OLIVE system into modular pieces that facilitate system upgrades, capability additions, incremental updates, and provide other benefits such as allowing for tuning models to improve processing in targeted audio conditions or allowing for multiple tools or options to choose from to accomplish a given task. Each plugin has a specific type, which defines the task that it is capable of performing (see Plugin Types below). They consist of two parts, the plugin proper, which contains the recipe or algorithm information on how to perform the task, and one or more domains, which contain the data models used to run the algorithm and perform the function. Since plugins are generally machine learning-based the same algorithm may have multiple strengths based on the specific data used to train it (e.g. telephone audio, push-to-talk audio, high effort vocalization, conversational speech). This separation between the algorithm and the data model allows us to deliver new functionality that is based on training data independently of the algorithm by delivering a new domain. Classes and enrollments (described below) are associated with a domain.","title":"Anatomy of a Plugin"},{"location":"plugins.html#plugin-types","text":"Plugin function types refer to the core task the plugin performs and have standard abbreviations. Most plugins are designed to perform one specific function (for example, language identification, keyword spotting). We refer to plugins as working on audio segments, since OLIVE can process both audio files (on the file system) and audio buffers (sent as data through the API). For more information regarding each plugin type, but not a specific plugin or domain, including plugin type definitions, general output formats, and use cases, please click on the name of the plugin in the 'Function Type' column of the table below to visit that plugin type's information page:","title":"Plugin Types"},{"location":"plugins.html#table-of-plugin-function-types","text":"Function type Abbreviation Scoring type Classes Description Speech Activity Detection SAD Frame / Region speech Identifies speech regions in an audio segment Speaker Identification SID Global Enrolled speakers Identifies whether a single-talker audio segment contains a target speaker Speaker Detection SDD Region Enrolled speakers Detects a target speaker in audio with multiple talkers Speaker Diarization DIA Region Detect each unlabeled speaker Segments audio into clusters of unique speakers Language Identification LID Global Languages in training data and/or enrolled languages in some plug-ins Language and dialect detection Keyword Detection KWS Region Enrolled text keywords Language-specific approach to keyword detection using speech recognition and text Query by Example Keyword Spotting QBE Region Enrolled audio keywords Language independent approach to word spotting using one or more audio examples Gender Identification GID Global male, female Determines whether the audio segment was spoken by a male or female voice Topic Detection TPD Region Enrolled topics Detects topic regions in an audio segment Speech Enhancement ENH N/A N/A Reduces noise in an audio segment Voice Type Discrimination VTD Frame live-speech Detects presence of live-produced human speech, differentiating from silence, noise, speech coming from electronic device","title":"Table of Plugin Function Types"},{"location":"plugins.html#scoring-types","text":"Different function types score audio segments on different levels of granularity. Some plugin functionality differences are essentially differences is how an audio segment is treated -- as a single unit or potentially multiple units. For example, the main difference between speaker identification and speaker detection is how a segment is scored, in that speaker identification assumes that the audio segment sent to it for scoring is homogenous and comes from a single speaker, where speaker detection will instead allow for the possibility of the presence of multiple speakers in a given audio segment. There are three major scoring types: Frame - Assigns a score for each 10ms frame of the audio segment submitted for scoring. Region - Assigns and reports time boundaries defining region(s) within the audio segment, and for each region, an accompanying score for each detected class . Global - Assigns a single score for the entire audio segment for each of the plugin's classes . For more information on these scoring types, refer to the Plugin Traits page.","title":"Scoring Types"},{"location":"plugins.html#classes","text":"Certain plugin types have classes as an attribute. These can be common, cross-mission categories that are often pre-trained - like speech, languages, or dialects - or they can be ad-hoc mission-specific classes like speakers or topics. A plugin\u2019s classes may be completely fixed as in gender identification (male, female) or speech activity detection (speech) or an open set as in language identification (English, Spanish, Mandarin, etc.), topics, or speakers. Some plugins allow the user to add new classes or modify existing classes. Some class sets are inherently closed, like SAD and GID, where the plugin is complete and covers the world of possible classes. Others, like LID/SID/TPD plugin will probably never be complete in covering all classes and thus will always need to be able to treat a segment as though it may not be from among the classes the plugin recognizes (i.e. \u2018out of set\u2019).","title":"Classes"},{"location":"plugins.html#enrollments","text":"Enrollments are a sub-set of classes that the user can create and/or modify. Both creation of a class and modification of an existing class are class modification requests, where the first class modification request for a given class also has the effect of creating the new class if it does not yet exist. Enrollments may be generated by end users with examples from their own data and can be learned from a single or small number of examples (SID, QBE) to a relatively large number of examples (LID, TID). Speakers are typically enrollments, as are query-based keywords and topics. Languages can also be enrolled and augmented with certain plugins. Since enrollments are dynamic, they may be incrementally updated \u201con the fly\u201d with new examples. For integration details regarding enrollments, refer to the Enrollments section of the API Integration page. To determine if a plugin supports enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page from the Specific Plugins list below.","title":"Enrollments"},{"location":"plugins.html#online-updates","text":"Considerable improvements to system accuracy and calibration can be found by updating a plugin post-deployment to better align with the conditions observed in recent history. Several plugins are able to perform unsupervised updates to certain submodules of the plugin. The updates do not require labels or human input and are based on automatically collected information during normal system use. In most cases, a system update must be invoked by the user via the API, and an option to determine if an update is ready to be applied is also provided in the API. For integration details regarding the update functionality, refer to the Update section of the API Integration page. To check if a plugin supports online updates, refer to its detailed information page from the Specific Plugins list below.","title":"Online Updates"},{"location":"plugins.html#adaptation","text":"Similarly to online updates, it can be possible to achieve even larger boosts in performance by updating a plugin by exposing it to the mission's audio conditions, or similarly representative audio conditions. Adaptation, however, requires human input, and in some cases, data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate.","title":"Adaptation"},{"location":"plugins.html#naming-conventions","text":"","title":"Naming Conventions"},{"location":"plugins.html#plugin-names","text":"Each plugin proper is given a three part name in the following form: function-attribute-version: function - three-letter abbreviation from column two of the Plugin Types Table above attribute - string that identifies the key attribute of the plugin, generally the algorithm name version - tracks the iteration of the plug-in in development in the form of v<digit> For example, sid-embed-v2 is a speaker identification plugin using speaker embed dings algorithm and is the second release or update of the approach. An additonal alphanumeric character may be appended to the version number if a plugin is re-released with bug fixes, but the performance is expected to be the same. For example, sad-dnn-v6a is a modified version of sad-dnn-v6, but the changes were meant to address errors or shortcomings in the plugin, not to change the algorithm or data used.","title":"Plugin Names"},{"location":"plugins.html#domain-names","text":"Domain names typically have two or three parts: condition-version or language-condition-version for plugins that have language-dependent domain components. Keyword spotting domains also contain the language for which the domain was trained. language - the language for which the domain was trained if language-dependent, or a representation of the set of languages contained within a LID plugin's domain condition - the specific audio environment for which the domain was trained, or \u201cmulti\u201d if the domain was developed to be condition independent version - tracks the iteration of the plug-in in development in the form of v<digit>","title":"Domain Names"},{"location":"plugins.html#specific-plugins","text":"For additional information about specific plugins, their options, implementation details and other, please refer to the specific plugin pages, accessible from each of the individual Plugin Task pages.","title":"Specific Plugins"},{"location":"raven.html","text":"Raven Web Graphical User Interface Raven Web GUI Overview The Raven Web GUI is a User Interface designed to primarily facilitate \"batch runs\" consisting of a large number of files through \"workflows\" consisting of multiple technologies. The Raven GUI can be used to run either local audio or serverside audio through one of several preconfigured HLT workflows. Below is a basic diagram of the Raven Web GUI with all the major panels highlighted: Panels: Media Menu Media File List Workflow List Submit Button Using the Raven Web GUI In order to submit audio to a workflow on the WebGUI you must: Select either \"Local Media\" or \"Remote Media\" from the Media Menu a. For \"Local Media\", upload all files you want to run via drag and drop or by clicking the Media File List to open a file browser b. For \"Remote Media\", navigate the files available on the server side and select all files you would like to submit Select a workflow to run from the \"Workflow List\" Click the submit button in the bottom right Below are images that can be used as reference for each step. 1. Select Local/Remote Media 2. Load Media 2a. Load Local Media 2b. Load Remote Media 3. Select Workflow 4. Submit Analyzing Results After running a set of data through a workflow you'll be directed to the results page which displays the HLT results for all plugins within the workflow that was run for each file that was submitted. The results page has a menu bar with several different ways to organize the files and scores on the page.","title":"OLIVE Raven Batch Web GUI"},{"location":"raven.html#raven-web-graphical-user-interface","text":"","title":"Raven Web Graphical User Interface"},{"location":"raven.html#raven-web-gui-overview","text":"The Raven Web GUI is a User Interface designed to primarily facilitate \"batch runs\" consisting of a large number of files through \"workflows\" consisting of multiple technologies. The Raven GUI can be used to run either local audio or serverside audio through one of several preconfigured HLT workflows. Below is a basic diagram of the Raven Web GUI with all the major panels highlighted: Panels: Media Menu Media File List Workflow List Submit Button","title":"Raven Web GUI Overview"},{"location":"raven.html#using-the-raven-web-gui","text":"In order to submit audio to a workflow on the WebGUI you must: Select either \"Local Media\" or \"Remote Media\" from the Media Menu a. For \"Local Media\", upload all files you want to run via drag and drop or by clicking the Media File List to open a file browser b. For \"Remote Media\", navigate the files available on the server side and select all files you would like to submit Select a workflow to run from the \"Workflow List\" Click the submit button in the bottom right Below are images that can be used as reference for each step. 1. Select Local/Remote Media 2. Load Media 2a. Load Local Media 2b. Load Remote Media 3. Select Workflow 4. Submit","title":"Using the Raven Web GUI"},{"location":"raven.html#analyzing-results","text":"After running a set of data through a workflow you'll be directed to the results page which displays the HLT results for all plugins within the workflow that was run for each file that was submitted. The results page has a menu bar with several different ways to organize the files and scores on the page.","title":"Analyzing Results"},{"location":"redaction.html","text":"Speaker Redaction Task This document covers how to use the features added to the OLIVE Nightingale GUI to support the task of \"Speaker Redaction\". This allows for the assisted labeling and discovery of all sections of speech within an audio recording that belong to a single speaker, whose audio is wished to be removed from the audio in order to protect his or her identity. Currently, the designated audio is replaced with a tone, and all content there is destroyed. In the future, different approaches may be available that disguise the identity of the voice while leaving the content of the speech audible. Speaker Redaction Task Overview Generally, the process of stepping through the Redaction task follows these summarized steps: Load the audio to be processed Find and select an example of the speaker whose voice is to be redacted Submit this audio to the system, so it can find and suggest additional regions where this speaker occurs Add additional selections from these candidates to the labeled regions Optionally iterate through steps 3 and 4 until all audio from the speaker to be redacted is appropriately labeled Submit this audio to the redaction system to replace the labeled regions with a tone Review the resulting audio to make sure all of the speaker's audio was removed or disguised Add regions and resubmit to redaction system if necessary This process is covered in more detail below. Speaker Redaction Walkthrough Getting Started To get started with this task, first open the OLIVE Nightingale GUI and load an audio file in by dragging and dropping a file into one of the GUI waveform slots, or by pressing ctrl+o or cmd+o while the GUI has focus to enter an 'Open File' dialog. Once the audio is loaded, select the \u20181. Set up tiers\u2019 button in the \u2018Redaction Task\u2019 panel. This will create two new tiers below the audio file: Suggested This tier is where the results from the speaker search algorithm will be displayed, providing suggested regions that the system believes belong to the speaker whose speech has been provided in the BLEEP regions of the tier below. BLEEP or keep This tier is where you provide 'seed' regions, as BLEEP labels to inform the speaker-search system what the speaker to redact sounds like. You can also use 'keep' labels to inform the system of regions that do not contain the speaker you're attempting to redact, so that those regions won't be suggested again in the future. All regions labeled as BLEEP in this tier are what will be removed by the redaction plugin during the final step of this process. All keep regions will be left alone, as will regions that have no labels. Only BLEEP regions will be affected. This initial setup step will also enable the \u20182. Find speaker\u2019 and \u20183. Create redacted audio\u2019 buttons on the \u2018Redaction task\u2019 panel. An in-app help pop-up with reminder instructions can be accessed by clicking the '2. Find speaker' button when no BLEEP regions have been added. Finding/Labeling Audio to Redact Start by selecting a portion of audio that contains only the speaker you wish to redact by clicking and dragging in the waveform portion of the audio after locating an appropriate speech segment. It is possible to perform this step with as little as 3-5 seconds of audio from the appropriate speaker, but providing more labeled audio at the start will make the system's suggestions for additional regions more accurate. Add this selection as a region to be 'bleeped' by clicking the '+' icon directly below your selection in the \" BLEEP or keep \" tier, and selecting BLEEP . Repeat this until you have selected at least 3-5 seconds of the speaker you wish to redact (more is better). Once you've selected some audio to get started, tell the system to find more speech from this speaker by clicking '2. Find speaker' in the 'Redaction Task' GUI panel. In the \u2018Suggested\u2019 tier you will see \u2018processing\u2019 followed by yellow boxes where the speaker of interest is likely to be speaking, this can be useful in finding additional areas with the speaker. Use the \u2018Suggested\u2019 tier to help review the rest of the file and add all other regions that you want to redact as BLEEP regions. You can perform the 'Find Speaker' step as many times as is desired to continue refining the provided suggestions. It will always use each BLEEP region together as a representation of the speaker you're attempting to find. Note that if the system suggests regions of speech that do not belong to the speaker to be redacted, you can stop future suggestions of this region by selecting the suggestion and labeling it as keep . The keep regions are not essential for this task, and can be used or not depending on user preference. Note that if you make a mistake and mark a region you wish to redact as keep , or a region you wish to be left alone as BLEEP , you can remove this selection by hovering over or clicking the respective label, and selecting the 'x' that appears in the top left corner. You will then be free to re-add this label as a different type, or adjust the boundaries and then do so. Redacting Selected Audio Once you have found and labeled all regions containing speech belonging to this speaker, you can proceed on to the final step in the process, of actually redacting the selected audio. This is done by selecting the '3. Create redacted audio' button from the 'Redactino Task' GUI panel. Once this is done, the GUI will prompt you with a file-save dialog to select where to save out the new audio file. Finally, a new slot is added below the BLEEP or keep tier that will display the redacted audio once it is created, and allow review of the redacted audio. Select which channel will be played back using the speaker icons to the left of each waveform slot, and review the final redacted audio. If you find while reviewing this audio that any audio from the speaker was missed and is still present in the final file, it is still possible to step through each part of this process again as necessary. A new BLEEP region (or regions) can be added at this point to catch the missed speech, and the audio can be resubmitted through '3. Create redacted audio'. At this point, the user can choose to save a new file, or overwrite the original redacted file. Cautions and Limitations Note that there is currently no mechanism for saving progress or state of the Redaction Task or BLEEP or keep tier. The final created audio file is saved in the location provided by the user, but it is not currently possible to save the GUI state while in the middle of labeling BLEEP regions and resume or complete the redaction task at a later time.","title":"Speaker Redaction"},{"location":"redaction.html#speaker-redaction-task","text":"This document covers how to use the features added to the OLIVE Nightingale GUI to support the task of \"Speaker Redaction\". This allows for the assisted labeling and discovery of all sections of speech within an audio recording that belong to a single speaker, whose audio is wished to be removed from the audio in order to protect his or her identity. Currently, the designated audio is replaced with a tone, and all content there is destroyed. In the future, different approaches may be available that disguise the identity of the voice while leaving the content of the speech audible.","title":"Speaker Redaction Task"},{"location":"redaction.html#speaker-redaction-task-overview","text":"Generally, the process of stepping through the Redaction task follows these summarized steps: Load the audio to be processed Find and select an example of the speaker whose voice is to be redacted Submit this audio to the system, so it can find and suggest additional regions where this speaker occurs Add additional selections from these candidates to the labeled regions Optionally iterate through steps 3 and 4 until all audio from the speaker to be redacted is appropriately labeled Submit this audio to the redaction system to replace the labeled regions with a tone Review the resulting audio to make sure all of the speaker's audio was removed or disguised Add regions and resubmit to redaction system if necessary This process is covered in more detail below.","title":"Speaker Redaction Task Overview"},{"location":"redaction.html#speaker-redaction-walkthrough","text":"","title":"Speaker Redaction Walkthrough"},{"location":"redaction.html#getting-started","text":"To get started with this task, first open the OLIVE Nightingale GUI and load an audio file in by dragging and dropping a file into one of the GUI waveform slots, or by pressing ctrl+o or cmd+o while the GUI has focus to enter an 'Open File' dialog. Once the audio is loaded, select the \u20181. Set up tiers\u2019 button in the \u2018Redaction Task\u2019 panel. This will create two new tiers below the audio file: Suggested This tier is where the results from the speaker search algorithm will be displayed, providing suggested regions that the system believes belong to the speaker whose speech has been provided in the BLEEP regions of the tier below. BLEEP or keep This tier is where you provide 'seed' regions, as BLEEP labels to inform the speaker-search system what the speaker to redact sounds like. You can also use 'keep' labels to inform the system of regions that do not contain the speaker you're attempting to redact, so that those regions won't be suggested again in the future. All regions labeled as BLEEP in this tier are what will be removed by the redaction plugin during the final step of this process. All keep regions will be left alone, as will regions that have no labels. Only BLEEP regions will be affected. This initial setup step will also enable the \u20182. Find speaker\u2019 and \u20183. Create redacted audio\u2019 buttons on the \u2018Redaction task\u2019 panel. An in-app help pop-up with reminder instructions can be accessed by clicking the '2. Find speaker' button when no BLEEP regions have been added.","title":"Getting Started"},{"location":"redaction.html#findinglabeling-audio-to-redact","text":"Start by selecting a portion of audio that contains only the speaker you wish to redact by clicking and dragging in the waveform portion of the audio after locating an appropriate speech segment. It is possible to perform this step with as little as 3-5 seconds of audio from the appropriate speaker, but providing more labeled audio at the start will make the system's suggestions for additional regions more accurate. Add this selection as a region to be 'bleeped' by clicking the '+' icon directly below your selection in the \" BLEEP or keep \" tier, and selecting BLEEP . Repeat this until you have selected at least 3-5 seconds of the speaker you wish to redact (more is better). Once you've selected some audio to get started, tell the system to find more speech from this speaker by clicking '2. Find speaker' in the 'Redaction Task' GUI panel. In the \u2018Suggested\u2019 tier you will see \u2018processing\u2019 followed by yellow boxes where the speaker of interest is likely to be speaking, this can be useful in finding additional areas with the speaker. Use the \u2018Suggested\u2019 tier to help review the rest of the file and add all other regions that you want to redact as BLEEP regions. You can perform the 'Find Speaker' step as many times as is desired to continue refining the provided suggestions. It will always use each BLEEP region together as a representation of the speaker you're attempting to find. Note that if the system suggests regions of speech that do not belong to the speaker to be redacted, you can stop future suggestions of this region by selecting the suggestion and labeling it as keep . The keep regions are not essential for this task, and can be used or not depending on user preference. Note that if you make a mistake and mark a region you wish to redact as keep , or a region you wish to be left alone as BLEEP , you can remove this selection by hovering over or clicking the respective label, and selecting the 'x' that appears in the top left corner. You will then be free to re-add this label as a different type, or adjust the boundaries and then do so.","title":"Finding/Labeling Audio to Redact"},{"location":"redaction.html#redacting-selected-audio","text":"Once you have found and labeled all regions containing speech belonging to this speaker, you can proceed on to the final step in the process, of actually redacting the selected audio. This is done by selecting the '3. Create redacted audio' button from the 'Redactino Task' GUI panel. Once this is done, the GUI will prompt you with a file-save dialog to select where to save out the new audio file. Finally, a new slot is added below the BLEEP or keep tier that will display the redacted audio once it is created, and allow review of the redacted audio. Select which channel will be played back using the speaker icons to the left of each waveform slot, and review the final redacted audio. If you find while reviewing this audio that any audio from the speaker was missed and is still present in the final file, it is still possible to step through each part of this process again as necessary. A new BLEEP region (or regions) can be added at this point to catch the missed speech, and the audio can be resubmitted through '3. Create redacted audio'. At this point, the user can choose to save a new file, or overwrite the original redacted file.","title":"Redacting Selected Audio"},{"location":"redaction.html#cautions-and-limitations","text":"Note that there is currently no mechanism for saving progress or state of the Redaction Task or BLEEP or keep tier. The final created audio file is saved in the location provided by the user, but it is not currently possible to save the GUI state while in the middle of labeling BLEEP regions and resume or complete the redaction task at a later time.","title":"Cautions and Limitations"},{"location":"releasePlugins.html","text":"OLIVE 5.1.0 Release Plugins The following plugins have been tested and certified for compatibility and release with the OLIVE 5.1.0 software package. ASDEC Specific Plugins Acoustic Event Detection (AED) aed-enrollable-v1.0.1 Waveform Alignment (ALN) aln-waveformAlignment-v1.0.0 Environmental Analysis (ENV) env-indoorOutdoor-v1.0.0 env-multiClass-v2.0.0 (Reverberation Detection) env-powerSupplyHum-v1.0.0 env-speakerCount-v1.0.0 Signal Energy Detection (SED) sed-rmsEnergy-v1.0.0 Voice Characterization (VOI) voi-speakingStyle-v1.0.0 voi-vocalEffort-v1.0.1 Speech Speech Activity Detection (SAD) sad-dnn-v7.0.1 Voice Type Discrimination (VTD) vtd-dnn-v7.0.1 Speaker Speaker Identification (SID) sid-dplda-v2.0.1 sid-embed-v6.0.1 Speaker Detection (SDD) sdd-sbcEmbed-v2.0.1 Speaker Diarization (DIA) dia-hybrid-v2.0.1 Speaker Highlighting (SHL) shl-sbcEmbed-v1.0.1 Language Language Identification (LID) lid-embedplda-v2.0.1 Language Detection (LDD) ldd-sbcEmbed-v1.0.1 Keyword Query By Example (QBE) qbe-tdnn-v5.0.0 Transcription Automatic Speech Recognition (ASR) asr-dynapy-v2.0.2 Translation Text Machine Translation (TMT) tmt-statistical-v1.0.1 Topic Topic Detection (TPD) tpd-dynapy-v3.0.0 tpd-embed-v3.0.0 For further information on each individual plugin and their capabilities, please refer to the pages linked above. For more information about how to integrate with OLIVE and get started using these plugins, refer to the appropriate documentation linked from the landing page .","title":"Release Plugins"},{"location":"releasePlugins.html#olive-510-release-plugins","text":"The following plugins have been tested and certified for compatibility and release with the OLIVE 5.1.0 software package.","title":"OLIVE 5.1.0 Release Plugins"},{"location":"releasePlugins.html#asdec-specific-plugins","text":"Acoustic Event Detection (AED) aed-enrollable-v1.0.1 Waveform Alignment (ALN) aln-waveformAlignment-v1.0.0 Environmental Analysis (ENV) env-indoorOutdoor-v1.0.0 env-multiClass-v2.0.0 (Reverberation Detection) env-powerSupplyHum-v1.0.0 env-speakerCount-v1.0.0 Signal Energy Detection (SED) sed-rmsEnergy-v1.0.0 Voice Characterization (VOI) voi-speakingStyle-v1.0.0 voi-vocalEffort-v1.0.1","title":"ASDEC Specific Plugins"},{"location":"releasePlugins.html#speech","text":"Speech Activity Detection (SAD) sad-dnn-v7.0.1 Voice Type Discrimination (VTD) vtd-dnn-v7.0.1","title":"Speech"},{"location":"releasePlugins.html#speaker","text":"Speaker Identification (SID) sid-dplda-v2.0.1 sid-embed-v6.0.1 Speaker Detection (SDD) sdd-sbcEmbed-v2.0.1 Speaker Diarization (DIA) dia-hybrid-v2.0.1 Speaker Highlighting (SHL) shl-sbcEmbed-v1.0.1","title":"Speaker"},{"location":"releasePlugins.html#language","text":"Language Identification (LID) lid-embedplda-v2.0.1 Language Detection (LDD) ldd-sbcEmbed-v1.0.1","title":"Language"},{"location":"releasePlugins.html#keyword","text":"Query By Example (QBE) qbe-tdnn-v5.0.0","title":"Keyword"},{"location":"releasePlugins.html#transcription","text":"Automatic Speech Recognition (ASR) asr-dynapy-v2.0.2","title":"Transcription"},{"location":"releasePlugins.html#translation","text":"Text Machine Translation (TMT) tmt-statistical-v1.0.1","title":"Translation"},{"location":"releasePlugins.html#topic","text":"Topic Detection (TPD) tpd-dynapy-v3.0.0 tpd-embed-v3.0.0 For further information on each individual plugin and their capabilities, please refer to the pages linked above. For more information about how to integrate with OLIVE and get started using these plugins, refer to the appropriate documentation linked from the landing page .","title":"Topic"},{"location":"server.html","text":"OLIVE Server Overview In the OLIVE architecture, if the individual plugins are the muscles of the system, the Server serves as the brains. It provides coordination and tasking, and is responsible for properly receiving and interpreting messages from client applications, kicking off the appropriate plugin jobs that these messages may request, as well as routing the proper response or results from these jobs back to the requesting client. The OLIVE Enterprise API is client/server based. Therefore, you must run the OLIVE server and manage its lifecycle as part of your integration effort. The server is included as part of the primary system installation. The OLIVE server communicates with clients over two ports, 5588 and 5589 by default (configurable), using ZeroMQ and Google Protocol Buffers. It relies on other components to perform its duties; namely an assembly of third party and other libraries that are delivered with OLIVE as the OLIVE Runtime , two SRI-built libraries; Idento and dnn, that empower the final puzzle piece, the plugins themselves, to complete their assigned tasks. OLIVE is usually delivered with a simple startup script for ease of use and to facilitate a rapid deployment, so the actual details of the relationship between these pieces does not need to be considered too closely if sticking, but the next section outlines how to properly establish the environmental setup that the Server needs to properly function, and how to manually start and configure the server. Installation, Environment Setup and Startup This section outlines the steps needed to prepare the OLIVE environment for proper operation, describes the contents of the OLIVE runtime library, and covers starting the actual server. The final section of this page also outlines some common pitfalls encountered with running the OLIVE server, and offers troubleshooting advice. Installation If you haven't already installed the OLIVE software package, please jump over and refer to our OLIVE Installation Guide for information about the layout of the software and a quick-start setup guide before continuing on. Environment Variables As discussed in the Installation Guide , we recommend using the provided olive_env.sh scripts to do the heavy lifting and set up the OLIVE environment, but it may be helpful to know what some of the important environment variables being set are referring to. Below is a selection of some of the major variables, with a brief description of their function. OLIVE The OLIVE environment variable points to the actuall OLIVE installation location - If the software is installed as shown in the install page , this is typically something like $HOME/olive5.0.0/. From this, the OLIVE software is able to properly add the relevant bin/ and lib directories to the $PATH and $LD_LIBRARY_PATH environment variables, respectively, and find/set other important variables. OLIVE_RUNTIME OLIVE_RUNTIME points to the location of the OLIVE Runtime library outlined below. It's very important that this is set, as OLIVE will not function properly without access to the libraries and dependencies in the runtime. In the two examples in the install page , the OLIVE_RUNTIME variable would be set to: $HOME/olive5.0.0/runtime-5.0.0-centos-7.3.1611-x86_64/ OLIVE_APP_DATA This variable is very important if running the OLIVE Server, but not necessary when using the OLIVE command line tools. OLIVE_APP_DATA tells the Server where to find the OLIVE plugins, and also determines where it will store things like class enrollments (i.e. speakers for SID plugins, keyword examples for QBE plugins), as well as server logs and other files that must be written out by the system. OLIVE_APP_DATA must contain a directory called plugins that contains valid OLIVE plugins. In the two examples in the install page , the OLIVE_APP_DATA variable would be set to: $HOME/oliveAppData/ Starting the Server Once the OLIVE environment has been properly established, launching the server is as simple as entering this into the appropriate terminal: oliveserver Additional runtime/configuration options are available for advanced operation, and their details can be found in the oliveserver usage statement: usage : oliveserver [- h ] [-- version ] [-- verbose ] [-- interface INTERFACE ] [-- port REQUEST_PORT ] [-- workers WORKERS ] [-- work_dir WORK_DIR ] [-- enroll_dir ENROLL_DIR ] [-- debug ] [-- quiet ] [-- timeout TIMEOUT ] [-- options OPTIONS_PATH ] optional arguments : - h , -- help show this help message and exit -- version show program 's version number and exit --verbose increase server logging output --interface INTERFACE server binds to this address; default * (all) --port REQUEST_PORT ther first of three sequentially numbered ports used by the server. The request port is the first port in this range, followd by the status port, and then an interal port only used by the server; default value is 5588. --workers WORKERS, -j WORKERS specify number of parallel WORKERS to run; default is the number of local processors --work_dir WORK_DIR path to work dir to create. default value of environment variable $OLIVE_APP_DATA --enroll_dir ENROLL_DIR path for storage of enrollment data. default value of environment variable $OLIVE_APP_DATA --debug debug mode prevents deletion of logs and intermediate files on success --quiet, -q suppress sending log information to the console --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have more or more configuration sections . Currently only the server 'server' section is supported An example for setting up the OLIVE runtime environment using the first directory setup shown on the install page , and launching the OLIVE server: $ cd $ HOME / olive5 . 0.0 / runtime - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ cd $ HOME / olive5 . 0.0 / olive - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / $ oliveserver -- port 6678 -- work_dir / home / sysadmin / OLIVE In this example, the ports that the server will listen on and broadcast its status heartbeat messages on have been update, in addition to setting a custom WORK directory. For more information on the available server options, please refer to the OLIVE Server Options section below. Note that upon launching the server, if the operation was successful, the user will be greeted with a display of the plugins and domains that the server has access to, as well as a \u201cServer ready\u201d message. If a list of plugins does not appear, but the \u201cServer ready\u201d message does, the OLIVE_APP_DATA environment variable should be checked to ensure that it contains a valid plugins directory. An example output of a successful server startup: [ oliveuser@localhost olive5.0.0 ] $ oliveserver TASK PLUGIN DOMAINS ------ ------------ --------------------------------- LID lid - embed - v2 [ 'multi-v1' ] SID sid - embed - v2 [ 'multi-v1' ] SAD sad - dnn - v4a [ 'digPtt-v1', 'ptt-v1', 'tel-v1' ] --------- Server ready Fri Jan 11 12:43:26 2019 --------- The olive_env.sh file is included with OLIVE deliveries and does the bulk of the environment setup required for running the server. Important variables that are handled with this script and that are required for running the OLIVE Server include: OLIVE_RUNTIME Must be set to the path of the OLIVE runtime directory. Allows OLIVE to find the libraries it requires. Ex: /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/ OLIVE Contains the path to the OLIVE installation directory. Allows OLIVE to find the binaries and utilities it relies on to function. Ex: /home/user1/olive5.0.0-installation/olive-5.0.0-centos-7.3.1611-x86_64/ or /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/olive-5.0.0-centos-7.3.1611-x86_64/ OLIVE_APP_DATA This is the path used by the server to store class enrollments and log files. This is also where the plugins that the server will have access to must be stored, in a directory simply named plugins. If OLIVE_APP_DATA is set to a directory that does not contain a plugins folder, the server will launch with no plugins. Ex: /home/user1/oliveAppData/ Where the contents of oliveAppData might be: plugins/ sad-dnn-v4a sid-embed-v2 lid-embed-v2 server/ (generated by the server) enrollments/ processing/ If your server is protected by a firewall you must ensure that any ports you attempt to utilize for the server are able to send and receive network traffic. The path you provide to --work_dir will ideally point to a large and performant hard drive. The location will house the enrollments, log files and temporary workspaces utilized by the server. Server Options interface Use this option to specify a single interface for the server to use for communication, otherwise the ports used by the server (5588, 5589, 5590 by default ) are available to all interfaces on the host. For example, --interface 192.168.10.20 would restrict the server to using only that interface, and only available to clients on that interface port Use this option to change the set of ports used by the server for external and internal communication. For example --port 6000 instructs the server to use ports 6000 (requests), 6001 (heartbeats), 60002 (internal communication) workers Use this option to specify the number of current jobs the server can process, this value should be set between 1 and the number of processors on the host work_dir Use this option to specify an alternate location of the directory where the server stores log files, enrollments, and temporary data. Setting this does NOT change the location of the plugins, which should still be located at $OLIVE_APP_DATA/plugins enroll_dir Use this option to specify an alternate location of the enrollments directory, when specified the work directory and plugins location is not changed. debug Use this option to preserver all logs files and provide extra verbose Use this option with the --debug option to produce to maximize debug output quite Use this option to start the server without non-error output messages timeout Use this option adjust the timeout (in seconds) used for jobs, unless instructed by OLIVE technical support this value should not specified options Used to pass special options to the server, this option should only be used with guidance from OLIVE technical support Remote Access The OLIVE server uses three sequential ports for communication; by default these are ports are 5588, 5589, and 5590. An alternate set of ports can be specified using the --port argument, which instructs the server to use 3 ports starting at the given value, so if --port 6000 is specified then ports 6000 (request port), 6001 (status port), and 6002 (internal port) are used. Clients send and receive responses on the request port (5588 by default), while the server publishes health and status information on the status port (5589 by default). The internal IPC port (5590 by default) is only used for internal server communication. You can restrict these ports to a specific interface by using the --interface option, so that the server is only available on that network (by default the server should be available on all interfaces, which may not be desirable on a multi-homed server). For example, specifying an interface of --interface 192.168.10.99 would restrict the server to communication only on that port. You could also use the interface option to restrict the server to local processing, not networked, by setting interface as: --interface 127.0.0.1 (on some OSs you may be able to specify --interface localhost instead of the loopback address, 127.0.0.1). If using the localhost interface, the server is not accessible to remote clients. Important Information Audio Formats The OLIVE server can handle a wide range of input audio file formats, but does have some limitations. For more information on the details of these limitations, please refer to the OLIVE Audio Formats information page. Common Pitfalls One common mistake that's encountered is when informing the OLIVE Server where to find plugins. There is a directory within the OLIVE distribution, olive/plugins that is often mistaken for the directory where OLIVE plugins should oro do reside. This directory has a different purpose, however, and should not be used as such. Guidelines are provided above for general recommendations for where to place your OLIVE plugins. Runtime Description The OLIVE Runtime is a collection of third party server dependencies, which are typically Python packages and C libraries, that are distributed with OLIVE and necessary for proper/full functionality. A full list of the included packages/libraries included in the OLIVE 4.12.0 release can be found below, with links for more information, and license information. Name Version Description How Obtained License Link Python 3.7.8 (Anaconda 4.8.4 Distribution) Python distribution for scientific computing Open Source BSD-3 https://docs.anaconda.com/anaconda/eula Cython 0.29.21 python library for access to native C libraries Open Source Apache-2.0 https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 protobuf (python) 3.13.0 library to serialize data Open Source BSD https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 psutil 5.7.2 library to provide OS status Open Source BSD https://github.com/giampaolo/psutil numpy 1.19.1 python library of high level math functions Open Source BSD-3 https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 https://raw.githubusercontent.com/google/protobuf/master/LICENSE scipy 1.5.2 python library for scientific computing Open Source BSD/MIT https://github.com/scipy/scipy/blob/master/LICENSE.txt scikit-learn 0.23.2 python machine learning library Open Source BSD https://github.com/scikit-learn/scikit-learn/blob/master/COPYING numexpr 2.7.1 python mathematical library Open Source MIT https://github.com/pydata/numexpr h5py 2.10.10 python library for HDF5 binary data format Open Source PSF https://github.com/h5py/h5py ipython 7.17.0 interactive shell for python Open Source BSD-3 https://github.com/ipython/ipython lxml 4.5.1 python library for processing XML and HTML Open Source BSD https://github.com/lxml/lxml matplotlib 3.3.1 python plotting library Open Source PSF https://github.com/matplotlib/matplotlib/tree/master/LICENSE zeromq 4.3.2 messaging library Open Source LGPL http://zeromq.org/area:licensing pyzmq 19.0.1 messaging library Open Source BSD https://github.com/zeromq/pyzmq mpi4py 3.0.3 Message Passing Interface (MPI) Open Source BSD https://bitbucket.org/mpi4py/mpi4py/src/master/ nose 1.3.7 Testing Package Open Source LGPL https://nose.readthedocs.io/en/latest/ pytest 6.0.1 Testing Package Open Source MIT https://docs.pytest.org/ fastcluster 1.1.26 python library for hierarchical clustering Open Source BSD http://danifold.net av 8.0.2 Pythonic binding for the FFmpeg libraries Open Source BSD https://github.com/PyAV-Org/PyAV opencv 4.4.0 CPU-only OpenCV packages Open Source MIT https://github.com/skvark/opencv-python distro 1.5.0 OS platform information AP Open Source Apache v2.0 https://github.com/nir0s/distro ninja 1.10.0 Python build support Open Source BSD http://ninja-build.org/ pyyaml 5.3.1 Python YAML parser Open Source MIT https://github.com/yaml/pyyaml cffi 1.14.0 Foreign Function Interface for Python Open Source MIT http://cffi.readthedocs.org/ future 0.18.2 Python 2 to 3 compatible interface Open Source MIT https://python-future.org/ six 1.15.0 Python 2 and 3 compatibility library Open Source MIT https://github.com/benjaminp/six requests 2.24.0 HTTP library Open Source Apache 2.0 https://requests.readthedocs.io/ mkl-include 2020.2 Math Kernel Librar Open Source Intel Simplified Software License https://software.intel.com/en-us/oneapi/onemkl typing_extensions 3.7.4.2 Backported and Experimental Type Hints for Python Open Source Python Software Foundation License (PSF) https://github.com/python/typing/blob/master/typing_extensions/README.rst python-gnupg 0.4.6 cryptographic library Open Source GPL-3 https://github.com/isislovecruft/python-gnupg pysoundfile 0.10.2 audio library based on libsndfile, Open Source BSD https://github.com/bastibe/PySoundFile tabulate 0.8.7 python library for printing output Open Source MIT https://bitbucket.org/astanin/python-tabulate pytorch 1.6.0 open source machine learning library based on the Torch library Open Source BSD https://pytorch.org/ liblinear 1.8 mathematical library Open Source BSD https://www.csie.ntu.edu.tw/~cjlin/liblinear/ libogg 1.3.2 library for ogg audio Open Source BSD https://xiph.org/ogg/ libspeex 1.2 audio codec Open Source BSD https://www.speex.org/ libvorbis 1.3.5 vorbis audio codec Open Source BSD http://www.vorbis.com/ libflac 1.3.2 flac audio codec Open Source BSD https://xiph.org/flac/index.html libsndfile 1.0.28 library for audio files Open Source LGPL http://www.mega-nerd.com/libsndfile/ sox 14.3.2 audio processing utility Open Source LGPL http://sox.sourceforge.net/ dpwelib 5/16/13 audio processing library Open Source Public Domain http://www1.icsi.berkeley.edu/~dpwe/dpwelib.html get_f0_snd n/a audio library Open Source BSD https://github.com/MattShannon/HTS-demo_CMU-ARCTIC-SLT-STRAIGHT-AR-decision-tree/blob/master/License upfirdn 0.2.1 signal processing library Open Source BSD https://github.com/telegraphic/upfirdn gnupg 1.4.22 cryptographic library Open Source CC BY-SA 3.0 https://www.gnupg.org/ klepto 0.1.9 python caching library Open Source BSD http://www.cacr.caltech.edu/~mmckerns/klepto.htm facenet 1.0.1 Face recognition with Google\u2019s FaceNet deep neural network & TensorFlow Open Source MIT https://github.com/jonaphin/facenet libopenfst 1.6.2 library for processing finite-state transducers (FSTs) Open Source ASL http://www.openfst.org/twiki/bin/view/FST/WebHome libopenblas 0.2.19 mathematical library Open Source BSD https://raw.githubusercontent.com/xianyi/OpenBLAS/develop/LICENSE kaldi 5.1 speech processing library Open Source ASL 2.0 http://kaldi.sourceforge.net/ torchvggish 0.1 A torch-compatible port of VGGish Open Source Apache v2.0 https://github.com/harritaylor/torchvggish transformers 3.3.1 Natural Language Processing for PyTorch and TensorFlow Opne Source Apache https://github.com/huggingface/transformers","title":"Server Guide"},{"location":"server.html#olive-server","text":"","title":"OLIVE Server"},{"location":"server.html#overview","text":"In the OLIVE architecture, if the individual plugins are the muscles of the system, the Server serves as the brains. It provides coordination and tasking, and is responsible for properly receiving and interpreting messages from client applications, kicking off the appropriate plugin jobs that these messages may request, as well as routing the proper response or results from these jobs back to the requesting client. The OLIVE Enterprise API is client/server based. Therefore, you must run the OLIVE server and manage its lifecycle as part of your integration effort. The server is included as part of the primary system installation. The OLIVE server communicates with clients over two ports, 5588 and 5589 by default (configurable), using ZeroMQ and Google Protocol Buffers. It relies on other components to perform its duties; namely an assembly of third party and other libraries that are delivered with OLIVE as the OLIVE Runtime , two SRI-built libraries; Idento and dnn, that empower the final puzzle piece, the plugins themselves, to complete their assigned tasks. OLIVE is usually delivered with a simple startup script for ease of use and to facilitate a rapid deployment, so the actual details of the relationship between these pieces does not need to be considered too closely if sticking, but the next section outlines how to properly establish the environmental setup that the Server needs to properly function, and how to manually start and configure the server.","title":"Overview"},{"location":"server.html#installation-environment-setup-and-startup","text":"This section outlines the steps needed to prepare the OLIVE environment for proper operation, describes the contents of the OLIVE runtime library, and covers starting the actual server. The final section of this page also outlines some common pitfalls encountered with running the OLIVE server, and offers troubleshooting advice.","title":"Installation, Environment Setup and Startup"},{"location":"server.html#installation","text":"If you haven't already installed the OLIVE software package, please jump over and refer to our OLIVE Installation Guide for information about the layout of the software and a quick-start setup guide before continuing on.","title":"Installation"},{"location":"server.html#environment-variables","text":"As discussed in the Installation Guide , we recommend using the provided olive_env.sh scripts to do the heavy lifting and set up the OLIVE environment, but it may be helpful to know what some of the important environment variables being set are referring to. Below is a selection of some of the major variables, with a brief description of their function.","title":"Environment Variables"},{"location":"server.html#olive","text":"The OLIVE environment variable points to the actuall OLIVE installation location - If the software is installed as shown in the install page , this is typically something like $HOME/olive5.0.0/. From this, the OLIVE software is able to properly add the relevant bin/ and lib directories to the $PATH and $LD_LIBRARY_PATH environment variables, respectively, and find/set other important variables.","title":"OLIVE"},{"location":"server.html#olive_runtime","text":"OLIVE_RUNTIME points to the location of the OLIVE Runtime library outlined below. It's very important that this is set, as OLIVE will not function properly without access to the libraries and dependencies in the runtime. In the two examples in the install page , the OLIVE_RUNTIME variable would be set to: $HOME/olive5.0.0/runtime-5.0.0-centos-7.3.1611-x86_64/","title":"OLIVE_RUNTIME"},{"location":"server.html#olive_app_data","text":"This variable is very important if running the OLIVE Server, but not necessary when using the OLIVE command line tools. OLIVE_APP_DATA tells the Server where to find the OLIVE plugins, and also determines where it will store things like class enrollments (i.e. speakers for SID plugins, keyword examples for QBE plugins), as well as server logs and other files that must be written out by the system. OLIVE_APP_DATA must contain a directory called plugins that contains valid OLIVE plugins. In the two examples in the install page , the OLIVE_APP_DATA variable would be set to: $HOME/oliveAppData/","title":"OLIVE_APP_DATA"},{"location":"server.html#starting-the-server","text":"Once the OLIVE environment has been properly established, launching the server is as simple as entering this into the appropriate terminal: oliveserver Additional runtime/configuration options are available for advanced operation, and their details can be found in the oliveserver usage statement: usage : oliveserver [- h ] [-- version ] [-- verbose ] [-- interface INTERFACE ] [-- port REQUEST_PORT ] [-- workers WORKERS ] [-- work_dir WORK_DIR ] [-- enroll_dir ENROLL_DIR ] [-- debug ] [-- quiet ] [-- timeout TIMEOUT ] [-- options OPTIONS_PATH ] optional arguments : - h , -- help show this help message and exit -- version show program 's version number and exit --verbose increase server logging output --interface INTERFACE server binds to this address; default * (all) --port REQUEST_PORT ther first of three sequentially numbered ports used by the server. The request port is the first port in this range, followd by the status port, and then an interal port only used by the server; default value is 5588. --workers WORKERS, -j WORKERS specify number of parallel WORKERS to run; default is the number of local processors --work_dir WORK_DIR path to work dir to create. default value of environment variable $OLIVE_APP_DATA --enroll_dir ENROLL_DIR path for storage of enrollment data. default value of environment variable $OLIVE_APP_DATA --debug debug mode prevents deletion of logs and intermediate files on success --quiet, -q suppress sending log information to the console --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have more or more configuration sections . Currently only the server 'server' section is supported An example for setting up the OLIVE runtime environment using the first directory setup shown on the install page , and launching the OLIVE server: $ cd $ HOME / olive5 . 0.0 / runtime - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ cd $ HOME / olive5 . 0.0 / olive - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / $ oliveserver -- port 6678 -- work_dir / home / sysadmin / OLIVE In this example, the ports that the server will listen on and broadcast its status heartbeat messages on have been update, in addition to setting a custom WORK directory. For more information on the available server options, please refer to the OLIVE Server Options section below. Note that upon launching the server, if the operation was successful, the user will be greeted with a display of the plugins and domains that the server has access to, as well as a \u201cServer ready\u201d message. If a list of plugins does not appear, but the \u201cServer ready\u201d message does, the OLIVE_APP_DATA environment variable should be checked to ensure that it contains a valid plugins directory. An example output of a successful server startup: [ oliveuser@localhost olive5.0.0 ] $ oliveserver TASK PLUGIN DOMAINS ------ ------------ --------------------------------- LID lid - embed - v2 [ 'multi-v1' ] SID sid - embed - v2 [ 'multi-v1' ] SAD sad - dnn - v4a [ 'digPtt-v1', 'ptt-v1', 'tel-v1' ] --------- Server ready Fri Jan 11 12:43:26 2019 --------- The olive_env.sh file is included with OLIVE deliveries and does the bulk of the environment setup required for running the server. Important variables that are handled with this script and that are required for running the OLIVE Server include: OLIVE_RUNTIME Must be set to the path of the OLIVE runtime directory. Allows OLIVE to find the libraries it requires. Ex: /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/ OLIVE Contains the path to the OLIVE installation directory. Allows OLIVE to find the binaries and utilities it relies on to function. Ex: /home/user1/olive5.0.0-installation/olive-5.0.0-centos-7.3.1611-x86_64/ or /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/olive-5.0.0-centos-7.3.1611-x86_64/ OLIVE_APP_DATA This is the path used by the server to store class enrollments and log files. This is also where the plugins that the server will have access to must be stored, in a directory simply named plugins. If OLIVE_APP_DATA is set to a directory that does not contain a plugins folder, the server will launch with no plugins. Ex: /home/user1/oliveAppData/ Where the contents of oliveAppData might be: plugins/ sad-dnn-v4a sid-embed-v2 lid-embed-v2 server/ (generated by the server) enrollments/ processing/ If your server is protected by a firewall you must ensure that any ports you attempt to utilize for the server are able to send and receive network traffic. The path you provide to --work_dir will ideally point to a large and performant hard drive. The location will house the enrollments, log files and temporary workspaces utilized by the server.","title":"Starting the Server"},{"location":"server.html#server-options","text":"","title":"Server Options"},{"location":"server.html#interface","text":"Use this option to specify a single interface for the server to use for communication, otherwise the ports used by the server (5588, 5589, 5590 by default ) are available to all interfaces on the host. For example, --interface 192.168.10.20 would restrict the server to using only that interface, and only available to clients on that interface","title":"interface"},{"location":"server.html#port","text":"Use this option to change the set of ports used by the server for external and internal communication. For example --port 6000 instructs the server to use ports 6000 (requests), 6001 (heartbeats), 60002 (internal communication)","title":"port"},{"location":"server.html#workers","text":"Use this option to specify the number of current jobs the server can process, this value should be set between 1 and the number of processors on the host","title":"workers"},{"location":"server.html#work_dir","text":"Use this option to specify an alternate location of the directory where the server stores log files, enrollments, and temporary data. Setting this does NOT change the location of the plugins, which should still be located at $OLIVE_APP_DATA/plugins","title":"work_dir"},{"location":"server.html#enroll_dir","text":"Use this option to specify an alternate location of the enrollments directory, when specified the work directory and plugins location is not changed.","title":"enroll_dir"},{"location":"server.html#debug","text":"Use this option to preserver all logs files and provide extra","title":"debug"},{"location":"server.html#verbose","text":"Use this option with the --debug option to produce to maximize debug output","title":"verbose"},{"location":"server.html#quite","text":"Use this option to start the server without non-error output messages","title":"quite"},{"location":"server.html#timeout","text":"Use this option adjust the timeout (in seconds) used for jobs, unless instructed by OLIVE technical support this value should not specified","title":"timeout"},{"location":"server.html#options","text":"Used to pass special options to the server, this option should only be used with guidance from OLIVE technical support","title":"options"},{"location":"server.html#remote-access","text":"The OLIVE server uses three sequential ports for communication; by default these are ports are 5588, 5589, and 5590. An alternate set of ports can be specified using the --port argument, which instructs the server to use 3 ports starting at the given value, so if --port 6000 is specified then ports 6000 (request port), 6001 (status port), and 6002 (internal port) are used. Clients send and receive responses on the request port (5588 by default), while the server publishes health and status information on the status port (5589 by default). The internal IPC port (5590 by default) is only used for internal server communication. You can restrict these ports to a specific interface by using the --interface option, so that the server is only available on that network (by default the server should be available on all interfaces, which may not be desirable on a multi-homed server). For example, specifying an interface of --interface 192.168.10.99 would restrict the server to communication only on that port. You could also use the interface option to restrict the server to local processing, not networked, by setting interface as: --interface 127.0.0.1 (on some OSs you may be able to specify --interface localhost instead of the loopback address, 127.0.0.1). If using the localhost interface, the server is not accessible to remote clients.","title":"Remote Access"},{"location":"server.html#important-information","text":"","title":"Important Information"},{"location":"server.html#audio-formats","text":"The OLIVE server can handle a wide range of input audio file formats, but does have some limitations. For more information on the details of these limitations, please refer to the OLIVE Audio Formats information page.","title":"Audio Formats"},{"location":"server.html#common-pitfalls","text":"One common mistake that's encountered is when informing the OLIVE Server where to find plugins. There is a directory within the OLIVE distribution, olive/plugins that is often mistaken for the directory where OLIVE plugins should oro do reside. This directory has a different purpose, however, and should not be used as such. Guidelines are provided above for general recommendations for where to place your OLIVE plugins.","title":"Common Pitfalls"},{"location":"server.html#runtime-description","text":"The OLIVE Runtime is a collection of third party server dependencies, which are typically Python packages and C libraries, that are distributed with OLIVE and necessary for proper/full functionality. A full list of the included packages/libraries included in the OLIVE 4.12.0 release can be found below, with links for more information, and license information. Name Version Description How Obtained License Link Python 3.7.8 (Anaconda 4.8.4 Distribution) Python distribution for scientific computing Open Source BSD-3 https://docs.anaconda.com/anaconda/eula Cython 0.29.21 python library for access to native C libraries Open Source Apache-2.0 https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 protobuf (python) 3.13.0 library to serialize data Open Source BSD https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 psutil 5.7.2 library to provide OS status Open Source BSD https://github.com/giampaolo/psutil numpy 1.19.1 python library of high level math functions Open Source BSD-3 https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 https://raw.githubusercontent.com/google/protobuf/master/LICENSE scipy 1.5.2 python library for scientific computing Open Source BSD/MIT https://github.com/scipy/scipy/blob/master/LICENSE.txt scikit-learn 0.23.2 python machine learning library Open Source BSD https://github.com/scikit-learn/scikit-learn/blob/master/COPYING numexpr 2.7.1 python mathematical library Open Source MIT https://github.com/pydata/numexpr h5py 2.10.10 python library for HDF5 binary data format Open Source PSF https://github.com/h5py/h5py ipython 7.17.0 interactive shell for python Open Source BSD-3 https://github.com/ipython/ipython lxml 4.5.1 python library for processing XML and HTML Open Source BSD https://github.com/lxml/lxml matplotlib 3.3.1 python plotting library Open Source PSF https://github.com/matplotlib/matplotlib/tree/master/LICENSE zeromq 4.3.2 messaging library Open Source LGPL http://zeromq.org/area:licensing pyzmq 19.0.1 messaging library Open Source BSD https://github.com/zeromq/pyzmq mpi4py 3.0.3 Message Passing Interface (MPI) Open Source BSD https://bitbucket.org/mpi4py/mpi4py/src/master/ nose 1.3.7 Testing Package Open Source LGPL https://nose.readthedocs.io/en/latest/ pytest 6.0.1 Testing Package Open Source MIT https://docs.pytest.org/ fastcluster 1.1.26 python library for hierarchical clustering Open Source BSD http://danifold.net av 8.0.2 Pythonic binding for the FFmpeg libraries Open Source BSD https://github.com/PyAV-Org/PyAV opencv 4.4.0 CPU-only OpenCV packages Open Source MIT https://github.com/skvark/opencv-python distro 1.5.0 OS platform information AP Open Source Apache v2.0 https://github.com/nir0s/distro ninja 1.10.0 Python build support Open Source BSD http://ninja-build.org/ pyyaml 5.3.1 Python YAML parser Open Source MIT https://github.com/yaml/pyyaml cffi 1.14.0 Foreign Function Interface for Python Open Source MIT http://cffi.readthedocs.org/ future 0.18.2 Python 2 to 3 compatible interface Open Source MIT https://python-future.org/ six 1.15.0 Python 2 and 3 compatibility library Open Source MIT https://github.com/benjaminp/six requests 2.24.0 HTTP library Open Source Apache 2.0 https://requests.readthedocs.io/ mkl-include 2020.2 Math Kernel Librar Open Source Intel Simplified Software License https://software.intel.com/en-us/oneapi/onemkl typing_extensions 3.7.4.2 Backported and Experimental Type Hints for Python Open Source Python Software Foundation License (PSF) https://github.com/python/typing/blob/master/typing_extensions/README.rst python-gnupg 0.4.6 cryptographic library Open Source GPL-3 https://github.com/isislovecruft/python-gnupg pysoundfile 0.10.2 audio library based on libsndfile, Open Source BSD https://github.com/bastibe/PySoundFile tabulate 0.8.7 python library for printing output Open Source MIT https://bitbucket.org/astanin/python-tabulate pytorch 1.6.0 open source machine learning library based on the Torch library Open Source BSD https://pytorch.org/ liblinear 1.8 mathematical library Open Source BSD https://www.csie.ntu.edu.tw/~cjlin/liblinear/ libogg 1.3.2 library for ogg audio Open Source BSD https://xiph.org/ogg/ libspeex 1.2 audio codec Open Source BSD https://www.speex.org/ libvorbis 1.3.5 vorbis audio codec Open Source BSD http://www.vorbis.com/ libflac 1.3.2 flac audio codec Open Source BSD https://xiph.org/flac/index.html libsndfile 1.0.28 library for audio files Open Source LGPL http://www.mega-nerd.com/libsndfile/ sox 14.3.2 audio processing utility Open Source LGPL http://sox.sourceforge.net/ dpwelib 5/16/13 audio processing library Open Source Public Domain http://www1.icsi.berkeley.edu/~dpwe/dpwelib.html get_f0_snd n/a audio library Open Source BSD https://github.com/MattShannon/HTS-demo_CMU-ARCTIC-SLT-STRAIGHT-AR-decision-tree/blob/master/License upfirdn 0.2.1 signal processing library Open Source BSD https://github.com/telegraphic/upfirdn gnupg 1.4.22 cryptographic library Open Source CC BY-SA 3.0 https://www.gnupg.org/ klepto 0.1.9 python caching library Open Source BSD http://www.cacr.caltech.edu/~mmckerns/klepto.htm facenet 1.0.1 Face recognition with Google\u2019s FaceNet deep neural network & TensorFlow Open Source MIT https://github.com/jonaphin/facenet libopenfst 1.6.2 library for processing finite-state transducers (FSTs) Open Source ASL http://www.openfst.org/twiki/bin/view/FST/WebHome libopenblas 0.2.19 mathematical library Open Source BSD https://raw.githubusercontent.com/xianyi/OpenBLAS/develop/LICENSE kaldi 5.1 speech processing library Open Source ASL 2.0 http://kaldi.sourceforge.net/ torchvggish 0.1 A torch-compatible port of VGGish Open Source Apache v2.0 https://github.com/harritaylor/torchvggish transformers 3.3.1 Natural Language Processing for PyTorch and TensorFlow Opne Source Apache https://github.com/huggingface/transformers","title":"Runtime Description"},{"location":"traits.html","text":"OLIVE Plugin Traits Traits Overview The functionality of each OLIVE plugin is defined by the API Traits that it implements. Each Trait defines a message or set of messages that the plugin must implement to perform an associated task. The available Traits are listed below, along with their associated implemented API messages, in the format: Trait Implemented Message 1 ... Implemented Message N Traits and their Corresponding Messages The Trait and Message List follows. Each Trait name links to the relevant section on this page . Each API message entry links to the relevant portion of the Protocol Buffers Message Definition Reference Page. GlobalScorer - ( Tutorial ) GlobalScorerRequest GlobalScorerStereoRequest RegionScorer - ( Tutorial ) RegionScorerRequest RegionScorerStereoRequest FrameScorer - ( Tutorial ) FrameScorerRequest FrameScorerStereoRequest ClassModifier - ( Tutorial ) ClassModificationRequest ClassRemovalRequest ClassExporter ClassExportRequest ClassImportRequest AudioConverter AudioModificationRequest AudioVectorizer PluginAudioVectorRequest LearningTrait (SupervisedAdapter, SupervisedTrainer, UnsupervisedAdapter) - ( Tutorial ) PreprocessAudioTrainRequest PreprocessAudioAdaptRequest SupervisedTrainingRequest UnsupervisedAdaptationRequest UpdateTrait GetUpdateStatusRequest ApplyUpdateRequest GlobalComparer GlobalComparerRequest TextTransformer TextTransformationRequest AudioAlignmentScorer AudioAlignmentScoreRequest In addition to the messages above, the following messages exist for interacting with the server itself and are independent of individual plugins or plugin Traits: GetStatusRequest PluginDirectoryRequest LoadPluginDomainRequest RemovePluginDomainRequest Full message definition details for all of the messages mentioned here Request/Result Message Pairs In general, each Request message is paired with a Result message, that defines the structure and contents of the server's reply to a given client Request message. The table below defines the pairs of the Result messages for the Request messages mentioned above; and the full message definition details can be found in the OLIVE API Message Protocol Documentation , which each entry below is linked to. Request Message Result (Response) Message GlobalScorerRequest GlobalScorerResult RegionScorerRequest RegionScorerResult FrameScorerRequest FrameScorerResult ClassModificationRequest ClassModificationResult ClassRemovalRequest ClassRemovalResult ClassExportRequest ClassExportResult ClassImportRequest ClassImportResult AudioModificationRequest AudioModificationResult PluginAudioVectorRequest PluginAudioVectorResult PreprocessAudioTrainRequest PreprocessAudioTrainResult PreprocessAudioAdaptRequest PreprocessAudioAdaptResult SupervisedTrainingRequest SupervisedTrainingResult UnsupervisedAdaptationRequest UnsupervisedAdaptationResult GetUpdateStatusRequest GetUpdateStatusResult ApplyUpdateRequest ApplyUpdateResult GlobalComparerRequest GlobalComparerResult TextTransformationRequest TextTransformationResult AudioAlignmentScoreRequest AudioAlignmentScoreResult or Task Request Message Result (Response) Message Global Score GlobalScorerRequest GlobalScorerResult Region Score RegionScorerRequest RegionScorerResult Frame Score FrameScorerRequest FrameScorerResult Create or modify a class enrollment ClassModificationRequest ClassModificationResult Remove an enrolled class ClassRemovalRequest ClassRemovalResult Export a class for later use or for model sharing ClassExportRequest ClassExportResult Import a previously exported class ClassImportRequest ClassImportResult Submit audio for modification (enhancement) AudioModificationRequest AudioModificationResult Request a vectorized representation of audio PluginAudioVectorRequest PluginAudioVectorResult Prepare audio for submission to training a new domain from scratch PreprocessAudioTrainRequest PreprocessAudioTrainResult Prepare audio for submission to adapt an existing domain PreprocessAudioAdaptRequest PreprocessAudioAdaptResult Query a plugin to see if it is ready to apply an \"Update\" GetUpdateStatusRequest GetUpdateStatusResult Instruct a plugin to perform an \"Update\" ApplyUpdateRequest ApplyUpdateResult Submit two audio files for forensic comparison GlobalComparerRequest GlobalComparerResult Submit text (string) for translation TextTransformationRequest TextTransformationResult Submit two or more audio files for alignment shift scores AudioAlignmentScoreRequest AudioAlignmentScoreResult Traits Deep Dive This section provides more details on what the \"real world\" usage and definition of each of the above Traits actually entails. If anything is unclear or could use more expansion, please contact us and we will provide updates and/or clarification. GlobalScorer A GlobalScorer plugin has the output of reporting a single score for each relevant plugin class representing the likelihood that the entire audio file or clip contains this class. For example, when audio is sent to a Speaker Identification (SID) plugin with the GlobalScorer Trait, if there are 3 enrolled speakers at that time, the plugin will return a single score for each enrolled speaker representing the likelihood that the audio is comprised of speech from each speaker, respectively. The output of such a request will contain information that looks something like this: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 Keep in mind that speakers and speaker identification are just examples in this case, and that this generalizes to any global scoring plugin and whatever classes it is meant to distinguish between. This same example could be represented more generically as: /data/sid/audio/file1.wav class1 -0.5348 /data/sid/audio/file1.wav class2 3.2122 /data/sid/audio/file1.wav class3 -5.5340 Due to the nature of global scoring plugins, they are best used when the attribute that's being estimated is known to be, or very likely to be static. For example, using global scoring SID on one side of a 4-wire telephone conversation, where the speaker is likely to remain the same throughout, or using global scoring LID on a TV broadcast that is expected to only contain a single language. These plugins can miss the nuances of or be confused by things like code-switching within recordings, or unexpected speaker changes. The benefits of global scoring plugins are that they are often very fast, since they treat the entirety of the submitted audio as a single unit, and do not worry about trying to subdivide or chunk it in any way. For a first-pass or quick triage approach, or when the data is known or suspected to be homogenous in the ways discussed above, these plugins are very effective. When finer grained results are necessary, though, a RegionScorer or FrameScorer may be more appropriate. RegionScorer For each audio file or recording submitted to a RegionScorer plugin, results are returned consisting of 0 or more regions with an associated score and plugin class. Typically, regions are returned in the case of a 'detection' of an instance of the respective class. A 'region' is a pair of timestamps, referring to the start time and end time of the detected class presence, and includes the name of the respective class, and an associated score. An example of this might be a keyword spotting plugin returning the keyword class name, as well as the location and likelihood of that keyword's presence. This might look something like: /data/test/testFile1.wav 0.630 1.170 Airplane 4.3725 /data/test/testFile1.wav 1.520 2.010 Watermelon -1.1978 Another example of output you may see from a region scoring plugin follows, showing what a region scoring speaker detection plugin might output. In this example, an enrolled speaker, speaker2 was detected in testFile1.wav from 0.630 s to 1.170 s with a likelihood score of 4.3725. Likewise for 1.520 s to 2.010 s in the same file for the enrolled speaker called speaker1 , this time with a likelihood score of -1.1978. /data/test/testFile1.wav 0.630 1.170 speaker2 4.3725 /data/test/testFile1.wav 1.520 2.010 speaker1 -1.1978 An even simpler output of this type may just label the regions within a file that the plugin determines contain speech. Again, these are just arbitrary examples using a specific plugin type to more easily describe the scoring type; a more generic output example could be: /data/test/testFile1.wav 0.630 1.170 class1 4.3725 /data/test/testFile1.wav 1.520 2.010 class2 -1.1978 RegionScorer plugins allow a finer resolution with respect to results granularity, and allow plugins to be more flexible and deal with transitions between languages or speakers or other classes of interest within a given audio file or recording. Sometimes this comes at a cost of increased processing complexity and/or slower runtime. FrameScorer A plugin with the FrameScorer Trait that is queried with a FrameScorerRequest will provide score results for each frame of whatever audio has been submitted. Unless otherwise noted, an audio frame is 10 milliseconds. The most common OLIVE plugin that has the FrameScorer Trait are speech activity detection (SAD) plugins, where the score for each frame represents the likelihood of speech being present in that 10 ms audio frame. The output in this case is simply a sequential list of numbers, corresponding to the output score for each frame, in order: 1.9047 1.8088 1.2382 -0.8862 -2.5509 In this example, these frames can then be processed to turn them into region scores, labeling the locations where speech has been detected as present within the file. Returning raw frame scores as a result allows more down-stream flexibility, allowing thresholds to be adjusted and regions re-labeled if desired, for example to allow for tuning for more difficult or unexpected audio conditions. ClassModifier Any plugin capable of adding or removing classes to or from its set of enrollments or target classes carries the ClassModifier Trait. This trait means the set of classes the plugin is interested in is mutable, and can be altered. Typically this is done by providing labeled data that belongs to the new class of interest to the server, which then enrolls a new model representing what it has learned about distinguishing this class from others. Existing class models can also be augmented by providing the system with additional data with this class label. In addition to adding new classes, and improving/augmenting existing ones, it is also possible to remove enrolled classes from domains of plugins carrying this trait, using the ClassRemovalRequest message. ClassExporter A plugin that implements the ClassExporter Trait is capable of exporting enrolled class models in a form that can be either imported back into the same system, as a way to save the model for preservation, or it can be imported into a different system, so long as that system has the same plugin and domain that was used to initially create the class model. This allows enrollments to be shared between systems. In general, exported models are specific to the plugin and domain that created them, so care must be taken to ensure models are not mixed in to other plugins. It is up to the client program or end user to keep tabs on where the exported models came from and what class they represent, and to manage these models once they've been exported. AudioConverter An AudioConverter plugin has audio as both its input and as its output. This Trait allows the system to take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. The only current plugin to implement the AudioConverter Trait is a speech enhancement plugin for enhancing the speech intelligibility of the submitted audio. AudioVectorizer This is a prototype feature that allows the system to preprocess an audio file or model to perform the compute-heavy steps of feature extraction and others, so that at a later time a user can either enroll a model or score a file very quickly, since the most time-consuming steps have already been performed. Like the ClassExporter Trait and exported classes/models, the vectorized audio representations are plugin/domain specific and cannot be used with plugins other than the one that created them. This is helpful if enrollments will be frequently rolled in or out of the system, or if the same audio files will be frequently re-tested, to avoid wasted repeat compute cycles. LearningTrait There are three total LearningTrait s: SupervisedAdapter SupervisedTrainer UnsupervisedAdapter Only SupervisedAdapter is currently supported by plugins in the OLIVE ecosystem; the others are deprecated or prototype features and should be ignored. SupervisedAdapter Plugins with the SupervisedAdapter Trait enable the ability to perform Supervised Adaptation of a domain. This is human assisted improvement of a plugin's domain, generally with feedback to the system in the form of annotations of target phenomena, or in some cases, error corrections. For more information on Adaptation, refer to this section . UpdateTrait Certain systems, such as recent speaker and language recognition plugins, sid-embed-v5 and lid-embedplda-v1 , have the capability to adapt themselves based purely on raw data provided by the user in the normal use of the system (enrollment and test data). These systems collect statistics over from the data feed through the system that can be used to update system parameters in unsupervised (autonomous) adaptation, thereby improving the performance of the plugin in the conditions in which is has been deployed. Since the use of this data in adaptation changes the behavior of the plugin the system does not automatically update itself, but rather requires the end user to \"trigger\" the update and use the data the system has collected to adapt. Implementing and invoking the associated Update Request Messages to start the Update process will use the accrued data from the test and enroll conditions to pdate system parameters and apply the update. One can always revert to the plugin's original state by clearing out the data and statistics collected the learn directory from the server's storage. GlobalComparer A plugin that supports the GlobalComparer Trait has the capability of accepting two waveforms as input, performing some sort of analysis or comparison of the two, and returning a PDF report summarizing the analysis. TextTransformer A plugin with the TextTransformer Trait is used to translate a text input when queried with a TextTransformationRequest, providing translation results for the submitted string (it does not take an audio input, unlike other scoring traits). The output in this case is simply a string, which is the translation result AudioAlignmentScorer A plugin with the AudioAlignmentScorer Trait can be used to provide alignment shift scores for two or more audio inputs using an AudioAlignmentScoreRequest. The output is a set of shift scores betwen each combination of two audio inputs in the AudioAlignmentScoreRequest","title":"OLIVE Plugin Traits"},{"location":"traits.html#olive-plugin-traits","text":"","title":"OLIVE Plugin Traits"},{"location":"traits.html#traits-overview","text":"The functionality of each OLIVE plugin is defined by the API Traits that it implements. Each Trait defines a message or set of messages that the plugin must implement to perform an associated task. The available Traits are listed below, along with their associated implemented API messages, in the format: Trait Implemented Message 1 ... Implemented Message N","title":"Traits Overview"},{"location":"traits.html#traits-and-their-corresponding-messages","text":"The Trait and Message List follows. Each Trait name links to the relevant section on this page . Each API message entry links to the relevant portion of the Protocol Buffers Message Definition Reference Page. GlobalScorer - ( Tutorial ) GlobalScorerRequest GlobalScorerStereoRequest RegionScorer - ( Tutorial ) RegionScorerRequest RegionScorerStereoRequest FrameScorer - ( Tutorial ) FrameScorerRequest FrameScorerStereoRequest ClassModifier - ( Tutorial ) ClassModificationRequest ClassRemovalRequest ClassExporter ClassExportRequest ClassImportRequest AudioConverter AudioModificationRequest AudioVectorizer PluginAudioVectorRequest LearningTrait (SupervisedAdapter, SupervisedTrainer, UnsupervisedAdapter) - ( Tutorial ) PreprocessAudioTrainRequest PreprocessAudioAdaptRequest SupervisedTrainingRequest UnsupervisedAdaptationRequest UpdateTrait GetUpdateStatusRequest ApplyUpdateRequest GlobalComparer GlobalComparerRequest TextTransformer TextTransformationRequest AudioAlignmentScorer AudioAlignmentScoreRequest In addition to the messages above, the following messages exist for interacting with the server itself and are independent of individual plugins or plugin Traits: GetStatusRequest PluginDirectoryRequest LoadPluginDomainRequest RemovePluginDomainRequest Full message definition details for all of the messages mentioned here","title":"Traits and their Corresponding Messages"},{"location":"traits.html#requestresult-message-pairs","text":"In general, each Request message is paired with a Result message, that defines the structure and contents of the server's reply to a given client Request message. The table below defines the pairs of the Result messages for the Request messages mentioned above; and the full message definition details can be found in the OLIVE API Message Protocol Documentation , which each entry below is linked to. Request Message Result (Response) Message GlobalScorerRequest GlobalScorerResult RegionScorerRequest RegionScorerResult FrameScorerRequest FrameScorerResult ClassModificationRequest ClassModificationResult ClassRemovalRequest ClassRemovalResult ClassExportRequest ClassExportResult ClassImportRequest ClassImportResult AudioModificationRequest AudioModificationResult PluginAudioVectorRequest PluginAudioVectorResult PreprocessAudioTrainRequest PreprocessAudioTrainResult PreprocessAudioAdaptRequest PreprocessAudioAdaptResult SupervisedTrainingRequest SupervisedTrainingResult UnsupervisedAdaptationRequest UnsupervisedAdaptationResult GetUpdateStatusRequest GetUpdateStatusResult ApplyUpdateRequest ApplyUpdateResult GlobalComparerRequest GlobalComparerResult TextTransformationRequest TextTransformationResult AudioAlignmentScoreRequest AudioAlignmentScoreResult or Task Request Message Result (Response) Message Global Score GlobalScorerRequest GlobalScorerResult Region Score RegionScorerRequest RegionScorerResult Frame Score FrameScorerRequest FrameScorerResult Create or modify a class enrollment ClassModificationRequest ClassModificationResult Remove an enrolled class ClassRemovalRequest ClassRemovalResult Export a class for later use or for model sharing ClassExportRequest ClassExportResult Import a previously exported class ClassImportRequest ClassImportResult Submit audio for modification (enhancement) AudioModificationRequest AudioModificationResult Request a vectorized representation of audio PluginAudioVectorRequest PluginAudioVectorResult Prepare audio for submission to training a new domain from scratch PreprocessAudioTrainRequest PreprocessAudioTrainResult Prepare audio for submission to adapt an existing domain PreprocessAudioAdaptRequest PreprocessAudioAdaptResult Query a plugin to see if it is ready to apply an \"Update\" GetUpdateStatusRequest GetUpdateStatusResult Instruct a plugin to perform an \"Update\" ApplyUpdateRequest ApplyUpdateResult Submit two audio files for forensic comparison GlobalComparerRequest GlobalComparerResult Submit text (string) for translation TextTransformationRequest TextTransformationResult Submit two or more audio files for alignment shift scores AudioAlignmentScoreRequest AudioAlignmentScoreResult","title":"Request/Result Message Pairs"},{"location":"traits.html#traits-deep-dive","text":"This section provides more details on what the \"real world\" usage and definition of each of the above Traits actually entails. If anything is unclear or could use more expansion, please contact us and we will provide updates and/or clarification.","title":"Traits Deep Dive"},{"location":"traits.html#globalscorer","text":"A GlobalScorer plugin has the output of reporting a single score for each relevant plugin class representing the likelihood that the entire audio file or clip contains this class. For example, when audio is sent to a Speaker Identification (SID) plugin with the GlobalScorer Trait, if there are 3 enrolled speakers at that time, the plugin will return a single score for each enrolled speaker representing the likelihood that the audio is comprised of speech from each speaker, respectively. The output of such a request will contain information that looks something like this: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 Keep in mind that speakers and speaker identification are just examples in this case, and that this generalizes to any global scoring plugin and whatever classes it is meant to distinguish between. This same example could be represented more generically as: /data/sid/audio/file1.wav class1 -0.5348 /data/sid/audio/file1.wav class2 3.2122 /data/sid/audio/file1.wav class3 -5.5340 Due to the nature of global scoring plugins, they are best used when the attribute that's being estimated is known to be, or very likely to be static. For example, using global scoring SID on one side of a 4-wire telephone conversation, where the speaker is likely to remain the same throughout, or using global scoring LID on a TV broadcast that is expected to only contain a single language. These plugins can miss the nuances of or be confused by things like code-switching within recordings, or unexpected speaker changes. The benefits of global scoring plugins are that they are often very fast, since they treat the entirety of the submitted audio as a single unit, and do not worry about trying to subdivide or chunk it in any way. For a first-pass or quick triage approach, or when the data is known or suspected to be homogenous in the ways discussed above, these plugins are very effective. When finer grained results are necessary, though, a RegionScorer or FrameScorer may be more appropriate.","title":"GlobalScorer"},{"location":"traits.html#regionscorer","text":"For each audio file or recording submitted to a RegionScorer plugin, results are returned consisting of 0 or more regions with an associated score and plugin class. Typically, regions are returned in the case of a 'detection' of an instance of the respective class. A 'region' is a pair of timestamps, referring to the start time and end time of the detected class presence, and includes the name of the respective class, and an associated score. An example of this might be a keyword spotting plugin returning the keyword class name, as well as the location and likelihood of that keyword's presence. This might look something like: /data/test/testFile1.wav 0.630 1.170 Airplane 4.3725 /data/test/testFile1.wav 1.520 2.010 Watermelon -1.1978 Another example of output you may see from a region scoring plugin follows, showing what a region scoring speaker detection plugin might output. In this example, an enrolled speaker, speaker2 was detected in testFile1.wav from 0.630 s to 1.170 s with a likelihood score of 4.3725. Likewise for 1.520 s to 2.010 s in the same file for the enrolled speaker called speaker1 , this time with a likelihood score of -1.1978. /data/test/testFile1.wav 0.630 1.170 speaker2 4.3725 /data/test/testFile1.wav 1.520 2.010 speaker1 -1.1978 An even simpler output of this type may just label the regions within a file that the plugin determines contain speech. Again, these are just arbitrary examples using a specific plugin type to more easily describe the scoring type; a more generic output example could be: /data/test/testFile1.wav 0.630 1.170 class1 4.3725 /data/test/testFile1.wav 1.520 2.010 class2 -1.1978 RegionScorer plugins allow a finer resolution with respect to results granularity, and allow plugins to be more flexible and deal with transitions between languages or speakers or other classes of interest within a given audio file or recording. Sometimes this comes at a cost of increased processing complexity and/or slower runtime.","title":"RegionScorer"},{"location":"traits.html#framescorer","text":"A plugin with the FrameScorer Trait that is queried with a FrameScorerRequest will provide score results for each frame of whatever audio has been submitted. Unless otherwise noted, an audio frame is 10 milliseconds. The most common OLIVE plugin that has the FrameScorer Trait are speech activity detection (SAD) plugins, where the score for each frame represents the likelihood of speech being present in that 10 ms audio frame. The output in this case is simply a sequential list of numbers, corresponding to the output score for each frame, in order: 1.9047 1.8088 1.2382 -0.8862 -2.5509 In this example, these frames can then be processed to turn them into region scores, labeling the locations where speech has been detected as present within the file. Returning raw frame scores as a result allows more down-stream flexibility, allowing thresholds to be adjusted and regions re-labeled if desired, for example to allow for tuning for more difficult or unexpected audio conditions.","title":"FrameScorer"},{"location":"traits.html#classmodifier","text":"Any plugin capable of adding or removing classes to or from its set of enrollments or target classes carries the ClassModifier Trait. This trait means the set of classes the plugin is interested in is mutable, and can be altered. Typically this is done by providing labeled data that belongs to the new class of interest to the server, which then enrolls a new model representing what it has learned about distinguishing this class from others. Existing class models can also be augmented by providing the system with additional data with this class label. In addition to adding new classes, and improving/augmenting existing ones, it is also possible to remove enrolled classes from domains of plugins carrying this trait, using the ClassRemovalRequest message.","title":"ClassModifier"},{"location":"traits.html#classexporter","text":"A plugin that implements the ClassExporter Trait is capable of exporting enrolled class models in a form that can be either imported back into the same system, as a way to save the model for preservation, or it can be imported into a different system, so long as that system has the same plugin and domain that was used to initially create the class model. This allows enrollments to be shared between systems. In general, exported models are specific to the plugin and domain that created them, so care must be taken to ensure models are not mixed in to other plugins. It is up to the client program or end user to keep tabs on where the exported models came from and what class they represent, and to manage these models once they've been exported.","title":"ClassExporter"},{"location":"traits.html#audioconverter","text":"An AudioConverter plugin has audio as both its input and as its output. This Trait allows the system to take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. The only current plugin to implement the AudioConverter Trait is a speech enhancement plugin for enhancing the speech intelligibility of the submitted audio.","title":"AudioConverter"},{"location":"traits.html#audiovectorizer","text":"This is a prototype feature that allows the system to preprocess an audio file or model to perform the compute-heavy steps of feature extraction and others, so that at a later time a user can either enroll a model or score a file very quickly, since the most time-consuming steps have already been performed. Like the ClassExporter Trait and exported classes/models, the vectorized audio representations are plugin/domain specific and cannot be used with plugins other than the one that created them. This is helpful if enrollments will be frequently rolled in or out of the system, or if the same audio files will be frequently re-tested, to avoid wasted repeat compute cycles.","title":"AudioVectorizer"},{"location":"traits.html#learningtrait","text":"There are three total LearningTrait s: SupervisedAdapter SupervisedTrainer UnsupervisedAdapter Only SupervisedAdapter is currently supported by plugins in the OLIVE ecosystem; the others are deprecated or prototype features and should be ignored.","title":"LearningTrait"},{"location":"traits.html#supervisedadapter","text":"Plugins with the SupervisedAdapter Trait enable the ability to perform Supervised Adaptation of a domain. This is human assisted improvement of a plugin's domain, generally with feedback to the system in the form of annotations of target phenomena, or in some cases, error corrections. For more information on Adaptation, refer to this section .","title":"SupervisedAdapter"},{"location":"traits.html#updatetrait","text":"Certain systems, such as recent speaker and language recognition plugins, sid-embed-v5 and lid-embedplda-v1 , have the capability to adapt themselves based purely on raw data provided by the user in the normal use of the system (enrollment and test data). These systems collect statistics over from the data feed through the system that can be used to update system parameters in unsupervised (autonomous) adaptation, thereby improving the performance of the plugin in the conditions in which is has been deployed. Since the use of this data in adaptation changes the behavior of the plugin the system does not automatically update itself, but rather requires the end user to \"trigger\" the update and use the data the system has collected to adapt. Implementing and invoking the associated Update Request Messages to start the Update process will use the accrued data from the test and enroll conditions to pdate system parameters and apply the update. One can always revert to the plugin's original state by clearing out the data and statistics collected the learn directory from the server's storage.","title":"UpdateTrait"},{"location":"traits.html#globalcomparer","text":"A plugin that supports the GlobalComparer Trait has the capability of accepting two waveforms as input, performing some sort of analysis or comparison of the two, and returning a PDF report summarizing the analysis.","title":"GlobalComparer"},{"location":"traits.html#texttransformer","text":"A plugin with the TextTransformer Trait is used to translate a text input when queried with a TextTransformationRequest, providing translation results for the submitted string (it does not take an audio input, unlike other scoring traits). The output in this case is simply a string, which is the translation result","title":"TextTransformer"},{"location":"traits.html#audioalignmentscorer","text":"A plugin with the AudioAlignmentScorer Trait can be used to provide alignment shift scores for two or more audio inputs using an AudioAlignmentScoreRequest. The output is a set of shift scores betwen each combination of two audio inputs in the AudioAlignmentScoreRequest","title":"AudioAlignmentScorer"},{"location":"workflows.html","text":"OLIVE Python Workflow API Introduction The OLIVE Workflow API extends the OLIVE Enterprise API to simplify working with an OLIVE server, allowing clients to request multiple OLIVE tasks with one API call. This eliminates the more verbose and complex calls necessary when using the Enterprise API. In particular, this framework will encapsulate 'feeding' functionality, to link together tasks, like speech activity detection and a plugin that uses speech regions or frames in its processing, or to 'bundle' multiple requests in a single call, rather than having every API call to a plugin be a separate action. The Workflow API is based around binary or text \"Workflow Definition\" files that SRI distributes to clients. These files contain a 'recipe' to be executed on an OLIVE server to perform analysis, enrollment, and eventually adaption using one or more audio files/inputs. This places the work of specifying how to execute a complex task to execute within the OLIVE server, instead of the client. To use this API, a client submits the SRI provided Workflow Definition file to an OLIVE server. The server verifies it can run the Workflow Definition through a process called \"actualization\". If successful, an activated Workflow is returned to the client. This activated Workflow is then ready for one or more analysis (or enrollment) requests. These requests can be made numerous times with one or more audio submissions. OLIVE still supports the original OLIVE Enterprise API that was based on creating and sending Google Protocol Buffers (Protobuf) message for each request, so if desired, clients can combine classic OLIVE API calls with the Workflow API to implement advanced functionality. For more information about working with OLIVE, please see: The OLIVE Server The OLIVE Plugins The OLIVE Enterprise API Useful Concepts to Know Tasks: a Workflow is composed of one or more tasks, such as SAD, LID, SID, QBE, ASR, etc. A task typically maps to a traditional plugin/domain, but as we expand the capabilities of the Workflow API, 'tasks' are likely to include functionality that is not implemented by a traditional OLIVE plugin, but by private helper plug-ins that facilitate the process but do not return values to the user. Job: A set of tasks and the audio (data) used with those tasks. Currently, a Workflow only includes one job, with most jobs accepting only one audio input. Workflow Definition - distributed as a read-only binary or text file with these characteristics: Similar to Plugins, workflows are independent of the OLIVE software and can be updated and/or distributed outside of an OLIVE software release. Can be submitted to multiple OLIVE servers for actualization in parallel. Each actualized Workflow Definition is unique to the server where it was actualized. Actualized Workflows use the names of plugins and domains in the users environment to execute tasks. These names are assumed to be the standard names they were delivered with. Changing the name of a plugin or domain will cause workflows that use them to cease to function. Limitations The current implementation of workflows is a first release. This release covers two types of behavior: bundling of disparate tasks together into a single call and restricted feeding processes, wherein the output of one plugin may be fed into one or more other \"downstream\" plugins that will use this information in their processing. The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the downstream plugin(s) must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). Further, the current implementation of workflows does not support tasks that create entirely new domains, like adaptation. This is much easier to accomplish with a direct call to the plugin, as bundling and feeding behavior is not relevant to this activity. Python Client Install The OLIVE Python API is distributed as a Python wheel package. To install, navigate into the 'api' folder distributed with your OLIVE delivery, and install it using your native pip3: $ pip3 install olivepy-5.1.0rc1-py3-none-any.whl This installs OLIVE and its dependencies into your local Python distribution. The client source is also available in the olivepy-5.1.0rc1.tar.gz package. OLIVE client dependencies include: protobuf soundfile numpy zmq python3 Integration Analysis Request With a Workflow Definition file, it only takes a few steps to make an analysis request: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> buffer = workflow . package_audio ( workflow . serialize_audio ( 'filename.wav' ), label = os . path . basename ( 'filename.wav' )) >>> response = workflow . analyze ([ buffer ]) The analysis response can be pretty printed as JSON: >>> print ( \"Workflow Results: {}\" . format ( json . dumps ( response . get_response_as_json (), indent = 1 ))) Workflow results: [ { \"job_name\" : \"Simple SAD and LID workflow\" , \"data\" : [ { \"dataId\" : \"test_audio.wav\" , \"msgType\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sampleRate\" : 8000 , \"durationSeconds\" : 10.509625434875488 , \"numberChannels\" : 1 , \"label\" : \"~/audio/test_audio.wav\" , \"id\" : \"edd6f2d8abb69e754cc4bd03c2d3f60379255048fdb20a8924858c0723ea4bbd\" } ], \"tasks\" : { \"SAD\" : { \"taskTrait\" : \"REGION_SCORER\" , \"taskType\" : \"SAD\" , \"messageType\" : \"REGION_SCORER_RESULT\" , \"analysis\" : { \"region\" : [ { \"startT\" : 3.3399999141693115 , \"endT\" : 10.5 , \"classId\" : \"speech\" , \"score\" : 0.0 } ] }, \"plugin\" : \"sad-dnn\" , \"domain\" : \"vtd-v1\" }, \"LID\" : { \"taskTrait\" : \"GLOBAL_SCORER\" , \"taskType\" : \"LID\" , \"messageType\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"classId\" : \"eng\" , \"score\" : 2.5195693969726562 }, { \"classId\" : \"apc\" , \"score\" : - 3.509812355041504 }, { \"classId\" : \"tgl\" , \"score\" : - 3.566483974456787 }, { \"classId\" : \"arb\" , \"score\" : - 3.994529962539673 }, { \"classId\" : \"fre\" , \"score\" : - 21.55596160888672 } ] }, \"plugin\" : \"lid-embedplda\" , \"domain\" : \"multi-v1\" } } } ] Enrollment Request Some workflows support enrollment for one or more tasks. To list the tasks that support enrollment in a workflow, use the OliveWorkflow get_enrollment_tasks method: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> print ( \"Enrollment Tasks: {}\" . format ( workflow . get_enrollment_tasks ())) Enrollment Tasks : [ 'SID' ] Shown above, a workflow that supports SAD, LID, and SID analysis also supports SID enrollment. To enroll a speaker via this workflow, use the OliveWorkflow's enroll method: ... >>> buffer = workflow . package_audio ( workflow . serialize_audio ( 'filename.wav' ), label = os . path . basename ( 'filename.wav' )) >>> response = workflow . enroll ( [ buffer ] , 'example speaker' , [ 'SID' ] ) >>> print ( \"Workflow enrollment: {}\" . format ( json . dumps ( response . get_response_as_json (), indent = 1 ))) Workflow enrollment : { \"successful\" : true } For the OLIVE 5.1 release only one task can be enrolled per request, even though the enrollment interface accepts multiple enrollment tasks. Other Enrollment Options Enrollment can also be done via the Python CLI tool, olivepyenroll: olivepyenroll -p sid-embed-v6.0.1 --domain multicond-v1 -e Test_Speaker -w test.wav Or incorporate the code from OliveClient (or AsyncOliveClient if client your client needs non-blocking calls): >>> import olivepy.oliveclient as oc client = oc . OliveClient ( 'test client' ) client . enroll ( 'sid-embed-v6.0.1' , 'multicond-v1' , 'Test_Speaker' , '/olive/data/test.wav' ) Advanced Workflow Features Audio Submission Options Audio can be submitted as a path name or as a buffer. If submitted as a path name, then the path must be accessible by the OLIVE server. If submitting audio as a buffer then that buffer can contain either: A 'serialized' file, where the file contents are read into a buffer. OliveWorkflow's serialize_audio() was added as a helper utility to load a local file into a serialized buffer. See more below. PCM-16 encoded samples. If the client has decoded a audio source, then those samples can be submitted directly in the audio buffer. The audio pathname or buffer is wrapped in a WorkflowDataRequest object before submitting to OLIVE. Sending a local file as a serialized buffer: ... # Submit the serialize the audio file into a buffer for submission to OLIVE audio_filename = \"~/audio/test_audio.wav\" # OliveWorkflow provides a helper method to serialize a file: buffer = workflow.serialize_audio(audio_filename) # Wrap the buffer in a WorkflowDataRequest: olive_data = workflow.package_audio(buffer, label=os.path.basename(audio_filename)) Or to send the path to a local file (assuming the server and client are running on the same host or have access to a shared file system): ... # Submit the path of the audio file: audio_filename = \"~/audio/test_audio.wav\" olive_data = workflow.package_audio(audio_filename, mode=msgutil.AudioTransferType.AUDIO_PATH) Once the data has been wrapped in a WorkflowDataRequest it can be submitted for analysis or enrollment. Audio Annotations The audio submitted for analysis (or enrollment) can be annotated with start/end regions when packaging audio using the OliveWorkflow.package_audio() method. For example, here is how to specify two regions within a file: ... filename = '/home/olive/test.wav' # Provide annotations for two regions: 0.3 to 1.7 seconds, and 2.4 to 3.3 seconds in filename: regions = [(0.3, 1.7), (2.4, 3.3)] # serialize the file: buffer = workflow.serialize_audio(filename) audio = workflow.package_audio(buffer, annotations=regions) ... Handling A Workflow Analysis Response (Python) A successful Workflow analysis produces a response that includes information about the audio analyzed and the results of one or more tasks. The Workflow API includes methods workflow.get_analysis_tasks(), and get_analysis_task_info() (discussed below) to help clients identify and prepare for the types of tasks a Workflow will produce. Inspecting Tasks in Workflow Definition A Python OliveWorkflow object (olivepy.workflow.OliveWorkflow) provides two methods to query the underlying Workflow Definition about the data and tasks that it supports. These methods are get_analysis_tasks() and get_analysis_task_info(). The first method, get_analysis_tasks(), returns the names of the tasks supported by this workflow, while the second method, get_analysis_task_info(), returns the names of the tasks plus details such as the type of score (Fame, Region, Global) returned by this task and when possible the name of the plugin/domain used to implement this task. get_analysis_task_info() also returns information about the data used by the job. For example: print(\"Analysis Tasks: {}\".format(workflow.get_analysis_tasks())) Outputs Analysis Tasks: ['SAD', 'LID'] For more detailed information about these tasks use get_analysis_task_info(): tasks_json = workflow.get_analysis_task_info() # Pretty print the output using json.dumps: print(\"Analysis Task Info: {}\".format(json.dumps(tasks_json, indent=1))) Output: Analysis Task Info: [ { \"data\": { \"minNumberInputs\": 1, \"maxNumberInputs\": 1, \"type\": \"AUDIO\", \"preprocessingRequired\": true, \"resampleRate\": 8000, \"mode\": \"MONO\" }, \"SAD\": { \"messageType\": \"REGION_SCORER_REQUEST\", \"traitOutput\": \"REGION_SCORER\", \"task\": \"SAD\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"SAD\", \"returnResult\": true, \"job_name\": \"Basic SAD and LID workflow\", \"analysis\": { \"plugin\": \"sad-dnn\", \"domain\": \"multi-v1\" } }, \"LID\": { \"messageType\": \"GLOBAL_SCORER_REQUEST\", \"traitOutput\": \"GLOBAL_SCORER\", \"task\": \"LID\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"LID\", \"returnResult\": true, \"job_name\": \"Basic SAD and LID workflow\", \"analysis\": { \"plugin\": \"lid-embedplda\", \"domain\": \"multi-v1\" } } } ] In the above example, the 'traitOutput' attribute, identifies the type of score out produced by the task. This attribute lets clients know the type of score produced by this task. Currently, Workflow tasks return one of these three score (analysis) types: FRAME_SCORER - these tasks create a set of scores for each frame in the submitted audio. Typically frame scoring is returned by SAD plugins to provide a set of frame scores for 'speech'. For users of the Enterprise API, this the output from a FrameScorerResult message. GLOBAL_SCORER - these tasks create a single set of scores (one per class) for an entire audio. In the Enterprise API this maps to a GlobalScorerResult message. REGION_SCORER - these tasks create a set of scores for regions in the audio submission. In the Enterprise API this maps to a RegionScorerResult message Parsing Analysis Output Analysis output is grouped into one or more 'jobs', where a job is the audio info plus the analysis task result(s). In the example above, one audio file was serialized, then submitted to OLIVE for analysis by SAD and LID tasks. This resulted in an OLIVE response containing the SAD and LID task results, plus information about the audio used for analysis. Since only one file was submitted for analysis, only one 'job' was returned. Had two audio files been submitted, then OLIVE would have created two job results - one for each file submitted. Each job will have a data element that describes properties of the audio used for analysis and one or more 'task' elements that contain the analysis results which are either frame scores, global scores, or region scores. Frame Scorer Task Output Tasks that return frame scores, are based the Enterprise API FrameScoreResult message: // The results from a FrameScorerRequest message FrameScorerResult { repeated FrameScores result = 1 ; // List of frame scores by class_id } // The basic unit of a frame score , returned in a FrameScorerRequest message FrameScores { required string class_id = 1 ; // The class id to which the frame scores pertain required int32 frame_rate = 2 ; // The number of frames per second required double frame_offset = 3 ; // The offset to the center of the frame 'window' repeated double score = 4 [ packed = true ]; // The frame - level scores for the class_id } The JSON output for frame score results has the form: result [ {classId:str, frameRate:int, frameOffset:double, score:[double ] } ] Here is an example of SAD output as frame scores: \"tasks\": { ... \"SAD\": { \"taskTrait\": \"FRAME_SCORER\", \"taskType\": \"SAD\", \"messageType\": \"FRAME_SCORER_RESULT\", \"analysis\": { \"result\": [ { \"classId\": \"speech\", \"frameRate\": 100, \"frameOffset\": 0.0, \"score\": [ -1.65412407898345, -1.65412407898345, -1.6976179167708825, -1.7201831820448585, -1.7218198748053783, -1.7285406659981328, -1.7403455556231222, -1.7572345436803467, -1.779207630169806, -1.8052880586341415, ... ] } ] }, \"plugin\": \"sad-dnn\", \"domain\": \"multi-v1\" }, Global Scorer Tasks Tasks that return global scores, are based on the Enterprise API's GlobalScorerResult message: message GlobalScorerResult { repeated GlobalScore score = 1 ; // The class scores } // The global score for a class message GlobalScore { required string class_id = 1 ; // The class required float score = 2 ; // The score associated with the class optional float confidence = 3 ; // An optional confidence value when part of a calibration report optional string comment = 4 ; // An optional suggested action when part of a calibration report } The JSON output for global score results has the form: score [ {classId:str, score:[float ] } ] Here is an example of LID output as global scores: \"tasks\": { ... \"LID\": { \"taskTrait\": \"GLOBAL_SCORER\", \"taskType\": \"LID\", \"messageType\": \"GLOBAL_SCORER_RESULT\", \"analysis\": { \"score\": [ { \"classId\": \"eng\", \"score\": 2.5195693969726562 }, { \"classId\": \"apc\", \"score\": -3.509812355041504 }, { \"classId\": \"tgl\", \"score\": -3.566483974456787 }, { \"classId\": \"arb\", \"score\": -3.994529962539673 }, { \"classId\": \"fre\", \"score\": -21.55596160888672 } ] }, \"plugin\": \"lid-embedplda\", \"domain\": \"multi-v1\" } Region Scorer Tasks Tasks that return region scores, are based on the Enterprise API's RegionScorerResult message: // The region score result message RegionScorerResult { repeated RegionScore region = 1 ; // The scored regions } // The basic unit a region score. There may be multiple RegionScore values in a RegionScorerResult message RegionScore { required float start_t = 1 ; // Begin-time of the region (in seconds) required float end_t = 2 ; // End-time of t he region (in seconds) required string class_id = 3 ; // Class ID associated with region optional float score = 4 ; // Optional score associated with the class_id label } The JSON output for region score results has the form: score[ {startT:float, endT:float, classId:str, score:float} ] Here is an example of SAD output as region score: \"tasks\": { \"SAD\": { \"taskTrait\": \"REGION_SCORER\", \"taskType\": \"SAD\", \"messageType\": \"REGION_SCORER_RESULT\", \"analysis\": { \"region\": [ { \"startT\": 3.3399999141693115, \"endT\": 10.5, \"classId\": \"speech\", \"score\": 0.0 } ] }, \"plugin\": \"sad-dnn\", \"domain\": \"vtd-v1\" }, Enrolled Classes To list the current classes (such as speakers for a SID task, or languages for LID) in a workflow, use the OliveWorkflow get_analysis_class_ids method: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> class_status_response = workflow . get_analysis_class_ids () >>> print ( \"Task Class IDs: {}\" . format ( json . dumps ( class_status_response . get_response_as_json (), indent = 1 ) )) Enrollment Class IDs : [ { \"job_name\" : \"SAD LID, SID\" , \"tasks\" : { \"SAD\" : { \"classId\" : [ \"speech\" ] }, \"LID\" : { \"classId\" : [ \"amh\" , \"apc\" , \"arb\" , \"arz\" , \"cmn\" , \"eng\" , \"fas\" , \"fre\" , \"jpn\" , \"kor\" , \"pus\" , \"rus\" , \"spa\" , \"tgl\" , \"tha\" , \"tur\" , \"urd\" , \"vie\" , \"yue\" ] }, \"SID\" : { \"classId\" : [ \"test speaker\" ] } } } ] Note that some tasks, such as SID, support enrollment, so the list of class IDs can can change over time as new enrollments are added. Multi-channel Audio The default workflow behavior is to merge multi-channel audio into a single channel, which is known as 'MONO' mode. To perform analysis on each channel instead of a merged channel, then the Workflow Definition must be authored with a data mode of 'SPLIT'. When using the split mode, each channel in a multi-channel audio input is \"split\" into a job. Here is a typical workflow that merges any multi-channel audio into a single channel audio input: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> json_ouput = workflow . get_analysis_task_info () >>> Print ( json . dumps ( json_output , indent = 1 )) [ { \"Preprocess Data\" : { \"minNumberInputs\" : 1 , \"maxNumberInputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessingRequired\" : true , \"resampleRate\" : 8000 , \"mode\" : \"MONO\" }, ... Here is a workflow that will split each channel in a multi-channel (stereo) audio input into separate jobs: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/multi_channel_sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> json_ouput = workflow . get_analysis_task_info () >>> Print ( json . dumps ( json_output , indent = 1 )) [ { \"Preprocess Data\" : { \"minNumberInputs\" : 1 , \"maxNumberInputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessingRequired\" : true , \"resampleRate\" : 8000 , \"mode\" : \"SPLIT\" }, ... The above Workflow Definition when applied to stereo file will produce a set of SAD, LID, and SID scores for each channel. Adaption Using a Workflow Not yet implemented via a Workflow. The adaption process is complicated, time-consuming, and plugin/domain specific. Until Adaptation via a workflow is implemented, please use the SRI provided Python client (olivepylearn) or Java client (OliveLearn) to perform adaptation. To adapt using the olivepylearn utility: olivepylearn --plugin sad-dnn --domain multi-v1 -a TEST_NEW_DOMAIN -i /olive/sadRegression/lists/adapt_s.lst Where that adapt_s.lst looks like this: /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav S 20.469 21.719 /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav NS 10.8000 10.8229 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav S 72.898 73.748 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav NS 42.754 43.010 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav S 79.437 80.427 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav NS 61.459 62.003 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav S 11.0438 111.638 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav NS 69.058 73.090 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav S 112.936 113.656 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav NS 83.046 83.114 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav S 16.940 20.050 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav NS 59.794 59.858 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav S 87.280 88.651 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav NS 82.229 82.461 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav S 111.346 111.936 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav NS 83.736 84.446 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav S 77.291 78.421 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav NS 0 4.951 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav S 30.349 32.429 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav NS 100.299 101.647 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav S 46.527 49.147 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav NS 44.747 46.148 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav S 24.551 25.471 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav NS 52.033 52.211 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav S 88.358 93.418 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav NS 46.564 46.788 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav S 10.507 11.077 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav NS 41.099 41.227 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav S 61.072 63.002 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav NS 19.108 19.460 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav S 97.182 97.789 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav NS 71.711 71.732 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav S 114.312 117.115 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav NS 31.065 31.154 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav S 90.346 91.608 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav NS 50.028 51.377 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav S 75.986 76.596 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav NS 12.485 12.709 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav S 72.719 73.046 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav NS 51.923 53.379 /olive//sadRegression/audio/adapt/20131213T104502UTC_18520_A.wav NS 11.1192 112.761 /olive//sadRegression/audio/adapt/20131213T121501UTC_18530_A.wav NS 81.277 82.766 /olive//sadRegression/audio/adapt/20131213T124501UTC_18533_A.wav NS 83.702 84.501 /olive//sadRegression/audio/adapt/20131213T134502UTC_18567_A.wav NS 69.379 72.258 /olive//sadRegression/audio/adapt/20131217T015001UTC_18707_A.wav NS 5.099 10.507 You may also incorporate the adapt code from olivepy.oliveclient directly into your client: # Setup processing variables (get this config or via command line options plugin = \"sad-dnn\" domain = \"multi-v1\" new_domain_name = \"python_adapted_multi-v2\" # Build the list of files plus the regions in the those files to adapt by parsing the input file: file_annotations = self . parse_annotation_file ( \"lists/adapt.lst\" ) return self . adapt_supervised_old ( plugin , domain , file_annotations , new_domain_name ) Python Workflow Example As previously mentioned, SRI distributes Workflow Definitions as files, which are preconfigured to perform tasks such as SAD, LID, SID, etc. The example below uses a Workflow Definition file with the SRI implemented Python Workflow API to make a SAD and LID request via a Workflow Definition file: import os , json , sys import olivepy.olive_async_client as oc import olivepy.workflow as ow import olivepy.messaging.msgutil as msgutil # Create a connection to a local OLIVE server client = oc . AsyncOliveClient ( \"test olive client\" , 'localhost' ) client . connect () # Example workflow definition file: workflow_filename = \"~/sad_lid.workflow\" workflow_def = ow . OliveWorkflowDefinition ( workflow_filename ) # Submit the workflow definition to the client for actualization (instantiation or sometimes called activation): workflow = workflow_def . create_workflow ( client ) # Prepare a audio file to submit (as a serialzied file) audio_filename = \"~/audio/test_audio.wav\" buffers = [] # read in the file as raw bytes so a buffer can be sent to the server as a buffer: buffer = workflow . serialize_audio ( audio_filename ) data_wrapper = workflow . package_audio ( buffer , label = audio_filename ) buffers . append ( data_wrapper ) # OR - if the server and client share a filesystem, uncomment the following to send the path to the audio file instead of a buffer: # data_wrapper = workflow.package_audio(audio_filename, mode=msgutil.AudioTransferType.AUDIO_PATH) # Submit the workflow to OLIVE for analysis: response = workflow . analyze ( buffers ) # response will contain an error message if the workflow failed, or results for each task: print ( \"Workflow results:\" ) # Use json.dumps to pretty print the json output: print ( \"{}\" . format ( json . dumps ( response . get_response_as_json (), indent = 1 ))) # Be sure to close the connection to the server when done client . disconnect () For the above code, one would see JSON formatted output like this (of course your output will vary by the audio submitted for analysis): Workflow results: [ { \"job_name\": \"Simple SAD and LID workflow\", \"data\": [ { \"dataId\": \"test_audio.wav\", \"msgType\": \"PREPROCESSED_AUDIO_RESULT\", \"mode\": \"MONO\", \"merged\": false, \"sampleRate\": 8000, \"durationSeconds\": 10.509625434875488, \"numberChannels\": 1, \"label\": \"~/audio/test_audio.wav\", \"id\": \"edd6f2d8abb69e754cc4bd03c2d3f60379255048fdb20a8924858c0723ea4bbd\" } ], \"tasks\": { \"SAD\": { \"taskTrait\": \"REGION_SCORER\", \"taskType\": \"SAD\", \"messageType\": \"REGION_SCORER_RESULT\", \"analysis\": { \"region\": [ { \"startT\": 3.3399999141693115, \"endT\": 10.5, \"classId\": \"speech\", \"score\": 0.0 } ] }, \"plugin\": \"sad-dnn\", \"domain\": \"vtd-v1\" }, \"LID\": { \"taskTrait\": \"GLOBAL_SCORER\", \"taskType\": \"LID\", \"messageType\": \"GLOBAL_SCORER_RESULT\", \"analysis\": { \"score\": [ { \"classId\": \"eng\", \"score\": 2.5195693969726562 }, { \"classId\": \"apc\", \"score\": -3.509812355041504 }, { \"classId\": \"tgl\", \"score\": -3.566483974456787 }, { \"classId\": \"arb\", \"score\": -3.994529962539673 }, { \"classId\": \"fre\", \"score\": -21.55596160888672 } ] }, \"plugin\": \"lid-embedplda\", \"domain\": \"multi-v1\" } } } ] API Specification OliveWorkflowDefinition class olivepy.workflow.OliveWorkflowDefinition(filename: str) The class is used to load a Workflow Definition from a file. Parameters filename: the name of the workflow definition file create_workflow() create_workflow(client: olivepy.olive_async_client.AsyncOliveClient) Create a new Workflow for making OLIVE analysis, enrollment, or adaptation requests Parameters client: a valid, client connection to an OLIVE server Returns A new OliveWorkflow object, which has been actualized (activated) by the olive server and get_json() Create a JSON structure of the Workflow output Returns A JSON formated dictionary of the Workflow Definition WorkflowException exception olivepy.workflow.WorkflowException() Bases: Exception This exception means that an error occurred handling a Workflow OliveWorkflow class olivepy.workflow.OliveWorkflow(olive_async_client: olivepy.olive_async_client.AsyncOliveClient, actualized_workflow: olivepy.messaging.response.OliveWorkflowActualizedResponse) An OliveWorkflow instance represents a Workflow actualized by an OLIVE server, and is ready for analysis, enrollment, or adaptation. Parameters filename: the name of the workflow definition file Raises WorkflowException \u2013 If the workflow was not actualized serialize_audio() serialize_audio(filename: str) Read in an audio file as a serialized buffer. This is a helper utility for package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not yet been serialized Parameters filename: the local path to the file Returns The contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples analyze() analyze(data_inputs: List[olive_pb2.WorkflowDataRequest], callback=None, options: Optional[Dict[str, str]] = None) Perform a workflow analysis Parameters data_inputs \u2013 a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. callback \u2013 an optional function callback that is invoked with the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when done. Otherwise, this method immediately returns and the callback method is invoked when the response is received. The call back function must accept 3 parameters (request, response, and error message) options \u2013 a dictionary of name/value option pairs to include with the analysis request Returns An OliveWorkflowAnalysisResponse if no callback provided enroll() enroll(data_inputs: List[olive_pb2.WorkflowDataRequest], class_id: str, task_names: List[str], callback=None, options=None) Submit data for enrollment. Parameters data_inputs \u2013 a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. class_id \u2013 the name of the enrollment task_names \u2013 a list of task names, where the audio is enrolled with tasks (NOTE only one task currently supported) that support enrollment. This value can be None, in which case the data input(s) is enrolled for each enrollable task. callback \u2013 an optional function callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. options \u2013 a dictionary of name/value option pairs to include with the analysis request Returns An OliveWorkflowEnrollmentResponse if no callback provided get_analysis_class_ids() get_analysis_class_ids(callback=None) Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc) Parameters callback - a callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) Returns An OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object get_analysis_task_info() get_analysis_task_info() Reports information on the tasks used for analysis from the actualized workflow. When possible, this reports the plugins used in the workflow (although there can be cases when the final plugin/domain used is not known until runtime) Returns A list of dictionaries, where each dictionary contains information about an analysis task get_analysis_tasks() get_analysis_tasks(job_name: Optional[str] = None) Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) It is possible, although unlikely, that a Workflow could define two of the the same type of tasks in a job. The keys in the dictionary ensure that the tasks in a workflow have a unique name. Parameters job_name \u2013 the name of the analysis job. Optional since most workflows only support one analysis job and \u2018jobs; may be hidden from users in a future release. Returns a list of task names get_enrollment_tasks() get_enrollment_tasks(job_name: Optional[str] = None) Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) Parameters job_name \u2013 the name of the enrollment job. Optional since most workflows only support one job Returns a list of task names package_audio() package_audio ( audio_data : AnyStr , mode =< AudioTransferType . AUDIO_SERIALIZED : 3 > , annotations : Optional [ List[Tuple[float, float ] ]] = None , task_annotations : Optional [ List[Tuple[float, float ] ]] = None , selected_channel : Optional [ int ] = None , num_channels : Optional [ int ] = None , sample_rate : Optional [ int ] = None , num_samples : Optional [ int ] = None , validate_local_path : bool = True , label = None ) Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request Parameters audio_data \u2013 the audio input is a string file path if mode is \u2018AUDIO_PATH\u2019, otherwise it should be a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in PCM_16 encoded samples mode \u2013 specifies how the audio is sent to the server: either as the filename of the audio input (file path must be valid for the server), send as a serialized buffer, or send PCM-16 encoded samples annotations \u2013 optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) task_annotations \u2013 optional regions (start/end regions in seconds) for a task. for example: {\u2018SHL\u2019: {\u2018speaker\u2019\u2019:[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the \u2018SHL\u2019 task, which are labeled for as class \u2018speaker\u2019 and has regions 0.5 to 4.5 and 6.8 to 9.2 (use get_analysis_tasks() to get the name of workflow tasks). selected_channel \u2013 optional - the channel to use in the workflow if the audio input is multi-channel num_channels \u2013 The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored sample_rate \u2013 The sample rate if audio input is a list of decoded (PCM-16) samples, if not usingma buffer of PCM-16 samples this is value is ignored num_samples \u2013 The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored validate_local_path \u2013 If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired label \u2013 an optional name to use with the audio Returns A populated WorkflowDataRequest to use in a workflow activity package_image() package_image(image_input) Not yet supported package_text() package_text(text_input: str) Not yet supported package_video() package_video(video_input) Not yet supported adapt() adapt(data_input, callback, options=None, finalize=True) Not yet supported","title":"Workflows"},{"location":"workflows.html#olive-python-workflow-api","text":"","title":"OLIVE Python Workflow API"},{"location":"workflows.html#introduction","text":"The OLIVE Workflow API extends the OLIVE Enterprise API to simplify working with an OLIVE server, allowing clients to request multiple OLIVE tasks with one API call. This eliminates the more verbose and complex calls necessary when using the Enterprise API. In particular, this framework will encapsulate 'feeding' functionality, to link together tasks, like speech activity detection and a plugin that uses speech regions or frames in its processing, or to 'bundle' multiple requests in a single call, rather than having every API call to a plugin be a separate action. The Workflow API is based around binary or text \"Workflow Definition\" files that SRI distributes to clients. These files contain a 'recipe' to be executed on an OLIVE server to perform analysis, enrollment, and eventually adaption using one or more audio files/inputs. This places the work of specifying how to execute a complex task to execute within the OLIVE server, instead of the client. To use this API, a client submits the SRI provided Workflow Definition file to an OLIVE server. The server verifies it can run the Workflow Definition through a process called \"actualization\". If successful, an activated Workflow is returned to the client. This activated Workflow is then ready for one or more analysis (or enrollment) requests. These requests can be made numerous times with one or more audio submissions. OLIVE still supports the original OLIVE Enterprise API that was based on creating and sending Google Protocol Buffers (Protobuf) message for each request, so if desired, clients can combine classic OLIVE API calls with the Workflow API to implement advanced functionality. For more information about working with OLIVE, please see: The OLIVE Server The OLIVE Plugins The OLIVE Enterprise API","title":"Introduction"},{"location":"workflows.html#useful-concepts-to-know","text":"Tasks: a Workflow is composed of one or more tasks, such as SAD, LID, SID, QBE, ASR, etc. A task typically maps to a traditional plugin/domain, but as we expand the capabilities of the Workflow API, 'tasks' are likely to include functionality that is not implemented by a traditional OLIVE plugin, but by private helper plug-ins that facilitate the process but do not return values to the user. Job: A set of tasks and the audio (data) used with those tasks. Currently, a Workflow only includes one job, with most jobs accepting only one audio input. Workflow Definition - distributed as a read-only binary or text file with these characteristics: Similar to Plugins, workflows are independent of the OLIVE software and can be updated and/or distributed outside of an OLIVE software release. Can be submitted to multiple OLIVE servers for actualization in parallel. Each actualized Workflow Definition is unique to the server where it was actualized. Actualized Workflows use the names of plugins and domains in the users environment to execute tasks. These names are assumed to be the standard names they were delivered with. Changing the name of a plugin or domain will cause workflows that use them to cease to function.","title":"Useful Concepts to Know"},{"location":"workflows.html#limitations","text":"The current implementation of workflows is a first release. This release covers two types of behavior: bundling of disparate tasks together into a single call and restricted feeding processes, wherein the output of one plugin may be fed into one or more other \"downstream\" plugins that will use this information in their processing. The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the downstream plugin(s) must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). Further, the current implementation of workflows does not support tasks that create entirely new domains, like adaptation. This is much easier to accomplish with a direct call to the plugin, as bundling and feeding behavior is not relevant to this activity.","title":"Limitations"},{"location":"workflows.html#python-client-install","text":"The OLIVE Python API is distributed as a Python wheel package. To install, navigate into the 'api' folder distributed with your OLIVE delivery, and install it using your native pip3: $ pip3 install olivepy-5.1.0rc1-py3-none-any.whl This installs OLIVE and its dependencies into your local Python distribution. The client source is also available in the olivepy-5.1.0rc1.tar.gz package. OLIVE client dependencies include: protobuf soundfile numpy zmq python3","title":"Python Client Install"},{"location":"workflows.html#integration","text":"","title":"Integration"},{"location":"workflows.html#analysis-request","text":"With a Workflow Definition file, it only takes a few steps to make an analysis request: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> buffer = workflow . package_audio ( workflow . serialize_audio ( 'filename.wav' ), label = os . path . basename ( 'filename.wav' )) >>> response = workflow . analyze ([ buffer ]) The analysis response can be pretty printed as JSON: >>> print ( \"Workflow Results: {}\" . format ( json . dumps ( response . get_response_as_json (), indent = 1 ))) Workflow results: [ { \"job_name\" : \"Simple SAD and LID workflow\" , \"data\" : [ { \"dataId\" : \"test_audio.wav\" , \"msgType\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sampleRate\" : 8000 , \"durationSeconds\" : 10.509625434875488 , \"numberChannels\" : 1 , \"label\" : \"~/audio/test_audio.wav\" , \"id\" : \"edd6f2d8abb69e754cc4bd03c2d3f60379255048fdb20a8924858c0723ea4bbd\" } ], \"tasks\" : { \"SAD\" : { \"taskTrait\" : \"REGION_SCORER\" , \"taskType\" : \"SAD\" , \"messageType\" : \"REGION_SCORER_RESULT\" , \"analysis\" : { \"region\" : [ { \"startT\" : 3.3399999141693115 , \"endT\" : 10.5 , \"classId\" : \"speech\" , \"score\" : 0.0 } ] }, \"plugin\" : \"sad-dnn\" , \"domain\" : \"vtd-v1\" }, \"LID\" : { \"taskTrait\" : \"GLOBAL_SCORER\" , \"taskType\" : \"LID\" , \"messageType\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"classId\" : \"eng\" , \"score\" : 2.5195693969726562 }, { \"classId\" : \"apc\" , \"score\" : - 3.509812355041504 }, { \"classId\" : \"tgl\" , \"score\" : - 3.566483974456787 }, { \"classId\" : \"arb\" , \"score\" : - 3.994529962539673 }, { \"classId\" : \"fre\" , \"score\" : - 21.55596160888672 } ] }, \"plugin\" : \"lid-embedplda\" , \"domain\" : \"multi-v1\" } } } ]","title":"Analysis Request"},{"location":"workflows.html#enrollment-request","text":"Some workflows support enrollment for one or more tasks. To list the tasks that support enrollment in a workflow, use the OliveWorkflow get_enrollment_tasks method: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> print ( \"Enrollment Tasks: {}\" . format ( workflow . get_enrollment_tasks ())) Enrollment Tasks : [ 'SID' ] Shown above, a workflow that supports SAD, LID, and SID analysis also supports SID enrollment. To enroll a speaker via this workflow, use the OliveWorkflow's enroll method: ... >>> buffer = workflow . package_audio ( workflow . serialize_audio ( 'filename.wav' ), label = os . path . basename ( 'filename.wav' )) >>> response = workflow . enroll ( [ buffer ] , 'example speaker' , [ 'SID' ] ) >>> print ( \"Workflow enrollment: {}\" . format ( json . dumps ( response . get_response_as_json (), indent = 1 ))) Workflow enrollment : { \"successful\" : true } For the OLIVE 5.1 release only one task can be enrolled per request, even though the enrollment interface accepts multiple enrollment tasks.","title":"Enrollment Request"},{"location":"workflows.html#other-enrollment-options","text":"Enrollment can also be done via the Python CLI tool, olivepyenroll: olivepyenroll -p sid-embed-v6.0.1 --domain multicond-v1 -e Test_Speaker -w test.wav Or incorporate the code from OliveClient (or AsyncOliveClient if client your client needs non-blocking calls): >>> import olivepy.oliveclient as oc client = oc . OliveClient ( 'test client' ) client . enroll ( 'sid-embed-v6.0.1' , 'multicond-v1' , 'Test_Speaker' , '/olive/data/test.wav' )","title":"Other Enrollment Options"},{"location":"workflows.html#advanced-workflow-features","text":"","title":"Advanced Workflow Features"},{"location":"workflows.html#audio-submission-options","text":"Audio can be submitted as a path name or as a buffer. If submitted as a path name, then the path must be accessible by the OLIVE server. If submitting audio as a buffer then that buffer can contain either: A 'serialized' file, where the file contents are read into a buffer. OliveWorkflow's serialize_audio() was added as a helper utility to load a local file into a serialized buffer. See more below. PCM-16 encoded samples. If the client has decoded a audio source, then those samples can be submitted directly in the audio buffer. The audio pathname or buffer is wrapped in a WorkflowDataRequest object before submitting to OLIVE. Sending a local file as a serialized buffer: ... # Submit the serialize the audio file into a buffer for submission to OLIVE audio_filename = \"~/audio/test_audio.wav\" # OliveWorkflow provides a helper method to serialize a file: buffer = workflow.serialize_audio(audio_filename) # Wrap the buffer in a WorkflowDataRequest: olive_data = workflow.package_audio(buffer, label=os.path.basename(audio_filename)) Or to send the path to a local file (assuming the server and client are running on the same host or have access to a shared file system): ... # Submit the path of the audio file: audio_filename = \"~/audio/test_audio.wav\" olive_data = workflow.package_audio(audio_filename, mode=msgutil.AudioTransferType.AUDIO_PATH) Once the data has been wrapped in a WorkflowDataRequest it can be submitted for analysis or enrollment.","title":"Audio Submission Options"},{"location":"workflows.html#audio-annotations","text":"The audio submitted for analysis (or enrollment) can be annotated with start/end regions when packaging audio using the OliveWorkflow.package_audio() method. For example, here is how to specify two regions within a file: ... filename = '/home/olive/test.wav' # Provide annotations for two regions: 0.3 to 1.7 seconds, and 2.4 to 3.3 seconds in filename: regions = [(0.3, 1.7), (2.4, 3.3)] # serialize the file: buffer = workflow.serialize_audio(filename) audio = workflow.package_audio(buffer, annotations=regions) ...","title":"Audio Annotations"},{"location":"workflows.html#handling-a-workflow-analysis-response-python","text":"A successful Workflow analysis produces a response that includes information about the audio analyzed and the results of one or more tasks. The Workflow API includes methods workflow.get_analysis_tasks(), and get_analysis_task_info() (discussed below) to help clients identify and prepare for the types of tasks a Workflow will produce.","title":"Handling A Workflow Analysis Response (Python)"},{"location":"workflows.html#inspecting-tasks-in-workflow-definition","text":"A Python OliveWorkflow object (olivepy.workflow.OliveWorkflow) provides two methods to query the underlying Workflow Definition about the data and tasks that it supports. These methods are get_analysis_tasks() and get_analysis_task_info(). The first method, get_analysis_tasks(), returns the names of the tasks supported by this workflow, while the second method, get_analysis_task_info(), returns the names of the tasks plus details such as the type of score (Fame, Region, Global) returned by this task and when possible the name of the plugin/domain used to implement this task. get_analysis_task_info() also returns information about the data used by the job. For example: print(\"Analysis Tasks: {}\".format(workflow.get_analysis_tasks())) Outputs Analysis Tasks: ['SAD', 'LID'] For more detailed information about these tasks use get_analysis_task_info(): tasks_json = workflow.get_analysis_task_info() # Pretty print the output using json.dumps: print(\"Analysis Task Info: {}\".format(json.dumps(tasks_json, indent=1))) Output: Analysis Task Info: [ { \"data\": { \"minNumberInputs\": 1, \"maxNumberInputs\": 1, \"type\": \"AUDIO\", \"preprocessingRequired\": true, \"resampleRate\": 8000, \"mode\": \"MONO\" }, \"SAD\": { \"messageType\": \"REGION_SCORER_REQUEST\", \"traitOutput\": \"REGION_SCORER\", \"task\": \"SAD\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"SAD\", \"returnResult\": true, \"job_name\": \"Basic SAD and LID workflow\", \"analysis\": { \"plugin\": \"sad-dnn\", \"domain\": \"multi-v1\" } }, \"LID\": { \"messageType\": \"GLOBAL_SCORER_REQUEST\", \"traitOutput\": \"GLOBAL_SCORER\", \"task\": \"LID\", \"consumerDataLabel\": \"audio\", \"consumerResultLabel\": \"LID\", \"returnResult\": true, \"job_name\": \"Basic SAD and LID workflow\", \"analysis\": { \"plugin\": \"lid-embedplda\", \"domain\": \"multi-v1\" } } } ] In the above example, the 'traitOutput' attribute, identifies the type of score out produced by the task. This attribute lets clients know the type of score produced by this task. Currently, Workflow tasks return one of these three score (analysis) types: FRAME_SCORER - these tasks create a set of scores for each frame in the submitted audio. Typically frame scoring is returned by SAD plugins to provide a set of frame scores for 'speech'. For users of the Enterprise API, this the output from a FrameScorerResult message. GLOBAL_SCORER - these tasks create a single set of scores (one per class) for an entire audio. In the Enterprise API this maps to a GlobalScorerResult message. REGION_SCORER - these tasks create a set of scores for regions in the audio submission. In the Enterprise API this maps to a RegionScorerResult message","title":"Inspecting Tasks in Workflow Definition"},{"location":"workflows.html#parsing-analysis-output","text":"Analysis output is grouped into one or more 'jobs', where a job is the audio info plus the analysis task result(s). In the example above, one audio file was serialized, then submitted to OLIVE for analysis by SAD and LID tasks. This resulted in an OLIVE response containing the SAD and LID task results, plus information about the audio used for analysis. Since only one file was submitted for analysis, only one 'job' was returned. Had two audio files been submitted, then OLIVE would have created two job results - one for each file submitted. Each job will have a data element that describes properties of the audio used for analysis and one or more 'task' elements that contain the analysis results which are either frame scores, global scores, or region scores.","title":"Parsing Analysis Output"},{"location":"workflows.html#frame-scorer-task-output","text":"Tasks that return frame scores, are based the Enterprise API FrameScoreResult message: // The results from a FrameScorerRequest message FrameScorerResult { repeated FrameScores result = 1 ; // List of frame scores by class_id } // The basic unit of a frame score , returned in a FrameScorerRequest message FrameScores { required string class_id = 1 ; // The class id to which the frame scores pertain required int32 frame_rate = 2 ; // The number of frames per second required double frame_offset = 3 ; // The offset to the center of the frame 'window' repeated double score = 4 [ packed = true ]; // The frame - level scores for the class_id } The JSON output for frame score results has the form: result [ {classId:str, frameRate:int, frameOffset:double, score:[double ] } ] Here is an example of SAD output as frame scores: \"tasks\": { ... \"SAD\": { \"taskTrait\": \"FRAME_SCORER\", \"taskType\": \"SAD\", \"messageType\": \"FRAME_SCORER_RESULT\", \"analysis\": { \"result\": [ { \"classId\": \"speech\", \"frameRate\": 100, \"frameOffset\": 0.0, \"score\": [ -1.65412407898345, -1.65412407898345, -1.6976179167708825, -1.7201831820448585, -1.7218198748053783, -1.7285406659981328, -1.7403455556231222, -1.7572345436803467, -1.779207630169806, -1.8052880586341415, ... ] } ] }, \"plugin\": \"sad-dnn\", \"domain\": \"multi-v1\" },","title":"Frame Scorer Task Output"},{"location":"workflows.html#global-scorer-tasks","text":"Tasks that return global scores, are based on the Enterprise API's GlobalScorerResult message: message GlobalScorerResult { repeated GlobalScore score = 1 ; // The class scores } // The global score for a class message GlobalScore { required string class_id = 1 ; // The class required float score = 2 ; // The score associated with the class optional float confidence = 3 ; // An optional confidence value when part of a calibration report optional string comment = 4 ; // An optional suggested action when part of a calibration report } The JSON output for global score results has the form: score [ {classId:str, score:[float ] } ] Here is an example of LID output as global scores: \"tasks\": { ... \"LID\": { \"taskTrait\": \"GLOBAL_SCORER\", \"taskType\": \"LID\", \"messageType\": \"GLOBAL_SCORER_RESULT\", \"analysis\": { \"score\": [ { \"classId\": \"eng\", \"score\": 2.5195693969726562 }, { \"classId\": \"apc\", \"score\": -3.509812355041504 }, { \"classId\": \"tgl\", \"score\": -3.566483974456787 }, { \"classId\": \"arb\", \"score\": -3.994529962539673 }, { \"classId\": \"fre\", \"score\": -21.55596160888672 } ] }, \"plugin\": \"lid-embedplda\", \"domain\": \"multi-v1\" }","title":"Global Scorer Tasks"},{"location":"workflows.html#region-scorer-tasks","text":"Tasks that return region scores, are based on the Enterprise API's RegionScorerResult message: // The region score result message RegionScorerResult { repeated RegionScore region = 1 ; // The scored regions } // The basic unit a region score. There may be multiple RegionScore values in a RegionScorerResult message RegionScore { required float start_t = 1 ; // Begin-time of the region (in seconds) required float end_t = 2 ; // End-time of t he region (in seconds) required string class_id = 3 ; // Class ID associated with region optional float score = 4 ; // Optional score associated with the class_id label } The JSON output for region score results has the form: score[ {startT:float, endT:float, classId:str, score:float} ] Here is an example of SAD output as region score: \"tasks\": { \"SAD\": { \"taskTrait\": \"REGION_SCORER\", \"taskType\": \"SAD\", \"messageType\": \"REGION_SCORER_RESULT\", \"analysis\": { \"region\": [ { \"startT\": 3.3399999141693115, \"endT\": 10.5, \"classId\": \"speech\", \"score\": 0.0 } ] }, \"plugin\": \"sad-dnn\", \"domain\": \"vtd-v1\" },","title":"Region Scorer Tasks"},{"location":"workflows.html#enrolled-classes","text":"To list the current classes (such as speakers for a SID task, or languages for LID) in a workflow, use the OliveWorkflow get_analysis_class_ids method: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> class_status_response = workflow . get_analysis_class_ids () >>> print ( \"Task Class IDs: {}\" . format ( json . dumps ( class_status_response . get_response_as_json (), indent = 1 ) )) Enrollment Class IDs : [ { \"job_name\" : \"SAD LID, SID\" , \"tasks\" : { \"SAD\" : { \"classId\" : [ \"speech\" ] }, \"LID\" : { \"classId\" : [ \"amh\" , \"apc\" , \"arb\" , \"arz\" , \"cmn\" , \"eng\" , \"fas\" , \"fre\" , \"jpn\" , \"kor\" , \"pus\" , \"rus\" , \"spa\" , \"tgl\" , \"tha\" , \"tur\" , \"urd\" , \"vie\" , \"yue\" ] }, \"SID\" : { \"classId\" : [ \"test speaker\" ] } } } ] Note that some tasks, such as SID, support enrollment, so the list of class IDs can can change over time as new enrollments are added.","title":"Enrolled Classes"},{"location":"workflows.html#multi-channel-audio","text":"The default workflow behavior is to merge multi-channel audio into a single channel, which is known as 'MONO' mode. To perform analysis on each channel instead of a merged channel, then the Workflow Definition must be authored with a data mode of 'SPLIT'. When using the split mode, each channel in a multi-channel audio input is \"split\" into a job. Here is a typical workflow that merges any multi-channel audio into a single channel audio input: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> json_ouput = workflow . get_analysis_task_info () >>> Print ( json . dumps ( json_output , indent = 1 )) [ { \"Preprocess Data\" : { \"minNumberInputs\" : 1 , \"maxNumberInputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessingRequired\" : true , \"resampleRate\" : 8000 , \"mode\" : \"MONO\" }, ... Here is a workflow that will split each channel in a multi-channel (stereo) audio input into separate jobs: >>> import olivepy.olive_async_client as oc >>> import olivepy.workflow as ow >>> client = oc . AsyncOliveClient ( \"example client\" ) >>> client . connect () >>> owd = ow . OliveWorkflowDefinition ( \"~/olive/multi_channel_sad_lid_sid.workflow\" ) >>> workflow = owd . create_workflow ( client ) >>> json_ouput = workflow . get_analysis_task_info () >>> Print ( json . dumps ( json_output , indent = 1 )) [ { \"Preprocess Data\" : { \"minNumberInputs\" : 1 , \"maxNumberInputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessingRequired\" : true , \"resampleRate\" : 8000 , \"mode\" : \"SPLIT\" }, ... The above Workflow Definition when applied to stereo file will produce a set of SAD, LID, and SID scores for each channel.","title":"Multi-channel Audio"},{"location":"workflows.html#adaption-using-a-workflow","text":"Not yet implemented via a Workflow. The adaption process is complicated, time-consuming, and plugin/domain specific. Until Adaptation via a workflow is implemented, please use the SRI provided Python client (olivepylearn) or Java client (OliveLearn) to perform adaptation. To adapt using the olivepylearn utility: olivepylearn --plugin sad-dnn --domain multi-v1 -a TEST_NEW_DOMAIN -i /olive/sadRegression/lists/adapt_s.lst Where that adapt_s.lst looks like this: /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav S 20.469 21.719 /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav NS 10.8000 10.8229 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav S 72.898 73.748 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav NS 42.754 43.010 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav S 79.437 80.427 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav NS 61.459 62.003 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav S 11.0438 111.638 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav NS 69.058 73.090 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav S 112.936 113.656 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav NS 83.046 83.114 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav S 16.940 20.050 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav NS 59.794 59.858 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav S 87.280 88.651 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav NS 82.229 82.461 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav S 111.346 111.936 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav NS 83.736 84.446 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav S 77.291 78.421 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav NS 0 4.951 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav S 30.349 32.429 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav NS 100.299 101.647 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav S 46.527 49.147 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav NS 44.747 46.148 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav S 24.551 25.471 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav NS 52.033 52.211 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav S 88.358 93.418 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav NS 46.564 46.788 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav S 10.507 11.077 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav NS 41.099 41.227 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav S 61.072 63.002 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav NS 19.108 19.460 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav S 97.182 97.789 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav NS 71.711 71.732 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav S 114.312 117.115 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav NS 31.065 31.154 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav S 90.346 91.608 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav NS 50.028 51.377 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav S 75.986 76.596 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav NS 12.485 12.709 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav S 72.719 73.046 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav NS 51.923 53.379 /olive//sadRegression/audio/adapt/20131213T104502UTC_18520_A.wav NS 11.1192 112.761 /olive//sadRegression/audio/adapt/20131213T121501UTC_18530_A.wav NS 81.277 82.766 /olive//sadRegression/audio/adapt/20131213T124501UTC_18533_A.wav NS 83.702 84.501 /olive//sadRegression/audio/adapt/20131213T134502UTC_18567_A.wav NS 69.379 72.258 /olive//sadRegression/audio/adapt/20131217T015001UTC_18707_A.wav NS 5.099 10.507 You may also incorporate the adapt code from olivepy.oliveclient directly into your client: # Setup processing variables (get this config or via command line options plugin = \"sad-dnn\" domain = \"multi-v1\" new_domain_name = \"python_adapted_multi-v2\" # Build the list of files plus the regions in the those files to adapt by parsing the input file: file_annotations = self . parse_annotation_file ( \"lists/adapt.lst\" ) return self . adapt_supervised_old ( plugin , domain , file_annotations , new_domain_name )","title":"Adaption Using a Workflow"},{"location":"workflows.html#python-workflow-example","text":"As previously mentioned, SRI distributes Workflow Definitions as files, which are preconfigured to perform tasks such as SAD, LID, SID, etc. The example below uses a Workflow Definition file with the SRI implemented Python Workflow API to make a SAD and LID request via a Workflow Definition file: import os , json , sys import olivepy.olive_async_client as oc import olivepy.workflow as ow import olivepy.messaging.msgutil as msgutil # Create a connection to a local OLIVE server client = oc . AsyncOliveClient ( \"test olive client\" , 'localhost' ) client . connect () # Example workflow definition file: workflow_filename = \"~/sad_lid.workflow\" workflow_def = ow . OliveWorkflowDefinition ( workflow_filename ) # Submit the workflow definition to the client for actualization (instantiation or sometimes called activation): workflow = workflow_def . create_workflow ( client ) # Prepare a audio file to submit (as a serialzied file) audio_filename = \"~/audio/test_audio.wav\" buffers = [] # read in the file as raw bytes so a buffer can be sent to the server as a buffer: buffer = workflow . serialize_audio ( audio_filename ) data_wrapper = workflow . package_audio ( buffer , label = audio_filename ) buffers . append ( data_wrapper ) # OR - if the server and client share a filesystem, uncomment the following to send the path to the audio file instead of a buffer: # data_wrapper = workflow.package_audio(audio_filename, mode=msgutil.AudioTransferType.AUDIO_PATH) # Submit the workflow to OLIVE for analysis: response = workflow . analyze ( buffers ) # response will contain an error message if the workflow failed, or results for each task: print ( \"Workflow results:\" ) # Use json.dumps to pretty print the json output: print ( \"{}\" . format ( json . dumps ( response . get_response_as_json (), indent = 1 ))) # Be sure to close the connection to the server when done client . disconnect () For the above code, one would see JSON formatted output like this (of course your output will vary by the audio submitted for analysis): Workflow results: [ { \"job_name\": \"Simple SAD and LID workflow\", \"data\": [ { \"dataId\": \"test_audio.wav\", \"msgType\": \"PREPROCESSED_AUDIO_RESULT\", \"mode\": \"MONO\", \"merged\": false, \"sampleRate\": 8000, \"durationSeconds\": 10.509625434875488, \"numberChannels\": 1, \"label\": \"~/audio/test_audio.wav\", \"id\": \"edd6f2d8abb69e754cc4bd03c2d3f60379255048fdb20a8924858c0723ea4bbd\" } ], \"tasks\": { \"SAD\": { \"taskTrait\": \"REGION_SCORER\", \"taskType\": \"SAD\", \"messageType\": \"REGION_SCORER_RESULT\", \"analysis\": { \"region\": [ { \"startT\": 3.3399999141693115, \"endT\": 10.5, \"classId\": \"speech\", \"score\": 0.0 } ] }, \"plugin\": \"sad-dnn\", \"domain\": \"vtd-v1\" }, \"LID\": { \"taskTrait\": \"GLOBAL_SCORER\", \"taskType\": \"LID\", \"messageType\": \"GLOBAL_SCORER_RESULT\", \"analysis\": { \"score\": [ { \"classId\": \"eng\", \"score\": 2.5195693969726562 }, { \"classId\": \"apc\", \"score\": -3.509812355041504 }, { \"classId\": \"tgl\", \"score\": -3.566483974456787 }, { \"classId\": \"arb\", \"score\": -3.994529962539673 }, { \"classId\": \"fre\", \"score\": -21.55596160888672 } ] }, \"plugin\": \"lid-embedplda\", \"domain\": \"multi-v1\" } } } ]","title":"Python Workflow Example"},{"location":"workflows.html#api-specification","text":"","title":"API Specification"},{"location":"workflows.html#oliveworkflowdefinition","text":"class olivepy.workflow.OliveWorkflowDefinition(filename: str) The class is used to load a Workflow Definition from a file. Parameters filename: the name of the workflow definition file","title":"OliveWorkflowDefinition"},{"location":"workflows.html#create_workflow","text":"create_workflow(client: olivepy.olive_async_client.AsyncOliveClient) Create a new Workflow for making OLIVE analysis, enrollment, or adaptation requests Parameters client: a valid, client connection to an OLIVE server Returns A new OliveWorkflow object, which has been actualized (activated) by the olive server and","title":"create_workflow()"},{"location":"workflows.html#get_json","text":"Create a JSON structure of the Workflow output Returns A JSON formated dictionary of the Workflow Definition","title":"get_json()"},{"location":"workflows.html#workflowexception","text":"exception olivepy.workflow.WorkflowException() Bases: Exception This exception means that an error occurred handling a Workflow","title":"WorkflowException"},{"location":"workflows.html#oliveworkflow","text":"class olivepy.workflow.OliveWorkflow(olive_async_client: olivepy.olive_async_client.AsyncOliveClient, actualized_workflow: olivepy.messaging.response.OliveWorkflowActualizedResponse) An OliveWorkflow instance represents a Workflow actualized by an OLIVE server, and is ready for analysis, enrollment, or adaptation. Parameters filename: the name of the workflow definition file Raises WorkflowException \u2013 If the workflow was not actualized","title":"OliveWorkflow"},{"location":"workflows.html#serialize_audio","text":"serialize_audio(filename: str) Read in an audio file as a serialized buffer. This is a helper utility for package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not yet been serialized Parameters filename: the local path to the file Returns The contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples","title":"serialize_audio()"},{"location":"workflows.html#analyze","text":"analyze(data_inputs: List[olive_pb2.WorkflowDataRequest], callback=None, options: Optional[Dict[str, str]] = None) Perform a workflow analysis Parameters data_inputs \u2013 a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. callback \u2013 an optional function callback that is invoked with the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when done. Otherwise, this method immediately returns and the callback method is invoked when the response is received. The call back function must accept 3 parameters (request, response, and error message) options \u2013 a dictionary of name/value option pairs to include with the analysis request Returns An OliveWorkflowAnalysisResponse if no callback provided","title":"analyze()"},{"location":"workflows.html#enroll","text":"enroll(data_inputs: List[olive_pb2.WorkflowDataRequest], class_id: str, task_names: List[str], callback=None, options=None) Submit data for enrollment. Parameters data_inputs \u2013 a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. class_id \u2013 the name of the enrollment task_names \u2013 a list of task names, where the audio is enrolled with tasks (NOTE only one task currently supported) that support enrollment. This value can be None, in which case the data input(s) is enrolled for each enrollable task. callback \u2013 an optional function callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. options \u2013 a dictionary of name/value option pairs to include with the analysis request Returns An OliveWorkflowEnrollmentResponse if no callback provided","title":"enroll()"},{"location":"workflows.html#get_analysis_class_ids","text":"get_analysis_class_ids(callback=None) Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc) Parameters callback - a callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) Returns An OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object","title":"get_analysis_class_ids()"},{"location":"workflows.html#get_analysis_task_info","text":"get_analysis_task_info() Reports information on the tasks used for analysis from the actualized workflow. When possible, this reports the plugins used in the workflow (although there can be cases when the final plugin/domain used is not known until runtime) Returns A list of dictionaries, where each dictionary contains information about an analysis task","title":"get_analysis_task_info()"},{"location":"workflows.html#get_analysis_tasks","text":"get_analysis_tasks(job_name: Optional[str] = None) Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) It is possible, although unlikely, that a Workflow could define two of the the same type of tasks in a job. The keys in the dictionary ensure that the tasks in a workflow have a unique name. Parameters job_name \u2013 the name of the analysis job. Optional since most workflows only support one analysis job and \u2018jobs; may be hidden from users in a future release. Returns a list of task names","title":"get_analysis_tasks()"},{"location":"workflows.html#get_enrollment_tasks","text":"get_enrollment_tasks(job_name: Optional[str] = None) Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) Parameters job_name \u2013 the name of the enrollment job. Optional since most workflows only support one job Returns a list of task names","title":"get_enrollment_tasks()"},{"location":"workflows.html#package_audio","text":"package_audio ( audio_data : AnyStr , mode =< AudioTransferType . AUDIO_SERIALIZED : 3 > , annotations : Optional [ List[Tuple[float, float ] ]] = None , task_annotations : Optional [ List[Tuple[float, float ] ]] = None , selected_channel : Optional [ int ] = None , num_channels : Optional [ int ] = None , sample_rate : Optional [ int ] = None , num_samples : Optional [ int ] = None , validate_local_path : bool = True , label = None ) Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request Parameters audio_data \u2013 the audio input is a string file path if mode is \u2018AUDIO_PATH\u2019, otherwise it should be a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in PCM_16 encoded samples mode \u2013 specifies how the audio is sent to the server: either as the filename of the audio input (file path must be valid for the server), send as a serialized buffer, or send PCM-16 encoded samples annotations \u2013 optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) task_annotations \u2013 optional regions (start/end regions in seconds) for a task. for example: {\u2018SHL\u2019: {\u2018speaker\u2019\u2019:[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the \u2018SHL\u2019 task, which are labeled for as class \u2018speaker\u2019 and has regions 0.5 to 4.5 and 6.8 to 9.2 (use get_analysis_tasks() to get the name of workflow tasks). selected_channel \u2013 optional - the channel to use in the workflow if the audio input is multi-channel num_channels \u2013 The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored sample_rate \u2013 The sample rate if audio input is a list of decoded (PCM-16) samples, if not usingma buffer of PCM-16 samples this is value is ignored num_samples \u2013 The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored validate_local_path \u2013 If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired label \u2013 an optional name to use with the audio Returns A populated WorkflowDataRequest to use in a workflow activity","title":"package_audio()"},{"location":"workflows.html#package_image","text":"package_image(image_input) Not yet supported","title":"package_image()"},{"location":"workflows.html#package_text","text":"package_text(text_input: str) Not yet supported","title":"package_text()"},{"location":"workflows.html#package_video","text":"package_video(video_input) Not yet supported","title":"package_video()"},{"location":"workflows.html#adapt","text":"adapt(data_input, callback, options=None, finalize=True) Not yet supported","title":"adapt()"},{"location":"plugins/aed-enrollable-v1.html","text":"aed-enrollable-v1.0.0 (Acoustic Event Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 v1.0.1 Various bug fixes and stability improvements, including fix to event/overlap deconfliction Description The goal of acoustic event detection is to identify and label enrolled sound classes within an audio file. This capability is designed to detect sound class regions within an audio file. Regions are discrete segments of the waveform where the event is detected. This plugin comes with a core set of enrolled classes: bird song dog bark door slamming restaurant noise traffic noise music wind gunshot explosion This plugin also allows the users to enroll new classes with example data similarly to speaker recognition. This capability is based on sound embeddings with Probabilistic Linear Discriminant Analysis (PLDA) backend and multi-class linear calibration and capable of enrolling new sound classes. This is the first release of this acoustic event detection (AED) plugin. This plugin detects and labels both discrete (things like gunshots and door slams) and continuous (things like music, winds, traffic, etc.) acoustic events. This plugin allows the detection of multiple sound classes within the same audio file or buffer, depending on the classes in the plugin. This plugin is capable of enrolling new sound classes from samples of audio. A minimum of five examples is needed to begin a reasonably effective model. Domains default-v1 Default domain meant for general-purpose audio conditions. Inputs For enrollment, an audio file or buffer with a corresponding sound labels. This assumes that the entire audio file is the event. To identify sub-regions of the file time annotations should also be provided using the standard methods. An example: Input-audio_1.wav MUSIC Input-audio_2.wav MUSIC Input-audio_3.wav DOG Input-audio_4.wav DOG Input-audio_5.wav TRAFFIC For scoring, an audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specified the entire audio file or buffer will be scored. Outputs The AED plugin returns a list of regions with a score for each detected classes. Regions are represented in seconds. Scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The list of results has a file name (or in the case of a buffer, an identifier), class label, a score for each detected sound class and the begin and end seconds of where the class was detected. The command-line interface output is formatted in this form: <file_name> <start _seconds> <end _seconds> <class> <score> An example output excerpt: audio.wav 0.00 1.49 GUNSHOT 4.38631201 audio.wav 2.45 7.49 MUSIC 3.77045155 audio.wav 8.70 10.74 DOOR 3.85196638 audio.wav 10.74 11.74 TRAFFIC 0.08894229 audio.wav 12.20 13.74 WIND 0.65049744 Enrollments This plugin allows class modifications. A class modification is essentially the capability to enroll a new class of acoustic event with sample(s) of a new sound class. A new sound enrollment is created with the class modification functionality, which consists of sending the system one or more audio samples from a sound class along with a label for that class. These samples must be passed in with annotations indicating where in the file the event is located, or the sample event must be manually segmented out by the users so the whole file submitted consists only of the event being enrolled. These enrollments can be augmented with subsequent class modification requests by adding more segmented audio with the same sound label. The same process can be used to augment an existing class (Example: BIRD, DOG, etc.) with new samples. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected event of interest and corresponding score for this event RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new event models or augment existing event models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Minimum Audio Length The system will only attempt to perform sound detection on audio segments longer than 1 second. Enrollment Limitations User can use a single file to enroll a new model, but multiple sound files of at least five or more waveforms are expected (more is better) to produce a better sound model. Enrolled Classes The initial enrolled classes are bird song, dog bark, door slamming, restaurant noise, traffic noise, music, wind, gunshot, and explosion. Global Options Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false detection segments. Reduce the threshold value if there are too many missed segments. 0.0 -5.0 to 5.0 min_output_seg Minimum output segment duration: Rejects all detections less than minimum output segment duration. 0.5 0.0 to 1.0","title":"Acoustic Event Detection (AED)"},{"location":"plugins/aed-enrollable-v1.html#aed-enrollable-v100-acoustic-event-detection","text":"","title":"aed-enrollable-v1.0.0 (Acoustic Event Detection)"},{"location":"plugins/aed-enrollable-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 v1.0.1 Various bug fixes and stability improvements, including fix to event/overlap deconfliction","title":"Version Changelog"},{"location":"plugins/aed-enrollable-v1.html#description","text":"The goal of acoustic event detection is to identify and label enrolled sound classes within an audio file. This capability is designed to detect sound class regions within an audio file. Regions are discrete segments of the waveform where the event is detected. This plugin comes with a core set of enrolled classes: bird song dog bark door slamming restaurant noise traffic noise music wind gunshot explosion This plugin also allows the users to enroll new classes with example data similarly to speaker recognition. This capability is based on sound embeddings with Probabilistic Linear Discriminant Analysis (PLDA) backend and multi-class linear calibration and capable of enrolling new sound classes. This is the first release of this acoustic event detection (AED) plugin. This plugin detects and labels both discrete (things like gunshots and door slams) and continuous (things like music, winds, traffic, etc.) acoustic events. This plugin allows the detection of multiple sound classes within the same audio file or buffer, depending on the classes in the plugin. This plugin is capable of enrolling new sound classes from samples of audio. A minimum of five examples is needed to begin a reasonably effective model.","title":"Description"},{"location":"plugins/aed-enrollable-v1.html#domains","text":"default-v1 Default domain meant for general-purpose audio conditions.","title":"Domains"},{"location":"plugins/aed-enrollable-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding sound labels. This assumes that the entire audio file is the event. To identify sub-regions of the file time annotations should also be provided using the standard methods. An example: Input-audio_1.wav MUSIC Input-audio_2.wav MUSIC Input-audio_3.wav DOG Input-audio_4.wav DOG Input-audio_5.wav TRAFFIC For scoring, an audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specified the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/aed-enrollable-v1.html#outputs","text":"The AED plugin returns a list of regions with a score for each detected classes. Regions are represented in seconds. Scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The list of results has a file name (or in the case of a buffer, an identifier), class label, a score for each detected sound class and the begin and end seconds of where the class was detected. The command-line interface output is formatted in this form: <file_name> <start _seconds> <end _seconds> <class> <score> An example output excerpt: audio.wav 0.00 1.49 GUNSHOT 4.38631201 audio.wav 2.45 7.49 MUSIC 3.77045155 audio.wav 8.70 10.74 DOOR 3.85196638 audio.wav 10.74 11.74 TRAFFIC 0.08894229 audio.wav 12.20 13.74 WIND 0.65049744","title":"Outputs"},{"location":"plugins/aed-enrollable-v1.html#enrollments","text":"This plugin allows class modifications. A class modification is essentially the capability to enroll a new class of acoustic event with sample(s) of a new sound class. A new sound enrollment is created with the class modification functionality, which consists of sending the system one or more audio samples from a sound class along with a label for that class. These samples must be passed in with annotations indicating where in the file the event is located, or the sample event must be manually segmented out by the users so the whole file submitted consists only of the event being enrolled. These enrollments can be augmented with subsequent class modification requests by adding more segmented audio with the same sound label. The same process can be used to augment an existing class (Example: BIRD, DOG, etc.) with new samples.","title":"Enrollments"},{"location":"plugins/aed-enrollable-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected event of interest and corresponding score for this event RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new event models or augment existing event models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/aed-enrollable-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/aed-enrollable-v1.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/aed-enrollable-v1.html#minimum-audio-length","text":"The system will only attempt to perform sound detection on audio segments longer than 1 second.","title":"Minimum Audio Length"},{"location":"plugins/aed-enrollable-v1.html#enrollment-limitations","text":"User can use a single file to enroll a new model, but multiple sound files of at least five or more waveforms are expected (more is better) to produce a better sound model.","title":"Enrollment Limitations"},{"location":"plugins/aed-enrollable-v1.html#enrolled-classes","text":"The initial enrolled classes are bird song, dog bark, door slamming, restaurant noise, traffic noise, music, wind, gunshot, and explosion.","title":"Enrolled Classes"},{"location":"plugins/aed-enrollable-v1.html#global-options","text":"Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false detection segments. Reduce the threshold value if there are too many missed segments. 0.0 -5.0 to 5.0 min_output_seg Minimum output segment duration: Rejects all detections less than minimum output segment duration. 0.5 0.0 to 1.0","title":"Global Options"},{"location":"plugins/aln-waveformAlignment-v1.html","text":"aln-waveformAlignment-v1 (Waveform Alignment) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Core behavior This plugin temporally aligns a group of waveforms of different durations, to a common reference waveform. The approach was developed based on the idea that there is a reference event in a scene (yelling, gunshot, accident, etc.) in a reference file, and the user would like to align other files to this event. Hence, we assume that the default case is where the reference is shorter than the other files being searched. In the core cases the system thus uses the first input waveform as the reference and generates a temporal offset for each of the other (\"target\") waveforms, if they can be aligned. In the exceptional case, the reference waveform is shorter than a target, then this comparison is aligned with respect to the target start time. Thus, if the beginning of the reference aligns with 50 seconds from the start of the target, the alignment is returned as a \"50\". This is depicted in the figure below, where each \"-\" is 10 seconds: XXXXXXXXXXXXXX Reference -----XXXXXXXXXXXXXX----- Target If the reference is longer than the target then the alignment is reversed, and the results are returned back as negative numbers. The figure below would return \"-50\". Returning this as a negative number is just a convention to indicate how the file should be aligned, results are always offset from the start of the longer file. -----XXXXXXXXXXXXXX----- Reference XXXXXXXXXXXXXX Target For cases where two waveforms are found to align with the same start time, a \"0\" is returned. The alignment of the files is calculated using the highest correlation of the candidate alignments beyond a threshold. If the two files never align with a correlation greater than the threshold no results are returned for that comparison. Thus one may pass in five targets and a reference and receive back less than five results. In the default case (when the reference is smaller) the reference waveform (first in the list) is swept across each target waveform in two-second increments. At each position, the correlation value is recorded. The same processing is applied to each target window. In cases where the target is shorter, the same process is performed using the target. Then, the index of the maximum correlation value that crosses the detection threshold is used to indicate where the reference file aligns to the different targets. Each detection uses the correlation value itself as a proxy for confidence. Annotations Input files may also be accompanied by \"annotations\" that indicate those regions the plugin should process, ignoring other parts of the waveform. This is useful to allow the user to specify a short region in the reference file containing an event of interest that should be searched for in the other files. In the case where annotations are used they must be provided or all files. In general we would expect that the reference waveform would have an annotated region that is relatively short and that the annotations for the target files would simply indicate their start to end times, though this does not have to be the case. wave1.wav 130.5 135.8 (reference) wave2.wav 0 150.9 (target) wave3.wav 0 180.0 (target) wave4.wav 0 254.2 (target) Annotations are optional, but, as noted, when annotations are used they must be provided for all files in the call. Domains default-v1 Default processing domain - performs alignment using correlation values. Inputs The input is a group of audio files, which is different from most OLIVE plugins. The first file is treated as the event, or reference, that must be found in the other longer files. Outputs The plugin returns an estimated lag for each unique pair of the reference waveform with every other waveform. That is, for every (reference, target) pair it returns the relative lag in seconds where reference occurs in target along with a confidence score. referenceFile.wav file1.wav -2.47 0.776 referenceFile.wav file2.wav 7.98 0.84 referenceFile.wav file3.wav 15.91 0.49 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AudioAlignmentPlugin \u2013 First implementation of a plugin that can take multiple input waveforms at the same time. Compatibility OLIVE 5.1+ Limitations If the reference is not alignable with a target, nothing is returned to the user for this comparison. If none of the waveforms can be aligned then nothing is returned. As noted above, if the reference waveform is longer than a target, then the estimated lag will be negative and based off the start time of the longer waveform. Hence \"50\" means that the reference file is aligned with the point 50 seconds from the start of the target waveform. \"-50\" means that the target file is aligned with the point 50 seconds from the start of the reference waveform. In cases where the two waveforms align in such a way that that the very start of the reference waveform aligns with the very end of the target, the attempt to align may fail. This is a know bug, and is unlikely to arise when the reference event is short in duration compared with the target waveform being searched. The figure below is likely to fail to produce an alignment, due to algorithmic limitation that can be overcome with future work to pad the detection process. XXX-------------- --------------XXX The case in the figure below will succeed to align, since no padding is required. This behavior is an artifact of the understanding that users would generally be searching longer target files with limited duration reference events, which may not always be the case. XXX --------------XXX Comments Currently, the plugin uses the first input waveform in the list as the reference event to localize. If the reference is not found in a target, then the user is notified of this via a message. The following message will appear: `'Warning: Reference \"{REF_NAME}\" was not found in Target \"{TARGET_NAME}\" at detection threshold={THRESHOLD}.'` Where the values in {} will be replaced with the appropriate waveform names and internal plugin threshold. Global Options Option Name Description Default Expected Range threshold Threshold for signal energy detection. Has only been tested and verified with default value. 0.1 0 to 1","title":"Waveform Alignment (ALN)"},{"location":"plugins/aln-waveformAlignment-v1.html#aln-waveformalignment-v1-waveform-alignment","text":"","title":"aln-waveformAlignment-v1 (Waveform Alignment)"},{"location":"plugins/aln-waveformAlignment-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/aln-waveformAlignment-v1.html#description","text":"","title":"Description"},{"location":"plugins/aln-waveformAlignment-v1.html#core-behavior","text":"This plugin temporally aligns a group of waveforms of different durations, to a common reference waveform. The approach was developed based on the idea that there is a reference event in a scene (yelling, gunshot, accident, etc.) in a reference file, and the user would like to align other files to this event. Hence, we assume that the default case is where the reference is shorter than the other files being searched. In the core cases the system thus uses the first input waveform as the reference and generates a temporal offset for each of the other (\"target\") waveforms, if they can be aligned. In the exceptional case, the reference waveform is shorter than a target, then this comparison is aligned with respect to the target start time. Thus, if the beginning of the reference aligns with 50 seconds from the start of the target, the alignment is returned as a \"50\". This is depicted in the figure below, where each \"-\" is 10 seconds: XXXXXXXXXXXXXX Reference -----XXXXXXXXXXXXXX----- Target If the reference is longer than the target then the alignment is reversed, and the results are returned back as negative numbers. The figure below would return \"-50\". Returning this as a negative number is just a convention to indicate how the file should be aligned, results are always offset from the start of the longer file. -----XXXXXXXXXXXXXX----- Reference XXXXXXXXXXXXXX Target For cases where two waveforms are found to align with the same start time, a \"0\" is returned. The alignment of the files is calculated using the highest correlation of the candidate alignments beyond a threshold. If the two files never align with a correlation greater than the threshold no results are returned for that comparison. Thus one may pass in five targets and a reference and receive back less than five results. In the default case (when the reference is smaller) the reference waveform (first in the list) is swept across each target waveform in two-second increments. At each position, the correlation value is recorded. The same processing is applied to each target window. In cases where the target is shorter, the same process is performed using the target. Then, the index of the maximum correlation value that crosses the detection threshold is used to indicate where the reference file aligns to the different targets. Each detection uses the correlation value itself as a proxy for confidence.","title":"Core behavior"},{"location":"plugins/aln-waveformAlignment-v1.html#annotations","text":"Input files may also be accompanied by \"annotations\" that indicate those regions the plugin should process, ignoring other parts of the waveform. This is useful to allow the user to specify a short region in the reference file containing an event of interest that should be searched for in the other files. In the case where annotations are used they must be provided or all files. In general we would expect that the reference waveform would have an annotated region that is relatively short and that the annotations for the target files would simply indicate their start to end times, though this does not have to be the case. wave1.wav 130.5 135.8 (reference) wave2.wav 0 150.9 (target) wave3.wav 0 180.0 (target) wave4.wav 0 254.2 (target) Annotations are optional, but, as noted, when annotations are used they must be provided for all files in the call.","title":"Annotations"},{"location":"plugins/aln-waveformAlignment-v1.html#domains","text":"default-v1 Default processing domain - performs alignment using correlation values.","title":"Domains"},{"location":"plugins/aln-waveformAlignment-v1.html#inputs","text":"The input is a group of audio files, which is different from most OLIVE plugins. The first file is treated as the event, or reference, that must be found in the other longer files.","title":"Inputs"},{"location":"plugins/aln-waveformAlignment-v1.html#outputs","text":"The plugin returns an estimated lag for each unique pair of the reference waveform with every other waveform. That is, for every (reference, target) pair it returns the relative lag in seconds where reference occurs in target along with a confidence score. referenceFile.wav file1.wav -2.47 0.776 referenceFile.wav file2.wav 7.98 0.84 referenceFile.wav file3.wav 15.91 0.49","title":"Outputs"},{"location":"plugins/aln-waveformAlignment-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AudioAlignmentPlugin \u2013 First implementation of a plugin that can take multiple input waveforms at the same time.","title":"Functionality (Traits)"},{"location":"plugins/aln-waveformAlignment-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/aln-waveformAlignment-v1.html#limitations","text":"If the reference is not alignable with a target, nothing is returned to the user for this comparison. If none of the waveforms can be aligned then nothing is returned. As noted above, if the reference waveform is longer than a target, then the estimated lag will be negative and based off the start time of the longer waveform. Hence \"50\" means that the reference file is aligned with the point 50 seconds from the start of the target waveform. \"-50\" means that the target file is aligned with the point 50 seconds from the start of the reference waveform. In cases where the two waveforms align in such a way that that the very start of the reference waveform aligns with the very end of the target, the attempt to align may fail. This is a know bug, and is unlikely to arise when the reference event is short in duration compared with the target waveform being searched. The figure below is likely to fail to produce an alignment, due to algorithmic limitation that can be overcome with future work to pad the detection process. XXX-------------- --------------XXX The case in the figure below will succeed to align, since no padding is required. This behavior is an artifact of the understanding that users would generally be searching longer target files with limited duration reference events, which may not always be the case. XXX --------------XXX","title":"Limitations"},{"location":"plugins/aln-waveformAlignment-v1.html#comments","text":"Currently, the plugin uses the first input waveform in the list as the reference event to localize. If the reference is not found in a target, then the user is notified of this via a message. The following message will appear: `'Warning: Reference \"{REF_NAME}\" was not found in Target \"{TARGET_NAME}\" at detection threshold={THRESHOLD}.'` Where the values in {} will be replaced with the appropriate waveform names and internal plugin threshold.","title":"Comments"},{"location":"plugins/aln-waveformAlignment-v1.html#global-options","text":"Option Name Description Default Expected Range threshold Threshold for signal energy detection. Has only been tested and verified with default value. 0.1 0 to 1","title":"Global Options"},{"location":"plugins/asr-dynapy-v2.html","text":"asr-dynapy-v2 (Automatic Speech Recognition) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is functionally identical to its predecessor,asr-dynapy-v1, but has been converted to be compatible with OLIVE 5.x. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks(TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models. Domains cmn-tdnnChain-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. eng-tdnnChain-tel-v1 English domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. fas-tdnn-tel-v1 Farsi domain focused on conversational telephony speech. This domain features non-chain TDNN networks, and reports word posterior scores. rus-tdnnChain-tel-v1 Russian domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. It also features a very large and complex language model that makes it quite memory-intensive to use. See the \"Resources\" Limitation listed below for more information. spa-tdnnChain-tel-v1 Spanish domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Out of Vocabulary (OOV) Words, Names The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications. Confidences vs Word Posteriors Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release - so there may be transcription errors resulting from these audio chunk break points. Resources The Russian domain, rus-tdnnChain-tel-v1, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain. Arabic Script Languages Note that for the Farsi domain, fas-tdnn-tel-v1, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Non-Verbal Recognizer Output Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0","title":"Automatic Speech Recognition (ASR)"},{"location":"plugins/asr-dynapy-v2.html#asr-dynapy-v2-automatic-speech-recognition","text":"","title":"asr-dynapy-v2 (Automatic Speech Recognition)"},{"location":"plugins/asr-dynapy-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/asr-dynapy-v2.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is functionally identical to its predecessor,asr-dynapy-v1, but has been converted to be compatible with OLIVE 5.x. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks(TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models.","title":"Description"},{"location":"plugins/asr-dynapy-v2.html#domains","text":"cmn-tdnnChain-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. eng-tdnnChain-tel-v1 English domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. fas-tdnn-tel-v1 Farsi domain focused on conversational telephony speech. This domain features non-chain TDNN networks, and reports word posterior scores. rus-tdnnChain-tel-v1 Russian domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. It also features a very large and complex language model that makes it quite memory-intensive to use. See the \"Resources\" Limitation listed below for more information. spa-tdnnChain-tel-v1 Spanish domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores.","title":"Domains"},{"location":"plugins/asr-dynapy-v2.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-dynapy-v2.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Outputs"},{"location":"plugins/asr-dynapy-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-dynapy-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/asr-dynapy-v2.html#limitations","text":"As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-dynapy-v2.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-dynapy-v2.html#out-of-vocabulary-oov-words-names","text":"The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications.","title":"Out of Vocabulary (OOV) Words, Names"},{"location":"plugins/asr-dynapy-v2.html#confidences-vs-word-posteriors","text":"Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores.","title":"Confidences vs Word Posteriors"},{"location":"plugins/asr-dynapy-v2.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release - so there may be transcription errors resulting from these audio chunk break points.","title":"Overlap"},{"location":"plugins/asr-dynapy-v2.html#resources","text":"The Russian domain, rus-tdnnChain-tel-v1, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain.","title":"Resources"},{"location":"plugins/asr-dynapy-v2.html#arabic-script-languages","text":"Note that for the Farsi domain, fas-tdnn-tel-v1, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Arabic Script Languages"},{"location":"plugins/asr-dynapy-v2.html#non-verbal-recognizer-output","text":"Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations.","title":"Non-Verbal Recognizer Output"},{"location":"plugins/asr-dynapy-v2.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-dynapy-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-dynapy-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/dia-hybrid-v2.html","text":"dia-hybrid-v2 (Speaker Diarization) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1, but updated to be compatible with 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Speaker Diarization plugins segment the submitted audio to determine 'who spoke when.' In constrast to SDD plugins, there is no enrollment functionality or any concept of 'speakers of interest' or 'target speakers.' Instead, speaker clusters are defined automatically with clas names such as 'spk1', 'spk2', etc. This plugin is based on Variational Bayes Diarization in an i-vector space defined by SRI\u2019s hybrid alignment framework, which is powered by deep neural network bottleneck features. Domains multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Inputs An audio file or buffer to be scored. Outputs In the basic case, Diarization returns a list of regions labelled with \u201cspkN\u201d. \u2018N\u2019 is an integer denoting an unknown speaker, and the maximum N is the total number of unknown speakers in the file. All regions of speaker N are deemed to be spoken by the same speaker. Regions are represented in seconds. The 'score' field is reported as -100.0; this is not a confidence value, but a placeholder to maintain output format compatibility with other region-scoring OLIVE plugins. input-audio.wav 0.470 8.210 spk3 1.0 input-audio.wav 8.320 13.110 spk4 1.0 input-audio.wav 13.280 29.960 spk3 1.0 input-audio.wav 30.350 32.030 spk3 1.0 input-audio.wav 32.310 46.980 spk1 1.0 input-audio.wav 47.790 51.120 spk2 1.0 input-audio.wav 51.360 54.290 spk3 1.0 input-audio.wav 54.340 55.400 spk2 1.0 input-audio.wav 55.550 58.790 spk2 1.0 input-audio.wav 58.820 76.340 spk1 1.0 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations Diarization plugins perform their clustering blind of any specific speaker knowledge. These plugins have no concept of individual speakers of interest or enrolled speaker models. Therefore, the only labels that will come back from the plugin will be 'spk1', 'spk2', and so on, corresponding to proposed individual speakers within the audio. If your target use case involves searching for a known individual speaker or speaker(s), consider using a speaker detection or speaker identification plugin instead. Speed and Memory Usage The current approach to diarization is exceptionally slow (close to real-time). Work is on-going to replace the Variational Bayes Diarization approach with a segmentation by classification approach that will combine segmentation and speaker detection into a single stage and improve system speed. Speaker Persistence and Labeling Across Files The definition of spkN for one processing instance is not retained for use in other processing instances. For instance, spk1 in fileA is not necessarily the same as spk1 in fileB. If speaker persistence and inter-file label consistency is required, please consider speaker detection technology instead. Minimum Speech Duration The system will only attempt to perform speaker diarization on submitted audio if there is more than 5 seconds of total speech detected in the file or buffer. In addition, only segments that are 2 seconds or longer will be considered for clustering and given a diarization score and proposed speaker label. Maximum number of speakers Currently, the plugin is configured to differentiate a maximum of 6 distinct/unique speakers within any scored audio. If you need to distinguish between a larger set of unknown speakers within files, or if 6 speakers is more than you ever expect to see for your given audio conditions, contact SRI for ways that we can maximize performance for your use case. Comments Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Speaker Diarization (DIA)"},{"location":"plugins/dia-hybrid-v2.html#dia-hybrid-v2-speaker-diarization","text":"","title":"dia-hybrid-v2 (Speaker Diarization)"},{"location":"plugins/dia-hybrid-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1, but updated to be compatible with 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/dia-hybrid-v2.html#description","text":"Speaker Diarization plugins segment the submitted audio to determine 'who spoke when.' In constrast to SDD plugins, there is no enrollment functionality or any concept of 'speakers of interest' or 'target speakers.' Instead, speaker clusters are defined automatically with clas names such as 'spk1', 'spk2', etc. This plugin is based on Variational Bayes Diarization in an i-vector space defined by SRI\u2019s hybrid alignment framework, which is powered by deep neural network bottleneck features.","title":"Description"},{"location":"plugins/dia-hybrid-v2.html#domains","text":"multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/dia-hybrid-v2.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/dia-hybrid-v2.html#outputs","text":"In the basic case, Diarization returns a list of regions labelled with \u201cspkN\u201d. \u2018N\u2019 is an integer denoting an unknown speaker, and the maximum N is the total number of unknown speakers in the file. All regions of speaker N are deemed to be spoken by the same speaker. Regions are represented in seconds. The 'score' field is reported as -100.0; this is not a confidence value, but a placeholder to maintain output format compatibility with other region-scoring OLIVE plugins. input-audio.wav 0.470 8.210 spk3 1.0 input-audio.wav 8.320 13.110 spk4 1.0 input-audio.wav 13.280 29.960 spk3 1.0 input-audio.wav 30.350 32.030 spk3 1.0 input-audio.wav 32.310 46.980 spk1 1.0 input-audio.wav 47.790 51.120 spk2 1.0 input-audio.wav 51.360 54.290 spk3 1.0 input-audio.wav 54.340 55.400 spk2 1.0 input-audio.wav 55.550 58.790 spk2 1.0 input-audio.wav 58.820 76.340 spk1 1.0","title":"Outputs"},{"location":"plugins/dia-hybrid-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/dia-hybrid-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/dia-hybrid-v2.html#limitations","text":"Diarization plugins perform their clustering blind of any specific speaker knowledge. These plugins have no concept of individual speakers of interest or enrolled speaker models. Therefore, the only labels that will come back from the plugin will be 'spk1', 'spk2', and so on, corresponding to proposed individual speakers within the audio. If your target use case involves searching for a known individual speaker or speaker(s), consider using a speaker detection or speaker identification plugin instead.","title":"Limitations"},{"location":"plugins/dia-hybrid-v2.html#speed-and-memory-usage","text":"The current approach to diarization is exceptionally slow (close to real-time). Work is on-going to replace the Variational Bayes Diarization approach with a segmentation by classification approach that will combine segmentation and speaker detection into a single stage and improve system speed.","title":"Speed and Memory Usage"},{"location":"plugins/dia-hybrid-v2.html#speaker-persistence-and-labeling-across-files","text":"The definition of spkN for one processing instance is not retained for use in other processing instances. For instance, spk1 in fileA is not necessarily the same as spk1 in fileB. If speaker persistence and inter-file label consistency is required, please consider speaker detection technology instead.","title":"Speaker Persistence and Labeling Across Files"},{"location":"plugins/dia-hybrid-v2.html#minimum-speech-duration","text":"The system will only attempt to perform speaker diarization on submitted audio if there is more than 5 seconds of total speech detected in the file or buffer. In addition, only segments that are 2 seconds or longer will be considered for clustering and given a diarization score and proposed speaker label.","title":"Minimum Speech Duration"},{"location":"plugins/dia-hybrid-v2.html#maximum-number-of-speakers","text":"Currently, the plugin is configured to differentiate a maximum of 6 distinct/unique speakers within any scored audio. If you need to distinguish between a larger set of unknown speakers within files, or if 6 speakers is more than you ever expect to see for your given audio conditions, contact SRI for ways that we can maximize performance for your use case.","title":"Maximum number of speakers"},{"location":"plugins/dia-hybrid-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/dia-hybrid-v2.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/env-indoorOutdoor-v1.html","text":"env-indoorOutdoor-v1.0.0 (Environmental Analysis - Indoor vs. Outdoor) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description The goal of indoor-outdoor acoustic environment detection capability is to determine if a given audio segment was collected indoors or outdoors. This plug-in provides a global score for the entire segment. The plugin is based on sound embeddings with a CPLDA backend and binary calibration. This is the first release of indoor-outdoor acoustic environment detection plugin. This plugin provides a global score indicating whether a given audio segment was collected indoors or outdoors. This capability is based on sound embeddings powered by CPLDA backend and binary calibration Domains default-v1 Default domain meant for general-purpose audio conditions. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs Returns score for the detected sound class based on the entire wave form passed in. Scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. If none of the scores crosses the threshold, it will be deemed as no-decision and there will be no output. The list of results has a file name (or in the case of a buffer, an identifier), class label, a score for each detected sound class. An example output excerpt: Input-audio-1.wav INDOOR 1.2345 Input-audio-2.wav OUTDOOR 2.003 Input-audio-3.wav OUTDOOR 0.5643 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single label for the entire audio. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations Minimum Audio Length A minimum duration of 1 second audio file is required in order to produce any meaningful detection. This plugin does not allow any class modification, so users may not alter the behavior by adding data through adaptation of class augmentation. Global Options Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false detection segments. Reduce the threshold value if there are too many missed segments. 0.0 -5.0 to 5.0","title":"Environmental Analysis (ENV) Indoor/Outdoor"},{"location":"plugins/env-indoorOutdoor-v1.html#env-indooroutdoor-v100-environmental-analysis-indoor-vs-outdoor","text":"","title":"env-indoorOutdoor-v1.0.0 (Environmental Analysis - Indoor vs. Outdoor)"},{"location":"plugins/env-indoorOutdoor-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/env-indoorOutdoor-v1.html#description","text":"The goal of indoor-outdoor acoustic environment detection capability is to determine if a given audio segment was collected indoors or outdoors. This plug-in provides a global score for the entire segment. The plugin is based on sound embeddings with a CPLDA backend and binary calibration. This is the first release of indoor-outdoor acoustic environment detection plugin. This plugin provides a global score indicating whether a given audio segment was collected indoors or outdoors. This capability is based on sound embeddings powered by CPLDA backend and binary calibration","title":"Description"},{"location":"plugins/env-indoorOutdoor-v1.html#domains","text":"default-v1 Default domain meant for general-purpose audio conditions.","title":"Domains"},{"location":"plugins/env-indoorOutdoor-v1.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/env-indoorOutdoor-v1.html#outputs","text":"Returns score for the detected sound class based on the entire wave form passed in. Scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. If none of the scores crosses the threshold, it will be deemed as no-decision and there will be no output. The list of results has a file name (or in the case of a buffer, an identifier), class label, a score for each detected sound class. An example output excerpt: Input-audio-1.wav INDOOR 1.2345 Input-audio-2.wav OUTDOOR 2.003 Input-audio-3.wav OUTDOOR 0.5643","title":"Outputs"},{"location":"plugins/env-indoorOutdoor-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single label for the entire audio. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/env-indoorOutdoor-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/env-indoorOutdoor-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/env-indoorOutdoor-v1.html#minimum-audio-length","text":"A minimum duration of 1 second audio file is required in order to produce any meaningful detection. This plugin does not allow any class modification, so users may not alter the behavior by adding data through adaptation of class augmentation.","title":"Minimum Audio Length"},{"location":"plugins/env-indoorOutdoor-v1.html#global-options","text":"Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false detection segments. Reduce the threshold value if there are too many missed segments. 0.0 -5.0 to 5.0","title":"Global Options"},{"location":"plugins/env-multiClass-v2.html","text":"env-multiClass-v2 (Environmental Analysis) Version Changelog Plugin Version Change v2.0.0 Initial plugin release with OLIVE 5.1.0, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.1.0 Description ENV plugins are designed to analyze submitted audio to glean information from the audio. Often not necessarily meant for detecting specific events, languages, or speakers, these plugins are instead pulling information about the environment or conditions of the audio recording, like the noise type(s) and level, codec, background conditions, reverberation, or other such characteristics. The specific classes that are reported will vary from plugin-to-plugin, so for more information regarding these, refer below. This plugin extracts information regarding the acoustic conditions and content of the audio file being analyzed, such as language, codec, SNR levels, etc. This plugin features: Model Ensemble The plugin leverages 7 systems for extracting 40 subclasses of information related to Language, Codec, SNR, Emotion, Reverb, Channel and Gender. Scaled Scores To faciltate interpretation, all scores for a given class (out of 7) are scaled to sum to 1.0. An exception is for Language due to the scores being LLRs and accounting for unknown languages in the model. Supported Classes The table below shows each class and the associated subclasses that this plugin reports scores for. Class Subclasses Channel ac (Acoustic projection via a mobile phone recording of a speakerphone voice output mic (Microphone) tel (Telephone) Codec aac amrnb clean (Uncompressed) g723_1 g726 gsm ilbc mp3 opus real144 speex vorbis Emotion A (Aroused) H (Happy) N (Neutral) Gender f (Female) m (Male) Language amh (Amharic) ara (Arabic) cmn (Chinese Mandarin) eng (English) fas (Farsi) fre (French) hau (Hausa) jpn (Japanese) kor (Korean) pus (Pushto) rus (Russian) spa (Spanish) tur (Turkish) urd (Urdu) vie (Vietnamese) Reverb nml (Limited to no reverberation) rev (Moderate to high levels of reverberation) SNR clean (SNR better than 30dB) snr20db (SNR of approximately 20dB) snr08db (SNR of 8dB or lower) Domains multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. Inputs Audio file or buffer and an optional identifier. Outputs Generally, a list of scores for all classes in the domain, for the entire segment. Unlike with SAD and SID, where scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection, most of the classes within an ENV plugin are set up differently. Some ENV plugins, like this one will have a 'parent' superclass that has several subclasses within it. For example, the Gender superclass has subclasses of m (male) and f (female); the Channel superclass may have subclasses of mic (microphone), tel (telephone), and ac (acoustic coupling). When the plugin reports its results, these classes are reported as: Gender-f <score> Gender-m <score> and Channel-ac <score> Channel-mic <score> Channel-tel <score> respectively. Note that within many of these superclasses, the scores are scaled so that the sum of all subclasses within the superclass will sum to one, to make the scores more legible and easier to interpret. In these cases, the highest score represents the most likely subclass for that category. An example output excerpt: input-audio.wav Channel-ac 0.0 input-audio.wav Channel-mic 1.0 input-audio.wav Channel-tel 0.0 input-audio.wav Codec-aac 0.0 input-audio.wav Codec-amrnb 0.0 input-audio.wav Codec-clean 1.0 input-audio.wav Codec-g723_1 0.0 input-audio.wav Codec-g726 0.0 input-audio.wav Codec-gsm 0.0 input-audio.wav Codec-ilbc 0.0 input-audio.wav Codec-mp3 0.0 input-audio.wav Codec-opus 0.0 input-audio.wav Codec-real144 0.0 input-audio.wav Codec-speex 0.0 input-audio.wav Codec-vorbis 0.0 input-audio.wav Emotion-A 0.0 input-audio.wav Emotion-H 0.0 input-audio.wav Emotion-N 0.9999999 input-audio.wav Gender-f 0.0 input-audio.wav Gender-m 1.0 input-audio.wav Language-amh 0.0 input-audio.wav Language-ara 0.0 input-audio.wav Language-cmn 0.0 input-audio.wav Language-eng 0.99999642 input-audio.wav Language-fas 0.0 input-audio.wav Language-fre 0.0040021786 input-audio.wav Language-hau 0.0 input-audio.wav Language-jpn 0.0 input-audio.wav Language-kor 0.0 input-audio.wav Language-pus 0.0 input-audio.wav Language-rus 0.0 input-audio.wav Language-spa 0.00023994729 input-audio.wav Language-tur 0.0 input-audio.wav Language-urd 0.0 input-audio.wav Language-vie 0.0 input-audio.wav Reverb-nml 1.0 input-audio.wav Reverb-rev 0.0 input-audio.wav SNR-clean 0.90663459 input-audio.wav SNR-snr08db 0.0 input-audio.wav SNR-snr20db 0.093361041 Functionality ( Traits The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Changing Audio Conditions This plugin is a global scoring plugin. The scores reported will represent a summarization of the entire submitted audio buffer or file with respect to each of the classes in the plugin. If conditions change, or there are multiple speakers, or multiple languages spoken, if the environment changes in any way, it may be very difficult or impossible for these changes to be captured and represented by the results from this plugin. Comments Calibration This plugin applies calibration only to the scores from the Language class. All other classes have the sum of scores for that class scaled to sum to 1.0. Subsetting Classes If only a limited number of the metadata classes are of interest, it is possible to subset the report back from the plugin by passing these classes as --ids with the scoring request. Refer to the Command Line Interface or API Documentation for instructions on how to do this, depending on how you are interacting with the plugin. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 0.0 to 1.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"Environmental Analysis (ENV) Multi Class"},{"location":"plugins/env-multiClass-v2.html#env-multiclass-v2-environmental-analysis","text":"","title":"env-multiClass-v2 (Environmental Analysis)"},{"location":"plugins/env-multiClass-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release with OLIVE 5.1.0, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/env-multiClass-v2.html#description","text":"ENV plugins are designed to analyze submitted audio to glean information from the audio. Often not necessarily meant for detecting specific events, languages, or speakers, these plugins are instead pulling information about the environment or conditions of the audio recording, like the noise type(s) and level, codec, background conditions, reverberation, or other such characteristics. The specific classes that are reported will vary from plugin-to-plugin, so for more information regarding these, refer below. This plugin extracts information regarding the acoustic conditions and content of the audio file being analyzed, such as language, codec, SNR levels, etc. This plugin features: Model Ensemble The plugin leverages 7 systems for extracting 40 subclasses of information related to Language, Codec, SNR, Emotion, Reverb, Channel and Gender. Scaled Scores To faciltate interpretation, all scores for a given class (out of 7) are scaled to sum to 1.0. An exception is for Language due to the scores being LLRs and accounting for unknown languages in the model.","title":"Description"},{"location":"plugins/env-multiClass-v2.html#supported-classes","text":"The table below shows each class and the associated subclasses that this plugin reports scores for. Class Subclasses Channel ac (Acoustic projection via a mobile phone recording of a speakerphone voice output mic (Microphone) tel (Telephone) Codec aac amrnb clean (Uncompressed) g723_1 g726 gsm ilbc mp3 opus real144 speex vorbis Emotion A (Aroused) H (Happy) N (Neutral) Gender f (Female) m (Male) Language amh (Amharic) ara (Arabic) cmn (Chinese Mandarin) eng (English) fas (Farsi) fre (French) hau (Hausa) jpn (Japanese) kor (Korean) pus (Pushto) rus (Russian) spa (Spanish) tur (Turkish) urd (Urdu) vie (Vietnamese) Reverb nml (Limited to no reverberation) rev (Moderate to high levels of reverberation) SNR clean (SNR better than 30dB) snr20db (SNR of approximately 20dB) snr08db (SNR of 8dB or lower)","title":"Supported Classes"},{"location":"plugins/env-multiClass-v2.html#domains","text":"multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs.","title":"Domains"},{"location":"plugins/env-multiClass-v2.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/env-multiClass-v2.html#outputs","text":"Generally, a list of scores for all classes in the domain, for the entire segment. Unlike with SAD and SID, where scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection, most of the classes within an ENV plugin are set up differently. Some ENV plugins, like this one will have a 'parent' superclass that has several subclasses within it. For example, the Gender superclass has subclasses of m (male) and f (female); the Channel superclass may have subclasses of mic (microphone), tel (telephone), and ac (acoustic coupling). When the plugin reports its results, these classes are reported as: Gender-f <score> Gender-m <score> and Channel-ac <score> Channel-mic <score> Channel-tel <score> respectively. Note that within many of these superclasses, the scores are scaled so that the sum of all subclasses within the superclass will sum to one, to make the scores more legible and easier to interpret. In these cases, the highest score represents the most likely subclass for that category. An example output excerpt: input-audio.wav Channel-ac 0.0 input-audio.wav Channel-mic 1.0 input-audio.wav Channel-tel 0.0 input-audio.wav Codec-aac 0.0 input-audio.wav Codec-amrnb 0.0 input-audio.wav Codec-clean 1.0 input-audio.wav Codec-g723_1 0.0 input-audio.wav Codec-g726 0.0 input-audio.wav Codec-gsm 0.0 input-audio.wav Codec-ilbc 0.0 input-audio.wav Codec-mp3 0.0 input-audio.wav Codec-opus 0.0 input-audio.wav Codec-real144 0.0 input-audio.wav Codec-speex 0.0 input-audio.wav Codec-vorbis 0.0 input-audio.wav Emotion-A 0.0 input-audio.wav Emotion-H 0.0 input-audio.wav Emotion-N 0.9999999 input-audio.wav Gender-f 0.0 input-audio.wav Gender-m 1.0 input-audio.wav Language-amh 0.0 input-audio.wav Language-ara 0.0 input-audio.wav Language-cmn 0.0 input-audio.wav Language-eng 0.99999642 input-audio.wav Language-fas 0.0 input-audio.wav Language-fre 0.0040021786 input-audio.wav Language-hau 0.0 input-audio.wav Language-jpn 0.0 input-audio.wav Language-kor 0.0 input-audio.wav Language-pus 0.0 input-audio.wav Language-rus 0.0 input-audio.wav Language-spa 0.00023994729 input-audio.wav Language-tur 0.0 input-audio.wav Language-urd 0.0 input-audio.wav Language-vie 0.0 input-audio.wav Reverb-nml 1.0 input-audio.wav Reverb-rev 0.0 input-audio.wav SNR-clean 0.90663459 input-audio.wav SNR-snr08db 0.0 input-audio.wav SNR-snr20db 0.093361041","title":"Outputs"},{"location":"plugins/env-multiClass-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest","title":"Functionality (Traits"},{"location":"plugins/env-multiClass-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/env-multiClass-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/env-multiClass-v2.html#changing-audio-conditions","text":"This plugin is a global scoring plugin. The scores reported will represent a summarization of the entire submitted audio buffer or file with respect to each of the classes in the plugin. If conditions change, or there are multiple speakers, or multiple languages spoken, if the environment changes in any way, it may be very difficult or impossible for these changes to be captured and represented by the results from this plugin.","title":"Changing Audio Conditions"},{"location":"plugins/env-multiClass-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/env-multiClass-v2.html#calibration","text":"This plugin applies calibration only to the scores from the Language class. All other classes have the sum of scores for that class scaled to sum to 1.0.","title":"Calibration"},{"location":"plugins/env-multiClass-v2.html#subsetting-classes","text":"If only a limited number of the metadata classes are of interest, it is possible to subset the report back from the plugin by passing these classes as --ids with the scoring request. Refer to the Command Line Interface or API Documentation for instructions on how to do this, depending on how you are interacting with the plugin.","title":"Subsetting Classes"},{"location":"plugins/env-multiClass-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 0.0 to 1.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/env-powerSupplyHum-v1.html","text":"env-powerSupplyHum-v1.0.0 (Environmental Analysis - Power Supply Hum) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Environmental Analysis plugins attempt to characterize the acoustic environment of an audio file in some way. This particular plugin is attempting to detect and label power supply hum contamination and is searching audio waveforms for either 50 or 60 Hz frequencies that come from a mains hum, also known as a line frequency or power line hum. These hums come from the alternating currents running through electrical lines and sockets. Different countries and regions will have either 50 or 60 Hz power line hums determined by their regulations and infrastructure. For example, audio recorded in the United States could have a 60 Hz hum while data recorded in Europe could have a 50 Hz hum. If a mains hum is detected, it gives a potential hint for the geographic provenance of the waveform. This plugin was trained on waveforms from several datasets (ESC-50, DARES-G1, DEMAND, QUT Noise, TUT 2018, AASP-INFO) with 50 and 60 Hz tones artificially mixed in. The data also included waveforms without inserted tones and pure tones. Seeing data without tones helps the model determine when there is no line frequency present. Presenting pure tones is motivated by speech, where systems learn better if they are exposed to the clean signal as well. The model is a Residual Network (ResNet) that was originally trained on ImageNet to differentiate 1000 different classes of objects. We fine-tuned this model on high-resolution spectrograms that were zoomed in to the lower frequency range where power line hums are present. Domains default-v1 Model trained on several varied datasets that showed the best performance on unseen data with real power line frequencies. Inputs The audio waveforms to be searched for power line frequencies. Outputs Detections for 50 and 60 Hz line frequencies in the input waveforms if any were found. The plugin also reports the log-likelihood ratio for the potential detected classes. If no mains hums were found, the plugin reports that the input waveforms have no line frequencies. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single label for the entire audio. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. 50 Cycle Bias The model performs best on detecting 50 Hz hums and the lack of a hum. It struggles most with 60 Hz hums when there is other nearby low frequency activity. Helping the model better discriminate between 60 Hz and nearby activity would be one of the first steps to further improvement. The improvements in 60 Hz would also likely benefit the 50 Hz and lack of hum classes. Training Data Applicability It is difficult to find real-world data with line frequencies, since it is one of the first issues corrected for or removed in official and academic data collects. To overcome this challenge, we artificially inserted 50 and 60 Hz tones into noisy waveforms to simulate power line frequencies while training the models. There is a risk that the model trained with artificial data will struggle to generalize on real, operational data.","title":"Environmental Analysis (ENV) Power Supply"},{"location":"plugins/env-powerSupplyHum-v1.html#env-powersupplyhum-v100-environmental-analysis-power-supply-hum","text":"","title":"env-powerSupplyHum-v1.0.0 (Environmental Analysis - Power Supply Hum)"},{"location":"plugins/env-powerSupplyHum-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/env-powerSupplyHum-v1.html#description","text":"Environmental Analysis plugins attempt to characterize the acoustic environment of an audio file in some way. This particular plugin is attempting to detect and label power supply hum contamination and is searching audio waveforms for either 50 or 60 Hz frequencies that come from a mains hum, also known as a line frequency or power line hum. These hums come from the alternating currents running through electrical lines and sockets. Different countries and regions will have either 50 or 60 Hz power line hums determined by their regulations and infrastructure. For example, audio recorded in the United States could have a 60 Hz hum while data recorded in Europe could have a 50 Hz hum. If a mains hum is detected, it gives a potential hint for the geographic provenance of the waveform. This plugin was trained on waveforms from several datasets (ESC-50, DARES-G1, DEMAND, QUT Noise, TUT 2018, AASP-INFO) with 50 and 60 Hz tones artificially mixed in. The data also included waveforms without inserted tones and pure tones. Seeing data without tones helps the model determine when there is no line frequency present. Presenting pure tones is motivated by speech, where systems learn better if they are exposed to the clean signal as well. The model is a Residual Network (ResNet) that was originally trained on ImageNet to differentiate 1000 different classes of objects. We fine-tuned this model on high-resolution spectrograms that were zoomed in to the lower frequency range where power line hums are present.","title":"Description"},{"location":"plugins/env-powerSupplyHum-v1.html#domains","text":"default-v1 Model trained on several varied datasets that showed the best performance on unseen data with real power line frequencies.","title":"Domains"},{"location":"plugins/env-powerSupplyHum-v1.html#inputs","text":"The audio waveforms to be searched for power line frequencies.","title":"Inputs"},{"location":"plugins/env-powerSupplyHum-v1.html#outputs","text":"Detections for 50 and 60 Hz line frequencies in the input waveforms if any were found. The plugin also reports the log-likelihood ratio for the potential detected classes. If no mains hums were found, the plugin reports that the input waveforms have no line frequencies.","title":"Outputs"},{"location":"plugins/env-powerSupplyHum-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single label for the entire audio. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/env-powerSupplyHum-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/env-powerSupplyHum-v1.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/env-powerSupplyHum-v1.html#50-cycle-bias","text":"The model performs best on detecting 50 Hz hums and the lack of a hum. It struggles most with 60 Hz hums when there is other nearby low frequency activity. Helping the model better discriminate between 60 Hz and nearby activity would be one of the first steps to further improvement. The improvements in 60 Hz would also likely benefit the 50 Hz and lack of hum classes.","title":"50 Cycle Bias"},{"location":"plugins/env-powerSupplyHum-v1.html#training-data-applicability","text":"It is difficult to find real-world data with line frequencies, since it is one of the first issues corrected for or removed in official and academic data collects. To overcome this challenge, we artificially inserted 50 and 60 Hz tones into noisy waveforms to simulate power line frequencies while training the models. There is a risk that the model trained with artificial data will struggle to generalize on real, operational data.","title":"Training Data Applicability"},{"location":"plugins/env-speakerCount-v1.html","text":"env-speakerCount-v1 (Environmental Analysis - Speaker Count) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Speaker Count (mex-speakerCount) plugins identifies the total number of speakers in a given audio segment. Domains multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Inputs An audio file or buffer to be scored. Outputs In the basic case, Speaker Count plugins returns a list of audio files/segments labelled with the number of speakers. input-audio1.wav 1 1.0 input-audio2.wav 2 1.0 input-audio3.wav 2+ 1.0 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning labeled number of speakers within the submitted audio. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations All current env-speakerCount plugins assume that an audio segment contains at least 1 non-overlapped speaker. The current plugin limits the speaker counting to 1, 2 or more than 2 speakers. In many cases, a minimum duration of speech of 5 seconds is required in order to output scores. This value can optionally be overwritten by the user, but the number of speaker provided for such short segments will be less reliable.","title":"Environmental Analysis (ENV) Speaker Count"},{"location":"plugins/env-speakerCount-v1.html#env-speakercount-v1-environmental-analysis-speaker-count","text":"","title":"env-speakerCount-v1 (Environmental Analysis - Speaker Count)"},{"location":"plugins/env-speakerCount-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/env-speakerCount-v1.html#description","text":"Speaker Count (mex-speakerCount) plugins identifies the total number of speakers in a given audio segment.","title":"Description"},{"location":"plugins/env-speakerCount-v1.html#domains","text":"multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/env-speakerCount-v1.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/env-speakerCount-v1.html#outputs","text":"In the basic case, Speaker Count plugins returns a list of audio files/segments labelled with the number of speakers. input-audio1.wav 1 1.0 input-audio2.wav 2 1.0 input-audio3.wav 2+ 1.0","title":"Outputs"},{"location":"plugins/env-speakerCount-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning labeled number of speakers within the submitted audio. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/env-speakerCount-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/env-speakerCount-v1.html#limitations","text":"All current env-speakerCount plugins assume that an audio segment contains at least 1 non-overlapped speaker. The current plugin limits the speaker counting to 1, 2 or more than 2 speakers. In many cases, a minimum duration of speech of 5 seconds is required in order to output scores. This value can optionally be overwritten by the user, but the number of speaker provided for such short segments will be less reliable.","title":"Limitations"},{"location":"plugins/gid.html","text":"Gender Identification GID Plugins All available LID plugins currently fit the description on this page. Refer to the table below for the currently supported Diarization plugins. Legacy Plugins (OLIVE 4.x Compatible) OLIVE Version Plugin Description OLIVE 4.9+ gid-gb-v1 Gender ID powered by a Gaussian backend Description GID plugins attempt to identify the gender of the talker in the audio submitted to the plugin for scoring. The goal is to distinguish between male and female speakers as a quick triage of incoming data, or to serve as a front-end for other plugins that may have a gender-specific workflow. Inputs Audio file or buffer and an optional identifier. Outputs Gender ID plugins report a score for each gender, in the format shown below. The plugins created so far feature score scaling, meaning that the scores returned for male and female are re-scaled so that their values sum to 1.0, in order to facilitate legibility. input-audio.wav f 0.1 input-audio.wav m 0.9 Limitations Labeling Granularity GID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple speakers who may or may not be of a different gender, the entire segment will still be scored as a unit. Ensuring that audio submitted for scoring consists of a single speaker is important for reliable results. Age All current Gender ID plugins are trained only on adult male and female speech. Speech from children will likely be confused with female speech, or be otherwise volatile. Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Gender Identification"},{"location":"plugins/gid.html#gender-identification","text":"","title":"Gender Identification"},{"location":"plugins/gid.html#gid-plugins","text":"All available LID plugins currently fit the description on this page. Refer to the table below for the currently supported Diarization plugins.","title":"GID Plugins"},{"location":"plugins/gid.html#legacy-plugins-olive-4x-compatible","text":"OLIVE Version Plugin Description OLIVE 4.9+ gid-gb-v1 Gender ID powered by a Gaussian backend","title":"Legacy Plugins (OLIVE 4.x Compatible)"},{"location":"plugins/gid.html#description","text":"GID plugins attempt to identify the gender of the talker in the audio submitted to the plugin for scoring. The goal is to distinguish between male and female speakers as a quick triage of incoming data, or to serve as a front-end for other plugins that may have a gender-specific workflow.","title":"Description"},{"location":"plugins/gid.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/gid.html#outputs","text":"Gender ID plugins report a score for each gender, in the format shown below. The plugins created so far feature score scaling, meaning that the scores returned for male and female are re-scaled so that their values sum to 1.0, in order to facilitate legibility. input-audio.wav f 0.1 input-audio.wav m 0.9","title":"Outputs"},{"location":"plugins/gid.html#limitations","text":"","title":"Limitations"},{"location":"plugins/gid.html#labeling-granularity","text":"GID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple speakers who may or may not be of a different gender, the entire segment will still be scored as a unit. Ensuring that audio submitted for scoring consists of a single speaker is important for reliable results.","title":"Labeling Granularity"},{"location":"plugins/gid.html#age","text":"All current Gender ID plugins are trained only on adult male and female speech. Speech from children will likely be confused with female speech, or be otherwise volatile.","title":"Age"},{"location":"plugins/gid.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/kws.html","text":"Keyword Spotting (KWS) Released Plugins All of our current QBE plugins match the description on this page. Refer to the list below for the currently supported Speaker Detection plugins. Legacy Plugins (OLIVE 4.x Compatible) OLIVE Version Plugin Description OLIVE 4.12+ kws-dynapy-v1 DynaPy-based keyword spotting plugin improving the overall KWS system infrastructure and performance. Description Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by text input from the user at score time. It is based on automatic speech recognition (ASR) technology, featuring speech-to-text transcription and language modeling. This means that every KWS domain is language dependent, and that keywords can only be detected if they exist in the underlying ASR system's dictionary, making out-of-vocabulary keywords a potential problem. Inputs An audio file or buffer and a list of desired keywords or keyphrases to detect. Outputs When one or more of the enrolled keywords has been detected in the submitted audio, KWS returns a region or list of timestamped regions, each with a score for the keyword that has been detected. Enrollments KWS plugins do not support enrollments; instead, the set of classes the plugin is searching for is provided as a list of text keywords with each scoring request. Note that each domain is language dependent, and that a word or phrase in a language other than the one the domain is trained in is likely to be out-of-vocabulary. This means the word or phrase will be difficult or impossible to recall. For details on how to set or pass these keywords, please refer to the appropriate sections within the OLIVE CLI User Guide or OLIVE API Documentation. Limitations As was previously mentioned, traditional KWS relies on an underlying ASR system, making each KWS domain completely language dependent. This places several real restrictions on the users. First, this means that for keywords to be detectable, the words must be part of the ASR system's dictionary. This may make it difficult to find some keywords or phrases, like names, brands, slang or other colloquialisms, if they are out-of-vocabulary. This also makes it more difficult to deal with speakers or situations that may involve code switching. This reliance on ASR also makes KWS plugins quite heavy with respect to resource requirements, and also quite slow compared to other plugin types. An additional limitation stemming from the language dependence of the system is that if a user would like to detect keywords in a new language that isn't currently offered by SRI, a new domain for that language would need to be created, which requires a large amount of transcribed audio in that language. Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Keyword Spotting (KWS)"},{"location":"plugins/kws.html#keyword-spotting-kws","text":"","title":"Keyword Spotting (KWS)"},{"location":"plugins/kws.html#released-plugins","text":"All of our current QBE plugins match the description on this page. Refer to the list below for the currently supported Speaker Detection plugins.","title":"Released Plugins"},{"location":"plugins/kws.html#legacy-plugins-olive-4x-compatible","text":"OLIVE Version Plugin Description OLIVE 4.12+ kws-dynapy-v1 DynaPy-based keyword spotting plugin improving the overall KWS system infrastructure and performance.","title":"Legacy Plugins (OLIVE 4.x Compatible)"},{"location":"plugins/kws.html#description","text":"Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by text input from the user at score time. It is based on automatic speech recognition (ASR) technology, featuring speech-to-text transcription and language modeling. This means that every KWS domain is language dependent, and that keywords can only be detected if they exist in the underlying ASR system's dictionary, making out-of-vocabulary keywords a potential problem.","title":"Description"},{"location":"plugins/kws.html#inputs","text":"An audio file or buffer and a list of desired keywords or keyphrases to detect.","title":"Inputs"},{"location":"plugins/kws.html#outputs","text":"When one or more of the enrolled keywords has been detected in the submitted audio, KWS returns a region or list of timestamped regions, each with a score for the keyword that has been detected.","title":"Outputs"},{"location":"plugins/kws.html#enrollments","text":"KWS plugins do not support enrollments; instead, the set of classes the plugin is searching for is provided as a list of text keywords with each scoring request. Note that each domain is language dependent, and that a word or phrase in a language other than the one the domain is trained in is likely to be out-of-vocabulary. This means the word or phrase will be difficult or impossible to recall. For details on how to set or pass these keywords, please refer to the appropriate sections within the OLIVE CLI User Guide or OLIVE API Documentation.","title":"Enrollments"},{"location":"plugins/kws.html#limitations","text":"As was previously mentioned, traditional KWS relies on an underlying ASR system, making each KWS domain completely language dependent. This places several real restrictions on the users. First, this means that for keywords to be detectable, the words must be part of the ASR system's dictionary. This may make it difficult to find some keywords or phrases, like names, brands, slang or other colloquialisms, if they are out-of-vocabulary. This also makes it more difficult to deal with speakers or situations that may involve code switching. This reliance on ASR also makes KWS plugins quite heavy with respect to resource requirements, and also quite slow compared to other plugin types. An additional limitation stemming from the language dependence of the system is that if a user would like to detect keywords in a new language that isn't currently offered by SRI, a new domain for that language would need to be created, which requires a large amount of transcribed audio in that language.","title":"Limitations"},{"location":"plugins/kws.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/ldd-sbcEmbed-v1.html","text":"ldd-sbcEmbed-v1 (Language Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one of the enrolled languages is found. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. The table below displays the list of currently available languages and dialects from SRI. Individual plugin pages will cover the languages actually available in each plugin and/or domain. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 10 languages configured (optionally configurable to up to 63 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 fre 11.40590000 input-audio.wav 43.500 77.500 fre 11.29558277 input-audio.wav 77.500 78.500 arz 2.63369179 input-audio.wav 78.500 80.500 arb 2.25519705 input-audio.wav 85.500 86.500 arz 2.06612849 input-audio.wav 97.500 98.500 fas 3.74665093 input-audio.wav 98.500 99.500 arz 2.22936487 input-audio.wav 105.500 106.500 spa 2.72254372 input-audio.wav 107.500 108.500 fas 2.60355234 input-audio.wav 108.500 110.500 eng 2.76414633 input-audio.wav 109.500 113.140 eng 2.85003138 input-audio.wav 113.760 116.260 arz 2.50716114 input-audio.wav 120.260 140.260 kor 14.93032360 input-audio.wav 143.260 157.260 kor 12.62243176 input-audio.wav 158.260 161.260 cmn 3.24917603 input-audio.wav 161.260 162.260 jpn 2.73345900 input-audio.wav 165.260 177.260 kor 12.32051945 input-audio.wav 178.260 180.260 arz 2.41706276 input-audio.wav 186.320 188.820 arz 2.85040617 input-audio.wav 193.820 194.820 spa 2.21501803 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Comments Language/Dialect Detection Granularity LID plugins that are capable of dialect detection typically include functionality to fall back to the base language class in the case of limited confidence. This is typically done by outputting scores for all dialects (i.e. ara-arz, ara-apc, and ara-arb) as well as the base language (i.e. ara). Note that any language with dialect information does not have the base class enrolled, but this is determined from the maximum of the dialect detectors for the base language available within the plugin (whether exposed or not). In the case that a dialect score is sufficiently high, the base language score will be set to 0.001 lower than the highest-scoring dialect, and otherwise the base class is set to 0.001 higher than the highest-scoring dialect score. In this way, labelling the audio sample based on the maximum scoring will indicate a specific dialect if confident, and otherwise the base language. This default mode is defined as BASEAPPEND. There are two alternate modes available that can optionally be set: BASEAPPEND - Default behavior, described above. BASEONLY \u2013 Output only base language scores formed by the maximum of the dialect-specific scores for a given base language. STANDARD \u2013 Output scores based on enrolled classes without producing a base language summarization for dialect-compatible detectors. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Language Name amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French jpn Japanese kor Korean pus Pashto rus Russian spa Spanish tgl Tagolog tha Thai tur Turkish urd Urdu vie Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Language Name alb Albanian amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic aze Azerbaijani bel Belorussian ben Bengali bos Bosnian bul Bulgarian cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French geo Georgian ger German gre Greek hau Hausa hrv Croatian ind Indonesian ita Italian jpn Japanese khm Khmer kor Korean mac Macedonian mya Burmese nde Ndebele orm Oromo pan Punjabi pol Polish por Portuguese prs Dari pus Pashto ron Romanian rus Russian sna Shona som Somali spa Spanish srp Serbian swa Swahili tam Tamil tgl Tagalog tha Thai tib Tibetan tir Tigrinya tur Turkish ukr Ukranian urd Urdu uzb Uzbek vie Vietnamese Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut), and becomes usable when sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can augment these existing languages using their own data by enrolling audio with the same label as an existing language. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder using the command line interface and calling $ ./configure_languages.py to get all languages or $ ./configure_languages.py lang1,lang2,\u2026,langN for a subset of available languages. Please note that running ./configure_languages.py without any arguments should be done with extreme care. This will enable all languages and dialects in the domain; including those that were included solely for their utility in score calibration, that may not have enough training data to create a model that acts as a reliable detector. Enabling all languages may adversely affect the plugin\u2019s performance. Some plugins may support adjusting the language detection granularity discussed above, though this is for advanced users only. An example of changing this setting using the configure_langauges,py script is $ ./configure_languages.py lang1,lang2,...,langN BASEONLY Where the options for this setting are discussed above, if supported. This will be discussed on the individual plugin page if supported. Segmentation By Classification Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 1.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all languages scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring language (value= True ). False True or False Additional Option Notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels output_only_highest_scoring_detected_language The boolean output_only_highest_scoring_detected_language parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_language is set to False, the plugin will report all the languages above the threshold for a given segment. However, if output_only_highest_scoring_detected_language is set as True, the plugin will report only the language with the maximum score for a given segment even when multiple languages have scores above the threshold. An example of this behavior distance follows. If we have a segment (S) with scores for three different languages previously enrolled, S lang1 10.8 S lang2 8.2 S lang3 3.1 and the threshold is 5.0, then with output_only_highest_scoring_detected_language = True, the system reports: S lang1 10.8 However, with output_only_highest_scoring_detected_language = False, the system reports: S lang1 10.8 S lang2 8.2 The default behavior of this plugin is to have this parameter set to False and to report all language detections over the detection threshold for each region.","title":"Language Detection (LDD)"},{"location":"plugins/ldd-sbcEmbed-v1.html#ldd-sbcembed-v1-language-detection","text":"","title":"ldd-sbcEmbed-v1 (Language Detection)"},{"location":"plugins/ldd-sbcEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/ldd-sbcEmbed-v1.html#description","text":"Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one of the enrolled languages is found. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. The table below displays the list of currently available languages and dialects from SRI. Individual plugin pages will cover the languages actually available in each plugin and/or domain. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API.","title":"Description"},{"location":"plugins/ldd-sbcEmbed-v1.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 10 languages configured (optionally configurable to up to 63 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/ldd-sbcEmbed-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/ldd-sbcEmbed-v1.html#outputs","text":"LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 fre 11.40590000 input-audio.wav 43.500 77.500 fre 11.29558277 input-audio.wav 77.500 78.500 arz 2.63369179 input-audio.wav 78.500 80.500 arb 2.25519705 input-audio.wav 85.500 86.500 arz 2.06612849 input-audio.wav 97.500 98.500 fas 3.74665093 input-audio.wav 98.500 99.500 arz 2.22936487 input-audio.wav 105.500 106.500 spa 2.72254372 input-audio.wav 107.500 108.500 fas 2.60355234 input-audio.wav 108.500 110.500 eng 2.76414633 input-audio.wav 109.500 113.140 eng 2.85003138 input-audio.wav 113.760 116.260 arz 2.50716114 input-audio.wav 120.260 140.260 kor 14.93032360 input-audio.wav 143.260 157.260 kor 12.62243176 input-audio.wav 158.260 161.260 cmn 3.24917603 input-audio.wav 161.260 162.260 jpn 2.73345900 input-audio.wav 165.260 177.260 kor 12.32051945 input-audio.wav 178.260 180.260 arz 2.41706276 input-audio.wav 186.320 188.820 arz 2.85040617 input-audio.wav 193.820 194.820 spa 2.21501803","title":"Outputs"},{"location":"plugins/ldd-sbcEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/ldd-sbcEmbed-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/ldd-sbcEmbed-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/ldd-sbcEmbed-v1.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/ldd-sbcEmbed-v1.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/ldd-sbcEmbed-v1.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/ldd-sbcEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/ldd-sbcEmbed-v1.html#languagedialect-detection-granularity","text":"LID plugins that are capable of dialect detection typically include functionality to fall back to the base language class in the case of limited confidence. This is typically done by outputting scores for all dialects (i.e. ara-arz, ara-apc, and ara-arb) as well as the base language (i.e. ara). Note that any language with dialect information does not have the base class enrolled, but this is determined from the maximum of the dialect detectors for the base language available within the plugin (whether exposed or not). In the case that a dialect score is sufficiently high, the base language score will be set to 0.001 lower than the highest-scoring dialect, and otherwise the base class is set to 0.001 higher than the highest-scoring dialect score. In this way, labelling the audio sample based on the maximum scoring will indicate a specific dialect if confident, and otherwise the base language. This default mode is defined as BASEAPPEND. There are two alternate modes available that can optionally be set: BASEAPPEND - Default behavior, described above. BASEONLY \u2013 Output only base language scores formed by the maximum of the dialect-specific scores for a given base language. STANDARD \u2013 Output scores based on enrolled classes without producing a base language summarization for dialect-compatible detectors.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/ldd-sbcEmbed-v1.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Language Name amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French jpn Japanese kor Korean pus Pashto rus Russian spa Spanish tgl Tagolog tha Thai tur Turkish urd Urdu vie Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/ldd-sbcEmbed-v1.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Language Name alb Albanian amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic aze Azerbaijani bel Belorussian ben Bengali bos Bosnian bul Bulgarian cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French geo Georgian ger German gre Greek hau Hausa hrv Croatian ind Indonesian ita Italian jpn Japanese khm Khmer kor Korean mac Macedonian mya Burmese nde Ndebele orm Oromo pan Punjabi pol Polish por Portuguese prs Dari pus Pashto ron Romanian rus Russian sna Shona som Somali spa Spanish srp Serbian swa Swahili tam Tamil tgl Tagalog tha Thai tib Tibetan tir Tigrinya tur Turkish ukr Ukranian urd Urdu uzb Uzbek vie Vietnamese","title":"Supported Languages"},{"location":"plugins/ldd-sbcEmbed-v1.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut), and becomes usable when sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can augment these existing languages using their own data by enrolling audio with the same label as an existing language.","title":"Enrollments"},{"location":"plugins/ldd-sbcEmbed-v1.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder using the command line interface and calling $ ./configure_languages.py to get all languages or $ ./configure_languages.py lang1,lang2,\u2026,langN for a subset of available languages. Please note that running ./configure_languages.py without any arguments should be done with extreme care. This will enable all languages and dialects in the domain; including those that were included solely for their utility in score calibration, that may not have enough training data to create a model that acts as a reliable detector. Enabling all languages may adversely affect the plugin\u2019s performance. Some plugins may support adjusting the language detection granularity discussed above, though this is for advanced users only. An example of changing this setting using the configure_langauges,py script is $ ./configure_languages.py lang1,lang2,...,langN BASEONLY Where the options for this setting are discussed above, if supported. This will be discussed on the individual plugin page if supported.","title":"Configuring Languages"},{"location":"plugins/ldd-sbcEmbed-v1.html#segmentation-by-classification","text":"Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/ldd-sbcEmbed-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 1.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all languages scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring language (value= True ). False True or False","title":"Global Options"},{"location":"plugins/ldd-sbcEmbed-v1.html#additional-option-notes","text":"","title":"Additional Option Notes"},{"location":"plugins/ldd-sbcEmbed-v1.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/ldd-sbcEmbed-v1.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels","title":"win_sec and step_sec"},{"location":"plugins/ldd-sbcEmbed-v1.html#output_only_highest_scoring_detected_language","text":"The boolean output_only_highest_scoring_detected_language parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_language is set to False, the plugin will report all the languages above the threshold for a given segment. However, if output_only_highest_scoring_detected_language is set as True, the plugin will report only the language with the maximum score for a given segment even when multiple languages have scores above the threshold. An example of this behavior distance follows. If we have a segment (S) with scores for three different languages previously enrolled, S lang1 10.8 S lang2 8.2 S lang3 3.1 and the threshold is 5.0, then with output_only_highest_scoring_detected_language = True, the system reports: S lang1 10.8 However, with output_only_highest_scoring_detected_language = False, the system reports: S lang1 10.8 S lang2 8.2 The default behavior of this plugin is to have this parameter set to False and to report all language detections over the detection threshold for each region.","title":"output_only_highest_scoring_detected_language"},{"location":"plugins/lid-embedplda-v2.html","text":"lid-embedplda-v2 (Language Identification) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description LID plugins detect one or more language or dialect classes in an audio segment as a global score. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration. This plug-in has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 10 languages configured (optionally configurable to up to 63 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav amh -19.9012123573 input-audio.wav ara -15.8882738579 input-audio.wav cmn -15.5530382622 input-audio.wav eng -14.1870705116 input-audio.wav fas -17.3224474419 input-audio.wav fre 10.1847232353 input-audio.wav hau -15.1134468544 input-audio.wav jpn -21.0655495155 input-audio.wav kor -18.3601671684 input-audio.wav pus -16.2738787163 input-audio.wav rus -10.4046117294 input-audio.wav spa -18.1588427055 input-audio.wav tur -14.0825478065 input-audio.wav urd -20.4127785194 input-audio.wav vie -18.552107476 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Comments Language/Dialect Detection Granularity LID plugins that are capable of dialect detection typically include functionality to fall back to the base language class in the case of limited confidence. This is typically done by outputting scores for all dialects (i.e. ara-arz, ara-apc, and ara-arb) as well as the base language (i.e. ara). Note that any language with dialect information does not have the base class enrolled, but this is determined from the maximum of the dialect detectors for the base language available within the plugin (whether exposed or not). In the case that a dialect score is sufficiently high, the base language score will be set to 0.001 lower than the highest-scoring dialect, and otherwise the base class is set to 0.001 higher than the highest-scoring dialect score. In this way, labelling the audio sample based on the maximum scoring will indicate a specific dialect if confident, and otherwise the base language. This default mode is defined as BASEAPPEND. There are two alternate modes available that can optionally be set: BASEAPPEND - Default behavior, described above. BASEONLY \u2013 Output only base language scores formed by the maximum of the dialect-specific scores for a given base language. STANDARD \u2013 Output scores based on enrolled classes without producing a base language summarization for dialect-compatible detectors. Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut), and becomes usable when sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can augment these existing languages using their own data by enrolling audio with the same label as an existing language. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder using the command line interface and calling $ ./configure_languages.py to get all languages or $ ./configure_languages.py lang1,lang2,\u2026,langN for a subset of available languages. Please note that running ./configure_languages.py without any arguments should be done with extreme care. This will enable all languages and dialects in the domain; including those that were included solely for their utility in score calibration, that may not have enough training data to create a model that acts as a reliable detector. Enabling all languages may adversely affect the plugin\u2019s performance. This plugin supports adjusting the language detection granularity discussed above, though this is for advanced users only. An example of changing this setting using the configure_langauges,py script is $ ./configure_languages.py lang1,lang2,...,langN BASEONLY Where the options for this setting are discussed above, if supported. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Language Name amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French jpn Japanese kor Korean pus Pashto rus Russian spa Spanish tgl Tagolog tha Thai tur Turkish urd Urdu vie Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Language Name alb Albanian amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic aze Azerbaijani bel Belorussian ben Bengali bos Bosnian bul Bulgarian cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French geo Georgian ger German gre Greek hau Hausa hrv Croatian ind Indonesian ita Italian jpn Japanese khm Khmer kor Korean mac Macedonian mya Burmese nde Ndebele orm Oromo pan Punjabi pol Polish por Portuguese prs Dari pus Pashto ron Romanian rus Russian sna Shona som Somali spa Spanish srp Serbian swa Swahili tam Tamil tgl Tagalog tha Thai tib Tibetan tir Tigrinya tur Turkish ukr Ukranian urd Urdu uzb Uzbek vie Vietnamese Global Options This plugin does not feature user-configurable option parameters. It does, however, offer configurable language models and language-reporting granularity. For details, refer here .","title":"Language Identification (LID)"},{"location":"plugins/lid-embedplda-v2.html#lid-embedplda-v2-language-identification","text":"","title":"lid-embedplda-v2 (Language Identification)"},{"location":"plugins/lid-embedplda-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/lid-embedplda-v2.html#description","text":"LID plugins detect one or more language or dialect classes in an audio segment as a global score. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration. This plug-in has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API.","title":"Description"},{"location":"plugins/lid-embedplda-v2.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 10 languages configured (optionally configurable to up to 63 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/lid-embedplda-v2.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/lid-embedplda-v2.html#outputs","text":"Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav amh -19.9012123573 input-audio.wav ara -15.8882738579 input-audio.wav cmn -15.5530382622 input-audio.wav eng -14.1870705116 input-audio.wav fas -17.3224474419 input-audio.wav fre 10.1847232353 input-audio.wav hau -15.1134468544 input-audio.wav jpn -21.0655495155 input-audio.wav kor -18.3601671684 input-audio.wav pus -16.2738787163 input-audio.wav rus -10.4046117294 input-audio.wav spa -18.1588427055 input-audio.wav tur -14.0825478065 input-audio.wav urd -20.4127785194 input-audio.wav vie -18.552107476","title":"Outputs"},{"location":"plugins/lid-embedplda-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/lid-embedplda-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/lid-embedplda-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile.","title":"Limitations"},{"location":"plugins/lid-embedplda-v2.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/lid-embedplda-v2.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/lid-embedplda-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/lid-embedplda-v2.html#languagedialect-detection-granularity","text":"LID plugins that are capable of dialect detection typically include functionality to fall back to the base language class in the case of limited confidence. This is typically done by outputting scores for all dialects (i.e. ara-arz, ara-apc, and ara-arb) as well as the base language (i.e. ara). Note that any language with dialect information does not have the base class enrolled, but this is determined from the maximum of the dialect detectors for the base language available within the plugin (whether exposed or not). In the case that a dialect score is sufficiently high, the base language score will be set to 0.001 lower than the highest-scoring dialect, and otherwise the base class is set to 0.001 higher than the highest-scoring dialect score. In this way, labelling the audio sample based on the maximum scoring will indicate a specific dialect if confident, and otherwise the base language. This default mode is defined as BASEAPPEND. There are two alternate modes available that can optionally be set: BASEAPPEND - Default behavior, described above. BASEONLY \u2013 Output only base language scores formed by the maximum of the dialect-specific scores for a given base language. STANDARD \u2013 Output scores based on enrolled classes without producing a base language summarization for dialect-compatible detectors.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/lid-embedplda-v2.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut), and becomes usable when sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can augment these existing languages using their own data by enrolling audio with the same label as an existing language.","title":"Enrollments"},{"location":"plugins/lid-embedplda-v2.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder using the command line interface and calling $ ./configure_languages.py to get all languages or $ ./configure_languages.py lang1,lang2,\u2026,langN for a subset of available languages. Please note that running ./configure_languages.py without any arguments should be done with extreme care. This will enable all languages and dialects in the domain; including those that were included solely for their utility in score calibration, that may not have enough training data to create a model that acts as a reliable detector. Enabling all languages may adversely affect the plugin\u2019s performance. This plugin supports adjusting the language detection granularity discussed above, though this is for advanced users only. An example of changing this setting using the configure_langauges,py script is $ ./configure_languages.py lang1,lang2,...,langN BASEONLY Where the options for this setting are discussed above, if supported.","title":"Configuring Languages"},{"location":"plugins/lid-embedplda-v2.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Language Name amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French jpn Japanese kor Korean pus Pashto rus Russian spa Spanish tgl Tagolog tha Thai tur Turkish urd Urdu vie Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/lid-embedplda-v2.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Language Name alb Albanian amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic aze Azerbaijani bel Belorussian ben Bengali bos Bosnian bul Bulgarian cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French geo Georgian ger German gre Greek hau Hausa hrv Croatian ind Indonesian ita Italian jpn Japanese khm Khmer kor Korean mac Macedonian mya Burmese nde Ndebele orm Oromo pan Punjabi pol Polish por Portuguese prs Dari pus Pashto ron Romanian rus Russian sna Shona som Somali spa Spanish srp Serbian swa Swahili tam Tamil tgl Tagalog tha Thai tib Tibetan tir Tigrinya tur Turkish ukr Ukranian urd Urdu uzb Uzbek vie Vietnamese","title":"Supported Languages"},{"location":"plugins/lid-embedplda-v2.html#global-options","text":"This plugin does not feature user-configurable option parameters. It does, however, offer configurable language models and language-reporting granularity. For details, refer here .","title":"Global Options"},{"location":"plugins/qbe-tdnn-v5.html","text":"qbe-tdnn-v5 (Query by Example Keyword Spotting) Version Changelog Plugin Version Change v5.0.0 Initial plugin release with OLIVE 5.1.0 v5.0.1 Increased threshold to minimze false alarms. Updated for OLIVE 5.1.0 Description Query by Example Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by audio sample enrollments. It has no language model or other constraints that accompany a traditional keyword spotting plugin, so it is language independent. This is a query by example plugin constructed with the latest TDNN architecture models, with score calibration and test-adaptive merging of examples. It features dynamic time warping to compensate for speed and cadence differences when detecting keywords. This version also removes Kaldi dependency, replacing a poor-performing bottleneck feature extractor with an in-house developed and trained model, and reduces false alarms by filtering overlapping lower-confidence detections. Domains multi-v1 Multi-condition domain meant for general-purpose audio conditions including telephone, broadband microphone, and other noisy situations without too many digital or PTT distortions. Inputs For enrollment, an audio file or buffer with a corresponding keyword or query label. For scoring, an audio buffer or file. Outputs When one or more of the enrolled keywords has been detected in the submitted audio, QBE returns a region or list of timestamped regions (in seconds), each with a score for the keyword that has been detected. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006 Enrollments Query by Example plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new keyword to enroll. A new enrollment is created with the first class modification request, which consists of essentially sending the system an audio sample of a new keyword or key phrase, along with a label for that query. The label is not used at all by the system for detection, and is only a reference for the user to help recall what the query was. This means that it's completely acceptable to enroll a sample where a speaker is saying something like \"buenas noches\", and to label it \"good night - spanish\" or \"good night\" or even \"lorem ipsum\" in the system. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same query label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected keyword and corresponding score for this keyword. RegionScorerRequest RegionScorerStereoRequest CLASS_MODIFIER \u2013 Enroll new keyword models or augment existing keyword models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Query/Keyword Recognizability The longer and more distinct the enrolled keyword or key phrase is, the better it will be recognized. Shorter keywords, that may occur often in speech (for example, enrolling the word 'a') or sound very similar to other words, may cause false alarms. In general, QBE plugins are language and speaker independent, and enrolled queries should be able to find speech from other talkers as well. However, if the enrolled keyword example is spoken by someone with a particular accent or non-standard pronunciation, it may not generalize with one or few samples. It's possible to enroll multiple samples and/or samples from multiple speakers to maximize the coverage of the query's model. Silence Sensitivity During Enrollment This version of the plugin is known to be very sensitive to including silence when enrolling new keyword queries, especially at the beginning or end of the query. Care should be taken to ensure that the boundaries of the enrollment submissions are as tight to the actual speech as possible. If excessive amounts of silence are included in the enrollment, the system could confuse this silence as part of the query, and the dynamic-time-warping algorithm may cause this to label keyword detections erroneously including large amounts of silence, and may also drastically increase the keyword search time. Future versions of the plugin will address this. Comments Very short keyword queries will be confusable with many other words, since the phonemes they consist of may be common or frequently occur as part of other words, or sound very similar to words or sounds that may commonly occur. The longer and more distinct a keyword is, the lower the likelihood of false alarms. Global Options This plugin does not feature user-configurable parameters.","title":"Query By Example (QBE)"},{"location":"plugins/qbe-tdnn-v5.html#qbe-tdnn-v5-query-by-example-keyword-spotting","text":"","title":"qbe-tdnn-v5 (Query by Example Keyword Spotting)"},{"location":"plugins/qbe-tdnn-v5.html#version-changelog","text":"Plugin Version Change v5.0.0 Initial plugin release with OLIVE 5.1.0 v5.0.1 Increased threshold to minimze false alarms. Updated for OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/qbe-tdnn-v5.html#description","text":"Query by Example Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by audio sample enrollments. It has no language model or other constraints that accompany a traditional keyword spotting plugin, so it is language independent. This is a query by example plugin constructed with the latest TDNN architecture models, with score calibration and test-adaptive merging of examples. It features dynamic time warping to compensate for speed and cadence differences when detecting keywords. This version also removes Kaldi dependency, replacing a poor-performing bottleneck feature extractor with an in-house developed and trained model, and reduces false alarms by filtering overlapping lower-confidence detections.","title":"Description"},{"location":"plugins/qbe-tdnn-v5.html#domains","text":"multi-v1 Multi-condition domain meant for general-purpose audio conditions including telephone, broadband microphone, and other noisy situations without too many digital or PTT distortions.","title":"Domains"},{"location":"plugins/qbe-tdnn-v5.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding keyword or query label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/qbe-tdnn-v5.html#outputs","text":"When one or more of the enrolled keywords has been detected in the submitted audio, QBE returns a region or list of timestamped regions (in seconds), each with a score for the keyword that has been detected. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006","title":"Outputs"},{"location":"plugins/qbe-tdnn-v5.html#enrollments","text":"Query by Example plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new keyword to enroll. A new enrollment is created with the first class modification request, which consists of essentially sending the system an audio sample of a new keyword or key phrase, along with a label for that query. The label is not used at all by the system for detection, and is only a reference for the user to help recall what the query was. This means that it's completely acceptable to enroll a sample where a speaker is saying something like \"buenas noches\", and to label it \"good night - spanish\" or \"good night\" or even \"lorem ipsum\" in the system. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same query label.","title":"Enrollments"},{"location":"plugins/qbe-tdnn-v5.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected keyword and corresponding score for this keyword. RegionScorerRequest RegionScorerStereoRequest CLASS_MODIFIER \u2013 Enroll new keyword models or augment existing keyword models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/qbe-tdnn-v5.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/qbe-tdnn-v5.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/qbe-tdnn-v5.html#querykeyword-recognizability","text":"The longer and more distinct the enrolled keyword or key phrase is, the better it will be recognized. Shorter keywords, that may occur often in speech (for example, enrolling the word 'a') or sound very similar to other words, may cause false alarms. In general, QBE plugins are language and speaker independent, and enrolled queries should be able to find speech from other talkers as well. However, if the enrolled keyword example is spoken by someone with a particular accent or non-standard pronunciation, it may not generalize with one or few samples. It's possible to enroll multiple samples and/or samples from multiple speakers to maximize the coverage of the query's model.","title":"Query/Keyword Recognizability"},{"location":"plugins/qbe-tdnn-v5.html#silence-sensitivity-during-enrollment","text":"This version of the plugin is known to be very sensitive to including silence when enrolling new keyword queries, especially at the beginning or end of the query. Care should be taken to ensure that the boundaries of the enrollment submissions are as tight to the actual speech as possible. If excessive amounts of silence are included in the enrollment, the system could confuse this silence as part of the query, and the dynamic-time-warping algorithm may cause this to label keyword detections erroneously including large amounts of silence, and may also drastically increase the keyword search time. Future versions of the plugin will address this.","title":"Silence Sensitivity During Enrollment"},{"location":"plugins/qbe-tdnn-v5.html#comments","text":"Very short keyword queries will be confusable with many other words, since the phonemes they consist of may be common or frequently occur as part of other words, or sound very similar to words or sounds that may commonly occur. The longer and more distinct a keyword is, the lower the likelihood of false alarms.","title":"Comments"},{"location":"plugins/qbe-tdnn-v5.html#global-options","text":"This plugin does not feature user-configurable parameters.","title":"Global Options"},{"location":"plugins/red-transform.html","text":"red-transform (Redaction - Voice Transformation) Version Changelog Plugin Version Change v1.0.1 Initial plugin release with OLIVE 5.2.0 Description red-transform plugin transforms regions of submitted audio with the goal of obscuring a speaker's identity. That being said, those with close knowledge of the original speaker may still be able to identify the transformed speaker through pronunciation, accent, and other unique identifiers. Domains Several pre-set domains are available. Ideally, the audio containing the speaker to be obscured is a clean close microphone and users would select domain A. As the input audio quality decreases, users will likely move down the domain list searching for a balance between obscurement and intelligibility. Moving further down the domain list (e.g. selecting domain C instead of domain A) also lessens robustness to reverse engineering. pre-set domains: * A-v1 * Domain \"A\" offers the highest amount of speaker obscurement, but suffers the most with respect to intelligibility in degraded audio conditions (noise, distant mic) * B-v1 * Domain \"B\" reduces some of the applied transformation processes in hopes of increasing intelligibility in less than ideal audio. * C-v1 * Domain \"C\" further reduces the applied transformation processes in pursuit of intelligibility. * D-v1 * Domain \"D\" offers the least amount of speaker obscurement, but offers the best robustness to degraded or noisy conditions. Inputs For redaction--voice transformation, an audio file and time-annotated regions corresponding to regions of speech to be obscured is required. Outputs The output of red-transform is an audio file that has been transformed according the domain specifications with the goal of creating audio where the speech is intelligible, but identifying features of the voice that could lead to recognizing the speaker have been obscured. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AUDIO_CONVERTER \u2013 Take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. AudioModificationRequestRequest Compatibility OLIVE 5.2+ Limitations Intelligibility & Audio conditions This plugin is very sensitive to audio conditions and can struggle to produce intelligible speech in degraded or distant microphone conditions. This is why multiple domains were provided, giving users the options of performing less intense transformation in difficult audio conditions where identity will already be partially obscured. Speed This plugin can be quite resource-intensive when it comes to processing speed. There can be significant speed differences between domains (e.g. domain D is much faster than domain A, due to less processing). Domain A may be near real-time processing speed, depending on hardware used for the transformation.","title":"red-transform (Redaction - Voice Transformation)"},{"location":"plugins/red-transform.html#red-transform-redaction-voice-transformation","text":"","title":"red-transform (Redaction - Voice Transformation)"},{"location":"plugins/red-transform.html#version-changelog","text":"Plugin Version Change v1.0.1 Initial plugin release with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/red-transform.html#description","text":"red-transform plugin transforms regions of submitted audio with the goal of obscuring a speaker's identity. That being said, those with close knowledge of the original speaker may still be able to identify the transformed speaker through pronunciation, accent, and other unique identifiers.","title":"Description"},{"location":"plugins/red-transform.html#domains","text":"Several pre-set domains are available. Ideally, the audio containing the speaker to be obscured is a clean close microphone and users would select domain A. As the input audio quality decreases, users will likely move down the domain list searching for a balance between obscurement and intelligibility. Moving further down the domain list (e.g. selecting domain C instead of domain A) also lessens robustness to reverse engineering. pre-set domains: * A-v1 * Domain \"A\" offers the highest amount of speaker obscurement, but suffers the most with respect to intelligibility in degraded audio conditions (noise, distant mic) * B-v1 * Domain \"B\" reduces some of the applied transformation processes in hopes of increasing intelligibility in less than ideal audio. * C-v1 * Domain \"C\" further reduces the applied transformation processes in pursuit of intelligibility. * D-v1 * Domain \"D\" offers the least amount of speaker obscurement, but offers the best robustness to degraded or noisy conditions.","title":"Domains"},{"location":"plugins/red-transform.html#inputs","text":"For redaction--voice transformation, an audio file and time-annotated regions corresponding to regions of speech to be obscured is required.","title":"Inputs"},{"location":"plugins/red-transform.html#outputs","text":"The output of red-transform is an audio file that has been transformed according the domain specifications with the goal of creating audio where the speech is intelligible, but identifying features of the voice that could lead to recognizing the speaker have been obscured.","title":"Outputs"},{"location":"plugins/red-transform.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AUDIO_CONVERTER \u2013 Take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. AudioModificationRequestRequest","title":"Functionality (Traits)"},{"location":"plugins/red-transform.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/red-transform.html#limitations","text":"","title":"Limitations"},{"location":"plugins/red-transform.html#intelligibility-audio-conditions","text":"This plugin is very sensitive to audio conditions and can struggle to produce intelligible speech in degraded or distant microphone conditions. This is why multiple domains were provided, giving users the options of performing less intense transformation in difficult audio conditions where identity will already be partially obscured.","title":"Intelligibility &amp; Audio conditions"},{"location":"plugins/red-transform.html#speed","text":"This plugin can be quite resource-intensive when it comes to processing speed. There can be significant speed differences between domains (e.g. domain D is much faster than domain A, due to less processing). Domain A may be near real-time processing speed, depending on hardware used for the transformation.","title":"Speed"},{"location":"plugins/sad-dnn-v7.html","text":"sad-dnn-v7 (Speech Activity Detection) Version Changelog Plugin Version Change v7.0.0 Initial plugin release, functionally identical to v6.0.0, but updated to be compatible with OLIVE 5.0.0 v7.0.1 (latest) Updated to be compatible with OLIVE 5.1.0, and introduces 'fast-multi-v1' domain Description Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring. Domains multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. fast-multi-v1 Multi-condition domain trained on the same data as the multi-v1 model above, but featuring configuration changes that allow it to process much more quickly, with a possible very slight trade off in accuracy in some circumstances. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins. Adaptation Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide . Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions. Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Speech Disclaimer A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced. DTMF False Alarms It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech. Minimum Audio Length A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection. Comments Speech Region Padding When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD. Supervised Adaptation Guidance When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Speech Activity Detection (SAD)"},{"location":"plugins/sad-dnn-v7.html#sad-dnn-v7-speech-activity-detection","text":"","title":"sad-dnn-v7 (Speech Activity Detection)"},{"location":"plugins/sad-dnn-v7.html#version-changelog","text":"Plugin Version Change v7.0.0 Initial plugin release, functionally identical to v6.0.0, but updated to be compatible with OLIVE 5.0.0 v7.0.1 (latest) Updated to be compatible with OLIVE 5.1.0, and introduces 'fast-multi-v1' domain","title":"Version Changelog"},{"location":"plugins/sad-dnn-v7.html#description","text":"Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring.","title":"Description"},{"location":"plugins/sad-dnn-v7.html#domains","text":"multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. fast-multi-v1 Multi-condition domain trained on the same data as the multi-v1 model above, but featuring configuration changes that allow it to process much more quickly, with a possible very slight trade off in accuracy in some circumstances.","title":"Domains"},{"location":"plugins/sad-dnn-v7.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/sad-dnn-v7.html#outputs","text":"SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/sad-dnn-v7.html#adaptation","text":"Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide .","title":"Adaptation"},{"location":"plugins/sad-dnn-v7.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/sad-dnn-v7.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sad-dnn-v7.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/sad-dnn-v7.html#speech-disclaimer","text":"A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced.","title":"Speech Disclaimer"},{"location":"plugins/sad-dnn-v7.html#dtmf-false-alarms","text":"It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech.","title":"DTMF False Alarms"},{"location":"plugins/sad-dnn-v7.html#minimum-audio-length","text":"A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection.","title":"Minimum Audio Length"},{"location":"plugins/sad-dnn-v7.html#comments","text":"","title":"Comments"},{"location":"plugins/sad-dnn-v7.html#speech-region-padding","text":"When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD.","title":"Speech Region Padding"},{"location":"plugins/sad-dnn-v7.html#supervised-adaptation-guidance","text":"When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported.","title":"Supervised Adaptation Guidance"},{"location":"plugins/sad-dnn-v7.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/sdd-sbcEmbed-v2.html","text":"sdd-sbcEmbed-v2 (Speaker Detection) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Speaker Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled speakers are detected. Unlike Speaker Identification (SID), SDD is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations of speakers when speech from one of the enrolled speakers is found. The goal of speaker detection is to identify and label regions within an audio file where enrolled target speakers are talking. This capability is designed to be used in files with multiple talkers speaking within the same file. For files where it is certain that only one talker will be present, either because it is collected this way or because a human has segmented the file, speaker recognition (SID) plugins should be used. This release of speaker detection is based on \"segmentation-by-classification\", in which the enrolled speakers are detected using a sliding and overlapping window over the file. This plugin does not do do \"diarization\"; it is only searching for regions where the particular enrolled speakers are talking and ignores all non-target speakers. The plugin is based on a core speaker recognition framework using speaker embeddings with PLDA backend and duration-aware calibration. This plugin improves on prior, deprecated technology based on diarization-and-classification using variational bayes diarization followed by scoring of discrete speaker regions. This approach had several major issues, including high use of computational resources, very long and unpredictable run-times (often 10 times slower than real time) and unpredictable performance. This release is average 30 times faster than the previous release, with much greater robustness for a variety of conditions and highly predictable performance. Domains micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording. telClosetalk-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 speaker1 -0.5348 /data/sid/audio/file1.wav 13.280 29.960 speaker2 3.2122 /data/sid/audio/file1.wav 30.350 32.030 speaker3 -5.5340 /data/sid/audio/file2.wav 32.310 46.980 speaker1 0.5333 /data/sid/audio/file2.wav 47.790 51.120 speaker2 -4.9444 /data/sid/audio/file2.wav 54.340 55.400 speaker3 -2.6564 Enrollments Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Comments Segmentation By Classification Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False Additional option notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels output_only_highest_scoring_detected_speaker The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (S) with scores for three different speakers previously enrolled, S spk1 10.8 S spk2 8.2 S spk3 3.1 and the threshold is 5.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: S spk1 10.8 However, with output_only_highest_scoring_detected_speaker = False, the system reports: S spk1 10.8 S spk2 8.2 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"Speaker Detection (SDD)"},{"location":"plugins/sdd-sbcEmbed-v2.html#sdd-sbcembed-v2-speaker-detection","text":"","title":"sdd-sbcEmbed-v2 (Speaker Detection)"},{"location":"plugins/sdd-sbcEmbed-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/sdd-sbcEmbed-v2.html#description","text":"Speaker Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled speakers are detected. Unlike Speaker Identification (SID), SDD is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations of speakers when speech from one of the enrolled speakers is found. The goal of speaker detection is to identify and label regions within an audio file where enrolled target speakers are talking. This capability is designed to be used in files with multiple talkers speaking within the same file. For files where it is certain that only one talker will be present, either because it is collected this way or because a human has segmented the file, speaker recognition (SID) plugins should be used. This release of speaker detection is based on \"segmentation-by-classification\", in which the enrolled speakers are detected using a sliding and overlapping window over the file. This plugin does not do do \"diarization\"; it is only searching for regions where the particular enrolled speakers are talking and ignores all non-target speakers. The plugin is based on a core speaker recognition framework using speaker embeddings with PLDA backend and duration-aware calibration. This plugin improves on prior, deprecated technology based on diarization-and-classification using variational bayes diarization followed by scoring of discrete speaker regions. This approach had several major issues, including high use of computational resources, very long and unpredictable run-times (often 10 times slower than real time) and unpredictable performance. This release is average 30 times faster than the previous release, with much greater robustness for a variety of conditions and highly predictable performance.","title":"Description"},{"location":"plugins/sdd-sbcEmbed-v2.html#domains","text":"micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording. telClosetalk-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations.","title":"Domains"},{"location":"plugins/sdd-sbcEmbed-v2.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sdd-sbcEmbed-v2.html#outputs","text":"In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 speaker1 -0.5348 /data/sid/audio/file1.wav 13.280 29.960 speaker2 3.2122 /data/sid/audio/file1.wav 30.350 32.030 speaker3 -5.5340 /data/sid/audio/file2.wav 32.310 46.980 speaker1 0.5333 /data/sid/audio/file2.wav 47.790 51.120 speaker2 -4.9444 /data/sid/audio/file2.wav 54.340 55.400 speaker3 -2.6564","title":"Outputs"},{"location":"plugins/sdd-sbcEmbed-v2.html#enrollments","text":"Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sdd-sbcEmbed-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sdd-sbcEmbed-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sdd-sbcEmbed-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sdd-sbcEmbed-v2.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/sdd-sbcEmbed-v2.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sdd-sbcEmbed-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/sdd-sbcEmbed-v2.html#segmentation-by-classification","text":"Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/sdd-sbcEmbed-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False","title":"Global Options"},{"location":"plugins/sdd-sbcEmbed-v2.html#additional-option-notes","text":"","title":"Additional option notes"},{"location":"plugins/sdd-sbcEmbed-v2.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/sdd-sbcEmbed-v2.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"win_sec and step_sec"},{"location":"plugins/sdd-sbcEmbed-v2.html#output_only_highest_scoring_detected_speaker","text":"The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (S) with scores for three different speakers previously enrolled, S spk1 10.8 S spk2 8.2 S spk3 3.1 and the threshold is 5.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: S spk1 10.8 However, with output_only_highest_scoring_detected_speaker = False, the system reports: S spk1 10.8 S spk2 8.2 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"output_only_highest_scoring_detected_speaker"},{"location":"plugins/sed-rmsEnergy-v1.html","text":"sed-rmsEnergy-v1 (Signal Energy Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Signal energy detection (SED) plugin detects the presence of any sound by measuring the energy level of the input signal. In contrast to the speech activity detector (SAD), this plugin can detect a wide range of sound activity, including speech and non-speech activity. Goal is to detect presence of any signal versus silence or low-level background noises (lower than 60 dB). Unlike SAD, this plugin is using signal processing techniques (RMS) to gate out low energy regions, rather than a machine learning model. Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring. Domains default-v1 Default domain meant for general-purpose audio conditions. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs SED has two possible output formats: frame scores and region scores, depending on which API is used to interface with the plug-in. Frame scoring interface returns a score (log-likelihood ratio) for each 10ms frame of the input audio segment. \u201c0\u201d is generally used as the threshold for detecting sound regions. Region scores internally post-process frame scores to return sound regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: 0.00223 0.00237 0.00250 0.00265 0.00280 0.00291 0.00305 0.00319 0.00332 0.00345 This can be transformed into a region scoring output, either at the client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where sound was detected in the audio. When the plugin is invoked with the region scoring interface, the conversion from frame scores to region scores is done internally by applying a threshold and padding to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions in seconds. An example of output SED region scores: Test.wav 0.50 1.55 sound 0.00000000 Test.wav 1.54 13.94 sound 0.00000000 Test.wav 14.42 17.06 sound 0.00000000 Test.wav 17.73 35.54 sound 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest This SED plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SED will return results only for those specified regions. Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Frame Scoring vs. Region Scoring We recommend using the \u2018region scoring\u2019 interface for most applications, since thresholding and padding the raw frame scores can be complicated and require expert knowledge. Threshold This plugin detects presence of signal based on the rms energy level of the signal, thus it may produce some false detections in heavy noise conditions likely higher than 60 dB. This can be adjusted by changing the threshold value. Minimum Audio Length A minimum waveform duration of 0.31 seconds is required to produce a meaningful detection. False Alarms in heavy noise condition This plugin detects presence of signal based on rms energy level of the signal, thus it may produce some false detections in heavy noise conditions likely higher than 60 dB. This can be adjusted by changing the threshold value. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 0.0 to 0.5","title":"Signal Energy Detection (SED)"},{"location":"plugins/sed-rmsEnergy-v1.html#sed-rmsenergy-v1-signal-energy-detection","text":"","title":"sed-rmsEnergy-v1 (Signal Energy Detection)"},{"location":"plugins/sed-rmsEnergy-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/sed-rmsEnergy-v1.html#description","text":"Signal energy detection (SED) plugin detects the presence of any sound by measuring the energy level of the input signal. In contrast to the speech activity detector (SAD), this plugin can detect a wide range of sound activity, including speech and non-speech activity. Goal is to detect presence of any signal versus silence or low-level background noises (lower than 60 dB). Unlike SAD, this plugin is using signal processing techniques (RMS) to gate out low energy regions, rather than a machine learning model. Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring.","title":"Description"},{"location":"plugins/sed-rmsEnergy-v1.html#domains","text":"default-v1 Default domain meant for general-purpose audio conditions.","title":"Domains"},{"location":"plugins/sed-rmsEnergy-v1.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/sed-rmsEnergy-v1.html#outputs","text":"SED has two possible output formats: frame scores and region scores, depending on which API is used to interface with the plug-in. Frame scoring interface returns a score (log-likelihood ratio) for each 10ms frame of the input audio segment. \u201c0\u201d is generally used as the threshold for detecting sound regions. Region scores internally post-process frame scores to return sound regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: 0.00223 0.00237 0.00250 0.00265 0.00280 0.00291 0.00305 0.00319 0.00332 0.00345 This can be transformed into a region scoring output, either at the client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where sound was detected in the audio. When the plugin is invoked with the region scoring interface, the conversion from frame scores to region scores is done internally by applying a threshold and padding to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions in seconds. An example of output SED region scores: Test.wav 0.50 1.55 sound 0.00000000 Test.wav 1.54 13.94 sound 0.00000000 Test.wav 14.42 17.06 sound 0.00000000 Test.wav 17.73 35.54 sound 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/sed-rmsEnergy-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest This SED plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SED will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/sed-rmsEnergy-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sed-rmsEnergy-v1.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/sed-rmsEnergy-v1.html#frame-scoring-vs-region-scoring","text":"We recommend using the \u2018region scoring\u2019 interface for most applications, since thresholding and padding the raw frame scores can be complicated and require expert knowledge.","title":"Frame Scoring vs. Region Scoring"},{"location":"plugins/sed-rmsEnergy-v1.html#threshold","text":"This plugin detects presence of signal based on the rms energy level of the signal, thus it may produce some false detections in heavy noise conditions likely higher than 60 dB. This can be adjusted by changing the threshold value.","title":"Threshold"},{"location":"plugins/sed-rmsEnergy-v1.html#minimum-audio-length","text":"A minimum waveform duration of 0.31 seconds is required to produce a meaningful detection.","title":"Minimum Audio Length"},{"location":"plugins/sed-rmsEnergy-v1.html#false-alarms-in-heavy-noise-condition","text":"This plugin detects presence of signal based on rms energy level of the signal, thus it may produce some false detections in heavy noise conditions likely higher than 60 dB. This can be adjusted by changing the threshold value.","title":"False Alarms in heavy noise condition"},{"location":"plugins/sed-rmsEnergy-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 0.0 to 0.5","title":"Global Options"},{"location":"plugins/shl-sbcEmbed-v1.html","text":"shl-sbcEmbed-v1 (Speaker Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Speaker Highlighting plugins will detect and label regions of speech in a submitted audio segment by searching for more speech within the audio that resembles the speaker in one or more user-provided 'seed' regions. Unlike Speaker Identification (SID), SHL is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations where the desired seed speaker is found. Unlike SID and SDD, Speaker Highlighting is not capable of performing any type of 'enrollment', and can only search for more examples of a speaker from within a given audio segment. There is no persistent speaker information retained for future trials, so a new target speaker seed must be supplied each time a new analysis is requested. Speaker Highlighting is meant to be used as a quick triage tool within a file, often longer files, where finding more or all speech from a given speaker is the goal. This can be done as a front-end for a task such as Speaker Redaction, where you'd like to remove, disguise, or otherwise process sections of a given speaker's voice, or as a 'helper' tool for Speaker Identification or Speaker Detection, to assist in building better speaker enrollment models for those plugins, by quickly finding additional candidate speech to add to an enrollment. Domains micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording. Inputs For scoring, an audio buffer or file, in addition to one or more timestamp regions denoting known locations of the speaker of interest. Outputs Speaker Highlighting returns a list of regions with an associated score for each region in the audio where the speaker is determined as 'detected'. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SHL plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/audio/file1.wav 8.320 13.110 speaker 0.5348 /data/audio/file1.wav 13.280 29.960 speaker 3.2122 /data/audio/file1.wav 30.350 32.030 speaker 5.5340 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Speaker Information Persistence Since Speaker Highlighting has no concept of enrollments, no information is retained between different audio analysis queries, and can only search for more of a given speaker within individual audio buffers or files. It can also only search audio for a single speaker at a time, since it assumes that all provided timestamped regions belong to the same speaker. Just as SID and SDD are sensitive to having \"good\" data provided as enrollment exemplars to perform properly, care must be applied when choosing the timestamp regions of the target speaker to 'seed' the system. If you provide a region that contains speech from multiple speakers, or is too short, or is noisy, or otherwise compromised, the performance of the system will degrade. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Comments Segmentation By Classification Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 Additional option notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"Speaker Highlighting (SHL)"},{"location":"plugins/shl-sbcEmbed-v1.html#shl-sbcembed-v1-speaker-detection","text":"","title":"shl-sbcEmbed-v1 (Speaker Detection)"},{"location":"plugins/shl-sbcEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/shl-sbcEmbed-v1.html#description","text":"Speaker Highlighting plugins will detect and label regions of speech in a submitted audio segment by searching for more speech within the audio that resembles the speaker in one or more user-provided 'seed' regions. Unlike Speaker Identification (SID), SHL is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations where the desired seed speaker is found. Unlike SID and SDD, Speaker Highlighting is not capable of performing any type of 'enrollment', and can only search for more examples of a speaker from within a given audio segment. There is no persistent speaker information retained for future trials, so a new target speaker seed must be supplied each time a new analysis is requested. Speaker Highlighting is meant to be used as a quick triage tool within a file, often longer files, where finding more or all speech from a given speaker is the goal. This can be done as a front-end for a task such as Speaker Redaction, where you'd like to remove, disguise, or otherwise process sections of a given speaker's voice, or as a 'helper' tool for Speaker Identification or Speaker Detection, to assist in building better speaker enrollment models for those plugins, by quickly finding additional candidate speech to add to an enrollment.","title":"Description"},{"location":"plugins/shl-sbcEmbed-v1.html#domains","text":"micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording.","title":"Domains"},{"location":"plugins/shl-sbcEmbed-v1.html#inputs","text":"For scoring, an audio buffer or file, in addition to one or more timestamp regions denoting known locations of the speaker of interest.","title":"Inputs"},{"location":"plugins/shl-sbcEmbed-v1.html#outputs","text":"Speaker Highlighting returns a list of regions with an associated score for each region in the audio where the speaker is determined as 'detected'. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SHL plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/audio/file1.wav 8.320 13.110 speaker 0.5348 /data/audio/file1.wav 13.280 29.960 speaker 3.2122 /data/audio/file1.wav 30.350 32.030 speaker 5.5340","title":"Outputs"},{"location":"plugins/shl-sbcEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/shl-sbcEmbed-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/shl-sbcEmbed-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/shl-sbcEmbed-v1.html#speaker-information-persistence","text":"Since Speaker Highlighting has no concept of enrollments, no information is retained between different audio analysis queries, and can only search for more of a given speaker within individual audio buffers or files. It can also only search audio for a single speaker at a time, since it assumes that all provided timestamped regions belong to the same speaker. Just as SID and SDD are sensitive to having \"good\" data provided as enrollment exemplars to perform properly, care must be applied when choosing the timestamp regions of the target speaker to 'seed' the system. If you provide a region that contains speech from multiple speakers, or is too short, or is noisy, or otherwise compromised, the performance of the system will degrade.","title":"Speaker Information Persistence"},{"location":"plugins/shl-sbcEmbed-v1.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/shl-sbcEmbed-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/shl-sbcEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/shl-sbcEmbed-v1.html#segmentation-by-classification","text":"Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/shl-sbcEmbed-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0","title":"Global Options"},{"location":"plugins/shl-sbcEmbed-v1.html#additional-option-notes","text":"","title":"Additional option notes"},{"location":"plugins/shl-sbcEmbed-v1.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/shl-sbcEmbed-v1.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"win_sec and step_sec"},{"location":"plugins/sid-dplda-v2.html","text":"sid-dplda-v2 (Speaker Identification) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. This is a SID plugin leveraging dynamic calibration and discrimination via a DNN-powered DPLDA backend. This plugin accounts for the conditions of the trial to provide superior calibration performance out-of-the-box relative to prior plugins. This plugin features: Discriminative PLDA: SRI-pioneered approach to modeling of speaker variability using a DNN-trained backend and internal calibration defined by conditions of the audio. This approach is considerably faster and more reliable than the prior approach from SRI termed Trial-based Calibration (TBC). Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained. Domains multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Enrollments SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Detection Granularity All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment. Minimum Speech Duration The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default). Comments The plugin will only process files with at least 0.5 seconds of detected speech (configurable). A custom option to output embeddings, the speech regions, and duration of speech is available by setting output_ivs_dump_path=$OUTPUT_PATH where OUTPUT_PATH is a user defined directory to store items. During enrollment and evaluation, if the information for an audio file (based on md5sum) exists in this directory, it is loaded instead of re-computed. The plugin is an audio vectorizer and class exporter/importer. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 20.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.5 0.5 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Speaker Identification (SID)"},{"location":"plugins/sid-dplda-v2.html#sid-dplda-v2-speaker-identification","text":"","title":"sid-dplda-v2 (Speaker Identification)"},{"location":"plugins/sid-dplda-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/sid-dplda-v2.html#description","text":"Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. This is a SID plugin leveraging dynamic calibration and discrimination via a DNN-powered DPLDA backend. This plugin accounts for the conditions of the trial to provide superior calibration performance out-of-the-box relative to prior plugins. This plugin features: Discriminative PLDA: SRI-pioneered approach to modeling of speaker variability using a DNN-trained backend and internal calibration defined by conditions of the audio. This approach is considerably faster and more reliable than the prior approach from SRI termed Trial-based Calibration (TBC). Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained.","title":"Description"},{"location":"plugins/sid-dplda-v2.html#domains","text":"multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs.","title":"Domains"},{"location":"plugins/sid-dplda-v2.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sid-dplda-v2.html#outputs","text":"Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Outputs"},{"location":"plugins/sid-dplda-v2.html#enrollments","text":"SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sid-dplda-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sid-dplda-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sid-dplda-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sid-dplda-v2.html#detection-granularity","text":"All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment.","title":"Detection Granularity"},{"location":"plugins/sid-dplda-v2.html#minimum-speech-duration","text":"The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sid-dplda-v2.html#comments","text":"The plugin will only process files with at least 0.5 seconds of detected speech (configurable). A custom option to output embeddings, the speech regions, and duration of speech is available by setting output_ivs_dump_path=$OUTPUT_PATH where OUTPUT_PATH is a user defined directory to store items. During enrollment and evaluation, if the information for an audio file (based on md5sum) exists in this directory, it is loaded instead of re-computed. The plugin is an audio vectorizer and class exporter/importer.","title":"Comments"},{"location":"plugins/sid-dplda-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 20.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.5 0.5 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/sid-embed-v6.html","text":"sid-embed-v6 (Speaker Identification) Version Changelog Plugin Version Change v6.0.0 Initial plugin release, functionally identical to v5.0.0, but updated to be compatible with OLIVE 5.0.0 v6.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. The release of the sid-embed-v6 plugin provides a new level of performance and automatic adaptation to new domains with a focus on cross-language and non-English speaker trials. This plugin features: Joint PLDA: SRI-pioneered approach to modeling of speaker land language variability jointly in the PLDA modeling space. Dynamic Mean Normalization: A novel approach to automatically adapting a single parameter (mean of the embedding space) based on the enrollment data available from the domain. This innovation provides a major improvement to performance in new domains while counteracting calibration mismatch prior to PLDA modeling. This simplifies the calibration process enabling the use of a single calibration model across domains. By default this option is turned off. Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained. Domains multicond-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. multilang-v1 A domain specifically optimized for cross-languages comparison trials and trained with a preponderance of non-English data, for enhanced performance on non-English data. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Enrollments SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Detection Granularity All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment. Minimum Speech Duration The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default). Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range sad_threshold SAD threshold for determining the audio to be used in metadata extraction 1.0 -5.0 - 6.0","title":"Speaker Identification (SID) Legacy"},{"location":"plugins/sid-embed-v6.html#sid-embed-v6-speaker-identification","text":"","title":"sid-embed-v6  (Speaker Identification)"},{"location":"plugins/sid-embed-v6.html#version-changelog","text":"Plugin Version Change v6.0.0 Initial plugin release, functionally identical to v5.0.0, but updated to be compatible with OLIVE 5.0.0 v6.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/sid-embed-v6.html#description","text":"Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. The release of the sid-embed-v6 plugin provides a new level of performance and automatic adaptation to new domains with a focus on cross-language and non-English speaker trials. This plugin features: Joint PLDA: SRI-pioneered approach to modeling of speaker land language variability jointly in the PLDA modeling space. Dynamic Mean Normalization: A novel approach to automatically adapting a single parameter (mean of the embedding space) based on the enrollment data available from the domain. This innovation provides a major improvement to performance in new domains while counteracting calibration mismatch prior to PLDA modeling. This simplifies the calibration process enabling the use of a single calibration model across domains. By default this option is turned off. Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained.","title":"Description"},{"location":"plugins/sid-embed-v6.html#domains","text":"multicond-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. multilang-v1 A domain specifically optimized for cross-languages comparison trials and trained with a preponderance of non-English data, for enhanced performance on non-English data.","title":"Domains"},{"location":"plugins/sid-embed-v6.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sid-embed-v6.html#outputs","text":"Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Outputs"},{"location":"plugins/sid-embed-v6.html#enrollments","text":"SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sid-embed-v6.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sid-embed-v6.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sid-embed-v6.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sid-embed-v6.html#detection-granularity","text":"All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment.","title":"Detection Granularity"},{"location":"plugins/sid-embed-v6.html#minimum-speech-duration","text":"The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sid-embed-v6.html#comments","text":"","title":"Comments"},{"location":"plugins/sid-embed-v6.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range sad_threshold SAD threshold for determining the audio to be used in metadata extraction 1.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/tmt-statistical-v1.html","text":"tmt-statistical-v1 (Text Machine Translation) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Text Machine Translation plugins perform translation of text from one language to another, typically from one language, specified by the domain, to English. All TMT domains are language-dependent, and each one is meant to work only with a single, specific language. TMT plugins may have some limitations or special processing considerations depending on the language(s) involved and their native alphabets, as well as the training data used to train the underlying models. This is the first Text Machine Translation plugin released for the OLIVE architecture. It offers statistical-model based translation of text, using SRI's SRInterp engine. Each domain provides translation from one language into English, with the two source languages currently available being Spanish and French. The input and output formats match those shown on the TMT information page. The goal of this plugin is to ingest text in one language, defined by the specified domain, and translate it to another language. In both currently available domains, the destination language is English. An example input string, in Spanish: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras Note that some punctuation and special characters will be stripped from the input during preprocessing. An example output string translation of the above example, as provided by the spa-eng-generic-v2 domain: manual of photography to learn everything essential about __UNKNOWN:fujifilm cameras Words that the system does not recognize or can't translate will be marked up with an __UNKNOWN: tag, as can be seen in the example output below. Note that all system output will be lowercase for case sensitive languages, and apart from the __UNKNOWN: tag, all output will be devoid of punctuation. Domains spa-eng-generic-v2 Translates Spanish text into English text. Trained mostly on OpenSubtitles data. fre-eng-generic-v1 Translates French text into English text. Trained mostly on OpenSubtitles data. Inputs For scoring, a text string or text-populated file is required. There is no verification performed by OLIVE or by TMT plugins that the text passed as input is actually in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Note that output may fail or be very confusing if the input language does not match the domain's capabilities. An example input string: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras There is a bit of preprocessing of input that occurs before a string is sent to translation. Thus, all input is lower-cased and frequent punctuation marks such as commas, exclamation marks or question marks are stripped from the string. However, no spelling error correction of any kind is performed. Outputs The output format for TMT plugins is simply text. Words that the system does not recognize or can't translate will be marked up with an __UNKNOWN: tag, as can be seen in the example output below. Note that all system output will be lowercase, and apart from the __UNKNOWN: tag, all output will be devoid of punctuation. An example output string translation of the input example: manual of photography to learn everything essential about __UNKNOWN:fujifilm cameras Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. TextTransformer \u2013 Plugin accepts and analyzes text string inputs, and outputs a new text string as output. In the case of TMT plugins, a text string in the source language should be provided as input, with the expectation that the output will be a text string translated to the desired destination language. TextTransformRequest TextTransformResult Compatibility OLIVE 5.1+ Limitations As the debut TMT plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. Like ASR, TMT plugins are language dependent and also largely text domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the translation models underlying each domain. Several factors contribute to what might limit the vocabulary of a translation model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, TMT plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. The size-on-disk of each domain can be quite large depending on the technology of the individual plugin, and some technologies/plugins may have additional constraints. See the individual plugin detail pages for more information. Multi-Job Threading This plugin is currently not capable of multi-threading or parallel processing. If using this plugin with an OLIVE server, the server must be run in single-worker mode, by specifying a maximum number of jobs of 1: scenicserver -j 1 or scenicserver --workers 1 Alternatively, if the connected client only submits jobs synchronously, waiting for the completion and response of each job before submitting additional queries, problems will be avoided. Due to this limitation, of performing machine translation with this plugin using the CLI tools (localanalyze), if a multi-line input file is provided, this same stipulation of maximizing the active workers to '1' must be used to ensure that the line order of the output text file matches the line order of the input file. Language Dependence Each domain of this TMT plugin is language specific, and is only capable of translating text from one single language to one other. There is no filter or any sort of verification performed by OLIVE to ensure that the text passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input text if the source language is unknown, though it is often easy to spot mismatched input languages by the high number of output words that appears with the \" __UNKNOWN: \" tag. Spelling Errors Note that there is no spell-checking or other types of spelling related pre-processing that occurs on the input data. Therefore any spelling mistakes in the input are likely to cause the system to output __UNKNOWN: tags. Out of Vocabulary (OOV) Words, Names The individual words that the plugin is capable of recognizing and translating is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be translated by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary or to the translation model. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications. Input Limit There is a maximum limit on the input that a single request/query can have. This may depend on input resources available and number of total characters, but currently seems to be roughly 600 words. Extra large inputs that may approach or exceed this number should be split into multiple job submissions. Resources (disk space) Because it is based on statistical MT, this plugin's performance generally directly corresponds to the size of the models it uses, as these models grow the system is exposed to more and more data to learn from. As a result, the included models are very large. Please ensure you have adequate disk space available before attempting to use this plugin. Future TMT plugins will be based on a neural MT architecture that will not have such extreme model size requirements. Comments Global Options This plugin does not expose any options to the user.","title":"Text Machine Translation (TMT)"},{"location":"plugins/tmt-statistical-v1.html#tmt-statistical-v1-text-machine-translation","text":"","title":"tmt-statistical-v1 (Text Machine Translation)"},{"location":"plugins/tmt-statistical-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/tmt-statistical-v1.html#description","text":"Text Machine Translation plugins perform translation of text from one language to another, typically from one language, specified by the domain, to English. All TMT domains are language-dependent, and each one is meant to work only with a single, specific language. TMT plugins may have some limitations or special processing considerations depending on the language(s) involved and their native alphabets, as well as the training data used to train the underlying models. This is the first Text Machine Translation plugin released for the OLIVE architecture. It offers statistical-model based translation of text, using SRI's SRInterp engine. Each domain provides translation from one language into English, with the two source languages currently available being Spanish and French. The input and output formats match those shown on the TMT information page. The goal of this plugin is to ingest text in one language, defined by the specified domain, and translate it to another language. In both currently available domains, the destination language is English. An example input string, in Spanish: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras Note that some punctuation and special characters will be stripped from the input during preprocessing. An example output string translation of the above example, as provided by the spa-eng-generic-v2 domain: manual of photography to learn everything essential about __UNKNOWN:fujifilm cameras Words that the system does not recognize or can't translate will be marked up with an __UNKNOWN: tag, as can be seen in the example output below. Note that all system output will be lowercase for case sensitive languages, and apart from the __UNKNOWN: tag, all output will be devoid of punctuation.","title":"Description"},{"location":"plugins/tmt-statistical-v1.html#domains","text":"spa-eng-generic-v2 Translates Spanish text into English text. Trained mostly on OpenSubtitles data. fre-eng-generic-v1 Translates French text into English text. Trained mostly on OpenSubtitles data.","title":"Domains"},{"location":"plugins/tmt-statistical-v1.html#inputs","text":"For scoring, a text string or text-populated file is required. There is no verification performed by OLIVE or by TMT plugins that the text passed as input is actually in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Note that output may fail or be very confusing if the input language does not match the domain's capabilities. An example input string: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras There is a bit of preprocessing of input that occurs before a string is sent to translation. Thus, all input is lower-cased and frequent punctuation marks such as commas, exclamation marks or question marks are stripped from the string. However, no spelling error correction of any kind is performed.","title":"Inputs"},{"location":"plugins/tmt-statistical-v1.html#outputs","text":"The output format for TMT plugins is simply text. Words that the system does not recognize or can't translate will be marked up with an __UNKNOWN: tag, as can be seen in the example output below. Note that all system output will be lowercase, and apart from the __UNKNOWN: tag, all output will be devoid of punctuation. An example output string translation of the input example: manual of photography to learn everything essential about __UNKNOWN:fujifilm cameras","title":"Outputs"},{"location":"plugins/tmt-statistical-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. TextTransformer \u2013 Plugin accepts and analyzes text string inputs, and outputs a new text string as output. In the case of TMT plugins, a text string in the source language should be provided as input, with the expectation that the output will be a text string translated to the desired destination language. TextTransformRequest TextTransformResult","title":"Functionality (Traits)"},{"location":"plugins/tmt-statistical-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/tmt-statistical-v1.html#limitations","text":"As the debut TMT plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. Like ASR, TMT plugins are language dependent and also largely text domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the translation models underlying each domain. Several factors contribute to what might limit the vocabulary of a translation model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, TMT plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. The size-on-disk of each domain can be quite large depending on the technology of the individual plugin, and some technologies/plugins may have additional constraints. See the individual plugin detail pages for more information.","title":"Limitations"},{"location":"plugins/tmt-statistical-v1.html#multi-job-threading","text":"This plugin is currently not capable of multi-threading or parallel processing. If using this plugin with an OLIVE server, the server must be run in single-worker mode, by specifying a maximum number of jobs of 1: scenicserver -j 1 or scenicserver --workers 1 Alternatively, if the connected client only submits jobs synchronously, waiting for the completion and response of each job before submitting additional queries, problems will be avoided. Due to this limitation, of performing machine translation with this plugin using the CLI tools (localanalyze), if a multi-line input file is provided, this same stipulation of maximizing the active workers to '1' must be used to ensure that the line order of the output text file matches the line order of the input file.","title":"Multi-Job Threading"},{"location":"plugins/tmt-statistical-v1.html#language-dependence","text":"Each domain of this TMT plugin is language specific, and is only capable of translating text from one single language to one other. There is no filter or any sort of verification performed by OLIVE to ensure that the text passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input text if the source language is unknown, though it is often easy to spot mismatched input languages by the high number of output words that appears with the \" __UNKNOWN: \" tag.","title":"Language Dependence"},{"location":"plugins/tmt-statistical-v1.html#spelling-errors","text":"Note that there is no spell-checking or other types of spelling related pre-processing that occurs on the input data. Therefore any spelling mistakes in the input are likely to cause the system to output __UNKNOWN: tags.","title":"Spelling Errors"},{"location":"plugins/tmt-statistical-v1.html#out-of-vocabulary-oov-words-names","text":"The individual words that the plugin is capable of recognizing and translating is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be translated by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary or to the translation model. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications.","title":"Out of Vocabulary (OOV) Words, Names"},{"location":"plugins/tmt-statistical-v1.html#input-limit","text":"There is a maximum limit on the input that a single request/query can have. This may depend on input resources available and number of total characters, but currently seems to be roughly 600 words. Extra large inputs that may approach or exceed this number should be split into multiple job submissions.","title":"Input Limit"},{"location":"plugins/tmt-statistical-v1.html#resources-disk-space","text":"Because it is based on statistical MT, this plugin's performance generally directly corresponds to the size of the models it uses, as these models grow the system is exposed to more and more data to learn from. As a result, the included models are very large. Please ensure you have adequate disk space available before attempting to use this plugin. Future TMT plugins will be based on a neural MT architecture that will not have such extreme model size requirements.","title":"Resources (disk space)"},{"location":"plugins/tmt-statistical-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/tmt-statistical-v1.html#global-options","text":"This plugin does not expose any options to the user.","title":"Global Options"},{"location":"plugins/tpd-dynapy-v3.html","text":"tpd-dynapy-v3 (Topic Detection) Version Changelog Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v2.0.0, but updated to be compatible with OLIVE 5.1.0 Description Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic detection framework using ASR-derived BERT embeddings, with PLDA backend scoring and multi-class calibration. This plugin improves on prior topic detection technology using updated automatic speech recognition (ASR) models, and BERT embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided. Domains eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-cts-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech. Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language. Comments The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below. Base Topic Data eng-cts-v1 BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 ACTIVITIES CHILDREN COMPUTERS-INTERNET-TECHNOLOGY-SOCIAL_MEDIA EDUCATION-CLASS-EXAMS-HOMEWORK-SCHOOLS ENTERTAINMENT-CONCERTS-SHOWS-FESTIVALS FOOD-DRINK-COOKING-DIET-RESTAURANT-GROCERY_SHOPPING FRIENDS-RELATIVES GET-TOGETHER HEALTH-ILLNESS-INJURY-TREATMENT-INSURANCE HOME-RESIDENCE-NEIGHBORHOOD-HOTEL-REAL_ESTATE LIFE-PHILOSOPHY-RELATIONSHIPS SPEECH_COLLECTION_PROJECT SPORTS-GAMES-HOBBIES-LEISURE TRANSPORTATION-VEHICLES TRAVEL WORK-PROFESSION cmn-cts-v1 ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_tpd_dynapy_v2_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 45.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Topic Detection (TPD) ASR-based"},{"location":"plugins/tpd-dynapy-v3.html#tpd-dynapy-v3-topic-detection","text":"","title":"tpd-dynapy-v3 (Topic Detection)"},{"location":"plugins/tpd-dynapy-v3.html#version-changelog","text":"Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v2.0.0, but updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/tpd-dynapy-v3.html#description","text":"Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic detection framework using ASR-derived BERT embeddings, with PLDA backend scoring and multi-class calibration. This plugin improves on prior topic detection technology using updated automatic speech recognition (ASR) models, and BERT embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided.","title":"Description"},{"location":"plugins/tpd-dynapy-v3.html#domains","text":"eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-cts-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection.","title":"Domains"},{"location":"plugins/tpd-dynapy-v3.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-dynapy-v3.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-dynapy-v3.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s).","title":"Enrollments"},{"location":"plugins/tpd-dynapy-v3.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-dynapy-v3.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/tpd-dynapy-v3.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-dynapy-v3.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-dynapy-v3.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-dynapy-v3.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech.","title":"Minimum Speech Duration"},{"location":"plugins/tpd-dynapy-v3.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language.","title":"Language Dependence"},{"location":"plugins/tpd-dynapy-v3.html#comments","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below.","title":"Comments"},{"location":"plugins/tpd-dynapy-v3.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-dynapy-v3.html#eng-cts-v1","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1"},{"location":"plugins/tpd-dynapy-v3.html#rus-cts-v1","text":"ACTIVITIES CHILDREN COMPUTERS-INTERNET-TECHNOLOGY-SOCIAL_MEDIA EDUCATION-CLASS-EXAMS-HOMEWORK-SCHOOLS ENTERTAINMENT-CONCERTS-SHOWS-FESTIVALS FOOD-DRINK-COOKING-DIET-RESTAURANT-GROCERY_SHOPPING FRIENDS-RELATIVES GET-TOGETHER HEALTH-ILLNESS-INJURY-TREATMENT-INSURANCE HOME-RESIDENCE-NEIGHBORHOOD-HOTEL-REAL_ESTATE LIFE-PHILOSOPHY-RELATIONSHIPS SPEECH_COLLECTION_PROJECT SPORTS-GAMES-HOBBIES-LEISURE TRANSPORTATION-VEHICLES TRAVEL WORK-PROFESSION","title":"rus-cts-v1"},{"location":"plugins/tpd-dynapy-v3.html#cmn-cts-v1","text":"ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK","title":"cmn-cts-v1"},{"location":"plugins/tpd-dynapy-v3.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_tpd_dynapy_v2_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 45.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/tpd-dynapy-v4.html","text":"tpd-dynapy-v4 (Topic Detection) Version Changelog Plugin Version Change v4.0.0 Initial plugin release, tested to be compatible with OLIVE 5.1.0 Description Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic embeddings framework with PLDA backend scoring and multi-class calibration. This plugin improves on prior TPD technology using updated ASR models, and XLM-RoBERTa word embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided. Domains eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-cts-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech. Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language. Comments or Usage Notes The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below. Base Topic Data eng-cts-v1 BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK cmn-cts-v1 ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_tpd_dynapy_v2_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"tpd-dynapy-v4 (Topic Detection)"},{"location":"plugins/tpd-dynapy-v4.html#tpd-dynapy-v4-topic-detection","text":"","title":"tpd-dynapy-v4 (Topic Detection)"},{"location":"plugins/tpd-dynapy-v4.html#version-changelog","text":"Plugin Version Change v4.0.0 Initial plugin release, tested to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/tpd-dynapy-v4.html#description","text":"Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic embeddings framework with PLDA backend scoring and multi-class calibration. This plugin improves on prior TPD technology using updated ASR models, and XLM-RoBERTa word embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided.","title":"Description"},{"location":"plugins/tpd-dynapy-v4.html#domains","text":"eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-cts-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection.","title":"Domains"},{"location":"plugins/tpd-dynapy-v4.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-dynapy-v4.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-dynapy-v4.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s).","title":"Enrollments"},{"location":"plugins/tpd-dynapy-v4.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-dynapy-v4.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/tpd-dynapy-v4.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-dynapy-v4.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-dynapy-v4.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-dynapy-v4.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech.","title":"Minimum Speech Duration"},{"location":"plugins/tpd-dynapy-v4.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language.","title":"Language Dependence"},{"location":"plugins/tpd-dynapy-v4.html#comments-or-usage-notes","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below.","title":"Comments or Usage Notes"},{"location":"plugins/tpd-dynapy-v4.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-dynapy-v4.html#eng-cts-v1","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1"},{"location":"plugins/tpd-dynapy-v4.html#rus-cts-v1","text":"ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK","title":"rus-cts-v1"},{"location":"plugins/tpd-dynapy-v4.html#cmn-cts-v1","text":"ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK","title":"cmn-cts-v1"},{"location":"plugins/tpd-dynapy-v4.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_tpd_dynapy_v2_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/tpd-embed-v3.html","text":"tpd-embed-v3 (Topic Detection) Version Changelog Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.1.0 Description Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin features a topic embeddings framework based on phone-level acoustic representations rather than words. This version of the plugin uses both a built-in set of topics for multi-class calibration as well as any other enrolled classes/topics. This makes the plugin more accurate, due to the leverage of other topics in the target domains, but also creates a dependency between enrolled topics. This plugin improves on prior TPD technology in being 20x faster, having a very small footprint of 89M on disk and using less than 1G in RAM, while offering similar performance. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). There are no pre-enrolled classes for detection, user must add annotated topic data to the system (enrollment). TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. Domains eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain, since this plugin is using the other topics to improve its performance. If a given topic has few samples (such as 10), it will not perform as well as if it had 50 samples (ideal). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate against each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Topic Embeddings Trade-offs Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. This is a newer TPD plugin that is based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. Comments The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 base topics for each domain used to improve calibration when limited topics have been enrolled. Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Base Topic Data eng-cts-v1 Domain BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 Domain CHILDREN EDUCATION-CLASS-EXAMS-HOMEWORK-SCHOOLS FOOD-DRINK-COOKING-DIET-RESTAURANT-GROCERY_SHOPPING FRIENDS-RELATIVES GET-TOGETHER-FUTURE LIFE-PHILOSOPHY-RELATIONSHIPS SPORTS-GAMES-HOBBIES-LEISURE TRAVEL-FUTURE TRAVEL-PAST WORK-PROFESSION Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 45.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Topic Detection (TPD) Acoustic-based"},{"location":"plugins/tpd-embed-v3.html#tpd-embed-v3-topic-detection","text":"","title":"tpd-embed-v3 (Topic Detection)"},{"location":"plugins/tpd-embed-v3.html#version-changelog","text":"Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/tpd-embed-v3.html#description","text":"Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin features a topic embeddings framework based on phone-level acoustic representations rather than words. This version of the plugin uses both a built-in set of topics for multi-class calibration as well as any other enrolled classes/topics. This makes the plugin more accurate, due to the leverage of other topics in the target domains, but also creates a dependency between enrolled topics. This plugin improves on prior TPD technology in being 20x faster, having a very small footprint of 89M on disk and using less than 1G in RAM, while offering similar performance. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). There are no pre-enrolled classes for detection, user must add annotated topic data to the system (enrollment). TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations.","title":"Description"},{"location":"plugins/tpd-embed-v3.html#domains","text":"eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection.","title":"Domains"},{"location":"plugins/tpd-embed-v3.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-embed-v3.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-embed-v3.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s).","title":"Enrollments"},{"location":"plugins/tpd-embed-v3.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-embed-v3.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/tpd-embed-v3.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-embed-v3.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-embed-v3.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain, since this plugin is using the other topics to improve its performance. If a given topic has few samples (such as 10), it will not perform as well as if it had 50 samples (ideal). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate against each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-embed-v3.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ).","title":"Minimum Speech Duration"},{"location":"plugins/tpd-embed-v3.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2).","title":"Language Dependence"},{"location":"plugins/tpd-embed-v3.html#topic-embeddings-trade-offs","text":"Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. This is a newer TPD plugin that is based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach.","title":"Topic Embeddings Trade-offs"},{"location":"plugins/tpd-embed-v3.html#comments","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 base topics for each domain used to improve calibration when limited topics have been enrolled. Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below.","title":"Comments"},{"location":"plugins/tpd-embed-v3.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-embed-v3.html#eng-cts-v1-domain","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1 Domain"},{"location":"plugins/tpd-embed-v3.html#rus-cts-v1-domain","text":"CHILDREN EDUCATION-CLASS-EXAMS-HOMEWORK-SCHOOLS FOOD-DRINK-COOKING-DIET-RESTAURANT-GROCERY_SHOPPING FRIENDS-RELATIVES GET-TOGETHER-FUTURE LIFE-PHILOSOPHY-RELATIONSHIPS SPORTS-GAMES-HOBBIES-LEISURE TRAVEL-FUTURE TRAVEL-PAST WORK-PROFESSION","title":"rus-cts-v1 Domain"},{"location":"plugins/tpd-embed-v3.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 45.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/voi-speakingStyle-v1.html","text":"voi-speakingStyle-v1 (Voice Characterization - Speaking Style) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Speaking Style (voi-speakingStyle) plugins detect different types of speaking styles in a given audio segment. These speaking styles are \"conversation\" or \"oration\". Domains multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Inputs An audio file or buffer to be scored. Outputs In the basic case, Speaking Style plugins returns the top speaking style score for each input audio file, labeled with \"conversation\" or \"oration\". The 'score' field is log-likelihood ratio ranging 0 to 1. input-audio1.wav conversation 0.94832635 input-audio2.wav oration 0.99265623 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning labeled number of speakers within the submitted audio. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations All current voi-speakingStyle plugins assume that an audio segment contains single type of speaking. A minimum duration of speech of 5 seconds is required in order to output scores. This value can optionally be overwritten by the user, but scores provided for such short segments will be less reliable.","title":"Voice Characterization (VOI) Speaking Style"},{"location":"plugins/voi-speakingStyle-v1.html#voi-speakingstyle-v1-voice-characterization-speaking-style","text":"","title":"voi-speakingStyle-v1 (Voice Characterization - Speaking Style)"},{"location":"plugins/voi-speakingStyle-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/voi-speakingStyle-v1.html#description","text":"Speaking Style (voi-speakingStyle) plugins detect different types of speaking styles in a given audio segment. These speaking styles are \"conversation\" or \"oration\".","title":"Description"},{"location":"plugins/voi-speakingStyle-v1.html#domains","text":"multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/voi-speakingStyle-v1.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/voi-speakingStyle-v1.html#outputs","text":"In the basic case, Speaking Style plugins returns the top speaking style score for each input audio file, labeled with \"conversation\" or \"oration\". The 'score' field is log-likelihood ratio ranging 0 to 1. input-audio1.wav conversation 0.94832635 input-audio2.wav oration 0.99265623","title":"Outputs"},{"location":"plugins/voi-speakingStyle-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning labeled number of speakers within the submitted audio. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/voi-speakingStyle-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/voi-speakingStyle-v1.html#limitations","text":"All current voi-speakingStyle plugins assume that an audio segment contains single type of speaking. A minimum duration of speech of 5 seconds is required in order to output scores. This value can optionally be overwritten by the user, but scores provided for such short segments will be less reliable.","title":"Limitations"},{"location":"plugins/voi-vocalEffort-v1.html","text":"voi-vocalEffort-v1 (Voice Characterization - VocalEffort) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 v1.0.1 Various bug fixes and stability improvements, compatible with OLIVE 5.1.0 Description Vocal Effort (voi-vocalEffort) plugins detects level of vocal effort in an audio segment. The range of vocal effort includes \"whisper\", \"neutral\", or \"shout\". Domains multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Inputs An audio file or buffer to be scored. Outputs In the basic case, Vocal Effort plugins returns a list of regions labeled with \"whisper\", \"neutral\", or \"shout\". Regions are represented in seconds. The 'score' field is log-likelihood ratio ranging 0 to 1. input-audio1.wav 2.0 10.0 neutral 1.0 input-audio2.wav 2.0 11.0 whisper 0.97211498 input-audio3.wav 2.0 13.0 shout 0.99872446 input-audio4.wav 2.0 9.0 neutral 1.0 input-audio5.wav 2.0 9.0 neutral 1.0 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected level of vocal effort and corresponding score for this level. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations All current voi-speakingStyle plugins assume that an audio segment contains single type of speaking. A minimum duration of speech of 3 seconds is required in order to output scores. This value can optionally be overwritten by the user, but scores provided for such short segments will be less reliable.","title":"Voice Characterization (VOI) Vocal Effort"},{"location":"plugins/voi-vocalEffort-v1.html#voi-vocaleffort-v1-voice-characterization-vocaleffort","text":"","title":"voi-vocalEffort-v1 (Voice Characterization - VocalEffort)"},{"location":"plugins/voi-vocalEffort-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 v1.0.1 Various bug fixes and stability improvements, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/voi-vocalEffort-v1.html#description","text":"Vocal Effort (voi-vocalEffort) plugins detects level of vocal effort in an audio segment. The range of vocal effort includes \"whisper\", \"neutral\", or \"shout\".","title":"Description"},{"location":"plugins/voi-vocalEffort-v1.html#domains","text":"multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/voi-vocalEffort-v1.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/voi-vocalEffort-v1.html#outputs","text":"In the basic case, Vocal Effort plugins returns a list of regions labeled with \"whisper\", \"neutral\", or \"shout\". Regions are represented in seconds. The 'score' field is log-likelihood ratio ranging 0 to 1. input-audio1.wav 2.0 10.0 neutral 1.0 input-audio2.wav 2.0 11.0 whisper 0.97211498 input-audio3.wav 2.0 13.0 shout 0.99872446 input-audio4.wav 2.0 9.0 neutral 1.0 input-audio5.wav 2.0 9.0 neutral 1.0","title":"Outputs"},{"location":"plugins/voi-vocalEffort-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected level of vocal effort and corresponding score for this level. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/voi-vocalEffort-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/voi-vocalEffort-v1.html#limitations","text":"All current voi-speakingStyle plugins assume that an audio segment contains single type of speaking. A minimum duration of speech of 3 seconds is required in order to output scores. This value can optionally be overwritten by the user, but scores provided for such short segments will be less reliable.","title":"Limitations"},{"location":"plugins/voi-voiceCharacterization-v1.html","text":"Voice Characterization (VOI) - voi-voiceCharacterization-v1.0.0 Description Voice Characterization (VOI) plugins will detect and label regions of speech in a submitted audio segment where one or more classes, types, or artifacts of speech are detected. The actual classes being detected will depend on the domain being used. This plugin has domains with two goals: Speaking Style The speakingStyle domain will detect and label regions of speech where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. Vocal Effort In both cases, the plugin will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech is none of these styles and its score is below threshold, the plugins detect none of these. No options are provided for users. Speaking Style (voi-speakingStyle) plugins will detect and label regions of speech in a submitted audio segment where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. The plugins will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech is none of these styles and its score is below threshold, the plugins detect none of these. No options are provided for users. Vocal Effort (voi-vocalEffort) plugins will detect and label regions of speech in a submitted audio segment where one or more levels of vocal effort (low, normal, high, scream, shout) are detected. The plugins will provide timestamp region labels to point to the locations when one of these levels is found. When the input speech is none of these levels and its score is below threshold, the plugins detect none of these. No options are provided for users Inputs An audio file or buffer to be scored. Outputs Speaking Style plugins returns a list of regions labelled with \"dialogue\", \"oration\", \"read\" with scores. Regions are represented in seconds. The 'score' field is the posterior probability ranging 0 to 1. All scores 0.0 above will be detected as output by default. input-audio1.wav 2.00 23.18 dialogue 0.94832635 input-audio2.wav 2.00 29.99 read 0.99265623 input-audio3.wav 2.00 27.78 oration 0.98858994 Vocal Effort plugins returns a list of regions labelled with \"low\", \"normal\", \"high\", \"scream\", \"shout\" with scores. Regions are represented in seconds. The 'score' field is the posterior probability ranging 0 to 1. All scores 0.0 above will be detected as output. input-audio1.wav 0.50 10.20 low 1.0 input-audio2.wav 0.50 10.62 normal 0.97211498 input-audio3.wav 0.50 12.98 high 0.99872446 input-audio4.wav 0.50 9.97 shout 1.0 input-audio5.wav 0.50 9.17 scream 1.0 Limitations A minimum duration of speech of 5 seconds is required in order to output scores. If the duration of speech is lesser than 5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. A minimum duration of speech of 1.5 seconds is required in order to output scores. If the duration of speech is lesser than 1.5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide . voi-speakingStyle-v1 (Speaking Style) Description Speaking Style (voi-speakingStyle) plugins will detect and label regions of speech in a submitted audio segment where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. The plugins will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech contains none of these styles and its score is below the threshold, the plugins detect none of these. No options are provided for users. Domains default-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. default-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected type of speaking style and corresponding score for this type. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations A minimum duration of speech of 5 seconds is required in order to output scores. If the duration of speech is lesser than 5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. A minimum duration of speech of 1.5 seconds is required in order to output scores. If the duration of speech is lesser than 1.5 seconds or no speech is found in a given audio segment, the plugins will raise insufficient speech messages.","title":"Voice Characterization (VOI) - voi-voiceCharacterization-v1.0.0"},{"location":"plugins/voi-voiceCharacterization-v1.html#voice-characterization-voi-voi-voicecharacterization-v100","text":"","title":"Voice Characterization (VOI) - voi-voiceCharacterization-v1.0.0"},{"location":"plugins/voi-voiceCharacterization-v1.html#description","text":"Voice Characterization (VOI) plugins will detect and label regions of speech in a submitted audio segment where one or more classes, types, or artifacts of speech are detected. The actual classes being detected will depend on the domain being used. This plugin has domains with two goals: Speaking Style The speakingStyle domain will detect and label regions of speech where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. Vocal Effort In both cases, the plugin will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech is none of these styles and its score is below threshold, the plugins detect none of these. No options are provided for users. Speaking Style (voi-speakingStyle) plugins will detect and label regions of speech in a submitted audio segment where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. The plugins will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech is none of these styles and its score is below threshold, the plugins detect none of these. No options are provided for users. Vocal Effort (voi-vocalEffort) plugins will detect and label regions of speech in a submitted audio segment where one or more levels of vocal effort (low, normal, high, scream, shout) are detected. The plugins will provide timestamp region labels to point to the locations when one of these levels is found. When the input speech is none of these levels and its score is below threshold, the plugins detect none of these. No options are provided for users","title":"Description"},{"location":"plugins/voi-voiceCharacterization-v1.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/voi-voiceCharacterization-v1.html#outputs","text":"Speaking Style plugins returns a list of regions labelled with \"dialogue\", \"oration\", \"read\" with scores. Regions are represented in seconds. The 'score' field is the posterior probability ranging 0 to 1. All scores 0.0 above will be detected as output by default. input-audio1.wav 2.00 23.18 dialogue 0.94832635 input-audio2.wav 2.00 29.99 read 0.99265623 input-audio3.wav 2.00 27.78 oration 0.98858994 Vocal Effort plugins returns a list of regions labelled with \"low\", \"normal\", \"high\", \"scream\", \"shout\" with scores. Regions are represented in seconds. The 'score' field is the posterior probability ranging 0 to 1. All scores 0.0 above will be detected as output. input-audio1.wav 0.50 10.20 low 1.0 input-audio2.wav 0.50 10.62 normal 0.97211498 input-audio3.wav 0.50 12.98 high 0.99872446 input-audio4.wav 0.50 9.97 shout 1.0 input-audio5.wav 0.50 9.17 scream 1.0","title":"Outputs"},{"location":"plugins/voi-voiceCharacterization-v1.html#limitations","text":"A minimum duration of speech of 5 seconds is required in order to output scores. If the duration of speech is lesser than 5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. A minimum duration of speech of 1.5 seconds is required in order to output scores. If the duration of speech is lesser than 1.5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message.","title":"Limitations"},{"location":"plugins/voi-voiceCharacterization-v1.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/voi-voiceCharacterization-v1.html#voi-speakingstyle-v1-speaking-style","text":"","title":"voi-speakingStyle-v1 (Speaking Style)"},{"location":"plugins/voi-voiceCharacterization-v1.html#description_1","text":"Speaking Style (voi-speakingStyle) plugins will detect and label regions of speech in a submitted audio segment where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. The plugins will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech contains none of these styles and its score is below the threshold, the plugins detect none of these. No options are provided for users.","title":"Description"},{"location":"plugins/voi-voiceCharacterization-v1.html#domains","text":"default-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. default-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/voi-voiceCharacterization-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected type of speaking style and corresponding score for this type. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/voi-voiceCharacterization-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/voi-voiceCharacterization-v1.html#limitations_1","text":"A minimum duration of speech of 5 seconds is required in order to output scores. If the duration of speech is lesser than 5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. A minimum duration of speech of 1.5 seconds is required in order to output scores. If the duration of speech is lesser than 1.5 seconds or no speech is found in a given audio segment, the plugins will raise insufficient speech messages.","title":"Limitations"},{"location":"plugins/vtd-dnn-v7.html","text":"vtd-dnn-v7 (Voice Type Discrimination) Version Changelog Plugin Version Change v7.0.1 Initial plugin release, this plugin shares a codebase with sad-dnn-v7.0.1, with only the models and parameters configured for live speech detection. Released with OLIVE 5.1.0 Description Voice type discrimination (VTD) plugins are designed to detect the presence of speech coming from a live human talker. The goal of VTD is to be able to distinguish not only live-produced, human speech from silence or noise, but also from speech being played over an electronic speaker, such as from a television or phone. When live speech is detected, it is labeled with the timestamps corresponding to its location in the audio. Like SAD, this plug-in may be used either as a frame scorer or region scorer. Domains vtd-v1 Domain designed to detect live speech indoors, differentiating within room live-speech from background distractors like TV, radio, door sound, telephone ringing, traffic etc. Inputs An audio file or buffer and optional identifier and/or optional regions. Outputs The current VTD plugin is capable of performing both frame scoring and region scoring . For frame scoring, typically, a log-likelihood ratio (LLR) score of live-speech vs. non-speech/non-live-speech per each 10ms frame of the input audio segment is output (i.e. 100 audio frames per second). An LLR of greater than \u201c0\u201d indicates that the likelihood of live-speech is greater than the likelihood of non-speech or non-live-speech and \u201c0\u201d is generally used as the threshold for detecting these live-speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech or non-live-speech. VTD plugins may also post-process frame scores to return speech regions, though this is often done on the client-side for flexibility. An excerpt example of what this typically looks like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by adding a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 seconds, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of VTD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a place holder number to retain formatting compatibility with other region-scoring type plugins. Adaptation VTD does not currently support adaptation. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest FrameScorerStereoRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest RegionScorerStereoRequest? This plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, VTD will return results only for those specified regions. Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Speech Intelligibility The VTD plugin detects any and all live speech, whether it is intelligible or not. Live-speech Detection Difficulties It is especially difficult to detect live-speech or differentiate live-speech from pre-recorded or electronic-speaker-produced speech 1) when the microphone is placed very close to distractor sources like TV, radio, etc., 2) when these distractors are played at an unusually high volume or 3) in cases where the microphone is very distant from the source and the signal is weak. Minimum Audio Length A minimum waveform duration of 0.3 seconds is suggested to produce a meaningful live-speech detection. Comments Speech Region Padding When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false live-speech segments. Reduce the threshold value if there are too many missed live-speech segments. 0.0 -4.0 to 4.0","title":"Voice Type Discrimination (VTD)"},{"location":"plugins/vtd-dnn-v7.html#vtd-dnn-v7-voice-type-discrimination","text":"","title":"vtd-dnn-v7 (Voice Type Discrimination)"},{"location":"plugins/vtd-dnn-v7.html#version-changelog","text":"Plugin Version Change v7.0.1 Initial plugin release, this plugin shares a codebase with sad-dnn-v7.0.1, with only the models and parameters configured for live speech detection. Released with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/vtd-dnn-v7.html#description","text":"Voice type discrimination (VTD) plugins are designed to detect the presence of speech coming from a live human talker. The goal of VTD is to be able to distinguish not only live-produced, human speech from silence or noise, but also from speech being played over an electronic speaker, such as from a television or phone. When live speech is detected, it is labeled with the timestamps corresponding to its location in the audio. Like SAD, this plug-in may be used either as a frame scorer or region scorer.","title":"Description"},{"location":"plugins/vtd-dnn-v7.html#domains","text":"vtd-v1 Domain designed to detect live speech indoors, differentiating within room live-speech from background distractors like TV, radio, door sound, telephone ringing, traffic etc.","title":"Domains"},{"location":"plugins/vtd-dnn-v7.html#inputs","text":"An audio file or buffer and optional identifier and/or optional regions.","title":"Inputs"},{"location":"plugins/vtd-dnn-v7.html#outputs","text":"The current VTD plugin is capable of performing both frame scoring and region scoring . For frame scoring, typically, a log-likelihood ratio (LLR) score of live-speech vs. non-speech/non-live-speech per each 10ms frame of the input audio segment is output (i.e. 100 audio frames per second). An LLR of greater than \u201c0\u201d indicates that the likelihood of live-speech is greater than the likelihood of non-speech or non-live-speech and \u201c0\u201d is generally used as the threshold for detecting these live-speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech or non-live-speech. VTD plugins may also post-process frame scores to return speech regions, though this is often done on the client-side for flexibility. An excerpt example of what this typically looks like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by adding a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 seconds, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of VTD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a place holder number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/vtd-dnn-v7.html#adaptation","text":"VTD does not currently support adaptation.","title":"Adaptation"},{"location":"plugins/vtd-dnn-v7.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest FrameScorerStereoRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest RegionScorerStereoRequest? This plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, VTD will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/vtd-dnn-v7.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/vtd-dnn-v7.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/vtd-dnn-v7.html#speech-intelligibility","text":"The VTD plugin detects any and all live speech, whether it is intelligible or not.","title":"Speech Intelligibility"},{"location":"plugins/vtd-dnn-v7.html#live-speech-detection-difficulties","text":"It is especially difficult to detect live-speech or differentiate live-speech from pre-recorded or electronic-speaker-produced speech 1) when the microphone is placed very close to distractor sources like TV, radio, etc., 2) when these distractors are played at an unusually high volume or 3) in cases where the microphone is very distant from the source and the signal is weak.","title":"Live-speech Detection Difficulties"},{"location":"plugins/vtd-dnn-v7.html#minimum-audio-length","text":"A minimum waveform duration of 0.3 seconds is suggested to produce a meaningful live-speech detection.","title":"Minimum Audio Length"},{"location":"plugins/vtd-dnn-v7.html#comments","text":"","title":"Comments"},{"location":"plugins/vtd-dnn-v7.html#speech-region-padding","text":"When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD.","title":"Speech Region Padding"},{"location":"plugins/vtd-dnn-v7.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false live-speech segments. Reduce the threshold value if there are too many missed live-speech segments. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/sad/sad-adapt.html","text":"Adaptation Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retarining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes of speech and non-speech annotations, and performance will improve with as little as a minute. inputs to the plug-in should include both S and NS. Inputs do not need to be balanced but just a single type may not provide much benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide .","title":"Sad adapt"},{"location":"plugins/sad/sad-adapt.html#adaptation","text":"Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retarining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes of speech and non-speech annotations, and performance will improve with as little as a minute. inputs to the plug-in should include both S and NS. Inputs do not need to be balanced but just a single type may not provide much benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide .","title":"Adaptation"},{"location":"plugins/sad/sad-desc.html","text":"Description Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions.","title":"Sad desc"},{"location":"plugins/sad/sad-desc.html#description","text":"Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions.","title":"Description"},{"location":"plugins/sad/sad-dnn-v6-prototype.html","text":"sad-dnn-v6 (Speech Activity Detection) Description Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. A general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring. Domains multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest FrameScorerStereoRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest RegionScorerStereoRequest? SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in milliseconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10159 13219 speech 0.00000000 test_1.wav 149290 177110 speech 0.00000000 test_1.wav 188810 218849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins. Adaptation Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retarining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes of speech and non-speech annotations, and performance will improve with as little as a minute. inputs to the plug-in should include both S and NS. Inputs do not need to be balanced but just a single type may not provide much benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide . Compatibility OLIVE 4.14+ Limitations A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced. Any known or potential limitations with this specific plugin are listed below. DTMF False Alarms It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech. Minimum Audio Length A minimum waveform duration of 0.3 seconds is suggested to produce a meaningful speech detection. Comments Bug Fixes The current version of this plugin is sad-dnn-v6a . The changes incorporated into this plugin over the base sad-dnn-v6 are: Bug fix allowing processing of a subset of the submitted audio file. Drastically improved memory usage by improving efficiency of normalization functionality. Exposed threshold parameter in a config file (plugin_config.py) for configuration by the user. Speech Region Padding When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD. Supervised Adaptation Guidance When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"sad-dnn-v6 (Speech Activity Detection)"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#sad-dnn-v6-speech-activity-detection","text":"","title":"sad-dnn-v6 (Speech Activity Detection)"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#description","text":"Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. A general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring.","title":"Description"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#domains","text":"multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions.","title":"Domains"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest FrameScorerStereoRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest RegionScorerStereoRequest? SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#outputs","text":"SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in milliseconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10159 13219 speech 0.00000000 test_1.wav 149290 177110 speech 0.00000000 test_1.wav 188810 218849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#adaptation","text":"Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retarining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes of speech and non-speech annotations, and performance will improve with as little as a minute. inputs to the plug-in should include both S and NS. Inputs do not need to be balanced but just a single type may not provide much benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide .","title":"Adaptation"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#compatibility","text":"OLIVE 4.14+","title":"Compatibility"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#limitations","text":"A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced. Any known or potential limitations with this specific plugin are listed below.","title":"Limitations"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#dtmf-false-alarms","text":"It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech.","title":"DTMF False Alarms"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#minimum-audio-length","text":"A minimum waveform duration of 0.3 seconds is suggested to produce a meaningful speech detection.","title":"Minimum Audio Length"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#comments","text":"","title":"Comments"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#bug-fixes","text":"The current version of this plugin is sad-dnn-v6a . The changes incorporated into this plugin over the base sad-dnn-v6 are: Bug fix allowing processing of a subset of the submitted audio file. Drastically improved memory usage by improving efficiency of normalization functionality. Exposed threshold parameter in a config file (plugin_config.py) for configuration by the user.","title":"Bug Fixes"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#speech-region-padding","text":"When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD.","title":"Speech Region Padding"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#supervised-adaptation-guidance","text":"When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments.","title":"Supervised Adaptation Guidance"},{"location":"plugins/sad/sad-dnn-v6-prototype.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/sad/sad-inputs.html","text":"Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Sad inputs"},{"location":"plugins/sad/sad-inputs.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/sad/sad-interface.html","text":"Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Sad interface"},{"location":"plugins/sad/sad-interface.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/sad/sad-limitations.html","text":"Limitations A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced.","title":"Sad limitations"},{"location":"plugins/sad/sad-limitations.html#limitations","text":"A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced.","title":"Limitations"},{"location":"plugins/sad/sad-outputs.html","text":"Outputs SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in milliseconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10159 13219 speech 0.00000000 test_1.wav 149290 177110 speech 0.00000000 test_1.wav 188810 218849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Sad outputs"},{"location":"plugins/sad/sad-outputs.html#outputs","text":"SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in milliseconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10159 13219 speech 0.00000000 test_1.wav 149290 177110 speech 0.00000000 test_1.wav 188810 218849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"}]}; var search = { index: new Promise(resolve => setTimeout(() => resolve(local_index), 0)) }