const local_index = {"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Open Language Interface for Voice Exploitation (OLIVE) 5.5.1 OLIVE is a suite of audio processing software tools to enable a wide range of audio analysis for several mission types, including large scale audio triage, targeted close forensic analysis, detection of speech, speakers, languages, and keywords of interest. The highlight feature of OLIVE 5.5.1 is the first official release of OLIVE GPU support for select plugins. Running some plugin operations on the GPU allows some plugins to run dramatically faster than when running exclusively on CPU, taking advantage of the GPU architecture and processing power. It also features several bug fixes, speed improvements, and a number of stability enhancements, especially for Docker-based deliveries (including Martini), which have now migrated from using CentOS7 as the OS base image to Ubuntu 20.04. It maintains compatibility with the previous release's memory-efficient \"SmOlive\" (small OLIVE) plugins that take advantage of quantized models to reduce the resident memory size during processing, and drastically reducing RAM requirements. As well as the expanded Workflows capabilities, such as the SmartTranslation workflow that combines upstream language identification to route audio to the appropriate transcription plugin/domain (when available), and then feeds these transcription results to the appropriate machine translation plugin/domain (when available). A number of under-the-hood adjustments and bug fixes improve server stability and reduce overall memory usage, and to allow parallel use of different MT language domains have also been implemented, in addition to bug fixes for conditional workflow capabilities. The latest point release adds feature enhancements that allow GPU-enabled plugins to co-exist in one OLIVE server instance with CPU-only plugins without having to artificially restrict the CPU-only plugins to use a single server worker. For most users, this means OLIVE can continue to be used as you are used to; for users of OLIVE 5.5.0, this means it's no longer necessary to configure and start up, maintain, and communicate with two separate servers. OLIVE 5.5.1 now also supports Machine Translation results being displayed in the Raven Batch Web UI , as part of the fully-featured OLIVE Martini software package. For more information about the OLIVE 5.5.1 plugins that are currently released and their capabilities, refer to the OLIVE 5.5.1 Release Plugins list, that links to more information about each. There are many facets to OLIVE, several of which are documented here. Feel free to navigate through the documentation using the bar along the left side of each page. The links along the right side of each page provide navigation within that particular page. If you're unsure of the best place to look, this may help you choose where to get started: Administrator and General Setup Documentation Installation and Setup OLIVE Martini Docker-based Installation - Details for installing and setting up the OLIVE software package for docker-container-based deliveries. OLIVE Martini includes a web-based Raven Batch GUI, and other utilities such as a web broker that exposes the new OLIVE REST API. Redhat/CentOS 7 Native Linux Installation - Details for installing the OLIVE software package and getting up and running with a linux-native OLIVE package. This was the most common delivery method before OLIVE 5.1.0. OLIVE GPU Plugin Configuration - Information on how to properly configure OLIVE plugins and domains to use an available Nvidia GPU. OLIVE Server Guide - For reference for setting up the OLIVE environment and firing up an OLIVE server instance, along with important details about how the Server works with the OLIVE Runtime and the OLIVE Runtime contents. OLIVE System Requirements - Speed and memory benchmarking results for some plugins, and overall resource requirement information. User Centric Documentation User Interfaces OLIVE Nightingale GUI - Details for launching and interacting with the OLIVE tools through the the optional OLIVE Nightingale GUI. Speaker Redaction GUI Task - Details how to step through the process of using the OLIVE GUI for Speaker Redaction. Raven Web Batch GUI - Details how to launch and interact with the Raven web-based batch GUI for OLIVE, available in OLIVE Martini-based deliveries. Java and Python Client Utilities - How to install and get started with running the example Java and Python command-line client utilities. Command Line Interface Documentation (Legacy) - If you're interested in exploring the capabilities of OLIVE and its plugins as an evaluation tool just using the command line. This is recommended for light testing and evaluation only, when using the native linux OLIVE, not for integration or for use with docker-based OLIVE deliveries. Supported Audio Formats - Outlines the audio formats OLIVE can currently process and which situations these restrictions apply Integrator Specific Documentation Workflow Integration - Details for getting started submitting enrollment and analysis requests using OLIVE workflows, a powerful new tool allowing you to leverage several plugins with a single server request/API message. Workflow integration is recommended for most integrations, unless very specific functionality is required that's not yet supported by workflows. OLIVE REST API Documentation - Swagger documentation covering the OLIVE REST API exposed by the OLIVE Web Broker. If this documentation is being hosted by the OLIVE Martini container, this documentation will be interactive, allowing you to submit REST API messages straight from the documentation for experimentation. OLIVE Enterprise API Integration: Enterprise API Primer - If interested in details about the mechanisms that allow integrating the OLIVE Server and backend audio processing capabilities to an existing system or GUI. Creating an API Reference Implementation - Contains information to help guide the creation of a new API Reference Implementation if Java is not the target language for client integration. Enterprise API Message Definitions - Provides more of the low-level details on the available API messages, their structures and contents. OlivePy Python API Client Documentation - Provides auto-generated PyDoc style documentation for the OlivePy Python Client API (brings you out of the normal OLIVE documentation - press browser's 'back' button to return). Integrating the (Java) Client API - Provides code examples (currently only in Java) for how to perform several OLIVE tasks and integrate the OLIVE Java Reference Implementation to create a new client or augment a client with OLIVE functionality. OLIVE Plugin Traits - Information regarding plugins including terminology definitions for different plugin types and other types Plugin Information Release Plugins - List of the plugins released with this version of OLIVE, with links to more information on the specific plugins themselves. General Plugin Info - Information regarding the actual technologies and capabilities provided by OLIVE (such as speech activity detection, language identification, etc.), and details on implementing these technologies. OLIVE Plugin Resource Requirements - Information about the processing speed and memory requirements for a selection of OLIVE plugins. Additional Info Glossary - Definitions of important terms and concepts. Contact Us - Information for how to reach out to the OLIVE team Care was made to provide links to important contextual information regarding important terms, acronyms, API messages, etc. throughout the documentation. If you find any section to be unclear or lacking in important details, please let us know which area(s) require improvement. For any questions, comments, or concerns about this documentation contact olive-support@sri.com .","title":"Overview"},{"location":"index.html#open-language-interface-for-voice-exploitation-olive-551","text":"OLIVE is a suite of audio processing software tools to enable a wide range of audio analysis for several mission types, including large scale audio triage, targeted close forensic analysis, detection of speech, speakers, languages, and keywords of interest. The highlight feature of OLIVE 5.5.1 is the first official release of OLIVE GPU support for select plugins. Running some plugin operations on the GPU allows some plugins to run dramatically faster than when running exclusively on CPU, taking advantage of the GPU architecture and processing power. It also features several bug fixes, speed improvements, and a number of stability enhancements, especially for Docker-based deliveries (including Martini), which have now migrated from using CentOS7 as the OS base image to Ubuntu 20.04. It maintains compatibility with the previous release's memory-efficient \"SmOlive\" (small OLIVE) plugins that take advantage of quantized models to reduce the resident memory size during processing, and drastically reducing RAM requirements. As well as the expanded Workflows capabilities, such as the SmartTranslation workflow that combines upstream language identification to route audio to the appropriate transcription plugin/domain (when available), and then feeds these transcription results to the appropriate machine translation plugin/domain (when available). A number of under-the-hood adjustments and bug fixes improve server stability and reduce overall memory usage, and to allow parallel use of different MT language domains have also been implemented, in addition to bug fixes for conditional workflow capabilities. The latest point release adds feature enhancements that allow GPU-enabled plugins to co-exist in one OLIVE server instance with CPU-only plugins without having to artificially restrict the CPU-only plugins to use a single server worker. For most users, this means OLIVE can continue to be used as you are used to; for users of OLIVE 5.5.0, this means it's no longer necessary to configure and start up, maintain, and communicate with two separate servers. OLIVE 5.5.1 now also supports Machine Translation results being displayed in the Raven Batch Web UI , as part of the fully-featured OLIVE Martini software package. For more information about the OLIVE 5.5.1 plugins that are currently released and their capabilities, refer to the OLIVE 5.5.1 Release Plugins list, that links to more information about each. There are many facets to OLIVE, several of which are documented here. Feel free to navigate through the documentation using the bar along the left side of each page. The links along the right side of each page provide navigation within that particular page. If you're unsure of the best place to look, this may help you choose where to get started:","title":"Open Language Interface for Voice Exploitation (OLIVE) 5.5.1"},{"location":"index.html#administrator-and-general-setup-documentation","text":"Installation and Setup OLIVE Martini Docker-based Installation - Details for installing and setting up the OLIVE software package for docker-container-based deliveries. OLIVE Martini includes a web-based Raven Batch GUI, and other utilities such as a web broker that exposes the new OLIVE REST API. Redhat/CentOS 7 Native Linux Installation - Details for installing the OLIVE software package and getting up and running with a linux-native OLIVE package. This was the most common delivery method before OLIVE 5.1.0. OLIVE GPU Plugin Configuration - Information on how to properly configure OLIVE plugins and domains to use an available Nvidia GPU. OLIVE Server Guide - For reference for setting up the OLIVE environment and firing up an OLIVE server instance, along with important details about how the Server works with the OLIVE Runtime and the OLIVE Runtime contents. OLIVE System Requirements - Speed and memory benchmarking results for some plugins, and overall resource requirement information.","title":"Administrator and General Setup Documentation"},{"location":"index.html#user-centric-documentation","text":"User Interfaces OLIVE Nightingale GUI - Details for launching and interacting with the OLIVE tools through the the optional OLIVE Nightingale GUI. Speaker Redaction GUI Task - Details how to step through the process of using the OLIVE GUI for Speaker Redaction. Raven Web Batch GUI - Details how to launch and interact with the Raven web-based batch GUI for OLIVE, available in OLIVE Martini-based deliveries. Java and Python Client Utilities - How to install and get started with running the example Java and Python command-line client utilities. Command Line Interface Documentation (Legacy) - If you're interested in exploring the capabilities of OLIVE and its plugins as an evaluation tool just using the command line. This is recommended for light testing and evaluation only, when using the native linux OLIVE, not for integration or for use with docker-based OLIVE deliveries. Supported Audio Formats - Outlines the audio formats OLIVE can currently process and which situations these restrictions apply","title":"User Centric Documentation"},{"location":"index.html#integrator-specific-documentation","text":"Workflow Integration - Details for getting started submitting enrollment and analysis requests using OLIVE workflows, a powerful new tool allowing you to leverage several plugins with a single server request/API message. Workflow integration is recommended for most integrations, unless very specific functionality is required that's not yet supported by workflows. OLIVE REST API Documentation - Swagger documentation covering the OLIVE REST API exposed by the OLIVE Web Broker. If this documentation is being hosted by the OLIVE Martini container, this documentation will be interactive, allowing you to submit REST API messages straight from the documentation for experimentation. OLIVE Enterprise API Integration: Enterprise API Primer - If interested in details about the mechanisms that allow integrating the OLIVE Server and backend audio processing capabilities to an existing system or GUI. Creating an API Reference Implementation - Contains information to help guide the creation of a new API Reference Implementation if Java is not the target language for client integration. Enterprise API Message Definitions - Provides more of the low-level details on the available API messages, their structures and contents. OlivePy Python API Client Documentation - Provides auto-generated PyDoc style documentation for the OlivePy Python Client API (brings you out of the normal OLIVE documentation - press browser's 'back' button to return). Integrating the (Java) Client API - Provides code examples (currently only in Java) for how to perform several OLIVE tasks and integrate the OLIVE Java Reference Implementation to create a new client or augment a client with OLIVE functionality. OLIVE Plugin Traits - Information regarding plugins including terminology definitions for different plugin types and other types","title":"Integrator Specific Documentation"},{"location":"index.html#plugin-information","text":"Release Plugins - List of the plugins released with this version of OLIVE, with links to more information on the specific plugins themselves. General Plugin Info - Information regarding the actual technologies and capabilities provided by OLIVE (such as speech activity detection, language identification, etc.), and details on implementing these technologies. OLIVE Plugin Resource Requirements - Information about the processing speed and memory requirements for a selection of OLIVE plugins.","title":"Plugin Information"},{"location":"index.html#additional-info","text":"Glossary - Definitions of important terms and concepts. Contact Us - Information for how to reach out to the OLIVE team Care was made to provide links to important contextual information regarding important terms, acronyms, API messages, etc. throughout the documentation. If you find any section to be unclear or lacking in important details, please let us know which area(s) require improvement. For any questions, comments, or concerns about this documentation contact olive-support@sri.com .","title":"Additional Info"},{"location":"apiBuildReferenceImp.html","text":"Developing an OLIVE API Reference Implementation If SRI\u2019s Java client library does not meet your needs or you need to create an implementation in a language other than Java, then the following information is helpful for creating a new reference implementation of the OLIVE Enterprise API. Things to know before you start Before you start, it is a good idea to first develop an understanding of the current OLIVE Java Client API. Reading the API Primer page and reviewing the provided Java Client API source files are all good places to start. As an alternative, To enable users to quickly put together client codes, the OLIVE Java API hides many of the low level implementation details such as assemby of request messages, submission of requests over a port, etc. This section describes some of these implementation details you need to consider when creating a brand new OLIVE API. 1. Communicating with the OLIVE Server A messaging system called ZeroMQ , or ZMQ for short, is used as the backbone for communicating with the OLIVE Server. In order to communicate with a running server, you must initialize ZeroMQ (ZMQ) sockets in your client code. There are two sockets: request socket: This is the socket over which you send requests and receive replies, all of which are serialized protobuf messages. You must conect to this socket using the ZMQ.DEALER configuration. status socket: This socket provides a simple heartbeat broadcast from the server. You may optionally monitor this socket to determine the up/down status of the server. Connect to this socket using the ZMQ.PUB configuration. The code for initializing a ZMQ context and creating/connecting the necessary sockets can differ by language. Below is a ZMQ initialization example in Java. ZMQ . Context context = ZMQ . context (); ZMQ . Socket request_socket = context . socket ( ZMQ . DEALER ); ZMQ . Socket status_socket = context . socket ( ZMQ . SUB ); request_socket . connect ( \"tcp://myserver:6678\" ); status_socket . connect ( \"tcp://myserver:6679\" ); status_socket . subscribe ( \"\" ); // Don\u2019t forget this After connecting the sockets, you can begin sending requests and receiving responses over the request port. The specifics of how this operation is performed are described in the next section. 2. Serialization of messages to and from the OLIVE Server Messages are exchanged between the client and server in serialized form, over the request port. Serialization is provided by the Google Protobufs library. You should familiarize yourself with protobufs before beginning your integration. In order to utilize protobufs, you must first take the scenic.proto message definition file (provided in the example code package or available upon request ) and use protoc (or protobuf.net) to automatically generate classes that represent the OLIVE API messages. For more information see the Google Protocol Buffers Documentation . For example for the OLIVE Java Client API, protoc is used to produce the Java file Scenic.java, located in src/main/java/com/sri/scenic/api within the example code package. This file contains the class Scenic which is used in the Java API for all message related classes and definitions. Once you have compiled the OLIVE messages into your code base you can begin your integration. As you may have seen in the OLIVE Java Client API code, (Scenic.java), every OLIVE message is an instance of a class named Envelope . As its name implies, Envelope acts as a container for enclosing messages. Messages are instances of a class named ScenicMessage . An Envelope can contain multiple instances of ScenicMessage , allowing you to batch your communications to the server. Envelope and ScenicMessage are special because they are used for every communication across the request_port and are basically just wrappers. They\u2019re analogous to the envelope and paper when writing someone a letter. The remaining OLIVE messages comprise the actual API requests and responses. The remaining messages each have an entry in the MessageType enum, allowing you to request certain types when retrieving data from database as well as dynamically deserializing data returned by the server. Message Building Example To get started, your integration will probably need to retrieve some information from the server. For example, you may wish to know the list of available plugins. An example pseudocode excerpt accomplishing this is shown below. PluginDirectoryRequest . Builder req = PluginDirectoryRequest . newBuilder () String id = getUUIDString () ScenicMessage msg = ScenicMessage . newBuilder () . setMessageType ( MessageType . PLUGIN_DIRECTORY_REQUEST ) . addMessageData ( req . build (). toByteString ()) . setMessageId ( id ) . build (); Envelope env = Envelope . newBuilder () . setSenderId ( \u201c third - party - integration \u201d ) . addMessage ( msg ). build (); // Now send the message to the server request_socket . send ( env . toByteArray ()); Envelope resp = Envelope . parseFrom ( request_socket . recv ()); for ( ScenicMessage sm : resp . getMessageList ()) { // For purposes of this example, we assume the above message was // the first and only sent so we can assume things about the response // message, namely that it corresponds to our request. assert ( id == msg . getMessageId ()) \u2019 assert ( msg . getMessageType () == MessageType . PLUGIN_DIRECTORY_RESULT ); if ( sm . hasError ()) { System . out . println ( \u201c Dang : \u201c + sm . getError ()); continue ; } PluginDirectoryResult rep = PluginDirectoryResult . parseFrom ( sm . getMessageData ( 0 )); for ( Plugin p : rep . getPluginsList ()){ System . out . println ( p . getId () + \": \" + p . getDesc () ); } } Please note the following: We could have put other requests in the Envelope. Their responses may or may not have come back in the same envelope, but they would have come back in order. We are guaranteed that our messages are received in order by the server and responses sent in order. However, for messages such as scoring requests and class modification requests (enrollment), which are highly asynchronous, there is no guarantee about the order in which they will finish. The sure-fire way to ensure that you process a message from the server correctly is to base your actions on the message id (which you originally assigned in your request). To properly deserialize the data contained within a ScenicMessage , you must check or otherwise be sure of the MessageType . Some OLIVE plugins need to be preloaded by the server in order to fulfill a request. In such cases either the API or the client program must first send a Load Plugin Domain Request to have the targeted plugin preloaded. An Analysis (Scoring) Message Example Now let\u2019s assume we wish to perform language identification on an audio file. We can create a LID like request as follows: // Variable init String plugin = \u201c lid - embed - v2 \u201d String domain = \u201c multi - v1 \u201d String audioFilePath = \u201c / home / user / audio / file1 . wav \u201d // Build audio object Scenic . Audio . Builder audio = Scenic . Audio . newBuilder (). setPath ( audioFilePath . toAbsolutePath (). toString ; // Create LID request // If specifically processing stereo audio files and wish to score // both channels, please use the FrameScorerStereoRequest or // GlobalScorerStereoRequest messages, that will be responded to with // an FrameScoreStereo or GlobalScoreStereo message, containing score results // for both channels of the submitted audio. // If submitting a stereo audio file using the standard xScorerRequest functions, // and you don\u2019t desire to score both channels independently, there are two // options: // - Specify the channel you wish to be scored -> you will receive results // for that channel only. // - Do not specify a channel -> you will receive a single set of results // corresponding to the merged mono representation of the stereo file. Scenic . GlobalScorerRequest . Builder req = Scenic . GlobalScorerRequest . newBuilder () . setAudio ( audio ) . setPlugin ( plugin ) . setDomain ( domain ); Note that this example relies on code written in our Java Client API, but the general steps to perform the task are the same. Now we wrap the request in a ScenicMessage and Envelope like we did in the last example and send it across the request socket. Analysis requests as well as Enrollment requests take significant time to process on the server. It\u2019s likely you\u2019ll want your integration to be doing other things while it is waiting for the response, such as issuing further analysis requests. This is fully supported. However, you don\u2019t know when or in what order the responses to your analyze request will emerge from the server. Therefore, it\u2019s advantageous to track the message ids that you\u2019ve issued in a map of the form message_id -> request message , so that you know the request to which a newly received response pertains. Let\u2019s assume we\u2019ve received a GlobalScorerResult message and have deserialized it into a variable named res . We could process the result as follows: // Currently OLIVE (SCENIC) will only send back one score reply per score // request. Future releases may be able to send back multiple. // Because of this, we must iterate though all of the scores. List < Scenic . GlobalScore > scores = res . getScoreList (); for ( Scenic . GlobalScore gs : scores ){ system . out . println ( \"LID Score: class \" + gs . getClassId () \u201c = \u201c + gs . getScore ()); } For more details regarding the specifications and breadth of the possible requests, their replies, and the structure of each of these data objects, including how results are represented, please refer to the OLIVE API Message Reference documentation. \u2003 Request a new OLIVE API Reference Implementation If there is a need for an OLIVE API in another language, SRI International would be an ideal candidate to undertake the task because of its rich experience having already done it in Java. However, it will still be a complex software engineering undertaking which will definitely take up a significant amount of project resources. For more information about what this would entail, please reach out to olive-support@sri.com.","title":"Building an API Reference Implementation"},{"location":"apiBuildReferenceImp.html#developing-an-olive-api-reference-implementation","text":"If SRI\u2019s Java client library does not meet your needs or you need to create an implementation in a language other than Java, then the following information is helpful for creating a new reference implementation of the OLIVE Enterprise API.","title":"Developing an OLIVE API Reference Implementation"},{"location":"apiBuildReferenceImp.html#things-to-know-before-you-start","text":"Before you start, it is a good idea to first develop an understanding of the current OLIVE Java Client API. Reading the API Primer page and reviewing the provided Java Client API source files are all good places to start. As an alternative, To enable users to quickly put together client codes, the OLIVE Java API hides many of the low level implementation details such as assemby of request messages, submission of requests over a port, etc. This section describes some of these implementation details you need to consider when creating a brand new OLIVE API.","title":"Things to know before you start"},{"location":"apiBuildReferenceImp.html#1-communicating-with-the-olive-server","text":"A messaging system called ZeroMQ , or ZMQ for short, is used as the backbone for communicating with the OLIVE Server. In order to communicate with a running server, you must initialize ZeroMQ (ZMQ) sockets in your client code. There are two sockets: request socket: This is the socket over which you send requests and receive replies, all of which are serialized protobuf messages. You must conect to this socket using the ZMQ.DEALER configuration. status socket: This socket provides a simple heartbeat broadcast from the server. You may optionally monitor this socket to determine the up/down status of the server. Connect to this socket using the ZMQ.PUB configuration. The code for initializing a ZMQ context and creating/connecting the necessary sockets can differ by language. Below is a ZMQ initialization example in Java. ZMQ . Context context = ZMQ . context (); ZMQ . Socket request_socket = context . socket ( ZMQ . DEALER ); ZMQ . Socket status_socket = context . socket ( ZMQ . SUB ); request_socket . connect ( \"tcp://myserver:6678\" ); status_socket . connect ( \"tcp://myserver:6679\" ); status_socket . subscribe ( \"\" ); // Don\u2019t forget this After connecting the sockets, you can begin sending requests and receiving responses over the request port. The specifics of how this operation is performed are described in the next section.","title":"1. Communicating with the OLIVE Server"},{"location":"apiBuildReferenceImp.html#2-serialization-of-messages-to-and-from-the-olive-server","text":"Messages are exchanged between the client and server in serialized form, over the request port. Serialization is provided by the Google Protobufs library. You should familiarize yourself with protobufs before beginning your integration. In order to utilize protobufs, you must first take the scenic.proto message definition file (provided in the example code package or available upon request ) and use protoc (or protobuf.net) to automatically generate classes that represent the OLIVE API messages. For more information see the Google Protocol Buffers Documentation . For example for the OLIVE Java Client API, protoc is used to produce the Java file Scenic.java, located in src/main/java/com/sri/scenic/api within the example code package. This file contains the class Scenic which is used in the Java API for all message related classes and definitions. Once you have compiled the OLIVE messages into your code base you can begin your integration. As you may have seen in the OLIVE Java Client API code, (Scenic.java), every OLIVE message is an instance of a class named Envelope . As its name implies, Envelope acts as a container for enclosing messages. Messages are instances of a class named ScenicMessage . An Envelope can contain multiple instances of ScenicMessage , allowing you to batch your communications to the server. Envelope and ScenicMessage are special because they are used for every communication across the request_port and are basically just wrappers. They\u2019re analogous to the envelope and paper when writing someone a letter. The remaining OLIVE messages comprise the actual API requests and responses. The remaining messages each have an entry in the MessageType enum, allowing you to request certain types when retrieving data from database as well as dynamically deserializing data returned by the server.","title":"2.  Serialization of messages to and from the OLIVE Server"},{"location":"apiBuildReferenceImp.html#message-building-example","text":"To get started, your integration will probably need to retrieve some information from the server. For example, you may wish to know the list of available plugins. An example pseudocode excerpt accomplishing this is shown below. PluginDirectoryRequest . Builder req = PluginDirectoryRequest . newBuilder () String id = getUUIDString () ScenicMessage msg = ScenicMessage . newBuilder () . setMessageType ( MessageType . PLUGIN_DIRECTORY_REQUEST ) . addMessageData ( req . build (). toByteString ()) . setMessageId ( id ) . build (); Envelope env = Envelope . newBuilder () . setSenderId ( \u201c third - party - integration \u201d ) . addMessage ( msg ). build (); // Now send the message to the server request_socket . send ( env . toByteArray ()); Envelope resp = Envelope . parseFrom ( request_socket . recv ()); for ( ScenicMessage sm : resp . getMessageList ()) { // For purposes of this example, we assume the above message was // the first and only sent so we can assume things about the response // message, namely that it corresponds to our request. assert ( id == msg . getMessageId ()) \u2019 assert ( msg . getMessageType () == MessageType . PLUGIN_DIRECTORY_RESULT ); if ( sm . hasError ()) { System . out . println ( \u201c Dang : \u201c + sm . getError ()); continue ; } PluginDirectoryResult rep = PluginDirectoryResult . parseFrom ( sm . getMessageData ( 0 )); for ( Plugin p : rep . getPluginsList ()){ System . out . println ( p . getId () + \": \" + p . getDesc () ); } } Please note the following: We could have put other requests in the Envelope. Their responses may or may not have come back in the same envelope, but they would have come back in order. We are guaranteed that our messages are received in order by the server and responses sent in order. However, for messages such as scoring requests and class modification requests (enrollment), which are highly asynchronous, there is no guarantee about the order in which they will finish. The sure-fire way to ensure that you process a message from the server correctly is to base your actions on the message id (which you originally assigned in your request). To properly deserialize the data contained within a ScenicMessage , you must check or otherwise be sure of the MessageType . Some OLIVE plugins need to be preloaded by the server in order to fulfill a request. In such cases either the API or the client program must first send a Load Plugin Domain Request to have the targeted plugin preloaded.","title":"Message Building Example"},{"location":"apiBuildReferenceImp.html#an-analysis-scoring-message-example","text":"Now let\u2019s assume we wish to perform language identification on an audio file. We can create a LID like request as follows: // Variable init String plugin = \u201c lid - embed - v2 \u201d String domain = \u201c multi - v1 \u201d String audioFilePath = \u201c / home / user / audio / file1 . wav \u201d // Build audio object Scenic . Audio . Builder audio = Scenic . Audio . newBuilder (). setPath ( audioFilePath . toAbsolutePath (). toString ; // Create LID request // If specifically processing stereo audio files and wish to score // both channels, please use the FrameScorerStereoRequest or // GlobalScorerStereoRequest messages, that will be responded to with // an FrameScoreStereo or GlobalScoreStereo message, containing score results // for both channels of the submitted audio. // If submitting a stereo audio file using the standard xScorerRequest functions, // and you don\u2019t desire to score both channels independently, there are two // options: // - Specify the channel you wish to be scored -> you will receive results // for that channel only. // - Do not specify a channel -> you will receive a single set of results // corresponding to the merged mono representation of the stereo file. Scenic . GlobalScorerRequest . Builder req = Scenic . GlobalScorerRequest . newBuilder () . setAudio ( audio ) . setPlugin ( plugin ) . setDomain ( domain ); Note that this example relies on code written in our Java Client API, but the general steps to perform the task are the same. Now we wrap the request in a ScenicMessage and Envelope like we did in the last example and send it across the request socket. Analysis requests as well as Enrollment requests take significant time to process on the server. It\u2019s likely you\u2019ll want your integration to be doing other things while it is waiting for the response, such as issuing further analysis requests. This is fully supported. However, you don\u2019t know when or in what order the responses to your analyze request will emerge from the server. Therefore, it\u2019s advantageous to track the message ids that you\u2019ve issued in a map of the form message_id -> request message , so that you know the request to which a newly received response pertains. Let\u2019s assume we\u2019ve received a GlobalScorerResult message and have deserialized it into a variable named res . We could process the result as follows: // Currently OLIVE (SCENIC) will only send back one score reply per score // request. Future releases may be able to send back multiple. // Because of this, we must iterate though all of the scores. List < Scenic . GlobalScore > scores = res . getScoreList (); for ( Scenic . GlobalScore gs : scores ){ system . out . println ( \"LID Score: class \" + gs . getClassId () \u201c = \u201c + gs . getScore ()); } For more details regarding the specifications and breadth of the possible requests, their replies, and the structure of each of these data objects, including how results are represented, please refer to the OLIVE API Message Reference documentation.","title":"An Analysis (Scoring) Message Example"},{"location":"apiBuildReferenceImp.html#request-a-new-olive-api-reference-implementation","text":"If there is a need for an OLIVE API in another language, SRI International would be an ideal candidate to undertake the task because of its rich experience having already done it in Java. However, it will still be a complex software engineering undertaking which will definitely take up a significant amount of project resources. For more information about what this would entail, please reach out to olive-support@sri.com.","title":"Request a new OLIVE API Reference Implementation"},{"location":"apiCode.html","text":"Integrating a Client API with OLIVE Java Client IDE Setup Guide If you would like to import the provided OLIVE Java API sample code and reference implementation into an IDE to explore the code and/or get started with integrating this code, please refer over to the Java Client IDE Setup Guide page. If you've already done this step, plan on integrating this functionality just using the provided JAR file, or just wish to browse code samples, continue below. The OLIVE Java API While the OLIVE system allows client integration via Protobuf messages sent from a variety of languages such as Java, Python, C++ and C#, and machine types such as Windows, Linux, and MacOS, the currently available OLIVE reference API implementation is Java-based only, and is also referred to as the OLIVE Java Client API. Therefore all instructions and code examples presented in this section currently assume that client programs are also in Java. Pseudocode and Python versions of the included code will be rolled into this page as they are available. Fundamentally any OLIVE API implementation is based on exchanging Protobuf messages between the client and server. These messages are defined in the API Message Reference Documentation. If you would like to create a reference implementation of the OLIVE API in another language, please refer to the information in the Developing an OLIVE API Reference Implementation page, that should prove to be helpful. If you would like an API reference implementation in another language to be created, please reach out to olive-support@sri.com to discuss your needs with the team. Basic Recipe for using the OLIVE Java Client API This section covers the first steps towards building a client based on implementing SRI's OLIVE API Java Reference implementation in order to use the OLIVE Java API for speech processing. The sections immediately following cover setting up an OLIVE server instance to connect to, then move into a number of steps necessary to connect a client to this server, and finally cover how to build and submit scoring and enrollment requests from the client program. The first step towards integrating a client with the provided OLIVE Java Reference Implementation is to review the API Message Reference to understand the available request and response messages, then dive in by setting up your IDE to work with this code. We have created a step-by-step guide for this configuration and setup process. Note that these steps are not necessary for implementing the actual functionality from this software package, but could be very useful for exploring and learning the code. If only the functionality from this reference implementation is desired, SRI can provide an appropriate JAR file to include in your project. OLIVE server As mentioned above, the OLIVE Enterprise API operates on a client/server model, making the OLIVE server an essential part of the OLIVE system. You must run the OLIVE server and manage its lifecycle as part of your integration effort. For more information about the server's duties, how to interact with it, what options and functionality are available to it, and general setup and troubleshooting information, please refer to the OLIVE Server Information Page . Establish Server Connection Before making any task request, a client must establish a connection with the server. By default, the OLIVE server listens on ports 5588 (request port) and 5589 (status port) for client connection and status requests. These ports are configurable, but if the server has not been instructed to change its listening ports, the code below should establish a connection. Using the OLIVE Java API, connection to the server can be made with a single call, as shown below. This call is available from the API Server class , included in the package com.sri.scenic.api.Server. Java Server server = new Server (); server . connect ( \"scenic-client\" , //client-id \"localhost\" , //address of server 5588 , //request-port 5589 , //status-port 10000 //timeout for failed connection request ); Python --- Coming Soon --- Pseudocode connect ( server - host - name , 5588 , 5589 ) The request port (5588 by default) is used for call and response messages (Protobuf messages). Each request message sent to this port is guaranteed a response from the server (this is why the messages in the API Message Reference are often suffixed with 'Request' and 'Result'). There is no need to poll the server for information on a submitted request, as the result/response for the a request is returned to the client as soon as it is available. The status port (5589 by default) is used by the Server to publish health and status messages (Heartbeat) to client(s). Clients can not send requests on this port. Request Available Plugins In order to submit most server requests, the client must specify the plugin and domain to handle the request. To obtain the handle of a targeted plugin, the client first requests a list of all currently available valid plugins from the server. From the returned plugins list, the client looks up the specific plugin handle by the plugin's unique name (id) and its associated trait for the task to be performed. This handle can then be used in a future request message sent to the server. In the Java API Reference Implementation, this is accomplished by the utilities requestPlugin() and findPluginDomainByTrait(), within the included ClientUtils package. The steps are shown below. Java // ask the server for a list of currently available plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // Look up a specific plugin from the plugin list using // the plugin's unique name (pluginName and domainName) and associated trait e.g. FRAME_SCORER Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Scenic . TraitType . FRAME_SCORER , pluginList ); Python def request_plugins ( self ): request = PluginDirectoryRequest () # Wrap message in an Envelop request = self . _wrap_message ( request , FRAME_SCORER_REQUEST ) # Now send the message logging . debug ( \"Sending a Plugin request message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message...\" ) envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): server_msg = envelope . message [ i ] if broker_msg . HasField ( \"error\" ): print ( server_msg . error ) else : plugin_dir_msg = PluginDirectoryResult () plugin_dir_msg . ParseFromString ( server_msg . message_data [ 0 ]) # There is likely to be multiple plugin/domains print ( \"Server has {} plugins \" . format ( len ( plugin_dir_msg . plugins ))) for p in plugin_dir_msg . plugins : print ( \"Plugin {} has {} domains\" . format ( p . id , len ( p . domain ))) Pseudocode plugin - list = getListOfAvailablePlugins ( server ); targeted - plugin = lookupPlugin ( plugin - list , pluginName , pluginTrait ); The targeted plugin handle, pd in the above example, can then be used with other utilities within the Client API to submit requests, or to otherwise interact with the plugin. For example, this code below shows how one might check what Traits that each plugin in the pluginList returned above support: Java for ( Pair < Scenic . Plugin , Scenic . Domain > pair : pluginList ){ for ( Scenic . Trait t : pair . getFirst (). getTraitList ()) { if ( t . getType () == Scenic . TraitType . FRAME_SCORER ){ log . info ( \"Plugin {} supports frame scoring\" , pair . getFirst (). getId ()); } if ( t . getType () == Scenic . TraitType . GLOBAL_SCORER ){ // Supports global scoring (i.e. SID or LID) log . info ( \"Plugin {} supports global scoring\" , pair . getFirst (). getId ()); } if ( t . getType () == Scenic . TraitType . REGION_SCORER ){ // Supports region scoring (i.e. KWS) log . info ( \"Plugin {} supports region scoring\" , pair . getFirst (). getId ()); } } } Python -- coming soon -- Pseudocode --coming soon-- Audio Submission Guidelines One of the core client activities is submitting Audio with a request. In the OLIVE API, three ways are provided for a client to package audio data to send to the OLIVE server: file path buffer of raw audio sample data serialized file buffer object The enum AudioTransferType is used to specify they type of audio transfer to use. When the client and the OLIVE server share the same file system, the easiest way for the client to send audio data to the server is by specifying the audio's file path on disk. The OLIVE Client API provides the utility below to package audio files which are accessible to the server locally: Java packageAudioAsPath () // AudioTransferType.SEND_AS_PATH When the client and the server don't share the same file system, as in the case of a client making a remote connection to the OLIVE server, it is necessary to send the client's local audio files as a file buffer. To help package the client's audio data in a raw buffer, the OLIVE Client API provides the utility below: Java packageAudioAsRawBuffer () // AudioTransferType.SEND_SAMPLES_BUFFER When submitting audio to the server as a buffer of raw samples, it is important to include information characterizing the audio, such as the bit depth, audio encoding, sample rate, and number of channels, to ensure the server knows how to properly treat the buffer it receives. A third utility, shown below, also packages the client's audio data in a buffer. This utility passes the original file to the server in its entirety in one contiguous buffer, leaving the audio file header intact. Java packageAudioAsSerializedBuffer () // AudioTransferType.SEND_SERIALIZED_BUFFER Sending audio data as a serialized file buffer ensures that all audio header information is provided intact to the server. This allows the server to properly decode and process the audio once its received, since it can directly access the bit depth, encoding type, sample rate and other necessary information from the header itself. The tradeoff with serialized files is that there may be additional overhead needed to process the audio into a consumable form. If the client and server reside on the same hardware and file system, it is advisable to simply pass filepaths when possible. This saves the memory overhead burden of both the client and server loading audio into memory. If using common audio types, like 16-bit PCM .wav files, it may also be possible to simply pass a non-serialized file buffer. The OLIVE Java Client API provides utilities such as requestFrameScore(), requestEnrollClass(), etc. to handle various client message requests that were covered above. In these utilities, the enum argument transferType is used to select in what way the audio data is to be sent. For example, when transferType is set to AudioTransferType.SEND_SERIALIZED_BUFFER , audio data will be interpreted as if it were sent as a serialized buffer. Synchronous vs. Asynchronous Message Submission The OLIVE Client API allows the client to choose between processing a task request synchronously or asynchronously. Processing a task request synchronously means the client will block and wait for the task result to return before proceeding to other task requests. On the other hand, asynchronous processing means the client will not wait for the result to come back before moving on, allowing several jobs to be submitted in parallel. The examples below generally show submitting requests asynchronously. The argument async in the API utilities requestFrameRequest(), requestEnrollClass(), etc., can be used to select if the client intends to wait for the task result to return. When async is set to true , the client will not block when a request is sent to the server, so other task requests can be made before the results are received asynchronously and handled by the callback. Construct Request Message Information contained in a task request message may be different depending on the type of task to be performed. For a scoring task, the request message usually contains specific plugin and domain names, and the audio to be scored. For an adaptation or enrollment task, it also contains class ID information. To make a task request, the client program must first assemble the necessary information into a request message, and then send the message to the server. These 2 steps are shown in pseudo code below. Pseudocode request = packageRequest ( pluginName , pluginDomain , requestType , any other information specific to requestType ) sendRequest ( server , request , audio ) Client API Code Samples The OLIVE Reference API includes functionality pre-coded to accommodate many of the available request messages. Utilities such as requestFrameScore(), requestEnrollClass(), etc. not only do the packaging of request messages, but also take care of sending the request messages to the server, all in one call. They are available from the SRI Java API package sri.com.scenic.api.ClientUtils. The required parameters to send requests using these scoring utilities include: server handle (server - see here ) the plugin handle (pd - see here ) the name of the audio file to submit to the server (filename) channel number of the audio to be processed when audio has more than 1 channel (channelNumber) a callback function for handling results returned either asynchronously or synchronously (rc) whether the client will block for task result to return (async - see here ) an enum of how to submit audio to the server (transferType) optional lists of annotations of the submitted audio (regions) optional list of parameters for customizing plugin behavior (options) optional list of class IDs for filtering the results (classIDs) Performing an enrollment request adds an additional parameter: the ID of the class to be enrolled The primary requests covered below are: Frame score requests - used to make most SAD requests (note that some SAD plugins return regions, not frame scores) Global score requests - used to make LID, SID, or Gender score requests Region score requests - used to make QbE, KWS, Diarization, and sometimes SAD score requests Enrollment requests - used to enroll speakers or other class types for plugins that support the ClassEnroller trait Frame Score Request The example below provides sample code for a function MyFrameScoreRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Frame scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. Note that only a plugin that support the FrameScorer trait, cna handle this request. All SAD plugins support this train, while some also support the RegionScorer trait. For an example of how to call this code with a specific plugin, refer to the SAD Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyFrameScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pp , filename , 1 , rc , true , true , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } Python --- Coming Soon --- Pseudocode -- Coming Soon -- Global Score Request The example below provides sample code for a function MyGlobalScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Global scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. Note also that the code required to submit a GlobalScorerRequest message doesn't care what type of plugin is going to be doing the scoring, as long as the plugin implements the GlobalScorer Trait. This means that the exact same code can be used for submitting audio to global scoring SID plugins, LID plugins, or any other global scoring plugin. For an example of how to call this code with a specific plugin, refer to the SID Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyGlobalScorerRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a call back to handle the global scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ System . out . println ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { System . out . println ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // make the global scoring reqeust return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , true , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } Python def request_lid ( self , plugin , domain , filename , classes ): ''' Request a LID analysis of 'filename' :param plugin: the name of the LID plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param languages: optional list of languages trigraphs used to filter (restrict) results returned from the server. This list should only include languages supported by the specified plugin :return: the LID analysis as a list of (global) scores ''' request = GlobalScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio # send the name of the file to the server: audio . path = filename # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) request = self . _wrap_message ( request , GLOBAL_SCORER_REQUEST ) # Now send the message logging . debug ( \"Sending a LID (global score request) message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): broker_msg = envelope . message [ i ] if broker_msg . HasField ( \"error\" ): print broker_msg . error else : global_score_msg = GlobalScorerResult () global_score_msg . ParseFromString ( broker_msg . message_data [ 0 ]) # Assume there is only one result set (for 'speech'): frame_score_msg.result[0] print ( \"Received {} global scores\" . format ( len ( global_score_msg . result [ 0 ] . score ))) return global_score_msg . result [ 0 ] return None Pseudocode --- Coming Soon --- Region Score Request The example below provides sample code for a function MyRegionScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Region scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. For an example of how to call this code with a specific plugin, refer to the KWS Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyRegionScorerRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a call back to handle the global scoring request Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // scoring result received if ( ! r . hasError ()){ log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"SDD Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // make the region scoring reqeust return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } Python --- Coming soon --- Pseudocode -- Coming soon -- Enrollment Request The example below provides sample code for a function MyEnrollmentRequest that takes a server connection, a plugin/domain handle, the name of the class (speaker) to enroll, and a path to an audio file as arguments, and uses this information to build and submit a Region Scoring request to the connected server. Examples below show an enrollment request with one file, followed by an enrollment with multiple files (for one class). Enrollment Request with a single audio file/enrollment. Java public static boolean MyEnrollmentRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String classID , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it synchronous so we know enrollment is complete boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , classID , enrollmentFileName , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); /** can do something else now, enrollment is complete **/ return true ; } Python --- Coming soon --- Pseudocode --- Coming soon --- Batch enrollment request with multiple files. Java public static boolean MyEnrollmentUsingMultipleFilesRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String classID , ArrayList < String > audioFiles , int enrollmentFileCount ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****server processed 1 enrollment file*****\" ); responseCount ++ ; } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; for ( String audioFilename : audioFiles ) { // For simplicity, enrollments requests are done synchronously boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , classID , audioFilename , 0 , enrollmentCallback , false , AudioTransferType . SEND_AS_FILE , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out enrollment file processed } } // wait for all enrollment files to be processed while ( responseCount != enrollmentFileCount ) { try { TimeUnit . SECONDS . sleep ( 10 ); } catch ( InterruptedException e ) { System . out . println ( \"*****Timeout Exception*****\" ); } } return true ; // all enrollment files are processed, now we can proceed to do other things if necessary } Python --- Coming soon --- Pseudocode --- Coming soon --- The function call definitions for some of these scoring and enrollment request utilities follows: Java boolean requestFrameScore ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename , int channelNumber , Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a frame scoring request to the OLIVE server, e.g. SAD scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestGlobalScore ( Server server , Pair < Scenic . Plugin , Scenic . Domain > plugin , Scenic . TraitType trait , String filename , int channelNumber , Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a global scoring request to the OLIVE server, e.g. LID scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestRegionScores ( Server server , Pair < Scenic . Plugin , Scenic . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a region scoring request to the OLIVE server, e.g. KWS scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestEnrollClass ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String id , String wavePath , int channelNumber , Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options ) This call submits an enrollment request to the OLIVE server, e.g. enrolling speakers in a SID plugin. A class ID (id) is required for enrollment. If selected regions of an audio are used for enrollment, they are passed along in a list (regions). To see examples of how these utilities are used, consult sample client code provided in the next section. Plugin Specific Code Examples This section shows examples of using the functions just outlined to make calls to specific plugins, and demonstrate how the same code can be reused for several purposes - for example, requestGlobalScore is valid to request scoring from both SID and LID plugins. All client examples below can be found in the OLIVE-API-examples tree in the src/main/java/com/sri/scenic/api/client folder. SAD Scoring Example SID Enrollment and Scoring Example LID Enrollment and Scoring Example (synchronized approach) LID Enrollment and Scoring Example (asynchronized approach) KWS Scoring Example SDD Enrollment and Scoring Example SAD Supervised Adaptation Example TPD Enrollment and Scoring Example SAD Scoring Request This shows a full implementation of a client program which sends a frame scoring request to a SAD plugin. Upon return of the result, it outputs the received frame scores. Included also is a second version where a threshold is used to filter out frame scores which are higher. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.ArrayBlockingQueue ; import java.util.concurrent.BlockingQueue ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MySADFrameScorer { private static Logger log = LoggerFactory . getLogger ( MySADFrameScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"tel-v1\" ; private static String pluginName = \"sad-dnn-v6_rc5\" ; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // from the list of plugins, find the targeted plugin for the task Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Scenic . TraitType . FRAME_SCORER , pluginList ); // formulate SAD frame scoring request and send to server MyFrameScoreRequest ( server , pd , audioFileName ); } public static boolean MyFrameScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pp , filename , 1 , rc , true , true , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } } The code above outputs all frame scores of the input audio, this can generate massive amount of output, especially when the audio is long. One good way to trim down the output is to filter out regions with frame scores higher than a preset threshold value. The following shows how this can be done in the @Override section of the callback routine, using a threshold of 0.0. Java // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); // filter to output speech regions with scores > 0.0 int speech_start = 0 ; int speech_end = 0 ; double sum_scores = 0.0 ; int num_frames = 0 ; for ( int i = 0 ; i < scores . length ; i ++ ) { if ( scores [ i ] > 0.0 ) { if ( speech_end == speech_start ) { speech_start = i ; } speech_end = i + 1 ; sum_scores = sum_scores + scores [ i ] ; num_frames = num_frames + 1 ; } else { if ( speech_end > speech_start ) { int start = ( int ) ( 100 * speech_start / ( double ) rate ); int end = ( int ) ( 100 * speech_end / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , sum_scores / num_frames )); } speech_start = i ; speech_end = i ; sum_scores = 0.0 ; num_frames = 0 ; } } } } System . exit ( 0 ); } }; SID Enrollment and Scoring Request This example is a full implementation of a client program which sends an enrollment request to a SID plugin, followed by a scoring request to the same SID plugin. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.atomic.AtomicInteger ; public class MySIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multicond-v1\" ; private static String pluginName = \"sid-embed-v5b\" ; private static String enrollmentFileName ; private static String speakerName = \"Mr.X\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static String outputDirName = \"./\" ;; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain SID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform SID enrollment task MySIDEnrollmentAndScoreRequest ( server , pd , speakerName , enrollmentFileName , scoreWaveFileName ); } public static boolean MySIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String enrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it a synchronized call, so we know the speaker is enrolled before we make the score request boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); if ( enrolled ){ // Create a call back to handle the SID scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ System . out . println ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { System . out . println ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // SID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , AudioTransferType . SEND_SERIALIZED_BUFFER , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } } LID Enrollment and Scoring Request Synchronized Approach A LID enrollment of a new language may involve multiple enrollment files. The synchronized version of this client waits for the server to complete the enrollment request with each enrollment file sequentially, before requesting LID scores on an input file. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MysyncLIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MysyncLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v1-rc6\" ; private static String audioList ; private static ArrayList < String > audioFiles = new ArrayList < String > (); private static String audioFilename ; private static int enrollmentFileCount = 0 ; private static String languageName = \"JKL\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String audioList = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // read enrollment files from input list if ( ! Files . exists ( Paths . get ( audioList ). toAbsolutePath ())) { System . err . println ( \"ERROR: '\" + audioList + \"' does not exist\" ); } BufferedReader br = new BufferedReader ( new FileReader ( audioList )); while (( audioFilename = br . readLine ()) != null ){ audioFiles . add ( audioFilename ); } enrollmentFileCount = audioFiles . size (); // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain LID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform LID enrollment task MyLIDEnrollmentAndScoreRequest ( server , pd , languageName , audioFiles , enrollmentFileCount , scoreWaveFileName ); } public static boolean MyLIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String languageName , ArrayList < String > audioFiles , int enrollmentFileCount , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****Enrollment Succeeded*****\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment requests, with one enrollment file at a time // make a synchronized enrollment call, so we know the language is enrolled before we make the score request for ( String audioFilename : audioFiles ) { boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , languageName , audioFilename , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out name of enrollment file used } } // do the score request // First create a call back to handle the LID scoring result Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ log . info ( \"\\t{} = {}\" , gs . getClassId (), gs . getScore ()); } } else { log . error ( \"Global scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } } Asynchronized Approach This example provides an alternate LID client example that performs asynchronized enrollment and does not wait for the last enrollment request to complete before making another. Instead, it keeps count on server responses to enrollment requests made, and makes sure that all enrollment requests are completed before making a LID scoring request on an input file. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MyasyncLIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MyasyncLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v1-rc6\" ; private static String audioList ; private static ArrayList < String > audioFiles = new ArrayList < String > (); private static String audioFilename ; private static int enrollmentFileCount = 0 ; private static int responseCount = 0 ; private static String languageName = \"MNO\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static String outputDirName = \"./\" ;; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment list is passed in as an argument String audioList = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // read enrollment files from input list and keep count of enrollment files to be processed if ( ! Files . exists ( Paths . get ( audioList ). toAbsolutePath ())) { System . err . println ( \"ERROR: '\" + audioList + \"' does not exist\" ); } BufferedReader br = new BufferedReader ( new FileReader ( audioList )); while (( audioFilename = br . readLine ()) != null ){ audioFiles . add ( audioFilename ); } enrollmentFileCount = audioFiles . size (); // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain LID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform LID enrollment task MyLIDEnrollmentAndScoreRequest ( server , pd , languageName , audioFiles , enrollmentFileCount , scoreWaveFileName ); } public static boolean MyLIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String languageName , ArrayList < String > audioFiles , int enrollmentFileCount , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****server processed 1 enrollment file*****\" ); responseCount ++ ; } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; for ( String audioFilename : audioFiles ) { boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , languageName , audioFilename , 0 , enrollmentCallback , true , true , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out enrollment file processed } } // wait for all enrollments to complete while ( responseCount != enrollmentFileCount ) { try { TimeUnit . SECONDS . sleep ( 10 ); } catch ( InterruptedException e ) { System . out . println ( \"*****Timeout Exception*****\" ); } } // We next do the score request // First create a call back to handle the LID scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ log . info ( \"\\t{} = {}\" , gs . getClassId (), gs . getScore ()); } } else { log . error ( \"Global scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , true , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } } KWS Scoring Request This portrays a full implementation of a KWS scoring request, against the list of default keywords already trained. Note that unlike the previous examples that requested GlobalScores, this client issues a RegionScoreRequest because KWS is a \"RegionScorer\". Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MyKWSRegionScorer { private static Logger log = LoggerFactory . getLogger ( MyKWSRegionScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"r-tdnn-tel-v1\" ; private static String pluginName = \"kws-dynapy-v1\" ; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // formulate the frame scoring task request Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"KWS\" , Scenic . TraitType . REGION_SCORER , pluginList ); // Perform KWS frame scoring task MyRegionScoresRequest ( server , pd , audioFileName ); } public static boolean MyRegionScoresRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle KWS results from the server Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > rc = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; return ClientUtils . requestRegionScores ( server , pp , filename , 0 , rc , true , false , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } } SDD Enrollment And Scoring Request The following code shows a full implementation of a speaker enrollment request, followed by a scoring request made to a SDD plugin. The scoring request in this client is a RegionScorerRequest because SDD plugins are region scorers. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MySDDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySDDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multicond-v1\" ; private static String pluginName = \"sdd-sbc-embed-v1-1\" ; private static String enrollmentFileName ; private static String speakerName = \"Mr.X\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain SDD plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform SDD enrollment task MySDDEnrollmentAndScoreRequest ( server , pd , speakerName , enrollmentFileName , scoreWaveFileName ); } public static boolean MySDDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String enrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // enrollment result received if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make a synchronized enrollment request, so we know the speaker is enrolled before we make the score request boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); if ( enrolled ){ // do the SDD scoring request // First create a callback to handle scoring result from server Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // scoring result received if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"SDD Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // SDD is a region scorer, so make a region score reqeust: return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } SAD Supervised Adaptation Request Below is a full implementation of a SAD request to adapt a new domain from an existing domain. The list of adaptation training files is passed into the client as a file. The code handles both supervised (speech regions specified) and unsupervised (speech regions not specified) SAD adaptations. However, some SAD plugins may not have the unsupervised adaptation capability, in which case the client will exit with a failure message. Java package com.sri.scenic.api.client ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Scenic.AnnotationRegion ; import com.sri.scenic.api.Scenic.AudioAnnotation ; import com.sri.scenic.api.Scenic.Domain ; import com.sri.scenic.api.Scenic.Plugin ; import com.sri.scenic.api.Scenic.TraitType ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.ClientUtils ; import com.sri.scenic.api.utils.LearningParser ; import com.sri.scenic.api.utils.LearningParser.LearningDataType ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Paths ; import java.util.ArrayList ; import java.util.Collection ; import java.util.HashMap ; import java.util.List ; import java.util.Map ; import java.util.UUID ; import javax.sound.sampled.UnsupportedAudioFileException ; import org.apache.commons.cli.CommandLine ; import org.apache.commons.cli.CommandLineParser ; import org.apache.commons.cli.DefaultParser ; import org.apache.commons.cli.HelpFormatter ; import org.apache.commons.cli.Option ; import org.apache.commons.cli.Options ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; public class MySADAdaptation { private static Logger log = LoggerFactory . getLogger ( MySADAdaptation . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"tel-v1\" ; private static String newDomainName = \"new-tel-v1\" ; private static String pluginName = \"sad-dnn-v6_rc5\" ; private static LearningParser learningParser = new LearningParser (); private static LearningDataType dataType ; /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file list is passed as an argument String audioFileList = args [ 0 ] ; learningParser . parse ( audioFileList ); if ( ! learningParser . isValid ()) { System . err . println ( \"Invalid input file: \" + audioFileList ); System . exit ( - 1 ); } if ( learningParser . hasRegions ()) { dataType = LearningDataType . SUPERVISED_WITH_REGIONS ; } else if ( learningParser . hasClasses ()) { dataType = LearningDataType . SUPERVISED ; } else { dataType = LearningDataType . UNSUPERVISED ; } // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // find plugin handle for adaptation task Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , learningParser . isUnsupervised () ? Scenic . TraitType . UNSUPERVISED_ADAPTER : Scenic . TraitType . SUPERVISED_ADAPTER , pluginList ); // Preproces audio - doesn't matter if supervised or unsupervised String adaptID = UUID . randomUUID (). toString (); Plugin p = pd . getFirst (); Domain d = pd . getSecond (); // optional annotations, generated if found in the parser (supervised adaptation) Map < String , List < AudioAnnotation >> annotations = new HashMap <> (); // annotations -> <classID> ->* <AudioAnnotations>, annotations will be empty for unsupervised adaptation int numPreprocessed = OliveLearn . preprocessAllAudio ( server , p , d , learningParser , adaptID , annotations ); if ( ! learningParser . isUnsupervised ()) { // supervised adaptation if ( numPreprocessed > 0 ) { OliveLearn . finalizeSupervisedAdaptation ( server , p , d , adaptID , newDomainName , annotations ); } else { System . err . println ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } else { // unsupervised adaptation if ( numPreprocessed > 0 ) { OliveLearn . finalizeUnsupervisedAdaptation ( server , p , d , adaptID , newDomainName ); } else { System . err . println ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } System . out . println ( \"\" ); System . out . println ( \"Learning finished. Exiting...\" ); System . exit ( 0 ); } } TPD Enrollment and Scoring Request This final example shows a full implementation of a TPD topic enrollment request using positive examples from selected regions of an audio, followed by a scoring request of the enrolled topic on another audio. Enrollment using negative audio samples are also included in this example as commented out lines. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.atomic.AtomicInteger ; public class MyTPDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MyTPDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"r-tdnn-tel-v1\" ; private static String pluginName = \"tpd-dynapy-v1\" ; private static String posEnrollmentFileName ; private static String negEnrollmentFileName ; private static String topicName = \"hello\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); //private static String outputDirName = \"./\";; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String posEnrollmentFileName = args [ 0 ] ; String negEnrollmentFileName = args [ 1 ] ; String scoreWaveFileName = args [ 2 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // set audio mode to read from buffer //Scenic.Audio audio = SimpleClient.packageAudioAsRawBuffer(enrollmentFileName, 0, null); // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain TPD plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform TPD enrollment and scoring tasks MyTPDEnrollmentAndScoreRequest ( server , pd , topicName , posEnrollmentFileName , negEnrollmentFileName , scoreWaveFileName ); } public static boolean MyTPDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String posEnrollmentFileName , String negEnrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } //enroll a positive example List < RegionWord > posRegions = new ArrayList <> (); posRegions . add ( new RegionWord ( 0.100 , 2.500 )); // NOTE to add regions in seconds posRegions . add ( new RegionWord ( 7.500 , 10.000 )); // NOTE to add regions in seconds /** //we can also enroll a negative example enrollmentOptions.add(new Pair<>(\"isNegative\", \"True\")); List<RegionWord> negRegions = new ArrayList<>(); negRegions.add(new RegionWord(1.000, 1.500)); // NOTE to add regions in seconds negRegions.add(new RegionWord(5.000, 7.000)); // NOTE to add regions in seconds **/ // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it a synchronized call, so we know enrollment is complete before we score boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , topicName , posEnrollmentFileName , 0 , enrollmentCallback , false , false , posRegions , enrollmentOptions ); /** //goes with negative example enrolled=ClientUtils.requestEnrollClass(server, pp, topicName, negEnrollmentFileName, 0, enrollmentCallback, false, false, posRegions, enrollmentOptions); **/ if ( enrolled ){ // Create a call back to handle the TPD scoring request Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} regions:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()){ log . info ( \"\\t{} = {}, From {} to {}\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // TPD is a region scorer, so make a region score reqeust return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } }","title":"Integrating (Java) Client API"},{"location":"apiCode.html#integrating-a-client-api-with-olive","text":"","title":"Integrating a Client API with OLIVE"},{"location":"apiCode.html#java-client-ide-setup-guide","text":"If you would like to import the provided OLIVE Java API sample code and reference implementation into an IDE to explore the code and/or get started with integrating this code, please refer over to the Java Client IDE Setup Guide page. If you've already done this step, plan on integrating this functionality just using the provided JAR file, or just wish to browse code samples, continue below.","title":"Java Client IDE Setup Guide"},{"location":"apiCode.html#the-olive-java-api","text":"While the OLIVE system allows client integration via Protobuf messages sent from a variety of languages such as Java, Python, C++ and C#, and machine types such as Windows, Linux, and MacOS, the currently available OLIVE reference API implementation is Java-based only, and is also referred to as the OLIVE Java Client API. Therefore all instructions and code examples presented in this section currently assume that client programs are also in Java. Pseudocode and Python versions of the included code will be rolled into this page as they are available. Fundamentally any OLIVE API implementation is based on exchanging Protobuf messages between the client and server. These messages are defined in the API Message Reference Documentation. If you would like to create a reference implementation of the OLIVE API in another language, please refer to the information in the Developing an OLIVE API Reference Implementation page, that should prove to be helpful. If you would like an API reference implementation in another language to be created, please reach out to olive-support@sri.com to discuss your needs with the team.","title":"The OLIVE Java API"},{"location":"apiCode.html#basic-recipe-for-using-the-olive-java-client-api","text":"This section covers the first steps towards building a client based on implementing SRI's OLIVE API Java Reference implementation in order to use the OLIVE Java API for speech processing. The sections immediately following cover setting up an OLIVE server instance to connect to, then move into a number of steps necessary to connect a client to this server, and finally cover how to build and submit scoring and enrollment requests from the client program. The first step towards integrating a client with the provided OLIVE Java Reference Implementation is to review the API Message Reference to understand the available request and response messages, then dive in by setting up your IDE to work with this code. We have created a step-by-step guide for this configuration and setup process. Note that these steps are not necessary for implementing the actual functionality from this software package, but could be very useful for exploring and learning the code. If only the functionality from this reference implementation is desired, SRI can provide an appropriate JAR file to include in your project.","title":"Basic Recipe for using the OLIVE Java Client API"},{"location":"apiCode.html#olive-server","text":"As mentioned above, the OLIVE Enterprise API operates on a client/server model, making the OLIVE server an essential part of the OLIVE system. You must run the OLIVE server and manage its lifecycle as part of your integration effort. For more information about the server's duties, how to interact with it, what options and functionality are available to it, and general setup and troubleshooting information, please refer to the OLIVE Server Information Page .","title":"OLIVE server"},{"location":"apiCode.html#establish-server-connection","text":"Before making any task request, a client must establish a connection with the server. By default, the OLIVE server listens on ports 5588 (request port) and 5589 (status port) for client connection and status requests. These ports are configurable, but if the server has not been instructed to change its listening ports, the code below should establish a connection. Using the OLIVE Java API, connection to the server can be made with a single call, as shown below. This call is available from the API Server class , included in the package com.sri.scenic.api.Server. Java Server server = new Server (); server . connect ( \"scenic-client\" , //client-id \"localhost\" , //address of server 5588 , //request-port 5589 , //status-port 10000 //timeout for failed connection request ); Python --- Coming Soon --- Pseudocode connect ( server - host - name , 5588 , 5589 ) The request port (5588 by default) is used for call and response messages (Protobuf messages). Each request message sent to this port is guaranteed a response from the server (this is why the messages in the API Message Reference are often suffixed with 'Request' and 'Result'). There is no need to poll the server for information on a submitted request, as the result/response for the a request is returned to the client as soon as it is available. The status port (5589 by default) is used by the Server to publish health and status messages (Heartbeat) to client(s). Clients can not send requests on this port.","title":"Establish Server Connection"},{"location":"apiCode.html#request-available-plugins","text":"In order to submit most server requests, the client must specify the plugin and domain to handle the request. To obtain the handle of a targeted plugin, the client first requests a list of all currently available valid plugins from the server. From the returned plugins list, the client looks up the specific plugin handle by the plugin's unique name (id) and its associated trait for the task to be performed. This handle can then be used in a future request message sent to the server. In the Java API Reference Implementation, this is accomplished by the utilities requestPlugin() and findPluginDomainByTrait(), within the included ClientUtils package. The steps are shown below. Java // ask the server for a list of currently available plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // Look up a specific plugin from the plugin list using // the plugin's unique name (pluginName and domainName) and associated trait e.g. FRAME_SCORER Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Scenic . TraitType . FRAME_SCORER , pluginList ); Python def request_plugins ( self ): request = PluginDirectoryRequest () # Wrap message in an Envelop request = self . _wrap_message ( request , FRAME_SCORER_REQUEST ) # Now send the message logging . debug ( \"Sending a Plugin request message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message...\" ) envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): server_msg = envelope . message [ i ] if broker_msg . HasField ( \"error\" ): print ( server_msg . error ) else : plugin_dir_msg = PluginDirectoryResult () plugin_dir_msg . ParseFromString ( server_msg . message_data [ 0 ]) # There is likely to be multiple plugin/domains print ( \"Server has {} plugins \" . format ( len ( plugin_dir_msg . plugins ))) for p in plugin_dir_msg . plugins : print ( \"Plugin {} has {} domains\" . format ( p . id , len ( p . domain ))) Pseudocode plugin - list = getListOfAvailablePlugins ( server ); targeted - plugin = lookupPlugin ( plugin - list , pluginName , pluginTrait ); The targeted plugin handle, pd in the above example, can then be used with other utilities within the Client API to submit requests, or to otherwise interact with the plugin. For example, this code below shows how one might check what Traits that each plugin in the pluginList returned above support: Java for ( Pair < Scenic . Plugin , Scenic . Domain > pair : pluginList ){ for ( Scenic . Trait t : pair . getFirst (). getTraitList ()) { if ( t . getType () == Scenic . TraitType . FRAME_SCORER ){ log . info ( \"Plugin {} supports frame scoring\" , pair . getFirst (). getId ()); } if ( t . getType () == Scenic . TraitType . GLOBAL_SCORER ){ // Supports global scoring (i.e. SID or LID) log . info ( \"Plugin {} supports global scoring\" , pair . getFirst (). getId ()); } if ( t . getType () == Scenic . TraitType . REGION_SCORER ){ // Supports region scoring (i.e. KWS) log . info ( \"Plugin {} supports region scoring\" , pair . getFirst (). getId ()); } } } Python -- coming soon -- Pseudocode --coming soon--","title":"Request Available Plugins"},{"location":"apiCode.html#audio-submission-guidelines","text":"One of the core client activities is submitting Audio with a request. In the OLIVE API, three ways are provided for a client to package audio data to send to the OLIVE server: file path buffer of raw audio sample data serialized file buffer object The enum AudioTransferType is used to specify they type of audio transfer to use. When the client and the OLIVE server share the same file system, the easiest way for the client to send audio data to the server is by specifying the audio's file path on disk. The OLIVE Client API provides the utility below to package audio files which are accessible to the server locally: Java packageAudioAsPath () // AudioTransferType.SEND_AS_PATH When the client and the server don't share the same file system, as in the case of a client making a remote connection to the OLIVE server, it is necessary to send the client's local audio files as a file buffer. To help package the client's audio data in a raw buffer, the OLIVE Client API provides the utility below: Java packageAudioAsRawBuffer () // AudioTransferType.SEND_SAMPLES_BUFFER When submitting audio to the server as a buffer of raw samples, it is important to include information characterizing the audio, such as the bit depth, audio encoding, sample rate, and number of channels, to ensure the server knows how to properly treat the buffer it receives. A third utility, shown below, also packages the client's audio data in a buffer. This utility passes the original file to the server in its entirety in one contiguous buffer, leaving the audio file header intact. Java packageAudioAsSerializedBuffer () // AudioTransferType.SEND_SERIALIZED_BUFFER Sending audio data as a serialized file buffer ensures that all audio header information is provided intact to the server. This allows the server to properly decode and process the audio once its received, since it can directly access the bit depth, encoding type, sample rate and other necessary information from the header itself. The tradeoff with serialized files is that there may be additional overhead needed to process the audio into a consumable form. If the client and server reside on the same hardware and file system, it is advisable to simply pass filepaths when possible. This saves the memory overhead burden of both the client and server loading audio into memory. If using common audio types, like 16-bit PCM .wav files, it may also be possible to simply pass a non-serialized file buffer. The OLIVE Java Client API provides utilities such as requestFrameScore(), requestEnrollClass(), etc. to handle various client message requests that were covered above. In these utilities, the enum argument transferType is used to select in what way the audio data is to be sent. For example, when transferType is set to AudioTransferType.SEND_SERIALIZED_BUFFER , audio data will be interpreted as if it were sent as a serialized buffer.","title":"Audio Submission Guidelines"},{"location":"apiCode.html#synchronous-vs-asynchronous-message-submission","text":"The OLIVE Client API allows the client to choose between processing a task request synchronously or asynchronously. Processing a task request synchronously means the client will block and wait for the task result to return before proceeding to other task requests. On the other hand, asynchronous processing means the client will not wait for the result to come back before moving on, allowing several jobs to be submitted in parallel. The examples below generally show submitting requests asynchronously. The argument async in the API utilities requestFrameRequest(), requestEnrollClass(), etc., can be used to select if the client intends to wait for the task result to return. When async is set to true , the client will not block when a request is sent to the server, so other task requests can be made before the results are received asynchronously and handled by the callback.","title":"Synchronous vs. Asynchronous Message Submission"},{"location":"apiCode.html#construct-request-message","text":"Information contained in a task request message may be different depending on the type of task to be performed. For a scoring task, the request message usually contains specific plugin and domain names, and the audio to be scored. For an adaptation or enrollment task, it also contains class ID information. To make a task request, the client program must first assemble the necessary information into a request message, and then send the message to the server. These 2 steps are shown in pseudo code below. Pseudocode request = packageRequest ( pluginName , pluginDomain , requestType , any other information specific to requestType ) sendRequest ( server , request , audio )","title":"Construct Request Message"},{"location":"apiCode.html#client-api-code-samples","text":"The OLIVE Reference API includes functionality pre-coded to accommodate many of the available request messages. Utilities such as requestFrameScore(), requestEnrollClass(), etc. not only do the packaging of request messages, but also take care of sending the request messages to the server, all in one call. They are available from the SRI Java API package sri.com.scenic.api.ClientUtils. The required parameters to send requests using these scoring utilities include: server handle (server - see here ) the plugin handle (pd - see here ) the name of the audio file to submit to the server (filename) channel number of the audio to be processed when audio has more than 1 channel (channelNumber) a callback function for handling results returned either asynchronously or synchronously (rc) whether the client will block for task result to return (async - see here ) an enum of how to submit audio to the server (transferType) optional lists of annotations of the submitted audio (regions) optional list of parameters for customizing plugin behavior (options) optional list of class IDs for filtering the results (classIDs) Performing an enrollment request adds an additional parameter: the ID of the class to be enrolled The primary requests covered below are: Frame score requests - used to make most SAD requests (note that some SAD plugins return regions, not frame scores) Global score requests - used to make LID, SID, or Gender score requests Region score requests - used to make QbE, KWS, Diarization, and sometimes SAD score requests Enrollment requests - used to enroll speakers or other class types for plugins that support the ClassEnroller trait","title":"Client API Code Samples"},{"location":"apiCode.html#frame-score-request","text":"The example below provides sample code for a function MyFrameScoreRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Frame scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. Note that only a plugin that support the FrameScorer trait, cna handle this request. All SAD plugins support this train, while some also support the RegionScorer trait. For an example of how to call this code with a specific plugin, refer to the SAD Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyFrameScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pp , filename , 1 , rc , true , true , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } Python --- Coming Soon --- Pseudocode -- Coming Soon --","title":"Frame Score Request"},{"location":"apiCode.html#global-score-request","text":"The example below provides sample code for a function MyGlobalScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Global scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. Note also that the code required to submit a GlobalScorerRequest message doesn't care what type of plugin is going to be doing the scoring, as long as the plugin implements the GlobalScorer Trait. This means that the exact same code can be used for submitting audio to global scoring SID plugins, LID plugins, or any other global scoring plugin. For an example of how to call this code with a specific plugin, refer to the SID Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyGlobalScorerRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a call back to handle the global scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ System . out . println ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { System . out . println ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // make the global scoring reqeust return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , true , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } Python def request_lid ( self , plugin , domain , filename , classes ): ''' Request a LID analysis of 'filename' :param plugin: the name of the LID plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param languages: optional list of languages trigraphs used to filter (restrict) results returned from the server. This list should only include languages supported by the specified plugin :return: the LID analysis as a list of (global) scores ''' request = GlobalScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio # send the name of the file to the server: audio . path = filename # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) request = self . _wrap_message ( request , GLOBAL_SCORER_REQUEST ) # Now send the message logging . debug ( \"Sending a LID (global score request) message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): broker_msg = envelope . message [ i ] if broker_msg . HasField ( \"error\" ): print broker_msg . error else : global_score_msg = GlobalScorerResult () global_score_msg . ParseFromString ( broker_msg . message_data [ 0 ]) # Assume there is only one result set (for 'speech'): frame_score_msg.result[0] print ( \"Received {} global scores\" . format ( len ( global_score_msg . result [ 0 ] . score ))) return global_score_msg . result [ 0 ] return None Pseudocode --- Coming Soon ---","title":"Global Score Request"},{"location":"apiCode.html#region-score-request","text":"The example below provides sample code for a function MyRegionScorerRequest that takes a server connection, a plugin/domain handle, and a path to an audio file as arguments, and uses this information to build and submit a Region scoring request to the connected server. Note that this code passes the audio to the server using a simple file path, assuming that the client and server have a shared file system. It is also possible to perform this request using buffered audio samples or a serialized file. For an example of how to call this code with a specific plugin, refer to the KWS Scoring Request code example below. The full Java code file this example was pulled from is also available, showing the process of establishing a server connection, and polling the server for available plugins to retrieve the appropriate plugin/domain handle. Java public static boolean MyRegionScorerRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a call back to handle the global scoring request Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // scoring result received if ( ! r . hasError ()){ log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"SDD Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // make the region scoring reqeust return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } Python --- Coming soon --- Pseudocode -- Coming soon --","title":"Region Score Request"},{"location":"apiCode.html#enrollment-request","text":"The example below provides sample code for a function MyEnrollmentRequest that takes a server connection, a plugin/domain handle, the name of the class (speaker) to enroll, and a path to an audio file as arguments, and uses this information to build and submit a Region Scoring request to the connected server. Examples below show an enrollment request with one file, followed by an enrollment with multiple files (for one class).","title":"Enrollment Request"},{"location":"apiCode.html#enrollment-request-with-a-single-audio-fileenrollment","text":"Java public static boolean MyEnrollmentRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String classID , String enrollmentFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it synchronous so we know enrollment is complete boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , classID , enrollmentFileName , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); /** can do something else now, enrollment is complete **/ return true ; } Python --- Coming soon --- Pseudocode --- Coming soon ---","title":"Enrollment Request with a single audio file/enrollment."},{"location":"apiCode.html#batch-enrollment-request-with-multiple-files","text":"Java public static boolean MyEnrollmentUsingMultipleFilesRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String classID , ArrayList < String > audioFiles , int enrollmentFileCount ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****server processed 1 enrollment file*****\" ); responseCount ++ ; } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; for ( String audioFilename : audioFiles ) { // For simplicity, enrollments requests are done synchronously boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , classID , audioFilename , 0 , enrollmentCallback , false , AudioTransferType . SEND_AS_FILE , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out enrollment file processed } } // wait for all enrollment files to be processed while ( responseCount != enrollmentFileCount ) { try { TimeUnit . SECONDS . sleep ( 10 ); } catch ( InterruptedException e ) { System . out . println ( \"*****Timeout Exception*****\" ); } } return true ; // all enrollment files are processed, now we can proceed to do other things if necessary } Python --- Coming soon --- Pseudocode --- Coming soon --- The function call definitions for some of these scoring and enrollment request utilities follows: Java boolean requestFrameScore ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename , int channelNumber , Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a frame scoring request to the OLIVE server, e.g. SAD scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestGlobalScore ( Server server , Pair < Scenic . Plugin , Scenic . Domain > plugin , Scenic . TraitType trait , String filename , int channelNumber , Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a global scoring request to the OLIVE server, e.g. LID scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestRegionScores ( Server server , Pair < Scenic . Plugin , Scenic . Domain > plugin , String filename , int channelNumber , Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options , List < String > classIDs ) This call submits a region scoring request to the OLIVE server, e.g. KWS scoring. The returned results are a list of regions (regions and scores) and possibly a list of associated class IDs (classIDs). Java boolean requestEnrollClass ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String id , String wavePath , int channelNumber , Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > rc , boolean async , AudioTransferType transferType , List < RegionWord > regions , List < Pair < String , String >> options ) This call submits an enrollment request to the OLIVE server, e.g. enrolling speakers in a SID plugin. A class ID (id) is required for enrollment. If selected regions of an audio are used for enrollment, they are passed along in a list (regions). To see examples of how these utilities are used, consult sample client code provided in the next section.","title":"Batch enrollment request with multiple files."},{"location":"apiCode.html#plugin-specific-code-examples","text":"This section shows examples of using the functions just outlined to make calls to specific plugins, and demonstrate how the same code can be reused for several purposes - for example, requestGlobalScore is valid to request scoring from both SID and LID plugins. All client examples below can be found in the OLIVE-API-examples tree in the src/main/java/com/sri/scenic/api/client folder. SAD Scoring Example SID Enrollment and Scoring Example LID Enrollment and Scoring Example (synchronized approach) LID Enrollment and Scoring Example (asynchronized approach) KWS Scoring Example SDD Enrollment and Scoring Example SAD Supervised Adaptation Example TPD Enrollment and Scoring Example","title":"Plugin Specific Code Examples"},{"location":"apiCode.html#sad-scoring-request","text":"This shows a full implementation of a client program which sends a frame scoring request to a SAD plugin. Upon return of the result, it outputs the received frame scores. Included also is a second version where a threshold is used to filter out frame scores which are higher. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.ArrayBlockingQueue ; import java.util.concurrent.BlockingQueue ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MySADFrameScorer { private static Logger log = LoggerFactory . getLogger ( MySADFrameScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"tel-v1\" ; private static String pluginName = \"sad-dnn-v6_rc5\" ; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // from the list of plugins, find the targeted plugin for the task Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , Scenic . TraitType . FRAME_SCORER , pluginList ); // formulate SAD frame scoring request and send to server MyFrameScoreRequest ( server , pd , audioFileName ); } public static boolean MyFrameScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); for ( int i = 0 ; i < scores . length ; i ++ ) { int start = ( int ) ( 100 * i / ( double ) rate ); int end = ( int ) ( 100 * ( i + 1 ) / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , scores [ i ] )); } } } System . exit ( 0 ); } }; return ClientUtils . requestFrameScore ( server , pp , filename , 1 , rc , true , true , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } } The code above outputs all frame scores of the input audio, this can generate massive amount of output, especially when the audio is long. One good way to trim down the output is to filter out regions with frame scores higher than a preset threshold value. The following shows how this can be done in the @Override section of the callback routine, using a threshold of 0.0. Java // Create a callback to handle SAD results from the server Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > rc = new Server . ResultCallback < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > () { @Override public void call ( Server . Result < Scenic . FrameScorerRequest , Scenic . FrameScorerResult > r ) { // output frame scores if ( ! r . hasError ()) { for ( Scenic . FrameScores fs : r . getRep (). getResultList ()) { System . out . println ( String . format ( \"Received %d frame scores for '%s'\" , fs . getScoreCount (), fs . getClassId ())); Double [] scores = fs . getScoreList (). toArray ( new Double [ fs . getScoreList (). size () ] ); int rate = fs . getFrameRate (); // filter to output speech regions with scores > 0.0 int speech_start = 0 ; int speech_end = 0 ; double sum_scores = 0.0 ; int num_frames = 0 ; for ( int i = 0 ; i < scores . length ; i ++ ) { if ( scores [ i ] > 0.0 ) { if ( speech_end == speech_start ) { speech_start = i ; } speech_end = i + 1 ; sum_scores = sum_scores + scores [ i ] ; num_frames = num_frames + 1 ; } else { if ( speech_end > speech_start ) { int start = ( int ) ( 100 * speech_start / ( double ) rate ); int end = ( int ) ( 100 * speech_end / ( double ) rate ); System . out . println ( String . format ( \"start: '%d' end: '%d' score:'%f'\" , start , end , sum_scores / num_frames )); } speech_start = i ; speech_end = i ; sum_scores = 0.0 ; num_frames = 0 ; } } } } System . exit ( 0 ); } };","title":"SAD Scoring Request"},{"location":"apiCode.html#sid-enrollment-and-scoring-request","text":"This example is a full implementation of a client program which sends an enrollment request to a SID plugin, followed by a scoring request to the same SID plugin. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.atomic.AtomicInteger ; public class MySIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multicond-v1\" ; private static String pluginName = \"sid-embed-v5b\" ; private static String enrollmentFileName ; private static String speakerName = \"Mr.X\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static String outputDirName = \"./\" ;; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain SID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform SID enrollment task MySIDEnrollmentAndScoreRequest ( server , pd , speakerName , enrollmentFileName , scoreWaveFileName ); } public static boolean MySIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String enrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it a synchronized call, so we know the speaker is enrolled before we make the score request boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , AudioTransferType . SEND_SERIALIZED_BUFFER , new ArrayList <> (), enrollmentOptions ); if ( enrolled ){ // Create a call back to handle the SID scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ System . out . println ( String . format ( \"speaker{%s} = {%f}\" , gs . getClassId (), gs . getScore ())); } } else { System . out . println ( String . format ( \"Global scorer error: {%s}\" , r . getError ())); } System . exit ( 0 ); } }; // SID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , AudioTransferType . SEND_SERIALIZED_BUFFER , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } }","title":"SID Enrollment and Scoring Request"},{"location":"apiCode.html#lid-enrollment-and-scoring-request","text":"","title":"LID Enrollment and Scoring Request"},{"location":"apiCode.html#synchronized-approach","text":"A LID enrollment of a new language may involve multiple enrollment files. The synchronized version of this client waits for the server to complete the enrollment request with each enrollment file sequentially, before requesting LID scores on an input file. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MysyncLIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MysyncLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v1-rc6\" ; private static String audioList ; private static ArrayList < String > audioFiles = new ArrayList < String > (); private static String audioFilename ; private static int enrollmentFileCount = 0 ; private static String languageName = \"JKL\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String audioList = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // read enrollment files from input list if ( ! Files . exists ( Paths . get ( audioList ). toAbsolutePath ())) { System . err . println ( \"ERROR: '\" + audioList + \"' does not exist\" ); } BufferedReader br = new BufferedReader ( new FileReader ( audioList )); while (( audioFilename = br . readLine ()) != null ){ audioFiles . add ( audioFilename ); } enrollmentFileCount = audioFiles . size (); // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain LID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform LID enrollment task MyLIDEnrollmentAndScoreRequest ( server , pd , languageName , audioFiles , enrollmentFileCount , scoreWaveFileName ); } public static boolean MyLIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String languageName , ArrayList < String > audioFiles , int enrollmentFileCount , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****Enrollment Succeeded*****\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment requests, with one enrollment file at a time // make a synchronized enrollment call, so we know the language is enrolled before we make the score request for ( String audioFilename : audioFiles ) { boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , languageName , audioFilename , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out name of enrollment file used } } // do the score request // First create a call back to handle the LID scoring result Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ log . info ( \"\\t{} = {}\" , gs . getClassId (), gs . getScore ()); } } else { log . error ( \"Global scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } }","title":"Synchronized Approach"},{"location":"apiCode.html#asynchronized-approach","text":"This example provides an alternate LID client example that performs asynchronized enrollment and does not wait for the last enrollment request to complete before making another. Instead, it keeps count on server responses to enrollment requests made, and makes sure that all enrollment requests are completed before making a LID scoring request on an input file. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MyasyncLIDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MyasyncLIDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multi-v1\" ; private static String pluginName = \"lid-embedplda-v1-rc6\" ; private static String audioList ; private static ArrayList < String > audioFiles = new ArrayList < String > (); private static String audioFilename ; private static int enrollmentFileCount = 0 ; private static int responseCount = 0 ; private static String languageName = \"MNO\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static String outputDirName = \"./\" ;; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment list is passed in as an argument String audioList = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // read enrollment files from input list and keep count of enrollment files to be processed if ( ! Files . exists ( Paths . get ( audioList ). toAbsolutePath ())) { System . err . println ( \"ERROR: '\" + audioList + \"' does not exist\" ); } BufferedReader br = new BufferedReader ( new FileReader ( audioList )); while (( audioFilename = br . readLine ()) != null ){ audioFiles . add ( audioFilename ); } enrollmentFileCount = audioFiles . size (); // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain LID plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform LID enrollment task MyLIDEnrollmentAndScoreRequest ( server , pd , languageName , audioFiles , enrollmentFileCount , scoreWaveFileName ); } public static boolean MyLIDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String languageName , ArrayList < String > audioFiles , int enrollmentFileCount , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // do something with the results: if ( ! r . hasError ()){ System . err . println ( \"*****server processed 1 enrollment file*****\" ); responseCount ++ ; } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; for ( String audioFilename : audioFiles ) { boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , languageName , audioFilename , 0 , enrollmentCallback , true , true , new ArrayList <> (), enrollmentOptions ); if ( enrolled ) { // can do something here like print out enrollment file processed } } // wait for all enrollments to complete while ( responseCount != enrollmentFileCount ) { try { TimeUnit . SECONDS . sleep ( 10 ); } catch ( InterruptedException e ) { System . out . println ( \"*****Timeout Exception*****\" ); } } // We next do the score request // First create a call back to handle the LID scoring request Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > scoreCallback = new Server . ResultCallback < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > () { @Override public void call ( Server . Result < Scenic . GlobalScorerRequest , Scenic . GlobalScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} scores:\" , r . getRep (). getScoreCount ()); for ( Scenic . GlobalScore gs : r . getRep (). getScoreList ()){ log . info ( \"\\t{} = {}\" , gs . getClassId (), gs . getScore ()); } } else { log . error ( \"Global scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // LID is a global scorer, so make a global score reqeust: return ClientUtils . requestGlobalScore ( server , pp , Scenic . TraitType . GLOBAL_SCORER , scoreWaveFileName , 0 , scoreCallback , true , true , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } }","title":"Asynchronized Approach"},{"location":"apiCode.html#kws-scoring-request","text":"This portrays a full implementation of a KWS scoring request, against the list of default keywords already trained. Note that unlike the previous examples that requested GlobalScores, this client issues a RegionScoreRequest because KWS is a \"RegionScorer\". Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MyKWSRegionScorer { private static Logger log = LoggerFactory . getLogger ( MyKWSRegionScorer . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"r-tdnn-tel-v1\" ; private static String pluginName = \"kws-dynapy-v1\" ; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file name is passed as an argument String audioFileName = args [ 0 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // formulate the frame scoring task request Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"KWS\" , Scenic . TraitType . REGION_SCORER , pluginList ); // Perform KWS frame scoring task MyRegionScoresRequest ( server , pd , audioFileName ); } public static boolean MyRegionScoresRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String filename ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // Create a callback to handle KWS results from the server Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > rc = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; return ClientUtils . requestRegionScores ( server , pp , filename , 0 , rc , true , false , regionParser . getRegions ( filename ), new ArrayList <> (), new ArrayList <> ()); } }","title":"KWS Scoring Request"},{"location":"apiCode.html#sdd-enrollment-and-scoring-request","text":"The following code shows a full implementation of a speaker enrollment request, followed by a scoring request made to a SDD plugin. The scoring request in this client is a RegionScorerRequest because SDD plugins are region scorers. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.util.* ; import java.util.concurrent.TimeUnit ; import java.util.concurrent.atomic.AtomicInteger ; public class MySDDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MySDDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"multicond-v1\" ; private static String pluginName = \"sdd-sbc-embed-v1-1\" ; private static String enrollmentFileName ; private static String speakerName = \"Mr.X\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String enrollmentFileName = args [ 0 ] ; String scoreWaveFileName = args [ 1 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain SDD plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform SDD enrollment task MySDDEnrollmentAndScoreRequest ( server , pd , speakerName , enrollmentFileName , scoreWaveFileName ); } public static boolean MySDDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String enrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // enrollment result received if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // make a synchronized enrollment request, so we know the speaker is enrolled before we make the score request boolean enrolled = ClientUtils . requestEnrollClass ( server , pp , speakerName , enrollmentFileName , 0 , enrollmentCallback , false , false , new ArrayList <> (), enrollmentOptions ); if ( enrolled ){ // do the SDD scoring request // First create a callback to handle scoring result from server Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // scoring result received if ( ! r . hasError ()) { log . info ( \"Received {} region scores:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()) { log . info ( \"\\t{} = {}. From {} to {} \" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"SDD Region scoring error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // SDD is a region scorer, so make a region score reqeust: return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; }","title":"SDD Enrollment And Scoring Request"},{"location":"apiCode.html#sad-supervised-adaptation-request","text":"Below is a full implementation of a SAD request to adapt a new domain from an existing domain. The list of adaptation training files is passed into the client as a file. The code handles both supervised (speech regions specified) and unsupervised (speech regions not specified) SAD adaptations. However, some SAD plugins may not have the unsupervised adaptation capability, in which case the client will exit with a failure message. Java package com.sri.scenic.api.client ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Scenic.AnnotationRegion ; import com.sri.scenic.api.Scenic.AudioAnnotation ; import com.sri.scenic.api.Scenic.Domain ; import com.sri.scenic.api.Scenic.Plugin ; import com.sri.scenic.api.Scenic.TraitType ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.ClientUtils ; import com.sri.scenic.api.utils.LearningParser ; import com.sri.scenic.api.utils.LearningParser.LearningDataType ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Paths ; import java.util.ArrayList ; import java.util.Collection ; import java.util.HashMap ; import java.util.List ; import java.util.Map ; import java.util.UUID ; import javax.sound.sampled.UnsupportedAudioFileException ; import org.apache.commons.cli.CommandLine ; import org.apache.commons.cli.CommandLineParser ; import org.apache.commons.cli.DefaultParser ; import org.apache.commons.cli.HelpFormatter ; import org.apache.commons.cli.Option ; import org.apache.commons.cli.Options ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; public class MySADAdaptation { private static Logger log = LoggerFactory . getLogger ( MySADAdaptation . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"tel-v1\" ; private static String newDomainName = \"new-tel-v1\" ; private static String pluginName = \"sad-dnn-v6_rc5\" ; private static LearningParser learningParser = new LearningParser (); private static LearningDataType dataType ; /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // audio file list is passed as an argument String audioFileList = args [ 0 ] ; learningParser . parse ( audioFileList ); if ( ! learningParser . isValid ()) { System . err . println ( \"Invalid input file: \" + audioFileList ); System . exit ( - 1 ); } if ( learningParser . hasRegions ()) { dataType = LearningDataType . SUPERVISED_WITH_REGIONS ; } else if ( learningParser . hasClasses ()) { dataType = LearningDataType . SUPERVISED ; } else { dataType = LearningDataType . UNSUPERVISED ; } // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // find plugin handle for adaptation task Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , \"SAD\" , learningParser . isUnsupervised () ? Scenic . TraitType . UNSUPERVISED_ADAPTER : Scenic . TraitType . SUPERVISED_ADAPTER , pluginList ); // Preproces audio - doesn't matter if supervised or unsupervised String adaptID = UUID . randomUUID (). toString (); Plugin p = pd . getFirst (); Domain d = pd . getSecond (); // optional annotations, generated if found in the parser (supervised adaptation) Map < String , List < AudioAnnotation >> annotations = new HashMap <> (); // annotations -> <classID> ->* <AudioAnnotations>, annotations will be empty for unsupervised adaptation int numPreprocessed = OliveLearn . preprocessAllAudio ( server , p , d , learningParser , adaptID , annotations ); if ( ! learningParser . isUnsupervised ()) { // supervised adaptation if ( numPreprocessed > 0 ) { OliveLearn . finalizeSupervisedAdaptation ( server , p , d , adaptID , newDomainName , annotations ); } else { System . err . println ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } else { // unsupervised adaptation if ( numPreprocessed > 0 ) { OliveLearn . finalizeUnsupervisedAdaptation ( server , p , d , adaptID , newDomainName ); } else { System . err . println ( \"Can not adapt domain because all audio preprocessing attempts failed.\" ); } } System . out . println ( \"\" ); System . out . println ( \"Learning finished. Exiting...\" ); System . exit ( 0 ); } }","title":"SAD Supervised Adaptation Request"},{"location":"apiCode.html#tpd-enrollment-and-scoring-request","text":"This final example shows a full implementation of a TPD topic enrollment request using positive examples from selected regions of an audio, followed by a scoring request of the enrolled topic on another audio. Enrollment using negative audio samples are also included in this example as commented out lines. Java package com.sri.scenic.api.client ; import com.google.protobuf.InvalidProtocolBufferException ; import com.sri.scenic.api.Scenic ; import com.sri.scenic.api.Server ; import com.sri.scenic.api.utils.* ; import com.sri.scenic.api.client.* ; import org.apache.commons.cli.* ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import javax.sound.sampled.UnsupportedAudioFileException ; import java.io.* ; import java.nio.file.* ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; import java.util.* ; import java.util.concurrent.atomic.AtomicInteger ; public class MyTPDEnrollmentAndScore { private static Logger log = LoggerFactory . getLogger ( MyTPDEnrollmentAndScore . class ); private static final int TIMEOUT = 10000 ; private static final String DEFAULT_SERVERNAME = \"localhost\" ; private static final int DEFAULT_PORT = 5588 ; private static String domainName = \"r-tdnn-tel-v1\" ; private static String pluginName = \"tpd-dynapy-v1\" ; private static String posEnrollmentFileName ; private static String negEnrollmentFileName ; private static String topicName = \"hello\" ; private static String scoreWaveFileName ; private static List < Pair < String , String >> enrollmentOptions = new ArrayList <> (); //private static String outputDirName = \"./\";; private static RegionParser regionParser = new RegionParser (); /** * Main execution point * * @throws Exception if there was an error */ public static void main ( String [] args ) throws Exception { // enrollment file name is passed in as an argument String posEnrollmentFileName = args [ 0 ] ; String negEnrollmentFileName = args [ 1 ] ; String scoreWaveFileName = args [ 2 ] ; // Setup the connection to the (scenic) server Server server = new Server (); server . connect ( \"scenic-ui\" , DEFAULT_SERVERNAME , DEFAULT_PORT , DEFAULT_PORT + 1 , TIMEOUT ); // may need to adjust timeout // wait for the connection long start_t = System . currentTimeMillis (); while ( ! server . getConnected (). get () && System . currentTimeMillis () - start_t < TIMEOUT ) { try { synchronized ( server . getConnected ()) { server . getConnected (). wait ( TIMEOUT ); } } catch ( InterruptedException e ) { // Keep waiting } } // report if connection fails if ( ! server . getConnected (). get ()) { log . error ( \"Unable to connect to the SCENIC server: {}\" , DEFAULT_SERVERNAME ); throw new Exception ( \"Unable to connect to server\" ); } // set audio mode to read from buffer //Scenic.Audio audio = SimpleClient.packageAudioAsRawBuffer(enrollmentFileName, 0, null); // ask the server for a list of current plugins List < Pair < Scenic . Plugin , Scenic . Domain >> pluginList = SimpleClient . requestPlugins ( server , true , true ); // obtain TPD plugin handle Pair < Scenic . Plugin , Scenic . Domain > pd = ClientUtils . findPluginDomainByTrait ( pluginName , domainName , null , Scenic . TraitType . CLASS_ENROLLER , pluginList ); // Perform TPD enrollment and scoring tasks MyTPDEnrollmentAndScoreRequest ( server , pd , topicName , posEnrollmentFileName , negEnrollmentFileName , scoreWaveFileName ); } public static boolean MyTPDEnrollmentAndScoreRequest ( Server server , Pair < Scenic . Plugin , Scenic . Domain > pp , String speakerName , String posEnrollmentFileName , String negEnrollmentFileName , String scoreWaveFileName ) throws ClientException , IOException , UnsupportedAudioFileException { if ( null == pp ) { return false ; } //enroll a positive example List < RegionWord > posRegions = new ArrayList <> (); posRegions . add ( new RegionWord ( 0.100 , 2.500 )); // NOTE to add regions in seconds posRegions . add ( new RegionWord ( 7.500 , 10.000 )); // NOTE to add regions in seconds /** //we can also enroll a negative example enrollmentOptions.add(new Pair<>(\"isNegative\", \"True\")); List<RegionWord> negRegions = new ArrayList<>(); negRegions.add(new RegionWord(1.000, 1.500)); // NOTE to add regions in seconds negRegions.add(new RegionWord(5.000, 7.000)); // NOTE to add regions in seconds **/ // First create a callback that handles the enrollment result from the server Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > enrollmentCallback = new Server . ResultCallback < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > () { @Override public void call ( Server . Result < Scenic . ClassModificationRequest , Scenic . ClassModificationResult > r ) { // examine enrollment result if ( ! r . hasError ()){ System . out . println ( \"Enrollment succeeded\" ); } else { log . error ( \"Enrollment request failed: {}\" , r . getError ()); } } }; // do enrollment request // make it a synchronized call, so we know enrollment is complete before we score boolean enrolled = false ; enrolled = ClientUtils . requestEnrollClass ( server , pp , topicName , posEnrollmentFileName , 0 , enrollmentCallback , false , false , posRegions , enrollmentOptions ); /** //goes with negative example enrolled=ClientUtils.requestEnrollClass(server, pp, topicName, negEnrollmentFileName, 0, enrollmentCallback, false, false, posRegions, enrollmentOptions); **/ if ( enrolled ){ // Create a call back to handle the TPD scoring request Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > scoreCallback = new Server . ResultCallback < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > () { @Override public void call ( Server . Result < Scenic . RegionScorerRequest , Scenic . RegionScorerResult > r ) { // do something with the results: if ( ! r . hasError ()){ log . info ( \"Received {} regions:\" , r . getRep (). getRegionCount ()); for ( Scenic . RegionScore rs : r . getRep (). getRegionList ()){ log . info ( \"\\t{} = {}, From {} to {}\" , rs . getClassId (), rs . getScore (), rs . getStartT (), rs . getEndT ()); } } else { log . error ( \"Region scorer error: {}\" , r . getError ()); } System . exit ( 0 ); } }; // TPD is a region scorer, so make a region score reqeust return ClientUtils . requestRegionScores ( server , pp , scoreWaveFileName , 0 , scoreCallback , true , false , regionParser . getRegions ( scoreWaveFileName ), new ArrayList <> (), new ArrayList <> ()); } return false ; } }","title":"TPD Enrollment and Scoring Request"},{"location":"apiInfo.html","text":"OLIVE Enterprise API Primer Introduction The OLIVE Enterprise API enables third-party tools and existing workflows to interface with the OLIVE backend system. This page provides an introduction to the concepts and information needed to begin implementing OLIVE\u2019s speech processing tools and capabilities within a client application by integrating through the OLIVE Enterprise API (previously known as the SCENIC Enterprise API). In general, the three main components of an OLIVE-based audio processing system are: The OLIVE Server The OLIVE Plugins An OLIVE-enabled Client Links are provided above and throughout this page for more information about the OLIVE Server and the Plugins. The main focus of this page is covering how to help implement or create an OLIVE-enabled Client. SRI offers a Java-based Reference API implementation for OLIVE to allow integrators relying on Java to quickly start folding OLIVE functionality into their new or current project. This code is also available as example code for integrators using different programming languages to use as a general reference for accomplishing certain tasks. This guide covers both implementing the existing Java-based API reference implementation, as well as some information that you will need to build your own reference implentation, if desired. If you're interested in having a reference implementation built and provided in a language other than Java, please refer to this section below. The next section starts by introducing general operating concepts of the OLIVE system, with links to resources that expand on this information if you'd like to dig a bit deeper. Following that is a section that describes the steps a client uses to connect to the OLIVE server, submit requests for processing, and unpack results returned from the server. If you are interested in full Java examples of clients requesting various tasks to be performed using this API, continue on to the Java API Client Integration Guide . We also provide a page that describes in details how to set up your Java IDE , using IntelliJ as an example, to help you get started with building your own custom Java clients using our example code. Although the current reference OLIVE API is Java based, this message based API can be implemented in different language. Should you have a need for a non-Java OLIVE API, this page of the documentation contains useful information for building an entirely new OLIVE API in a language other than Java. Useful Concepts to Know Before you start using the OLIVE API to put together client programs, it is useful to have some background information about how the OLIVE system operates. The OLIVE system is client/server based, using Google Protocol Buffers (Protobuf) transported over ZMQ sockets to communicate between the client and server. Therefore any API implementation that allows client access to the backend OLIVE system must also follow the same client/server model of exchanging Google Protocol Buffer based messages over ZMQ sockets. The OLIVE server accepts client connections at pre-selected but configurable ports. Once a connection is established between a client and the server, task requests and results (implemented as Protobuf messages) can be sent and received over this connection. Via the server, client requests are directed to and performed by OLIVE speech processing agents (plugins). Also via the server, results of completed requests are returned to the client. Task requests and results are sent in the form of OLIVE messages which contain information pertinent to the type of request and the type of plugin performing the request. The contemts and details of all of the possible messages, both requests and responses, are detailed in the API Message Reference page, but more details about which of these will be important to you and how to actually use these messages follow below. If you are new to the OLIVE Enterprise API then you will quickly discover that the API does not offer explicit API calls for making \u201cSAD\u201d, \u201cSID\u201d, or \u201cLID\u201d requests. The reason for this is the API is based on the OLIVE Plugin Framework used by the OLIVE server, which abstracts speech systems such as SAD, SID, and LID into \u201ctraits\u201d. It is these \u201ctraits\u201d that are expressed in this API, so score and class modification requests are based on the plugins traits and not the type of speech system. The OLIVE server identifies plugins by the task types (SAD, SID, LID, etc.) they claim they can perform, and the details of how these plugins can go about those tasks are defined by the Traits that they implement. For example, a plugin with a Speaker Identification task often implements the GlobalScorer Trait. From the table below or from the Traits Info Page , you can see the messages that accompany implementing this Trait, which define the functionality associated with being a GlobalScorer, and let us see the types of output we can expect to retrieve from such a plugin. The following subsections describe each of these basic concepts in more details. OLIVE Plugin Tasks and Traits The OLIVE server identifies plugins by the type of tasks they can perform. For example, a plugin designed to recognize and identify the voice of a pre-enrolled speaker carries the Task of Speaker Identification , or SID. Likewise, a plugin that is designed to label speech regions within an audio clip has the Task of Speech Activity Detection , or SAD. In order to perform these Tasks, plugins require some abilities - for example, a SID task plugin requires the ability to perform enrollment of speakers and scoring of audio against those enrollments; a SAD type plugin must be able to perform a scoring task to calculate the likelihood of speech throughout an audio clip, and sometimes adaptation tasks; etc. These abilities are defined by what OLIVE calls Traits . More detailed information on the available OLIVE Traits can be found on the respective info page, but a quick primer is also included below. Continuing with the previous example, a SID type plugin needs to be able to perform the scoring task. Historically, most SID plugins assume that the audio being passed to it is homogenous, and consists of a single speaker, and it is therefore possible to score all of the audio and return a single score for each enrolled speaker model representing whether or not the candidate speech is likely from this speaker. This type of score is called a global score, and the Trait that the plugin implements to gain this functionality is called GlobalScorer . \"Implementing\" this Trait means that the plugin contains routine definitions that allow it to receive and appropriately respond to the the OLIVE API Messages associated with that Trait. In order to have speaker models to score against, a SID plugin also needs the ability to enroll speaker models as classes. The ability to modify enrolled classes comes from the ClassModifier Trait and its associated API messages. For a complete list of the available Traits and their associated request messages, including the appropriate reply message to each one, refer to the Plugin Traits info page and/or the API Message Reference page. For a high level overview of what this means in terms of available plugins, continue below. The following table shows the likely traits for the scoring functionality of selected OLIVE plugin task types: Plugin Type Scoring Trait Speech Activity Detection (SAD) FrameScorer and/or RegionScorer Language Identification (LID) GlobalScorer Speaker Identification (SID) GlobalScorer Speaker Diarization and Detection (SDD) RegionScorer Keyword Spotting (KWS) RegionScorer Query By Example KWS (QBE) RegionScorer Topic Detection (TPD) RegionScorer Speech Enhancement (ENH) AudioConverter Voice Type Discrimination (VTD) FrameScorer and/or RegionScorer For plugins that allow or require enrollment functionality, the associated Trait is ClassModifier . The following plugin types may currently have this Trait for the enrollment task: Plugin Type Enrollment Trait Language Identification (LID) * ClassModifier Speaker Identification (SID) ClassModifier Speaker Diarization and Detection (SDD) ClassModifier Query By Example KWS (QBE) ClassModifier Topic Detection (TPD) ClassModifier *Note that not all LID plugins allow or support language/class enrollment. When in doubt, refer to individual plugin documentation, or check the plugin's implemented Traits. Please remember, these tables may not be true for all plugins and some Plugins may support additional Traits. This mapping is only intended to help introduce the OLIVE Enterprise API and its underlying Plugin Framework to new developers. Some SAD plugins also allow the end user to perform domain adaptation to improve plugin performance in certain audio conditions. The Trait listed below is associated with this task. Plugin Type Adaptation Trait Speech Activity Detection (SAD) SupervisedAdapter OLIVE Message Requests / Results By Plugin Traits Now that you know a bit about the available Plugin Tasks and the Traits they're likely to implement, we will discuss the Messages that actually allow for requests to be made to the plugins, and for information to be passed back from the plugins to the client. A client connected to the OLIVE server can submit message to the server to request information from plugins. The table below shows what requests are generally available for selected plugin types. Note that it is possible to create plugins that may stray from this list and may implement a different Trait than what is shown below. Scoring Traits Plugin Trait Task Request Message Result Message Global Scorer LID , SID GlobalScorerRequest GlobalScorerStereoRequest GlobalScorerResult GlobalScorerStereoResult RegionScorer SDD , SAD *, KWS , QBE , TPD RegionScorerRequest RegionScorerStereoRequest RegionScorerResult RegionScorerStereoResult FrameScorer SAD * FrameScorerRequest FrameScorerStereoRequest FrameScorerResult FrameScorerStereoResult *Note that not all SAD plugins support FrameScorer and/or RegionScorer. Please refer to specific plugin documentation or consult with SRI if unsure. As you can see from this table, the same few API messages are reused for most scoring requests, meaning the actual code implementation for these tasks can be kept simple. Other Traits Plugin Trait Task Functionality Request Message Result Message AudioConverter ENH Audio Modification, Speech Enhancement AudioModificationRequest AudioModificationResult GlobalComparer FOR Forensic Audio Comparison GlobalComparerRequest GlobalComparerResult LearningTrait / SupervisedAdapter SAD Audio Condition Domain Adaptation SupervisedAdaptationRequest PreprocessAudioAdaptRequest SupervisedAdaptationResult PreprocessAudioAdaptResult Other Useful OLIVE Message Types Besides the messages related to plugin tasking and interaction mentioned in the two sections above, there are several additional messages which are useful to know for server management and other non-plugin-specific tasks. PluginDirectoryRequest and PluginDirectoryResult The client sends a PluginDirectoryRequest to the server to request a list of available plugins, and the server sends back to the client a PluginDirectoryResult . The PluginDirectoryResult response includes supported plugins, their traits, and optional parameters they support. A comprehensive list of OLIVE API Messages is available in OLIVE API Message Reference . Information Persistence As of OLIVE 4.0 the backend OLIVE server and API no longer support persistence. It is the responsibility of the client to store, manage, and reference results from the OLIVE server. The OLIVE server does persist enrolled class models and some collected adaptation information. Dependencies The OLIVE Enterprise API utilizes the following dependencies: Google Protocol Buffers 3.4: Used to define the messages that comprise the OLIVE API. Most messages are in the form of request/reply. ZeroMQ 3.2.3: Provides inter-process communication over several possible mechanisms including TCP. Protobuf-net : Optional - needed if you wish to integrate from a .NET/Mono application. You will need versions of these software dependencies appropriate for your system architecture/operating system in order to communicate with the OLIVE server. Supported Languages Given the dependencies described in the previous section it is possible to utilize the OLIVE API from the following programming languages/runtimes: Java (or other JVM languages that provide Java interoperability) C# (or other .NET language, via the protobuf.net library. This is an extra dependency) Python C++ Note that because the Java-based OLIVE UI utilizes the API, SRI has already developed a Java client library to facilitate use of the API from Java. For more information see the \u201cJava Client Library\u201d section of this document.","title":"Enterprise API Primer"},{"location":"apiInfo.html#olive-enterprise-api-primer","text":"","title":"OLIVE Enterprise API Primer"},{"location":"apiInfo.html#introduction","text":"The OLIVE Enterprise API enables third-party tools and existing workflows to interface with the OLIVE backend system. This page provides an introduction to the concepts and information needed to begin implementing OLIVE\u2019s speech processing tools and capabilities within a client application by integrating through the OLIVE Enterprise API (previously known as the SCENIC Enterprise API). In general, the three main components of an OLIVE-based audio processing system are: The OLIVE Server The OLIVE Plugins An OLIVE-enabled Client Links are provided above and throughout this page for more information about the OLIVE Server and the Plugins. The main focus of this page is covering how to help implement or create an OLIVE-enabled Client. SRI offers a Java-based Reference API implementation for OLIVE to allow integrators relying on Java to quickly start folding OLIVE functionality into their new or current project. This code is also available as example code for integrators using different programming languages to use as a general reference for accomplishing certain tasks. This guide covers both implementing the existing Java-based API reference implementation, as well as some information that you will need to build your own reference implentation, if desired. If you're interested in having a reference implementation built and provided in a language other than Java, please refer to this section below. The next section starts by introducing general operating concepts of the OLIVE system, with links to resources that expand on this information if you'd like to dig a bit deeper. Following that is a section that describes the steps a client uses to connect to the OLIVE server, submit requests for processing, and unpack results returned from the server. If you are interested in full Java examples of clients requesting various tasks to be performed using this API, continue on to the Java API Client Integration Guide . We also provide a page that describes in details how to set up your Java IDE , using IntelliJ as an example, to help you get started with building your own custom Java clients using our example code. Although the current reference OLIVE API is Java based, this message based API can be implemented in different language. Should you have a need for a non-Java OLIVE API, this page of the documentation contains useful information for building an entirely new OLIVE API in a language other than Java.","title":"Introduction"},{"location":"apiInfo.html#useful-concepts-to-know","text":"Before you start using the OLIVE API to put together client programs, it is useful to have some background information about how the OLIVE system operates. The OLIVE system is client/server based, using Google Protocol Buffers (Protobuf) transported over ZMQ sockets to communicate between the client and server. Therefore any API implementation that allows client access to the backend OLIVE system must also follow the same client/server model of exchanging Google Protocol Buffer based messages over ZMQ sockets. The OLIVE server accepts client connections at pre-selected but configurable ports. Once a connection is established between a client and the server, task requests and results (implemented as Protobuf messages) can be sent and received over this connection. Via the server, client requests are directed to and performed by OLIVE speech processing agents (plugins). Also via the server, results of completed requests are returned to the client. Task requests and results are sent in the form of OLIVE messages which contain information pertinent to the type of request and the type of plugin performing the request. The contemts and details of all of the possible messages, both requests and responses, are detailed in the API Message Reference page, but more details about which of these will be important to you and how to actually use these messages follow below. If you are new to the OLIVE Enterprise API then you will quickly discover that the API does not offer explicit API calls for making \u201cSAD\u201d, \u201cSID\u201d, or \u201cLID\u201d requests. The reason for this is the API is based on the OLIVE Plugin Framework used by the OLIVE server, which abstracts speech systems such as SAD, SID, and LID into \u201ctraits\u201d. It is these \u201ctraits\u201d that are expressed in this API, so score and class modification requests are based on the plugins traits and not the type of speech system. The OLIVE server identifies plugins by the task types (SAD, SID, LID, etc.) they claim they can perform, and the details of how these plugins can go about those tasks are defined by the Traits that they implement. For example, a plugin with a Speaker Identification task often implements the GlobalScorer Trait. From the table below or from the Traits Info Page , you can see the messages that accompany implementing this Trait, which define the functionality associated with being a GlobalScorer, and let us see the types of output we can expect to retrieve from such a plugin. The following subsections describe each of these basic concepts in more details.","title":"Useful Concepts to Know"},{"location":"apiInfo.html#olive-plugin-tasks-and-traits","text":"The OLIVE server identifies plugins by the type of tasks they can perform. For example, a plugin designed to recognize and identify the voice of a pre-enrolled speaker carries the Task of Speaker Identification , or SID. Likewise, a plugin that is designed to label speech regions within an audio clip has the Task of Speech Activity Detection , or SAD. In order to perform these Tasks, plugins require some abilities - for example, a SID task plugin requires the ability to perform enrollment of speakers and scoring of audio against those enrollments; a SAD type plugin must be able to perform a scoring task to calculate the likelihood of speech throughout an audio clip, and sometimes adaptation tasks; etc. These abilities are defined by what OLIVE calls Traits . More detailed information on the available OLIVE Traits can be found on the respective info page, but a quick primer is also included below. Continuing with the previous example, a SID type plugin needs to be able to perform the scoring task. Historically, most SID plugins assume that the audio being passed to it is homogenous, and consists of a single speaker, and it is therefore possible to score all of the audio and return a single score for each enrolled speaker model representing whether or not the candidate speech is likely from this speaker. This type of score is called a global score, and the Trait that the plugin implements to gain this functionality is called GlobalScorer . \"Implementing\" this Trait means that the plugin contains routine definitions that allow it to receive and appropriately respond to the the OLIVE API Messages associated with that Trait. In order to have speaker models to score against, a SID plugin also needs the ability to enroll speaker models as classes. The ability to modify enrolled classes comes from the ClassModifier Trait and its associated API messages. For a complete list of the available Traits and their associated request messages, including the appropriate reply message to each one, refer to the Plugin Traits info page and/or the API Message Reference page. For a high level overview of what this means in terms of available plugins, continue below. The following table shows the likely traits for the scoring functionality of selected OLIVE plugin task types: Plugin Type Scoring Trait Speech Activity Detection (SAD) FrameScorer and/or RegionScorer Language Identification (LID) GlobalScorer Speaker Identification (SID) GlobalScorer Speaker Diarization and Detection (SDD) RegionScorer Keyword Spotting (KWS) RegionScorer Query By Example KWS (QBE) RegionScorer Topic Detection (TPD) RegionScorer Speech Enhancement (ENH) AudioConverter Voice Type Discrimination (VTD) FrameScorer and/or RegionScorer For plugins that allow or require enrollment functionality, the associated Trait is ClassModifier . The following plugin types may currently have this Trait for the enrollment task: Plugin Type Enrollment Trait Language Identification (LID) * ClassModifier Speaker Identification (SID) ClassModifier Speaker Diarization and Detection (SDD) ClassModifier Query By Example KWS (QBE) ClassModifier Topic Detection (TPD) ClassModifier *Note that not all LID plugins allow or support language/class enrollment. When in doubt, refer to individual plugin documentation, or check the plugin's implemented Traits. Please remember, these tables may not be true for all plugins and some Plugins may support additional Traits. This mapping is only intended to help introduce the OLIVE Enterprise API and its underlying Plugin Framework to new developers. Some SAD plugins also allow the end user to perform domain adaptation to improve plugin performance in certain audio conditions. The Trait listed below is associated with this task. Plugin Type Adaptation Trait Speech Activity Detection (SAD) SupervisedAdapter","title":"OLIVE Plugin Tasks and Traits"},{"location":"apiInfo.html#olive-message-requests-results-by-plugin-traits","text":"Now that you know a bit about the available Plugin Tasks and the Traits they're likely to implement, we will discuss the Messages that actually allow for requests to be made to the plugins, and for information to be passed back from the plugins to the client. A client connected to the OLIVE server can submit message to the server to request information from plugins. The table below shows what requests are generally available for selected plugin types. Note that it is possible to create plugins that may stray from this list and may implement a different Trait than what is shown below.","title":"OLIVE Message Requests / Results By Plugin Traits"},{"location":"apiInfo.html#scoring-traits","text":"Plugin Trait Task Request Message Result Message Global Scorer LID , SID GlobalScorerRequest GlobalScorerStereoRequest GlobalScorerResult GlobalScorerStereoResult RegionScorer SDD , SAD *, KWS , QBE , TPD RegionScorerRequest RegionScorerStereoRequest RegionScorerResult RegionScorerStereoResult FrameScorer SAD * FrameScorerRequest FrameScorerStereoRequest FrameScorerResult FrameScorerStereoResult *Note that not all SAD plugins support FrameScorer and/or RegionScorer. Please refer to specific plugin documentation or consult with SRI if unsure. As you can see from this table, the same few API messages are reused for most scoring requests, meaning the actual code implementation for these tasks can be kept simple.","title":"Scoring Traits"},{"location":"apiInfo.html#other-traits","text":"Plugin Trait Task Functionality Request Message Result Message AudioConverter ENH Audio Modification, Speech Enhancement AudioModificationRequest AudioModificationResult GlobalComparer FOR Forensic Audio Comparison GlobalComparerRequest GlobalComparerResult LearningTrait / SupervisedAdapter SAD Audio Condition Domain Adaptation SupervisedAdaptationRequest PreprocessAudioAdaptRequest SupervisedAdaptationResult PreprocessAudioAdaptResult","title":"Other Traits"},{"location":"apiInfo.html#other-useful-olive-message-types","text":"Besides the messages related to plugin tasking and interaction mentioned in the two sections above, there are several additional messages which are useful to know for server management and other non-plugin-specific tasks. PluginDirectoryRequest and PluginDirectoryResult The client sends a PluginDirectoryRequest to the server to request a list of available plugins, and the server sends back to the client a PluginDirectoryResult . The PluginDirectoryResult response includes supported plugins, their traits, and optional parameters they support. A comprehensive list of OLIVE API Messages is available in OLIVE API Message Reference .","title":"Other Useful OLIVE Message Types"},{"location":"apiInfo.html#information-persistence","text":"As of OLIVE 4.0 the backend OLIVE server and API no longer support persistence. It is the responsibility of the client to store, manage, and reference results from the OLIVE server. The OLIVE server does persist enrolled class models and some collected adaptation information.","title":"Information Persistence"},{"location":"apiInfo.html#dependencies","text":"The OLIVE Enterprise API utilizes the following dependencies: Google Protocol Buffers 3.4: Used to define the messages that comprise the OLIVE API. Most messages are in the form of request/reply. ZeroMQ 3.2.3: Provides inter-process communication over several possible mechanisms including TCP. Protobuf-net : Optional - needed if you wish to integrate from a .NET/Mono application. You will need versions of these software dependencies appropriate for your system architecture/operating system in order to communicate with the OLIVE server.","title":"Dependencies"},{"location":"apiInfo.html#supported-languages","text":"Given the dependencies described in the previous section it is possible to utilize the OLIVE API from the following programming languages/runtimes: Java (or other JVM languages that provide Java interoperability) C# (or other .NET language, via the protobuf.net library. This is an extra dependency) Python C++ Note that because the Java-based OLIVE UI utilizes the API, SRI has already developed a Java client library to facilitate use of the API from Java. For more information see the \u201cJava Client Library\u201d section of this document.","title":"Supported Languages"},{"location":"apiMessage.html","text":"OLIVE API Message Protocol Documentation olive.proto Protocol Buffer Definitions The messages defined on this page are what define the OLIVE Enterprise API. These messages are how a client application will interact with and provide tasks to an OLIVE Server. The format used by the OLIVE API is based on the Google Protocol Buffer . For more information on how to integrate the ability to send and receive these messages into a client application using a provided Client API implementation from SRI, refer to the Integrating the (Java) Client API Guide. For more information on creating your own reference implementation with the functionality of these messages, head over to the Creating an API Reference Implementation page that offers guidelines and information. Server Management Messages GetActiveRequest Message to request the list of ScenicMessages that are still active GetActiveResult Response to GetActiveRequest containing the ScenicMessages that are still active Field Type Label Description message_id string repeated List containing the IDs of each message still being processed on the server total_num string required Total number of messages still being processed GetStatusRequest Request a simple server status message, similar to a heartbeat except the request reply is sent on the status port and is requested by the client GetStatusResult The result of a GetStatusRequest Field Type Label Description num_pending uint32 required The number of pending jobs num_busy uint32 required the number of active jobs num_finished uint32 required The number of finished jobs Heartbeat A heartbeat Message, acknowledging that the server is running, this message is continuously broadcast by the server on it's status port (this is the only message sent on the status port) Field Type Label Description stats ServerStats optional The current status of the server, optional since status is not available when the server first starts, but a heartbeat is still sent logdir string required The location where the server writes it log files ServerStats Current status of the OLIVE server, sent as part of a Heatbeat message. Field Type Label Description cpu_percent float required The current percentage of CPU used cpu_average float required The average CPU percentage used since the server was started mem_percent float required The percentage of memory used max_mem_percent float required The most memory used since the server was started swap_percent float required The current swap used max_swap_percent float required The max (most) swap space used since the server was started pool_busy uint32 required The number of jobs currently running on the server pool_pending uint32 required The number of jobs queued on the server pool_finished uint32 required The number of jobs completed pool_reinit bool required The number of jobs that need to be re-ran max_num_jobs uint32 optional The max number of concurrent jobs server_version string optional The current version of the server LoadPluginDomainRequest Request a plugin be pre-loaded to optimize later score request(s) Field Type Label Description plugin string required The plugin domain string required The domain LoadPluginDomainResult Acknowledgment that a plugin is being loaded Field Type Label Description successful bool required True if the request is being loaded (but receipt of this message does not guarantee the plugin has finished loaded) RemovePluginDomainRequest Unload (remove from memory) a previously loaded plugin. Use to free resources on the server or force reloading of a plugin/domain Field Type Label Description plugin string required The plugin to remove domain string optional The domain to remove, if omitted all domains removed for this plugin. RemovePluginDomainResult Acknowledgment that a plugin/domain has been removed (unloaded) Field Type Label Description successful bool required True if the plugin was been removed PluginDirectoryRequest Use a PluginDirectoryRequest message to receive the list of plugins available on the server. A Plugin performs tasks such as SAD, LID, SID, or KWS. There may be multiple plugins registered for a given task. A plugin typically has one or more Domains. Plugins contain the code of the recognizer, while Domains correspond to a particular training or adaptation sessions. Domains therefore represent the data/conditions. A plugin and domain together are necessary to perform scoring. PluginDirectoryResult The collection of plugins available on the server, response to PluginDirectoryRequest Field Type Label Description plugins Plugin repeated The available plugins Global Scorer Messages GlobalScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a GlobalScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a GlobalScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide. GlobalScore The global score for a class Field Type Label Description class_id string required The class score float required The score associated with the class confidence float optional An optional confidence value when part of a calibration report comment string optional An optional suggested action when part of a calibration report GlobalScorerRequest Request global scoring using the specified plugin. The plugin must implement the GlobalScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of global scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio optional The audio to analyze/score. Either audio or vector must be set. vector AudioVector optional The preprocessed audio vector to analyze/score. Either audio or vector must be set. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored GlobalScorerResult The result from a GlobalScorerRequest, having zero or more GlobalScore elements Field Type Label Description score GlobalScore repeated The class scores Region Scorer Messages RegionScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a RegionScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a RegionScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide. RegionScore The basic unit of a region score. There may be multiple RegionScore values in a RegionScorerResult Field Type Label Description start_t int32 required Begin-time of the region (in seconds) end_t int32 required End-time of the region (in seconds) class_id string required Class ID associated with region score float optional Optional score associated with the class_id label RegionScorerRequest Request region scoring for the specified plugin/domain. The plugin must implement the RegionScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of region scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored RegionScorerResult The set of region score results, response to RegionScorerRequest Field Type Label Description region RegionScore repeated The scored regions Frame Scorer Messages FrameScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a FrameScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a FrameScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide. FrameScorerRequest Request frame scoring using the specified plugin and audio. The plugin must implement the FrameScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of frame scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored FrameScorerResult The results from a FrameScorerRequest Field Type Label Description result FrameScores repeated List of frame scores by class_id FrameScores The basic unit of a frame score, returned in a FrameScorerRequest Field Type Label Description class_id string required The class ID to which the frame scores pertain frame_rate int32 required The number of frames per second frame_offset double required The offset to the center of the frame 'window' score double repeated The frame-level scores for the class_id Text Transformation Messages TextTransformer is an OLIVE Plugin Trait for scoring using text (instead of audio). For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page. TextTransformationRequest Request the transformation of a text/string using MT Field Type Label Description plugin string required The plugin to invoke domain string required The domain text string optional The string text to analyze/score, Optional as of OLIVE 5.0 since data input(s) can be specified as part of a workflow option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored TextTransformationResult The response to a TextTransformationRequest Field Type Label Description transformation TextTransformation repeated TextTransformation The text value returned in a TextTransformationResult Field Type Label Description class_id string required A classifier for this result, usually just 'text' transformed_text string required The text result Audio Alignment Messages AudioAlignmentScorer is an OLIVE Plugin Trait for alignment of two or more audio inputs. For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page. AudioAlignmentScoreRequest Request the alignment of two or more audio inputs Field Type Label Description plugin string required The plugin to invoke domain string required The domain audios Audio repeated The audio to analyze/score, Optional as of OLIVE 5.0 since Audio can be specified option OptionValue repeated as part of a workflow. If specified there should be two or more audio inputs Any options specified | | class_id | string | repeated | Optionally specify the classes to be scored | AudioAlignmentScoreResult The result of a AudioAlignmentScoreRequest Field Type Label Description scores AudioAlignmentScore repeated AudioAlignmentScore A score in an AudioAlignmentScoreResult Field Type Label Description reference_audio_label string required The source or reference audio name (file 1) other_audio_label string required The name of the audio input in comparison (file 2) shift_offset float required shift offset between the audion in the reference and confidence float required The confidence of this score Global Comparer Messages GlobalComparerReport The visual representation of a global comparison Field Type Label Description type ReportType required The type of report (normally a PDF) report_data bytes required The serialized report GlobalComparerRequest Request the comparison of two audio submission. The plugin must implement the GlobalComparer trait to handle this request Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio_one Audio required One of two audio submissions to analyze/score. audio_two Audio required One of two audio submissions to analyze/score. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored // todo remove! GlobalComparerResult The result of a GlobalComparerRequest Field Type Label Description results Metadata repeated The metadata/scores returned from a global compare analysis report GlobalComparerReport repeated A comparison report generated by the plugin ReportType Possible report formats Name Number Description PDF 1 PNG 2 GIF 3 JPEG 4 TIFF 5 Class Modifier Messages ClassModificationRequest Request a modification of a class for the specified plugin. The plugin must implement the ClassModifier Trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class being enrolled/modified addition Audio repeated List of Audio, action pairs to apply to the class removal Audio repeated List of Audio, action pairs to apply to the class addition_vector AudioVector repeated List of preprocessed audio vector to apply to the class finalize bool optional Whether or not to finalize the class. You can send multiple ClassModificationRequests and only finalize on the last request for efficiency. Default: true option OptionValue repeated Any modification options ClassModificationResult Response to ClassModificationRequest . Field Type Label Description addition_result AudioResult repeated Provides feedback about the success/failure of individual audio additions removal_result AudioResult repeated Provides feedback about the success/failure of individual audio removals vector_addition_result AudioResult repeated Provides feedback about the success/failure of individual audio vector additions ClassRemovalRequest Request removal of a class in the specified plugin/domain Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The id of the class to be removed ClassRemovalResult Acknowledgment that a ClassRemovalRequest was received AudioResult The feedback/description of class modification for a result in a ClassModificationResult message Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred on this audio Audio Converter Messages AudioModification The contents of an AudioModificationResult. Field Type Label Description audio AudioBuffer required The transformed audio message string required Description of how this audio was transformed, or an error description. Not sure if needed? scores Metadata repeated Zero or more scores (metadata) about the modifed audio. Metadata is a list of name/value pairs. AudioModificationRequest Request enhancement (modification) of the submitted audio. The plugin must support the AudioConverter trait to support this request Field Type Label Description plugin string required The plugin domain string required The domain requested_channels uint32 required Convert audio to have this number of channels requested_rate uint32 required Convert audio to this sample rate modifications Audio repeated List of Audio, action pairs to apply to the class - may have to limit to one audio submission per request, not sure how to handle multiple results option OptionValue repeated Any options specified AudioModificationResult The result of an AudioModificationRequest Field Type Label Description successful bool required Whether or not the individual audio modification succeeded modification_result AudioModification repeated Provides feedback about the success/failure of individual audio additions. Audio Vectorizer Messages AudioVector Represents audio preprocessed by a plugin/domain. Field Type Label Description plugin string required The origin plugin domain string required The origin domain data bytes required The audio vector data, varies by plugin params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing PluginAudioVectorRequest Request one or more audio submissions be vectorized (preprocesssed) by the specified plugin. The resulting vectorized audio can only be processed by the same plugin. A plugin must support the AudioVectorizer Trait to support this request. Field Type Label Description plugin string required The plugin domain string required The domain addition Audio repeated List of Audio to process PluginAudioVectorResult The result of a PluginAudioVectorRequest, containing a set of VectorResults Field Type Label Description vector_result VectorResult repeated The results of processing the submitted audio. One result per audio addition. VectorResult The status of the vector request, and if successful includes an AudioVector Field Type Label Description successful bool required Whether or not the audio was successfully processed message string optional Description of what occurred to cause an error audio_vector AudioVector optional If successful, the vectorized audio ClassExportRequest Exports an existing class enrollment (i.e. speaker enrollment) from the server for the specified class_id. The plugin must support the ClassExporter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class model to export ClassExportResult The result of an enrollment export Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred to cause an error enrollment EnrollmentModel optional If successful, then this is the exported model for the specified class. ClassImportRequest Used to import an enrollment model (exported via a ClassExportRequest). Only plugins that support the ClassExporter trait can handle this request. Only import an enrollment into the same plugin AND domain as previously exported. Field Type Label Description plugin string required The plugin domain string required The domain class_id string optional Import the model using this class name, instead of the original name enrollment EnrollmentModel required the enrollment to import ClassImportResult The status of a ClassImportRequest. Field Type Label Description successful bool required Whether or not the import succeeded message string optional Description of what occurred to cause an error EnrollmentModel An enrollment model for a specific plugin and domain. This is used to save a current enrollment or restore a class enrollment via a ClassImportRequest. This model is not used as an AudioVector in a scoring requests. Field Type Label Description plugin string required The origin plugin domain string required The origin domain class_id string required the class_id of the enrollment data bytes required The enrollment model data params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing Updater Messages ApplyUpdateRequest Used to request an update of a Plugin that supports the Update trait. Use GetUpdateStatusRequest to check if a plugin is ready for an update, otherwise this request is ignored by the server/plugin Field Type Label Description plugin string required The plugin to apply the update domain string required The domain params Metadata repeated Name/value options, plugin dependent ApplyUpdateResult This message is immediately returned after an ApplyUpdateRequest, as the updating process can take an extended time range to complete. Field Type Label Description successful bool required True if the plugin is being updated GetUpdateStatusRequest Used to request the status for a Plugin that supports the Update trait Field Type Label Description plugin string required The plugin domain string required The domain GetUpdateStatusResult The result of a GetUpdateStatusRequest message. Field Type Label Description update_ready bool required True if the plugin has determiend it is ready for an update last_update DateTime optional The date of the last update, if any params Metadata repeated Zero or Metadata values describing the update status of the plugin. Metadata is a list of name/value pairs. DateTime Date and time info Field Type Label Description year uint32 required Year month uint32 required Month day uint32 required Day hour uint32 required Hour min uint32 required Minute sec uint32 required Seconds Learning Trait Messages These messages are used by plugins that support adaptation and/or training. PreprocessAudioAdaptRequest Request preprocessing of this audio submission, which may be part of an adaptation set. Adaptation can be unsupervised (neither class_id, start_t, and end_t set) or supervised by setting class_id or class_id, start_t, and end_t. Adaptation should be finalized by calling either SupervisedAdaptationRequest or UnsupervisedAdaptationRequest. Plugins must support either the SupervisedAdapter or UnsupervisedAdapter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain audio Audio required The submitted audio adapt_space string required A unique name for the client where pre-processed data is stored class_id string optional The id of the class annotation being preprocessed (supervised training) start_t uint32 optional Begin-time of the region (in seconds) end_t uint32 optional End-time of the region (in seconds) PreprocessAudioAdaptResult The result of a PreprocessAudioAdaptRequest Field Type Label Description audio_id string required The ID of the preprocessed audio duration double required The duration of the audio Supervised Adapter Messages SupervisedAdaptationRequest Finalize adaptation of the specified plugin/domain using audio preprocessed using calls to PreprocessAudioAdaptRequest. The plugin must implement the SupervisedAdapter trait to handle this request. When you adapt or train, you are creating a new domain for the target plugin, that is based on the domain passed in to the 'domain' field of this call. This new domain is specific to a plugin, so it is created within the plugin, and will be named with the string passed to SupervisedAdaptationRequest as 'new_domain'. To actually use this new domain, future scoring or enrollment requests must specify this new domain name, instead of the original, using the value specified during adaptation as the 'new_domain' field. To have access to this new domain, either restart the server, or send a RemovePluginDomainRequest message to the server, which will force a reload of that plugin. Upon successful completion of SupervisedAdaptationRequest, a SupervisedAdaptationResult message should be received, containing the path to the newly created domain on the server's file system. The file sizes of the actual models will not change as a result of adaptation. Rather, the values stored inside these files will. Field Type Label Description plugin string required The plugin to invoke domain string required The domain to adapt new_domain string required the new domain name class_annotations ClassAnnotation repeated The annotations to use for adaptation, audio annotations are created via PreprocessAudioAdaptRequest calls adapt_space string required A unique name for the client where pre-processed data is stored SupervisedAdaptationResult Acknowledgment message that adaptation successfully completed. Informs the client of the full path of the new domain created by the SupervisedAdaptationRequest . Field Type Label Description new_domain string required Confirmation of the new domain name ClassAnnotation Set of annotations for a class Field Type Label Description class_id string required The class ID (such as speaker name or language name) annotations AudioAnnotation repeated the set of all audio annotations for this class. AudioAnnotation A set of audio annotations for a specific audio submission Field Type Label Description audio_id string required The audio ID returned in a PreprocessAudioAdaptResult or PreprocessAudioTrainResult message regions AnnotationRegion repeated Set of annotations Workflow Messages Not yet defined Basic Types Trait A Trait implemented by a plugin Field Type Label Description type TraitType optional The trait type options OptionDefinition repeated Any options specific to this plugin's implementation of the trait Plugin The description of a plugin Field Type Label Description id string optional The id of the plugin task string optional e.g. LID, SID, SAD, KWS, AED, etc. label string optional Display label for plugin desc string optional A brief description of how the plugin works/technologies it employs. vendor string optional A brief description of how the plugin works/technologies it employs. domain Domain repeated The domains owned by this plugin trait Trait repeated The traits (capabilities) of this plugin Domain A description of a domain. Field Type Label Description id string optional The ID of the domain label string optional Display label for the domain desc string optional A brief description of the domain conditions class_id string repeated The list of classes known to this domain Envelope Every message passed between the server and client is an instance of Envelope. An Envelope can contain multiple ScenicMessage instances, so it's important to iterate through them all when you receive an Envelope. Field Type Label Description message ScenicMessage repeated The messages to be sent sender_id string required string description of the message sender ScenicMessage A ScenicMessage represents a single logical message between a client and server. It is placed within an Envelope. It contains nested messages in serialized form. The message_type field is used to determine the type of the nested data. Not all ScenicMessage instance will have message_data, and some may have multiple, but they will all be of the same type. It depends on the value of message_type. Field Type Label Description message_id string required id issued by client (and unique to that client) used to track a request. Any reply for that request will have the same id. message_type MessageType required type of message message_data bytes repeated nested message data that can be deserialized according to message_type. Some messages do not have nested data, some have multiple records error string optional error message; if present an error has occurred on the server info string optional informational message, typically used to explain why message_data is empty but no error is reported MessageType The MessageType enum provides a value for each top-level SCENIC message. It is used within a ScenicMessage to indicate the type of the serialized message contained therein. Name Number Description PLUGIN_DIRECTORY_REQUEST 1 PLUGIN_DIRECTORY_RESULT 2 GLOBAL_SCORER_REQUEST 3 GLOBAL_SCORER_RESULT 4 REGION_SCORER_REQUEST 5 REGION_SCORER_RESULT 6 FRAME_SCORER_REQUEST 7 FRAME_SCORER_RESULT 8 CLASS_MODIFICATION_REQUEST 9 CLASS_MODIFICATION_RESULT 10 CLASS_REMOVAL_REQUEST 11 CLASS_REMOVAL_RESULT 12 GET_ACTIVE_REQUEST 13 GET_ACTIVE_RESULT 14 LOAD_PLUGIN_REQUEST 15 LOAD_PLUGIN_RESULT 16 GET_STATUS_REQUEST 17 GET_STATUS_RESULT 18 HEARTBEAT 19 PREPROCESS_AUDIO_TRAIN_REQUEST 20 PREPROCESS_AUDIO_TRAIN_RESULT 21 PREPROCESS_AUDIO_ADAPT_REQUEST 22 PREPROCESS_AUDIO_ADAPT_RESULT 23 SUPERVISED_TRAINING_REQUEST 24 SUPERVISED_TRAINING_RESULT 25 SUPERVISED_ADAPTATION_REQUEST 26 SUPERVISED_ADAPTATION_RESULT 27 UNSUPERVISED_ADAPTATION_REQUEST 28 UNSUPERVISED_ADAPTATION_RESULT 29 CLASS_ANNOTATION 30 AUDIO_ANNOTATION 31 ANNOTATION_REGION 32 REMOVE_PLUGIN_REQUEST 33 REMOVE_PLUGIN_RESULT 34 AUDIO_MODIFICATION_REQUEST 35 AUDIO_MODIFICATION_RESULT 36 PLUGIN_AUDIO_VECTOR_REQUEST 37 PLUGIN_AUDIO_VECTOR_RESULT 38 CLASS_EXPORT_REQUEST 39 CLASS_EXPORT_RESULT 40 CLASS_IMPORT_REQUEST 41 CLASS_IMPORT_RESULT 42 APPLY_UPDATE_REQUEST 43 APPLY_UPDATE_RESULT 44 GET_UPDATE_STATUS_REQUEST 45 GET_UPDATE_STATUS_RESULT 46 GLOBAL_COMPARER_REQUEST 47 GLOBAL_COMPARER_RESULT 48 | AUDIO_ALIGN_REQUEST | 68 | | | AUDIO_ALIGN_RESULT | 69 | | | TEXT_TRANSFORM_REQUEST | 70 | | | TEXT_TRANSFORM_RESULT | 71 | | | PREPROCESSED_AUDIO_RESULT | 72 | | | INVALID_MESSAGE | 73 | | OptionType Classifies how a OptionDefinition (TraitOption) should be represented in a UI widget Name Number Description BOOLEAN 1 CHOICE 2 TraitType The list of possible traits that a plugin can implement Name Number Description GLOBAL_SCORER 1 REGION_SCORER 2 FRAME_SCORER 3 CLASS_ENROLLER 4 CLASS_MODIFIER 5 SUPERVISED_TRAINER 6 SUPERVISED_ADAPTER 7 UNSUPERVISED_ADAPTER 8 AUDIO_CONVERTER 9 AUDIO_VECTORIZER 10 CLASS_EXPORTER 11 UPDATER 12 LEARNING_TRAIT 13 GLOBAL_COMPARER 14 TEXT_TRANSFORMER 15 AUDIO_ALIGNMENT_SCORER 16 TaskType Name Number Description SAD 1 Speech Activity Detection SID 2 Speaker ID SDD 3 Speaker ID, but output in regions LID 4 Language ID LDD 5 Language ID, but output in regions KWS 6 Keyword Spotting TPD 7 Topic Detection VTD 8 Voice Type Discrimination GID 9 Gender ID GDD 10 Gender ID, but output in regions ASR 11 Automatic Speech Recognition ENH 12 Audio Enhancement CMP 13 Audio Comparison SDD 14 Speaker Detection DIA 15 Diarization QBE 16 Query by Example SHL 17 Speaker Highlighing FID 18 Face ID TMT 19 Text Machine Translation ALN 20 Audio Alignment OptionDefinition A plugin TraitOption, describing how a plugin trait is used Field Type Label Description name string required The name/ID of the option label string required Display label for the option desc string optional A description of the option type OptionType required The type of the option (boolean, choice/drop-down, etc) choice string repeated Optional list of choices used by CHOICE type options default string optional The default option in the list of Options OptionValue A name/value property pair Field Type Label Description name string required The name/ID of the option value string required The option value as a string InputDataType Workflow(?) Input Data Types Name Number Description AUDIO 1 VIDEO 2 TEXT 3 IMAGE 4 InputType Name Number Description FRAME 1 REGION 2 JobClass Field Type Label Description job_name string required The parent job name in a Workflow JobDefinition task TaskClass repeated TaskClass Field Type Label Description task_name string required The ID from the associated WorkflowTask (consumer_result_label) class_id string repeated Zero or more class IDs available to this task. Some tasks do not support classes class_label string optional An optional label/name to describe the classes used by this task such a 'speaker' or 'language' classes_label string optional The speaker label when refering to plural classes, such as speakers, or languages Shared Types Audio Represents an audio. Can either refer to a local file or embed an audio buffer directly. The path and audioSamples fields should be treated as mutually exclusive, with one and only one of these fields implemented For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description path string optional Path to the audio file represented by this record (if not specified then audio is input as a buffer) audioSamples AudioBuffer optional Audio included as a buffer (if not specified, then path must be set) selected_channel uint32 optional Optional - if using multi-channel audio and 'mode' is SELECTED, then this channel is provided to the plugin(s). regions AnnotationRegion repeated Optional annotated regions for this audio label string optional Optional - label used to identify this audio input AnnotationRegion A single pair of timestamps (start and end) that make up an annotated region. Timestamps are in seconds. Field Type Label Description start_t uint32 required Begin-time of the region (in seconds) end_t uint32 required End-time of the region (in seconds) AudioBuffer Audio is contained in a buffer (and the path filed is NOT set in Audio) - by default the audio in the buffer should be PCM encoded, unless the buffer contains a serialized file (unencoded) in which case the serialized_file must be set to true. If the data has been decoded and is not PCM encoded data, then the encoding field must be specified For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description channels uint32 optional The number of channels contained in data, ignored for serialized buffers samples uint32 optional The number of samples (in each channel), ignored for serialized buffers rate uint32 optional The sample rate, ignored for serialized buffers bit_depth AudioBitDepth optional The number of bits in each sample, ignored for serialized buffers data bytes required Should be channels * samples long, striped by channels serialized_file bool optional Optional - true if data contains a serialized buffer encoding AudioEncodingType optional Optional - Not Yet supported - the audio encoding type. Assumed to be PCM if not specified. Ignored for serialized buffer AudioEncodingType Audio encoding types Name Number Description PCMU8 1 PCMS8 2 PCM16 3 PCM24 4 PCM32 5 FLOAT32 6 FLOAT64 7 ULAW 8 ALAW 9 IMA_ADPCM 10 MS_ADPCM 11 GSM610 12 G723_24 13 G721_32 14 DWW12 15 DWW16 16 DWW24 17 VORBIS 18 VOX_ADPCM 19 DPCM16 20 DPCM8 21 AudioBitDepth Number of bits in each audio sample Name Number Description BIT_DEPTH_8 1 BIT_DEPTH_16 2 BIT_DEPTH_24 3 BIT_DEPTH_32 4 Metadata The parent container for Metadata so that typed name/value properties can be transported in a generalized way Field Type Label Description type MetadataType required Indicates the type of this metadata, so it can be deserialized to the appropriate type name string required The name (key) for this metadata value bytes required The value is one of MetadataType, must be deserialized by the client into the type specified by type MetadataType Data types supported in an AudioModificationResult's metadata: Name Number Description STRING_META 1 INTEGER_META 2 DOUBLE_META 3 BOOLEAN_META 4 LIST_META 5 BooleanMetadata Value as boolean Field Type Label Description value bool required DoubleMetadata Value as a double Field Type Label Description value double required IntegerMetadata Value as an integer Field Type Label Description value int32 required ListMetadata Value as list of Metadata values Field Type Label Description type MetadataType repeated The type for the corresponding element value bytes repeated The value is one or more MetadataType elements, each element must be deserialized by the client into the type specified by type. For example, for the type, STRING_META, deserialize data as StringMetadata StringMetadata Value as a string Field Type Label Description value string required Scalar Value Types .proto Type Notes C++ Type Java Type Python Type double double double float float float float float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long uint32 Uses variable-length encoding. uint32 int int/long uint64 Uses variable-length encoding. uint64 long int/long sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long sfixed32 Always four bytes. int32 int int sfixed64 Always eight bytes. int64 long int/long bool bool boolean boolean string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode bytes May contain any arbitrary sequence of bytes. string ByteString str","title":"Enterprise API Message Reference"},{"location":"apiMessage.html#olive-api-message-protocol-documentation","text":"","title":"OLIVE API Message Protocol Documentation"},{"location":"apiMessage.html#oliveproto-protocol-buffer-definitions","text":"The messages defined on this page are what define the OLIVE Enterprise API. These messages are how a client application will interact with and provide tasks to an OLIVE Server. The format used by the OLIVE API is based on the Google Protocol Buffer . For more information on how to integrate the ability to send and receive these messages into a client application using a provided Client API implementation from SRI, refer to the Integrating the (Java) Client API Guide. For more information on creating your own reference implementation with the functionality of these messages, head over to the Creating an API Reference Implementation page that offers guidelines and information.","title":"olive.proto Protocol Buffer Definitions"},{"location":"apiMessage.html#server-management-messages","text":"","title":"Server Management Messages"},{"location":"apiMessage.html#getactiverequest","text":"Message to request the list of ScenicMessages that are still active","title":"GetActiveRequest"},{"location":"apiMessage.html#getactiveresult","text":"Response to GetActiveRequest containing the ScenicMessages that are still active Field Type Label Description message_id string repeated List containing the IDs of each message still being processed on the server total_num string required Total number of messages still being processed","title":"GetActiveResult"},{"location":"apiMessage.html#getstatusrequest","text":"Request a simple server status message, similar to a heartbeat except the request reply is sent on the status port and is requested by the client","title":"GetStatusRequest"},{"location":"apiMessage.html#getstatusresult","text":"The result of a GetStatusRequest Field Type Label Description num_pending uint32 required The number of pending jobs num_busy uint32 required the number of active jobs num_finished uint32 required The number of finished jobs","title":"GetStatusResult"},{"location":"apiMessage.html#heartbeat","text":"A heartbeat Message, acknowledging that the server is running, this message is continuously broadcast by the server on it's status port (this is the only message sent on the status port) Field Type Label Description stats ServerStats optional The current status of the server, optional since status is not available when the server first starts, but a heartbeat is still sent logdir string required The location where the server writes it log files","title":"Heartbeat"},{"location":"apiMessage.html#serverstats","text":"Current status of the OLIVE server, sent as part of a Heatbeat message. Field Type Label Description cpu_percent float required The current percentage of CPU used cpu_average float required The average CPU percentage used since the server was started mem_percent float required The percentage of memory used max_mem_percent float required The most memory used since the server was started swap_percent float required The current swap used max_swap_percent float required The max (most) swap space used since the server was started pool_busy uint32 required The number of jobs currently running on the server pool_pending uint32 required The number of jobs queued on the server pool_finished uint32 required The number of jobs completed pool_reinit bool required The number of jobs that need to be re-ran max_num_jobs uint32 optional The max number of concurrent jobs server_version string optional The current version of the server","title":"ServerStats"},{"location":"apiMessage.html#loadplugindomainrequest","text":"Request a plugin be pre-loaded to optimize later score request(s) Field Type Label Description plugin string required The plugin domain string required The domain","title":"LoadPluginDomainRequest"},{"location":"apiMessage.html#loadplugindomainresult","text":"Acknowledgment that a plugin is being loaded Field Type Label Description successful bool required True if the request is being loaded (but receipt of this message does not guarantee the plugin has finished loaded)","title":"LoadPluginDomainResult"},{"location":"apiMessage.html#removeplugindomainrequest","text":"Unload (remove from memory) a previously loaded plugin. Use to free resources on the server or force reloading of a plugin/domain Field Type Label Description plugin string required The plugin to remove domain string optional The domain to remove, if omitted all domains removed for this plugin.","title":"RemovePluginDomainRequest"},{"location":"apiMessage.html#removeplugindomainresult","text":"Acknowledgment that a plugin/domain has been removed (unloaded) Field Type Label Description successful bool required True if the plugin was been removed","title":"RemovePluginDomainResult"},{"location":"apiMessage.html#plugindirectoryrequest","text":"Use a PluginDirectoryRequest message to receive the list of plugins available on the server. A Plugin performs tasks such as SAD, LID, SID, or KWS. There may be multiple plugins registered for a given task. A plugin typically has one or more Domains. Plugins contain the code of the recognizer, while Domains correspond to a particular training or adaptation sessions. Domains therefore represent the data/conditions. A plugin and domain together are necessary to perform scoring.","title":"PluginDirectoryRequest"},{"location":"apiMessage.html#plugindirectoryresult","text":"The collection of plugins available on the server, response to PluginDirectoryRequest Field Type Label Description plugins Plugin repeated The available plugins","title":"PluginDirectoryResult"},{"location":"apiMessage.html#global-scorer-messages","text":"GlobalScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a GlobalScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a GlobalScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide.","title":"Global Scorer Messages"},{"location":"apiMessage.html#globalscore","text":"The global score for a class Field Type Label Description class_id string required The class score float required The score associated with the class confidence float optional An optional confidence value when part of a calibration report comment string optional An optional suggested action when part of a calibration report","title":"GlobalScore"},{"location":"apiMessage.html#globalscorerrequest","text":"Request global scoring using the specified plugin. The plugin must implement the GlobalScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of global scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio optional The audio to analyze/score. Either audio or vector must be set. vector AudioVector optional The preprocessed audio vector to analyze/score. Either audio or vector must be set. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"GlobalScorerRequest"},{"location":"apiMessage.html#globalscorerresult","text":"The result from a GlobalScorerRequest, having zero or more GlobalScore elements Field Type Label Description score GlobalScore repeated The class scores","title":"GlobalScorerResult"},{"location":"apiMessage.html#region-scorer-messages","text":"RegionScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a RegionScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a RegionScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide.","title":"Region Scorer Messages"},{"location":"apiMessage.html#regionscore","text":"The basic unit of a region score. There may be multiple RegionScore values in a RegionScorerResult Field Type Label Description start_t int32 required Begin-time of the region (in seconds) end_t int32 required End-time of the region (in seconds) class_id string required Class ID associated with region score float optional Optional score associated with the class_id label","title":"RegionScore"},{"location":"apiMessage.html#regionscorerrequest","text":"Request region scoring for the specified plugin/domain. The plugin must implement the RegionScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of region scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"RegionScorerRequest"},{"location":"apiMessage.html#regionscorerresult","text":"The set of region score results, response to RegionScorerRequest Field Type Label Description region RegionScore repeated The scored regions","title":"RegionScorerResult"},{"location":"apiMessage.html#frame-scorer-messages","text":"FrameScorer is an OLIVE Plugin Trait defining a/the type of scoring the plugin is capable of. For more details about what it means to be a FrameScorer, refer to the relevant section of the Plugin Traits page. For an example of the code that would be used to build and submit a FrameScorer request, refer to the relevant section of the Integrating the (Java) Client API Guide.","title":"Frame Scorer Messages"},{"location":"apiMessage.html#framescorerrequest","text":"Request frame scoring using the specified plugin and audio. The plugin must implement the FrameScorer trait to handle this request. If this request is successful, then one set of scores is returned since the audio submission is assumed to be mono. If submitting multichannel audio then the audio is merged (unless a channel specified in the Audio message, then that channel is used) to produce one set of frame scores. Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio Audio required The audio to analyze/score option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"FrameScorerRequest"},{"location":"apiMessage.html#framescorerresult","text":"The results from a FrameScorerRequest Field Type Label Description result FrameScores repeated List of frame scores by class_id","title":"FrameScorerResult"},{"location":"apiMessage.html#framescores","text":"The basic unit of a frame score, returned in a FrameScorerRequest Field Type Label Description class_id string required The class ID to which the frame scores pertain frame_rate int32 required The number of frames per second frame_offset double required The offset to the center of the frame 'window' score double repeated The frame-level scores for the class_id","title":"FrameScores"},{"location":"apiMessage.html#text-transformation-messages","text":"TextTransformer is an OLIVE Plugin Trait for scoring using text (instead of audio). For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page.","title":"Text Transformation Messages"},{"location":"apiMessage.html#texttransformationrequest","text":"Request the transformation of a text/string using MT Field Type Label Description plugin string required The plugin to invoke domain string required The domain text string optional The string text to analyze/score, Optional as of OLIVE 5.0 since data input(s) can be specified as part of a workflow option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored","title":"TextTransformationRequest"},{"location":"apiMessage.html#texttransformationresult","text":"The response to a TextTransformationRequest Field Type Label Description transformation TextTransformation repeated","title":"TextTransformationResult"},{"location":"apiMessage.html#texttransformation","text":"The text value returned in a TextTransformationResult Field Type Label Description class_id string required A classifier for this result, usually just 'text' transformed_text string required The text result","title":"TextTransformation"},{"location":"apiMessage.html#audio-alignment-messages","text":"AudioAlignmentScorer is an OLIVE Plugin Trait for alignment of two or more audio inputs. For more details about what it means to be a TextTransformer, refer to the relevant section of the Plugin Traits page.","title":"Audio Alignment  Messages"},{"location":"apiMessage.html#audioalignmentscorerequest","text":"Request the alignment of two or more audio inputs Field Type Label Description plugin string required The plugin to invoke domain string required The domain audios Audio repeated The audio to analyze/score, Optional as of OLIVE 5.0 since Audio can be specified option OptionValue repeated as part of a workflow. If specified there should be two or more audio inputs Any options specified | | class_id | string | repeated | Optionally specify the classes to be scored |","title":"AudioAlignmentScoreRequest"},{"location":"apiMessage.html#audioalignmentscoreresult","text":"The result of a AudioAlignmentScoreRequest Field Type Label Description scores AudioAlignmentScore repeated","title":"AudioAlignmentScoreResult"},{"location":"apiMessage.html#audioalignmentscore","text":"A score in an AudioAlignmentScoreResult Field Type Label Description reference_audio_label string required The source or reference audio name (file 1) other_audio_label string required The name of the audio input in comparison (file 2) shift_offset float required shift offset between the audion in the reference and confidence float required The confidence of this score","title":"AudioAlignmentScore"},{"location":"apiMessage.html#global-comparer-messages","text":"","title":"Global Comparer Messages"},{"location":"apiMessage.html#globalcomparerreport","text":"The visual representation of a global comparison Field Type Label Description type ReportType required The type of report (normally a PDF) report_data bytes required The serialized report","title":"GlobalComparerReport"},{"location":"apiMessage.html#globalcomparerrequest","text":"Request the comparison of two audio submission. The plugin must implement the GlobalComparer trait to handle this request Field Type Label Description plugin string required The plugin to invoke domain string required The domain audio_one Audio required One of two audio submissions to analyze/score. audio_two Audio required One of two audio submissions to analyze/score. option OptionValue repeated Any options specified class_id string repeated Optionally specify the classes to be scored // todo remove!","title":"GlobalComparerRequest"},{"location":"apiMessage.html#globalcomparerresult","text":"The result of a GlobalComparerRequest Field Type Label Description results Metadata repeated The metadata/scores returned from a global compare analysis report GlobalComparerReport repeated A comparison report generated by the plugin","title":"GlobalComparerResult"},{"location":"apiMessage.html#reporttype","text":"Possible report formats Name Number Description PDF 1 PNG 2 GIF 3 JPEG 4 TIFF 5","title":"ReportType"},{"location":"apiMessage.html#class-modifier-messages","text":"","title":"Class Modifier Messages"},{"location":"apiMessage.html#classmodificationrequest","text":"Request a modification of a class for the specified plugin. The plugin must implement the ClassModifier Trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class being enrolled/modified addition Audio repeated List of Audio, action pairs to apply to the class removal Audio repeated List of Audio, action pairs to apply to the class addition_vector AudioVector repeated List of preprocessed audio vector to apply to the class finalize bool optional Whether or not to finalize the class. You can send multiple ClassModificationRequests and only finalize on the last request for efficiency. Default: true option OptionValue repeated Any modification options","title":"ClassModificationRequest"},{"location":"apiMessage.html#classmodificationresult","text":"Response to ClassModificationRequest . Field Type Label Description addition_result AudioResult repeated Provides feedback about the success/failure of individual audio additions removal_result AudioResult repeated Provides feedback about the success/failure of individual audio removals vector_addition_result AudioResult repeated Provides feedback about the success/failure of individual audio vector additions","title":"ClassModificationResult"},{"location":"apiMessage.html#classremovalrequest","text":"Request removal of a class in the specified plugin/domain Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The id of the class to be removed","title":"ClassRemovalRequest"},{"location":"apiMessage.html#classremovalresult","text":"Acknowledgment that a ClassRemovalRequest was received","title":"ClassRemovalResult"},{"location":"apiMessage.html#audioresult","text":"The feedback/description of class modification for a result in a ClassModificationResult message Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred on this audio","title":"AudioResult"},{"location":"apiMessage.html#audio-converter-messages","text":"","title":"Audio Converter Messages"},{"location":"apiMessage.html#audiomodification","text":"The contents of an AudioModificationResult. Field Type Label Description audio AudioBuffer required The transformed audio message string required Description of how this audio was transformed, or an error description. Not sure if needed? scores Metadata repeated Zero or more scores (metadata) about the modifed audio. Metadata is a list of name/value pairs.","title":"AudioModification"},{"location":"apiMessage.html#audiomodificationrequest","text":"Request enhancement (modification) of the submitted audio. The plugin must support the AudioConverter trait to support this request Field Type Label Description plugin string required The plugin domain string required The domain requested_channels uint32 required Convert audio to have this number of channels requested_rate uint32 required Convert audio to this sample rate modifications Audio repeated List of Audio, action pairs to apply to the class - may have to limit to one audio submission per request, not sure how to handle multiple results option OptionValue repeated Any options specified","title":"AudioModificationRequest"},{"location":"apiMessage.html#audiomodificationresult","text":"The result of an AudioModificationRequest Field Type Label Description successful bool required Whether or not the individual audio modification succeeded modification_result AudioModification repeated Provides feedback about the success/failure of individual audio additions.","title":"AudioModificationResult"},{"location":"apiMessage.html#audio-vectorizer-messages","text":"","title":"Audio Vectorizer Messages"},{"location":"apiMessage.html#audiovector","text":"Represents audio preprocessed by a plugin/domain. Field Type Label Description plugin string required The origin plugin domain string required The origin domain data bytes required The audio vector data, varies by plugin params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing","title":"AudioVector"},{"location":"apiMessage.html#pluginaudiovectorrequest","text":"Request one or more audio submissions be vectorized (preprocesssed) by the specified plugin. The resulting vectorized audio can only be processed by the same plugin. A plugin must support the AudioVectorizer Trait to support this request. Field Type Label Description plugin string required The plugin domain string required The domain addition Audio repeated List of Audio to process","title":"PluginAudioVectorRequest"},{"location":"apiMessage.html#pluginaudiovectorresult","text":"The result of a PluginAudioVectorRequest, containing a set of VectorResults Field Type Label Description vector_result VectorResult repeated The results of processing the submitted audio. One result per audio addition.","title":"PluginAudioVectorResult"},{"location":"apiMessage.html#vectorresult","text":"The status of the vector request, and if successful includes an AudioVector Field Type Label Description successful bool required Whether or not the audio was successfully processed message string optional Description of what occurred to cause an error audio_vector AudioVector optional If successful, the vectorized audio","title":"VectorResult"},{"location":"apiMessage.html#classexportrequest","text":"Exports an existing class enrollment (i.e. speaker enrollment) from the server for the specified class_id. The plugin must support the ClassExporter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain class_id string required The ID of the class model to export","title":"ClassExportRequest"},{"location":"apiMessage.html#classexportresult","text":"The result of an enrollment export Field Type Label Description successful bool required Whether or not the individual audio succeeded message string optional Description of what occurred to cause an error enrollment EnrollmentModel optional If successful, then this is the exported model for the specified class.","title":"ClassExportResult"},{"location":"apiMessage.html#classimportrequest","text":"Used to import an enrollment model (exported via a ClassExportRequest). Only plugins that support the ClassExporter trait can handle this request. Only import an enrollment into the same plugin AND domain as previously exported. Field Type Label Description plugin string required The plugin domain string required The domain class_id string optional Import the model using this class name, instead of the original name enrollment EnrollmentModel required the enrollment to import","title":"ClassImportRequest"},{"location":"apiMessage.html#classimportresult","text":"The status of a ClassImportRequest. Field Type Label Description successful bool required Whether or not the import succeeded message string optional Description of what occurred to cause an error","title":"ClassImportResult"},{"location":"apiMessage.html#enrollmentmodel","text":"An enrollment model for a specific plugin and domain. This is used to save a current enrollment or restore a class enrollment via a ClassImportRequest. This model is not used as an AudioVector in a scoring requests. Field Type Label Description plugin string required The origin plugin domain string required The origin domain class_id string required the class_id of the enrollment data bytes required The enrollment model data params Metadata repeated Name/value parameter data generated by the plugin and needed for later processing","title":"EnrollmentModel"},{"location":"apiMessage.html#updater-messages","text":"","title":"Updater Messages"},{"location":"apiMessage.html#applyupdaterequest","text":"Used to request an update of a Plugin that supports the Update trait. Use GetUpdateStatusRequest to check if a plugin is ready for an update, otherwise this request is ignored by the server/plugin Field Type Label Description plugin string required The plugin to apply the update domain string required The domain params Metadata repeated Name/value options, plugin dependent","title":"ApplyUpdateRequest"},{"location":"apiMessage.html#applyupdateresult","text":"This message is immediately returned after an ApplyUpdateRequest, as the updating process can take an extended time range to complete. Field Type Label Description successful bool required True if the plugin is being updated","title":"ApplyUpdateResult"},{"location":"apiMessage.html#getupdatestatusrequest","text":"Used to request the status for a Plugin that supports the Update trait Field Type Label Description plugin string required The plugin domain string required The domain","title":"GetUpdateStatusRequest"},{"location":"apiMessage.html#getupdatestatusresult","text":"The result of a GetUpdateStatusRequest message. Field Type Label Description update_ready bool required True if the plugin has determiend it is ready for an update last_update DateTime optional The date of the last update, if any params Metadata repeated Zero or Metadata values describing the update status of the plugin. Metadata is a list of name/value pairs.","title":"GetUpdateStatusResult"},{"location":"apiMessage.html#datetime","text":"Date and time info Field Type Label Description year uint32 required Year month uint32 required Month day uint32 required Day hour uint32 required Hour min uint32 required Minute sec uint32 required Seconds","title":"DateTime"},{"location":"apiMessage.html#learning-trait-messages","text":"These messages are used by plugins that support adaptation and/or training.","title":"Learning Trait Messages"},{"location":"apiMessage.html#preprocessaudioadaptrequest","text":"Request preprocessing of this audio submission, which may be part of an adaptation set. Adaptation can be unsupervised (neither class_id, start_t, and end_t set) or supervised by setting class_id or class_id, start_t, and end_t. Adaptation should be finalized by calling either SupervisedAdaptationRequest or UnsupervisedAdaptationRequest. Plugins must support either the SupervisedAdapter or UnsupervisedAdapter trait to handle this request. Field Type Label Description plugin string required The plugin domain string required The domain audio Audio required The submitted audio adapt_space string required A unique name for the client where pre-processed data is stored class_id string optional The id of the class annotation being preprocessed (supervised training) start_t uint32 optional Begin-time of the region (in seconds) end_t uint32 optional End-time of the region (in seconds)","title":"PreprocessAudioAdaptRequest"},{"location":"apiMessage.html#preprocessaudioadaptresult","text":"The result of a PreprocessAudioAdaptRequest Field Type Label Description audio_id string required The ID of the preprocessed audio duration double required The duration of the audio","title":"PreprocessAudioAdaptResult"},{"location":"apiMessage.html#supervised-adapter-messages","text":"","title":"Supervised Adapter Messages"},{"location":"apiMessage.html#supervisedadaptationrequest","text":"Finalize adaptation of the specified plugin/domain using audio preprocessed using calls to PreprocessAudioAdaptRequest. The plugin must implement the SupervisedAdapter trait to handle this request. When you adapt or train, you are creating a new domain for the target plugin, that is based on the domain passed in to the 'domain' field of this call. This new domain is specific to a plugin, so it is created within the plugin, and will be named with the string passed to SupervisedAdaptationRequest as 'new_domain'. To actually use this new domain, future scoring or enrollment requests must specify this new domain name, instead of the original, using the value specified during adaptation as the 'new_domain' field. To have access to this new domain, either restart the server, or send a RemovePluginDomainRequest message to the server, which will force a reload of that plugin. Upon successful completion of SupervisedAdaptationRequest, a SupervisedAdaptationResult message should be received, containing the path to the newly created domain on the server's file system. The file sizes of the actual models will not change as a result of adaptation. Rather, the values stored inside these files will. Field Type Label Description plugin string required The plugin to invoke domain string required The domain to adapt new_domain string required the new domain name class_annotations ClassAnnotation repeated The annotations to use for adaptation, audio annotations are created via PreprocessAudioAdaptRequest calls adapt_space string required A unique name for the client where pre-processed data is stored","title":"SupervisedAdaptationRequest"},{"location":"apiMessage.html#supervisedadaptationresult","text":"Acknowledgment message that adaptation successfully completed. Informs the client of the full path of the new domain created by the SupervisedAdaptationRequest . Field Type Label Description new_domain string required Confirmation of the new domain name","title":"SupervisedAdaptationResult"},{"location":"apiMessage.html#classannotation","text":"Set of annotations for a class Field Type Label Description class_id string required The class ID (such as speaker name or language name) annotations AudioAnnotation repeated the set of all audio annotations for this class.","title":"ClassAnnotation"},{"location":"apiMessage.html#audioannotation","text":"A set of audio annotations for a specific audio submission Field Type Label Description audio_id string required The audio ID returned in a PreprocessAudioAdaptResult or PreprocessAudioTrainResult message regions AnnotationRegion repeated Set of annotations","title":"AudioAnnotation"},{"location":"apiMessage.html#workflow-messages","text":"Not yet defined","title":"Workflow Messages"},{"location":"apiMessage.html#basic-types","text":"","title":"Basic Types"},{"location":"apiMessage.html#trait","text":"A Trait implemented by a plugin Field Type Label Description type TraitType optional The trait type options OptionDefinition repeated Any options specific to this plugin's implementation of the trait","title":"Trait"},{"location":"apiMessage.html#plugin","text":"The description of a plugin Field Type Label Description id string optional The id of the plugin task string optional e.g. LID, SID, SAD, KWS, AED, etc. label string optional Display label for plugin desc string optional A brief description of how the plugin works/technologies it employs. vendor string optional A brief description of how the plugin works/technologies it employs. domain Domain repeated The domains owned by this plugin trait Trait repeated The traits (capabilities) of this plugin","title":"Plugin"},{"location":"apiMessage.html#domain","text":"A description of a domain. Field Type Label Description id string optional The ID of the domain label string optional Display label for the domain desc string optional A brief description of the domain conditions class_id string repeated The list of classes known to this domain","title":"Domain"},{"location":"apiMessage.html#envelope","text":"Every message passed between the server and client is an instance of Envelope. An Envelope can contain multiple ScenicMessage instances, so it's important to iterate through them all when you receive an Envelope. Field Type Label Description message ScenicMessage repeated The messages to be sent sender_id string required string description of the message sender","title":"Envelope"},{"location":"apiMessage.html#scenicmessage","text":"A ScenicMessage represents a single logical message between a client and server. It is placed within an Envelope. It contains nested messages in serialized form. The message_type field is used to determine the type of the nested data. Not all ScenicMessage instance will have message_data, and some may have multiple, but they will all be of the same type. It depends on the value of message_type. Field Type Label Description message_id string required id issued by client (and unique to that client) used to track a request. Any reply for that request will have the same id. message_type MessageType required type of message message_data bytes repeated nested message data that can be deserialized according to message_type. Some messages do not have nested data, some have multiple records error string optional error message; if present an error has occurred on the server info string optional informational message, typically used to explain why message_data is empty but no error is reported","title":"ScenicMessage"},{"location":"apiMessage.html#messagetype","text":"The MessageType enum provides a value for each top-level SCENIC message. It is used within a ScenicMessage to indicate the type of the serialized message contained therein. Name Number Description PLUGIN_DIRECTORY_REQUEST 1 PLUGIN_DIRECTORY_RESULT 2 GLOBAL_SCORER_REQUEST 3 GLOBAL_SCORER_RESULT 4 REGION_SCORER_REQUEST 5 REGION_SCORER_RESULT 6 FRAME_SCORER_REQUEST 7 FRAME_SCORER_RESULT 8 CLASS_MODIFICATION_REQUEST 9 CLASS_MODIFICATION_RESULT 10 CLASS_REMOVAL_REQUEST 11 CLASS_REMOVAL_RESULT 12 GET_ACTIVE_REQUEST 13 GET_ACTIVE_RESULT 14 LOAD_PLUGIN_REQUEST 15 LOAD_PLUGIN_RESULT 16 GET_STATUS_REQUEST 17 GET_STATUS_RESULT 18 HEARTBEAT 19 PREPROCESS_AUDIO_TRAIN_REQUEST 20 PREPROCESS_AUDIO_TRAIN_RESULT 21 PREPROCESS_AUDIO_ADAPT_REQUEST 22 PREPROCESS_AUDIO_ADAPT_RESULT 23 SUPERVISED_TRAINING_REQUEST 24 SUPERVISED_TRAINING_RESULT 25 SUPERVISED_ADAPTATION_REQUEST 26 SUPERVISED_ADAPTATION_RESULT 27 UNSUPERVISED_ADAPTATION_REQUEST 28 UNSUPERVISED_ADAPTATION_RESULT 29 CLASS_ANNOTATION 30 AUDIO_ANNOTATION 31 ANNOTATION_REGION 32 REMOVE_PLUGIN_REQUEST 33 REMOVE_PLUGIN_RESULT 34 AUDIO_MODIFICATION_REQUEST 35 AUDIO_MODIFICATION_RESULT 36 PLUGIN_AUDIO_VECTOR_REQUEST 37 PLUGIN_AUDIO_VECTOR_RESULT 38 CLASS_EXPORT_REQUEST 39 CLASS_EXPORT_RESULT 40 CLASS_IMPORT_REQUEST 41 CLASS_IMPORT_RESULT 42 APPLY_UPDATE_REQUEST 43 APPLY_UPDATE_RESULT 44 GET_UPDATE_STATUS_REQUEST 45 GET_UPDATE_STATUS_RESULT 46 GLOBAL_COMPARER_REQUEST 47 GLOBAL_COMPARER_RESULT 48 | AUDIO_ALIGN_REQUEST | 68 | | | AUDIO_ALIGN_RESULT | 69 | | | TEXT_TRANSFORM_REQUEST | 70 | | | TEXT_TRANSFORM_RESULT | 71 | | | PREPROCESSED_AUDIO_RESULT | 72 | | | INVALID_MESSAGE | 73 | |","title":"MessageType"},{"location":"apiMessage.html#optiontype","text":"Classifies how a OptionDefinition (TraitOption) should be represented in a UI widget Name Number Description BOOLEAN 1 CHOICE 2","title":"OptionType"},{"location":"apiMessage.html#traittype","text":"The list of possible traits that a plugin can implement Name Number Description GLOBAL_SCORER 1 REGION_SCORER 2 FRAME_SCORER 3 CLASS_ENROLLER 4 CLASS_MODIFIER 5 SUPERVISED_TRAINER 6 SUPERVISED_ADAPTER 7 UNSUPERVISED_ADAPTER 8 AUDIO_CONVERTER 9 AUDIO_VECTORIZER 10 CLASS_EXPORTER 11 UPDATER 12 LEARNING_TRAIT 13 GLOBAL_COMPARER 14 TEXT_TRANSFORMER 15 AUDIO_ALIGNMENT_SCORER 16","title":"TraitType"},{"location":"apiMessage.html#tasktype","text":"Name Number Description SAD 1 Speech Activity Detection SID 2 Speaker ID SDD 3 Speaker ID, but output in regions LID 4 Language ID LDD 5 Language ID, but output in regions KWS 6 Keyword Spotting TPD 7 Topic Detection VTD 8 Voice Type Discrimination GID 9 Gender ID GDD 10 Gender ID, but output in regions ASR 11 Automatic Speech Recognition ENH 12 Audio Enhancement CMP 13 Audio Comparison SDD 14 Speaker Detection DIA 15 Diarization QBE 16 Query by Example SHL 17 Speaker Highlighing FID 18 Face ID TMT 19 Text Machine Translation ALN 20 Audio Alignment","title":"TaskType"},{"location":"apiMessage.html#optiondefinition","text":"A plugin TraitOption, describing how a plugin trait is used Field Type Label Description name string required The name/ID of the option label string required Display label for the option desc string optional A description of the option type OptionType required The type of the option (boolean, choice/drop-down, etc) choice string repeated Optional list of choices used by CHOICE type options default string optional The default option in the list of Options","title":"OptionDefinition"},{"location":"apiMessage.html#optionvalue","text":"A name/value property pair Field Type Label Description name string required The name/ID of the option value string required The option value as a string","title":"OptionValue"},{"location":"apiMessage.html#inputdatatype","text":"Workflow(?) Input Data Types Name Number Description AUDIO 1 VIDEO 2 TEXT 3 IMAGE 4","title":"InputDataType"},{"location":"apiMessage.html#inputtype","text":"Name Number Description FRAME 1 REGION 2","title":"InputType"},{"location":"apiMessage.html#jobclass","text":"Field Type Label Description job_name string required The parent job name in a Workflow JobDefinition task TaskClass repeated","title":"JobClass"},{"location":"apiMessage.html#taskclass","text":"Field Type Label Description task_name string required The ID from the associated WorkflowTask (consumer_result_label) class_id string repeated Zero or more class IDs available to this task. Some tasks do not support classes class_label string optional An optional label/name to describe the classes used by this task such a 'speaker' or 'language' classes_label string optional The speaker label when refering to plural classes, such as speakers, or languages","title":"TaskClass"},{"location":"apiMessage.html#shared-types","text":"","title":"Shared Types"},{"location":"apiMessage.html#audio","text":"Represents an audio. Can either refer to a local file or embed an audio buffer directly. The path and audioSamples fields should be treated as mutually exclusive, with one and only one of these fields implemented For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description path string optional Path to the audio file represented by this record (if not specified then audio is input as a buffer) audioSamples AudioBuffer optional Audio included as a buffer (if not specified, then path must be set) selected_channel uint32 optional Optional - if using multi-channel audio and 'mode' is SELECTED, then this channel is provided to the plugin(s). regions AnnotationRegion repeated Optional annotated regions for this audio label string optional Optional - label used to identify this audio input","title":"Audio"},{"location":"apiMessage.html#annotationregion","text":"A single pair of timestamps (start and end) that make up an annotated region. Timestamps are in seconds. Field Type Label Description start_t uint32 required Begin-time of the region (in seconds) end_t uint32 required End-time of the region (in seconds)","title":"AnnotationRegion"},{"location":"apiMessage.html#audiobuffer","text":"Audio is contained in a buffer (and the path filed is NOT set in Audio) - by default the audio in the buffer should be PCM encoded, unless the buffer contains a serialized file (unencoded) in which case the serialized_file must be set to true. If the data has been decoded and is not PCM encoded data, then the encoding field must be specified For more information regarding what types of audio are currently supported by the OLIVE server, see the Supported Audio Formats page. Field Type Label Description channels uint32 optional The number of channels contained in data, ignored for serialized buffers samples uint32 optional The number of samples (in each channel), ignored for serialized buffers rate uint32 optional The sample rate, ignored for serialized buffers bit_depth AudioBitDepth optional The number of bits in each sample, ignored for serialized buffers data bytes required Should be channels * samples long, striped by channels serialized_file bool optional Optional - true if data contains a serialized buffer encoding AudioEncodingType optional Optional - Not Yet supported - the audio encoding type. Assumed to be PCM if not specified. Ignored for serialized buffer","title":"AudioBuffer"},{"location":"apiMessage.html#audioencodingtype","text":"Audio encoding types Name Number Description PCMU8 1 PCMS8 2 PCM16 3 PCM24 4 PCM32 5 FLOAT32 6 FLOAT64 7 ULAW 8 ALAW 9 IMA_ADPCM 10 MS_ADPCM 11 GSM610 12 G723_24 13 G721_32 14 DWW12 15 DWW16 16 DWW24 17 VORBIS 18 VOX_ADPCM 19 DPCM16 20 DPCM8 21","title":"AudioEncodingType"},{"location":"apiMessage.html#audiobitdepth","text":"Number of bits in each audio sample Name Number Description BIT_DEPTH_8 1 BIT_DEPTH_16 2 BIT_DEPTH_24 3 BIT_DEPTH_32 4","title":"AudioBitDepth"},{"location":"apiMessage.html#metadata","text":"The parent container for Metadata so that typed name/value properties can be transported in a generalized way Field Type Label Description type MetadataType required Indicates the type of this metadata, so it can be deserialized to the appropriate type name string required The name (key) for this metadata value bytes required The value is one of MetadataType, must be deserialized by the client into the type specified by type","title":"Metadata"},{"location":"apiMessage.html#metadatatype","text":"Data types supported in an AudioModificationResult's metadata: Name Number Description STRING_META 1 INTEGER_META 2 DOUBLE_META 3 BOOLEAN_META 4 LIST_META 5","title":"MetadataType"},{"location":"apiMessage.html#booleanmetadata","text":"Value as boolean Field Type Label Description value bool required","title":"BooleanMetadata"},{"location":"apiMessage.html#doublemetadata","text":"Value as a double Field Type Label Description value double required","title":"DoubleMetadata"},{"location":"apiMessage.html#integermetadata","text":"Value as an integer Field Type Label Description value int32 required","title":"IntegerMetadata"},{"location":"apiMessage.html#listmetadata","text":"Value as list of Metadata values Field Type Label Description type MetadataType repeated The type for the corresponding element value bytes repeated The value is one or more MetadataType elements, each element must be deserialized by the client into the type specified by type. For example, for the type, STRING_META, deserialize data as StringMetadata","title":"ListMetadata"},{"location":"apiMessage.html#stringmetadata","text":"Value as a string Field Type Label Description value string required","title":"StringMetadata"},{"location":"apiMessage.html#scalar-value-types","text":".proto Type Notes C++ Type Java Type Python Type double double double float float float float float int32 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint32 instead. int32 int int int64 Uses variable-length encoding. Inefficient for encoding negative numbers \u2013 if your field is likely to have negative values, use sint64 instead. int64 long int/long uint32 Uses variable-length encoding. uint32 int int/long uint64 Uses variable-length encoding. uint64 long int/long sint32 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int32s. int32 int int sint64 Uses variable-length encoding. Signed int value. These more efficiently encode negative numbers than regular int64s. int64 long int/long fixed32 Always four bytes. More efficient than uint32 if values are often greater than 2^28. uint32 int int fixed64 Always eight bytes. More efficient than uint64 if values are often greater than 2^56. uint64 long int/long sfixed32 Always four bytes. int32 int int sfixed64 Always eight bytes. int64 long int/long bool bool boolean boolean string A string must always contain UTF-8 encoded or 7-bit ASCII text. string String str/unicode bytes May contain any arbitrary sequence of bytes. string ByteString str","title":"Scalar Value Types"},{"location":"audioFormats.html","text":"OLIVE Supported Audio Formats Overview There are five main methods of interacting with the OLIVE system that carry different implications with respect to the support audio formats. They are as follows: NIGHTINGALE (Forensic) GUI \u2013 Submitting audio files using the Nightingale GUI, also known as the Forensic GUI. OLIVE (Batch) GUI \u2013 Submitting audio files using the OLIVE Batch GUI, also known as the Batch GUI or SCENIC Batch GUI, for submitting large file-based audio processing jobs. COMMAND LINE INTERFACE (CLI) TOOLS \u2013 submitting audio through the localenroll, localanalyze, localtrain command line tools. OLIVE API (BUFFERED) \u2013 sending pre-loaded memory buffers of audio samples to the server through the OLIVE API. OLIVE API (SERIALIZED) \u2013 sending a serialized object to the server that consists of an entire audio file with its header intact. These interaction methods can be combined into three groups that share limitations: Local file-based processing by server. Dictates compatibility for 2. OLIVE Batch GUI , 3. Command Line Interface Tools , and 5. OLIVE API (Serialized) . Audio files being opened and processed by Java. Dictates compatibility for 1. Nightingale GUI . Audio samples buffered into memory. Dictates compatibility for 4. OLIVE API (Buffered) . The limitations of each group are defined below. Local file-based processing by server The audio file compatibility for this group of OLIVE interactions is dictated by the libsndfile for reading and writing audio files. All files submitted to the OLIVE Batch GUI, through the localenroll, localanalyze, and localtrain command line tools, or as Serialized files through the OLIVE API can be of any audio file format and type supported by the libsndfile package. Stereo files are supported, but are merged into a single channel before scoring when submitting to the OLIVE Batch GUI and Command Line Interface Tools. When submitting files as serialized objects through the API, there is flexibility regarding how the channels are processed \u2013 please refer to the API Documentation for more details. Supported audio formats include: Microsoft WAV SGI/Apple AIFF/AIFC Sun AU/Snd Raw (headerless) Paris Audio File (PAF) Commodore IFF/SVX Sphere/NIST WAV IRCAM SF Creative VOC SoundForge W64 GNU Octave MAT4.4 Portable Voice Format Fasttracker 2 XI HMM Tool Kit HTK Apple CAF Sound Designer II SD2 Free Lossless Audio Codec (FLAC) Supported encodings vary by the format used (see the link below for a comprehensive compatibility table), but samples of several supported encodings are as follows: Unsigned and signed 8, 16, 24 and 32 bit PCM IEEE 32 and 64 floating point U-LAW A-LAW IMA ADPCM MS ADPCM GSM 6.10 G721/723 ADPCM 12/16/24 bit DWVW OK Dialogic ADPCM 8/16 DPCM More information on libsndfile supported audio formats can be found here: http://www.mega-nerd.com/libsndfile/#Features Audio files being opened and processed by Java Any files being opened in the Nightingale GUI for close analysis work must be able to be opened and read by the underlying Java code libraries: Java Media Framework JavaX.Sound FLAC Only mono files are currently supported. Supported sample rates include: 8 kHz multiples to 48 kHz 11.025 kHz multiples to 44.1 kHz Supported container formats: FLAC RIFF (.wav) AIFF AIFC AU Supported encoding formats: Compressed: FLAC PCM: 16 bit signed int, big or little endian 8 bit signed or unsigned int 32 bit float, little endian only (i.e. RIFF, not RIFX) 8 bit mulaw or alaw ADPCM: Microsoft or IMA ADPCM Audio samples buffered into memory Raw buffered audio samples being sent to the OLIVE server for enrollment or scoring are read and processed under the assumption that they are raw 16-bit Linear PCM sampes at an 8 kHz sampling rate. Serialized buffered audio files sent to the server are not processed by the client or assumed to be anything specific; rather they are interpreted by the server as a complete (header-intact!) audio file.","title":"Supported Audio Formats"},{"location":"audioFormats.html#olive-supported-audio-formats","text":"","title":"OLIVE Supported Audio Formats"},{"location":"audioFormats.html#overview","text":"There are five main methods of interacting with the OLIVE system that carry different implications with respect to the support audio formats. They are as follows: NIGHTINGALE (Forensic) GUI \u2013 Submitting audio files using the Nightingale GUI, also known as the Forensic GUI. OLIVE (Batch) GUI \u2013 Submitting audio files using the OLIVE Batch GUI, also known as the Batch GUI or SCENIC Batch GUI, for submitting large file-based audio processing jobs. COMMAND LINE INTERFACE (CLI) TOOLS \u2013 submitting audio through the localenroll, localanalyze, localtrain command line tools. OLIVE API (BUFFERED) \u2013 sending pre-loaded memory buffers of audio samples to the server through the OLIVE API. OLIVE API (SERIALIZED) \u2013 sending a serialized object to the server that consists of an entire audio file with its header intact. These interaction methods can be combined into three groups that share limitations: Local file-based processing by server. Dictates compatibility for 2. OLIVE Batch GUI , 3. Command Line Interface Tools , and 5. OLIVE API (Serialized) . Audio files being opened and processed by Java. Dictates compatibility for 1. Nightingale GUI . Audio samples buffered into memory. Dictates compatibility for 4. OLIVE API (Buffered) . The limitations of each group are defined below.","title":"Overview"},{"location":"audioFormats.html#local-file-based-processing-by-server","text":"The audio file compatibility for this group of OLIVE interactions is dictated by the libsndfile for reading and writing audio files. All files submitted to the OLIVE Batch GUI, through the localenroll, localanalyze, and localtrain command line tools, or as Serialized files through the OLIVE API can be of any audio file format and type supported by the libsndfile package. Stereo files are supported, but are merged into a single channel before scoring when submitting to the OLIVE Batch GUI and Command Line Interface Tools. When submitting files as serialized objects through the API, there is flexibility regarding how the channels are processed \u2013 please refer to the API Documentation for more details. Supported audio formats include: Microsoft WAV SGI/Apple AIFF/AIFC Sun AU/Snd Raw (headerless) Paris Audio File (PAF) Commodore IFF/SVX Sphere/NIST WAV IRCAM SF Creative VOC SoundForge W64 GNU Octave MAT4.4 Portable Voice Format Fasttracker 2 XI HMM Tool Kit HTK Apple CAF Sound Designer II SD2 Free Lossless Audio Codec (FLAC) Supported encodings vary by the format used (see the link below for a comprehensive compatibility table), but samples of several supported encodings are as follows: Unsigned and signed 8, 16, 24 and 32 bit PCM IEEE 32 and 64 floating point U-LAW A-LAW IMA ADPCM MS ADPCM GSM 6.10 G721/723 ADPCM 12/16/24 bit DWVW OK Dialogic ADPCM 8/16 DPCM More information on libsndfile supported audio formats can be found here: http://www.mega-nerd.com/libsndfile/#Features","title":"Local file-based processing by server"},{"location":"audioFormats.html#audio-files-being-opened-and-processed-by-java","text":"Any files being opened in the Nightingale GUI for close analysis work must be able to be opened and read by the underlying Java code libraries: Java Media Framework JavaX.Sound FLAC Only mono files are currently supported. Supported sample rates include: 8 kHz multiples to 48 kHz 11.025 kHz multiples to 44.1 kHz Supported container formats: FLAC RIFF (.wav) AIFF AIFC AU Supported encoding formats: Compressed: FLAC PCM: 16 bit signed int, big or little endian 8 bit signed or unsigned int 32 bit float, little endian only (i.e. RIFF, not RIFX) 8 bit mulaw or alaw ADPCM: Microsoft or IMA ADPCM","title":"Audio files being opened and processed by Java"},{"location":"audioFormats.html#audio-samples-buffered-into-memory","text":"Raw buffered audio samples being sent to the OLIVE server for enrollment or scoring are read and processed under the assumption that they are raw 16-bit Linear PCM sampes at an 8 kHz sampling rate. Serialized buffered audio files sent to the server are not processed by the client or assumed to be anything specific; rather they are interpreted by the server as a complete (header-intact!) audio file.","title":"Audio samples buffered into memory"},{"location":"cli.html","text":"OLIVE Command Line Interface Guide (Legacy) Disclaimer Note that the tools described below are legacy tools that are mostly used for internal testing and development. With docker-based deliveries, these utilities are difficult to access and have many performance tradeoffs versus using the OLIVE server through a client - they should not be used for integration, only for very basic experimentation. The functionality offered by these should instead be accessed through the provided Java example client (OliveAnalyze, OliveEnroll, etc.) or Python example client (olivepyanalyze, olivepyenroll, etc.). Documentation for these utilities is under construction and will be provided soon - but each utility has a help statement that provides instructions for running each. Introduction This document describes running the OLIVE (formerly SCENIC) system from a command line. Our command line applications are geared toward a variety of specialized users such as researchers, system evaluators (i.e. Leidos for the DARPA RATS program), and testers. Casual users should consider using our graphical application. However, our command line applications can function as general-purpose tools, but may require specially formatted files such the RATS XML files for audio analysis and LDC-format TSV files for training annotations. 1: Overview OLIVE command line interface (CLI) tools include: localenroll \u2013 Used to enroll \u2018targets\u2019 into the system, such as a target speaker for speaker identification (SID), a topic of interest for topic identification (TID), or a keyword or phrase of interest for query-by-example keyword spotting (QBE). localanalyze \u2013 Used to query the OLIVE server to score audio to find speech with a speech activity detection (SAD) plugin, report scores for potential speakers or languages of interest for SID or language identification (LID) plugins, report likelihood and location(s) of conversation topics or keywords of interest (TID, QBE, KWS). localtrain \u2013 Used to train or adapt plugins that support the LearningTrait (SupervisedAdapter, SupervisedTrainer, or UnsupervisedAdapter) with examples of new audio conditions to improve performance in such conditions. Also used to add new language recognition capabilities to a LID plugin, and to retrain the background models of a SID plugin to prepare it for new audio conditions. Training and adaptation are not available in all plugins, please refer to individual plugin documentation or plugin capabilities matrix to verify availability of training or adaptation. 2: Command Line Testing and Analysis A. Enrollment with localenroll The localenroll command is used to enroll audio for SID and TID. It can be invoked from a BASH or C-shell terminal. It takes a simply formatted text file as input and does not produce an output file. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TID, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in the Plugin Appendix. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7 The basic syntax for calling localenroll (more details and options below) is: $ ./localenroll <path_to_plugin_domain> <path_to_enrollment_file> Where an example of that may be: $ ./localenroll plugins/sid-embed-v1/domains/multi-v1/ /data/sid/smoke_enroll.lst The numerous options available in localenroll can be seen by executing localenroll --help, the output of which is shown below: usage : localenroll [- h ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path enrollment_file positional arguments : plugin_domain_path path to plugin domain used for analysis enrollment_file List of enrollments of the form < audio_path > < class_id > OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have a configuration section named enrollment . Only values from the enrollment section are read -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting B. Scoring and processing with localanalyze I. Invoking localanalyze The localanalyze utility is used to perform OLIVE scoring and analysis with most plugins (SAD, SID, SDD, LID, KWS, QBE, GID, TID), or processing with an ENH plugin, all on list-based input files. It can be invoked from a BASH or C-shell terminal. A path to a valid OLIVE plugin and domain as well as an audio paths input file are required for all tasks. For some plugins, like LID and SID, an optional IDs input file can be specified via the --class_ids argument to limit which languages or speakers are scored. This IDs input file is also how a keyword spotting plugin is informed what the keywords of interest are for a given analysis. The exact details for invoking localanalyze will depend upon the plugin technology being used, and may vary slightly depending upon the options available to each individual plugin, but the general format for running this utility is: $ localanalyze <path_to_plugin_domain> <list_of_files_to_analyze_or_process> With an example (SID): $ localanalyze plugins/sid-embed-v1/domains/multi-v1/ /data/sid/test_data.lst The format of the audio input file is simply a list of one or more newline-separated lines containing a path to an audio file: <audio_path> Example audio input file: /data/sid/test/unknownSpkr1.wav /data/sid/test/unknownSpkr27.wav As mentioned above, if you would only like to score a subset of the enrolled speakers or languages, you can optionally pass a list of these identifiers as a newline-separated list text file, with the --class_ids command line argument. This same argument is how you select keywords to search for when running localanalyze with a keyword spotting plugin (see KWS section in the Plugin Appendix for more information). IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Speaker Identification IDs example: Chris Billy Spkr3 A Keyword Spotting IDs example: turn left torpedo watermelon Example (KWS) of a localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/test-audio.lst Note that re-running localanalyze will overwrite the contents of the output.txt file or OUTPUT directory, depending on what type of plugin is being run. The OLIVE usage/help statement for localanalyze : usage : localanalyze [- h ] [-- output OUTPUT_PATH ] [-- thresholds THRESHOLDS ] [-- class_ids ID_LIST_PATH ] [-- options OPTIONS_PATH ] [-- regions REGION_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path audio_paths_file positional arguments : plugin_domain_path path to plugin domain used for analysis audio_paths_file List of audio files to analyze OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- output OUTPUT_PATH , - o OUTPUT_PATH path to output file or directory -- thresholds THRESHOLDS Optional comma - separated threshold values to apply to frame - level scores , e . g . 0.0 , 1.5 . Use syntax '-- thresholds=' for negative values , e . g -- thresholds =- 2.0 ,- 1.0 -- class_ids ID_LIST_PATH , - i ID_LIST_PATH Optional file that specifies class ids to be scored . E . g . limit the speakers that scored . -- options OPTIONS_PATH Optional file containing plugin specific name / value pairs . The option file may have more or more section headings s for each plugin type . Common section names are 'frame scoring' , , 'global scoring' or 'region scoring' -- regions REGION_PATH , - r REGION_PATH Optional flag indicating that the audio paths file should be supplemented with regions from a PEM formated file , it is up to the plugin to utilize these regions to supplement its scoring . This flag is ignored if the audio input list ( audio_paths_file ) is a PEM formatted file . -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting II. Output Plugin Scoring Types In general, the output format and location of a call to localanalyze will depend on the type of \u2018scorer\u2019 the plugin being used is. There are currently four types of plugins in OLIVE: Global scorer Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Currently SID, LID, and GID are the only global scoring plugins. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Example \u2013 sid-embed-v1, lid-embed-v1 Region scorer Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. TID, SDD, QBE, and KWS are all region scorers. Example \u2013 sdd-embed-v1, qbe-tdnn-v4, kws-batch-v9 Frame scorer A frame scoring plugin provides a score for every \u2018frame\u2019 of audio within every test file passed to localanalyze . This allows OLIVE to find distinct regions of speech with high precision in recordings with noise and/or silence. SAD is a frame scoring plugin. It is also possible to apply a threshold to a frame scoring plugin at run-time to report regions of detection instead of frame scores. For a plugin like SAD, this allows OLIVE to provide output in the form of speech regions. A frame is a short segment of audio that typically consists of 10 milliseconds of audio (100 frames per second). Example \u2013 sad-dnn-v4 Audio to audio This plugin takes an audio file as input, and also returns an audio file as output. Currently the only plugins that fall into this category are speech/audio enhancement plugins, where the goal is removing noise and distortion from an audio file to improve the human listening experience and intelligibility. Example \u2013 enh-mmse-v1 Global Scorer Output In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> The name and location of the output file can be overridden by passing it as the argument to the -o or --output argument when calling localanalyze . To see specific examples for each plugin type, please refer to the appropriate section of the Plugin Appendix. Region Scorer Output Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: < audio_path > < region_start_timestamp > < region_end_timestamp > < class_id > < score > Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. Specific examples can be found in the Plugin Appendix at the end of this document. Frame Scorer Output In the case of frame scorers like SAD, an output file is generated for each audio input file, where each audio output file contains a score for each frame in the audio input. There is one frame score per line. Alternatively, an option exists to produce segmentation scores from SAD results by using the --threshold argument. When using the --threshold argument, the output file adheres to standard 5-column PEM format. Without supplying a threshold to localanalyze , the frame scorer output looks like this: <frame_1_score> <frame_2_score> \u2026 <frame_N_score> When a threshold is provided, the output file will resemble the following: < filename > , < channel > , < label ( \u201c speech \u201d ) > , < speech region start time ( seconds ) > , < end time ( seconds ) > Audio to Audio Output An audio-to-audio plugin takes an audio file as input and returns a corresponding audio file as output. Currently, this plugin type is used to supply enhancement capabilities to OLIVE, to allow OLIVE to improve the quality, intelligibility, or just general human listening experience for an audio file. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav 3: Command Line Field Adaptation A. Command Line Field Adaptation Overview In general, training and adaptation are very resource and time intensive operations. Very large amounts of RAM are used at certain steps in training. When attempting to train or adapt, the machine should be dedicated to that operation. If the plugin path contains a domain then adaptation is implied, otherwise training is implied. The high-level difference between training and adaptation is that adaptation will use the new data supplied during adaptation in addition to the data already used to train the model used by the plugin/domain. Training, on the other hand, ignores the data originally used for training a model and retrains from scratch using only the new data provided. When performing training, none of the data in the base plugin will be used, but the feature configs will. Check the plugin\u2019s traits to determine if full training and/or adaptation are supported. B. Invoking localtrain Not to be confused with enrollment, the localtrain command line application is used to perform field adaptations for SAD, LID & SID. localtrain takes a plugin or plugin_domain path, and one or more data input files formatted for: Unsupervised data - a newline separated list of audio file paths Supervised data with file level annotations - a newline separated list of audio files paths with a class Id (i.e. \u201caudio_file1.flac fas\\n\u201d) Supervised data with region level annotations - a newline separated list of audio file paths, start time (seconds), end time (seconds), and class ID (i.e. \u201caudio_file1.flac 1.25 3.5 fas\\n\u201d) If multiple data files are specified then they must all use the same annotation format. The localtrain utility outputs a new domain in the plugin path. The details of the localtrain executable are below: usage : localtrain [- h ] -- domain - id DOMAIN_ID [-- overwrite ] [-- preprocess ] [-- finalize ] [-- unique ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_or_domain_path data [ data ...] Train or adapt OLIVE audio recognition systems positional arguments : plugin_or_domain_path Path to the plugin or domain . A plugin path implies full training . A domain path implies adaptation of the specified domain . data paths to data files for training / adapation . The files can have one of three forms . 1 : < audio_path >\\ n 2 : < audio_path > < class_id >\\ n 3 : < audio_path > < class_id > < start > < end > \\ n . The first form has no annotations and implies unsupervised . The second form provides for file - level annotations while the third form supports region - level annotations . Start and end times should be in seconds . If multiple files are specified , they must have the same form . optional arguments : - h , -- help show this help message and exit -- domain - id DOMAIN_ID The id of the new domain you 're creating through training or adaptation. Should be a string that is somewhat descriptive of the conditions --overwrite Forcefully overwite an existing domain --preprocess Pre-process audio only, do not finalize training/adaptation --finalize Pre-process audio only, do not finalize training/adaptation --unique gurantees log files are written to unique directoires/files. Helpful when running in SGE mode --options OPTIONS_PATH Optional file containing plugin specific name/value pairs. The option file must have one or more sections for each plugin type. Common section names are ' supervised trainer ', ,' supervised adapter ', ' unsupervised trainer ' or ' unsupervised adapter ' --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- version show program ' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting In order to use an adapted system plugin, simply pass the full path of the domain generated by localtrain to localenroll or localanalyze as the plugin_domain_path argument. For training, do not include the domain in the plugin path. When running on a SGE, you may split the audio processing from the finalization step by using the --preprocess flag to first pre-process audio files, then use invoke localtrain with the --finalize argument to finalize training. Guidelines for the minimum amount of audio data required to successfully execute localtrain are listed in the table below. Task Operation Speech Duration SAD Adapt to new channel 1h LID Adapt to new channel 20m LID Train a new language 3h SID Adapt to new channel 1h i. Examples SAD Adaptation Example: $ localtrain ./plugins/sad-dnn-v1/domains/ptt-v1/ adaptation-data.lst Where each line of adaptation-data.lst has the following format: /path/to/audio.wav label C. LID Training/Adaptation When training new channel conditions, it is recommended to train all supported languages in the LID model to produce the best results. The out of set language is labeled as \u2018xxx\u2019. Use this language ID when training to add languages that you do not want to target in the LID task but are known to be in the test dataset. 4: Log Files a. OLIVE Command Line Logging When executing localtrain , localenroll , and localanalyze , here are three named log files that may be of interest should something go awry. The top-level log file: This log file corresponds to the -l option to the localtrain , localenroll , and localanalyze utilities. By default, it is named the same as the utility being used with \u201c.log\u201d appended (i.e. localanalyze.log when running localanalyze ) and will be written to the directory from which you executed the utility. The pool executor log file: This file will be written to [work_directory]/logs/pool_executor.log, where work_directory corresponds to the -w option to localtrain / localenroll / localanalyze and defaults to your current directory/WORK. The pool executor log file is the best log file to look at if unexpected errors occur. It corresponds to our internal job scheduler also known as the pool executor. The pool monitor log file: This file will be written to [work_directory]/logs/pool_monitor.log, where work_directory corresponds to the -w option to the localtrain , localenroll , localanalyze utilities and defaults to your current directory/WORK/. This log contains stats about memory and CPU utilization. All three of these log files will exhibit log rotation behavior. In the event of errors, [work_directory]/logs may also contain log files named [order_id].failed , where order_id generally corresponds to the file names of the audio files being used for adaptation/training, enrollment, or analysis. The id can be used to tie errors in the pool executor log file to the \u201c.failed\u201d log files. If you run the OLIVE CLI utilities in debug mode (-d), all log files will be maintained, even if they were successful. b. Rotating Log Files OLIVE employs rotating log files in many places. In this context, rotating refers to a log file that is rewritten each time the application is run. The old log file, if any, is renamed with an integer suffix denoting how many invocations in the past it corresponds to. For instance, if you run localanalyze and don\u2019t specify a -l option, you\u2019ll get the default localanalyze.log file. If localanalyze.log already exists, it is moved to localanalyze.log.1 . The system will keep the 10 most recent log files. A file named localanalyze.log.8 means that the file corresponds to eight invocations ago. 5: Plugin Appendix Plugin Types and Acronyms Currently, OLIVE supports the plugin technologies listed in the following list. For operating instructions that apply to only a specific technology, refer to that section within this appendix. SAD \u2013 Speech activity detection. SID \u2013 Speaker identification. LID \u2013 Language identification. KWS \u2013 Keyword spotting. QBE \u2013 Query by example based keyword spotting. TID \u2013 Topic identification. SDD \u2013 Speaker diarization and detection. GID \u2013 Gender identification. ENH \u2013 Speech and audio enhancement. Speech Activity Detection (SAD) SAD plugins are frame scorers that take an audio list file and annotate the presence and location of speech in each audio file in that list. In standard operation, SAD plugins produce a single output file for each input file, by default in a directory called OUTPUT in the location localanalyze was called from. Output files carry the name of the original input file, but with a new extension \u201c.scores\u201d \u2013 for example, audioFile1.wav will become audioFile1.wav.scores, saved inside OUTPUT/. The format of these results files is a newline separated list of numerical values representing the likelihood that each 10ms frame of the audio file contains speech. Typically, a score above 0 represents speech detection, and a score below 0 represents no speech. SAD analysis example: $ localanalyze /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst Output files: OUTPUT/audioFile1.wav.scores OUTPUT/audioFile2.wav.scores Example audioFile1.wav.scores contents: -0.22624 -0.10081 0.00925 0.12365 Alternatively, SAD plugins can be run with the --thresholds flag to have localanalyze automatically convert the frame scores to regions of speech, by applying the provided threshold. SAD analysis example using thresholds: $ localanalyze --thresholds = 0 .0 /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst This will provide a single output file in OUTPUT directory corresponding to the provided threshold: 0.0.pem. If more than one threshold is provided, there will be a PEM file placed into OUTPUT corresponding to each provided threshold. Example PEM output: /data/sad/test/audioFile1.wav 1 speech 63.110 66.060 /data/sad/test/audioFile1.wav 1 speech 66.510 69.230 /data/sad/test/audioFile1.wav 1 speech 93.480 96.090 /data/sad/test/audioFile1.wav 1 speech 96.570 100.760 Note that if negative thresholds are to be used, it is very important to specify the thresholds using an \u2018=\u2019 character. For example, this threshold specification is valid: --thresholds=-2.0,4.0 And this is not valid: --thresholds -2.0,4.0 If only thresholds of 0 or above are going to be used, it is acceptable to omit the equals sign. Speaker Identification (SID) SID plugins are global scorers that take an audio list file and return a score for each enrolled speaker model scored against the audio in each input audio file. Generally, a score above 0 for an enrolled speaker model represents that speaker being detected in the respective audio file. In order to perform analysis on a file with a SID plugin you must first enroll one or more target speakers. The enrollment list file for a SID plugin follows this format for each line: <audio_file_path> <speaker_id> An example enroll.lst: /data/spkr_example_audio_5760.wav UIM1 /data/spkr_example_audio_5761.wav UIM1 /data/spkr_example_audio_5762.wav John /data/spkr_example_audio_5763.wav John Enrolling these speakers with localenroll : $ localenroll /path/to/plugins/sid-embed-v2/domains/multi-v1/ enroll.lst Example localanalyze call: $ localanalyze ./plugins/sid-embed-v2/domains/multi-v1/ ./data/sid/test/testAudio.lst By default, the output of this call is written to output.txt in the directory the command was run. The format of output.txt contains one line for each enrolled speaker model, for each input audio file, and the corresponding score: <audio_file_path> <speaker_id> <score> Example output.txt: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Trial-based Calibration Speaker Identification (SID TBC) Trial-based Calibration SID plugins are identified by \u2018tbc\u2019 in the plugin name. They are used very similarly to a basic SID plugin, using localenroll and localanalyze just like the examples shown in the previous example. The benefit of TBC plugins is that they allow OLIVE to perform calibration at test-time, based on the actual data conditions being encountered, rather than being forced to use a single, global calibration model that has been trained a priori. The basics of TBC enrollment and testing follow the previous SID examples, but the additional options and outputs available to TBC are detailed below. The standard approach to calibration uses a \u201cone size fits all\u201d calibration model based on the developer\u2019s best understanding of potential operating conditions. This is problematic when the user either doesn\u2019t know ahead of time what likely conditions are, or when operating conditions may vary widely. Trial-based calibration was developed as a means of providing calibration that is responsive to the particular conditions of a trial, and adapts its calibration model based on the conditions encountered. There are two ways we have developed to do this. The first draws from a pool of available data (either provided by the developer or augmented with user-provided data) and uses measures of the conditions found within this data and the trial conditions to build an ideal calibration set on the fly. This is useful in that this approach can also determine when a trial CANNOT be calibrated, and to measure the success of calibration when it is possible. The clear downside of this approach is that it is quite slow. A second approach to TBC is to use a model that has used a DNN to learn to predict both calibration parameters and confidence from large sets of trials and available calibration data. This approach is very fast (about 5000 times faster than the first approach) but has the downside that expanding the calibration set by the user\u2019s data isn\u2019t possible. This plug-in provides both approaches in the TBC plug-in, as two separate domains. In addition to the output score file detailed in the SID section, TBC plugins have additional possible outputs. Speech Detection Output Segmentation files are used to label time regions in the speech signal. We use this format for voice activity detection (VAD). If an output_ivs_dump_path is provided as an option to localenroll or localanalyze , the system will produce this file in a folder corresponding to the wav_id for all registered waveforms. The format is the following: md5sum start end ( in seconds ) Example: b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.060 0.060 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.090 0.090 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.110 0.170 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.200 0.200 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.560 3.550 Persistent I-Vectors For both localenroll and localanalyze , if the output_ivs_dump_path is defined via an options file with the --options flag, an i-vector from each audio file is saved in this directory for re-use. This reduces unnecessary computation when there exists overlap between lists of wave files to be processed. For instance, if the enroll and test wave file lists are identical (i.e., the case of an exhaustive comparison of a set of files), i-vector persistence will reduce overall computation by almost a factor of 2 since i-vector extraction consumes more of the computation required for an evaluation. I-vectors will be saved in a sub-directory of output_ivs_dump_path based on the base name of the wave file. In addition to this optional feature, the enrollment vectors are loaded in memory prior to verification and if the md5sum of a test audio file matches one used in the enrollment process, the corresponding vector will be used instead of re-processing the audio. This is because vector extraction is identical between enrollment and verification. Trial-based Calibration Trial-based calibration (TBC) does not change the way calibration works but changes the way calibration is used. It relaxes the constraint on the system developers to train a calibration model that is ideally matched to the end use conditions. Rather than train a calibration model a priori, the system postpones this training until the conditions of the particular verification trial are known to the system; a trial consists of comparing test audio to an enrolled speaker model. The goal of trial-based calibration is to use information about the trial to generate an ideal calibration set for the trial conditions using the reservoir of possible calibration audio files available. Using this set, a calibration model tailored to the conditions of the trial can be trained and used to effectively calibrate the verification score. The TBC operation and output differs from traditional SID plugins; it may choose to reject a trial and NOT output a score if insufficient data is available for calibrating for those conditions. For instance, the output may look similar to the following: waves/T6_ACK2.sph T6 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00) waves/T6_ACK3.sph T1 0.0 -inf Unable to calibrate with only 12 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T2 0.0 -inf Unable to calibrate with only 3 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T3 0.0 -inf Unable to calibrate with only 0 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T4 0.0 -inf Unable to calibrate with only 2 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T6 0.0 -inf Unable to calibrate with only 9 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T4_ACK3.sph T5 0.96153318882 4.04821968079 Used 95 target trials in calibration waves/T4_RC.sph T4 3.46785068512 4.07499170303 Used 95 target trials in calibration waves/T5_Tip2.sph T5 8.90770149231 4.07352733612 Used 98 target trials in calibration waves/T5_RC.sph T5 10.2386112213 4.03855705261 Used 47 target trials in calibration waves/T4_Tip2.sph T4 10.8663234711 4.07404613495 Used 218 target trials in calibration waves/T4_Tip1.sph T4 11.793006897 3.98730397224 Used 164 target trials in calibration waves/T4_ACK2.sph T4 11.8091144562 3.90610170364 Used 119 target trials in calibration waves/T4_ACK1.sph T4 12.2115001678 4.16342687607 Used 208 target trials in calibration waves/T5_ACK1.sph T5 13.8099250793 3.99625587463 Used 99 target trials in calibration waves/T5_Tip1.sph T5 14.9411458969 3.96994686127 Used 83 target trials in calibration waves/T4_ACK3.sph T4 16.003446579 4.05554199219 Used 146 target trials in calibration The output follows the structure: <testwave> <modelid> <score> <confidence> <info> In the instance of insufficient calibration segments being located, a score of 0.0 and a calibration confidence of -inf is given. In contrast, when sufficient data is found for calibration, the number of segments used in calibration is reported. There exist two options for applying calibration with the current plugin: DNN-assisted TBC, normal TBC, or global calibration. Each of these options use duration information to reduce the impact of duration variation impacting calibration performance. Note that changing calibration domains does NOT require re-enrollment of models as these are done in a domain-independent way for any TBC-enabled plugin. DNN-assisted Trial-based Calibration DNN-assisted TBC is invoked by passing the tbcdnn-v1 domain to localanalyze . This is a very fast and newly pioneered effort by SRI to reduce the computation needed to apply dynamic calibration methods to speaker recognition and operates with very low overhead compared to global calibration, and yet significantly benefits calibration performance in varying conditions or conditions that differ from the development conditions. localanalyze ... <plugin>/domains/tbcdnn-v1 test.lst Normal Trial-based Calibration TBC is applied by default with the 'sid-embedDnnTbc-v1' plugin. The data within the domain (such as 'tbc-v1') is used as candidate calibration data. localanalyze ... <plugin>/domains/tbc-v1 test.lst TBC is applied to verification scores on a trial-by-trial basis. As such, verification using TBC will operate at a speed much slower than global or DNN-assisted TBC depending on the size and make-up of the TBC data. This should be considered when using TBC in a cluster environment where it is the number of trials (model vs test comparisons) that determine the running time instead of the number of test files. Global Calibration Each domain can be used to invoke global calibration. This is particularly useful for user-defined data as it provides a rapid means of improving calibration performance without a dramatic increase in computation time. In this case, verification will operate at a much faster pace since TBC is essentially disabled and the global calibration model parameters are applied to all scores. In order to invoke global calibration, and optional parameter must be passed to localanalyze via an options file: echo \"[global scoring] global_calibration = True \" > options.lst localanalyze -- options options . lst ... < plugin >/ domains / tbc - v1 test . lst Optional Parameters The TBC-based plugins offer several tunable parameters via the options parameter to localenroll or localanalyze . These can be passed to the enrollment phase or verification phase by preceding the options in an ascii text file as such: $ cat options . lst [ enrollment ] ... enrollment options per line ... [ global scoring ] ... verification options per line ... The optional parameters and their purpose are provided below. tbc_confidence_threshold = 3 . 0 , # Similarity threshold for processing a trial with TBC score_threshold = 0 . 0 , # Score offset subtracted from output LLRs to assist in making 0 threshold output tgt_max = 300 , # The maxmimum number of target trials used for TBC of a trial imp_max = 3000 , # The maxmimum number of impostor trials used for TBC of a trial tgt_imp_min = 20 , # The mimum number of relevant target and impostor calibration trials needed to use TBC ( rejected otherwise ) global_calibration = False , # Apply global calibration instead of TBC ivs_dump_path = None , # Output path for dumping vectors and meta information sad_threshold = 0 . 5 , # Threshold for speech activity detection ( higher results in less speech ) sad_filter = 1 , # Smoothing of LLRs from SAD DNN prior to thresholding sad_interpolate = 1 , # If > 1 , a speed up of SAD by interpolating values between frames ( 4 works well ) Utilizing these parameters in an options file may look like this: echo \"[enrollment] sad_threshold = 1.0 ivs_dump_path = . / embeddings [ global scoring ] sad_threshold = 1.0 ivs_dump_path = . / embeddings tgt_max = 100 tgt_imp_min = 50 \" > options.lst localenroll -- options options . lst ... < plugin >/ domains /< domain > enroll . lst localanalyze -- options options . lst ... < plugin >/ domains /< domain > test . lst Verification Trial Output The format for the verification trial is the following. Note that for global calibration, the optional parameters (calibration_confidence and calibration_remarks) are not output. Output format: wav_id speaker_id score [ calibration_confidence calibration_remarks ] Here is an example of score executed with Global Calibration: waves/T1_ACK1.sph T6 5.19274568558 waves/T1_ACK1.sph T4 1.204241395 waves/T1_ACK1.sph T5 1.69025540352 Here is an example of scores executed with DNN-assisted TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used DNN-assisted TBC with confidence 5.751 waves/T1_ACK1.sph T4 1.204241395 5.12 Used DNN-assisted TBC with confidence 3.122 waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with confidence above threshold (3.00) Here is an example of scores executed with normal TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used 67 target trials in calibration waves/T1_ACK1.sph T4 1.204241395 5.12 Used 73 target trials in calibration waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00) Speaker Diarization and Detection (SDD) The overall goal of SDD plugins is to detect regions of speech within an audio recording that are associated with different speakers, and then identify those speakers if possible. SDD plugins have three different modes of operation, as outlined below. Changing the execution mode for SDD is done by passing an options file to localanalyze as an argument to the --options flag. The main behavior and premise of the technology and plugin remain the same, but each mode changes the format and information contained in the output file. Running the SDD plugin is very similar to running SID plugins, with the same syntax for enrollment and testing. Currently, training or adaptation through localtrain is not supported, but enrolling new speakers and testing against enrolled speakers is as simple as: $ localenroll /path/to/plugins/sdd-embed-v1/domains/multi-v1/ enrollmentAudio.lst $ localanalyze /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst The enrollment and test audio file lists in this example follow the same format as the lists used by SID plugins, described above. By default, if run as above with no options, the plugin will run in Speaker Detection mode, and provide the output described above. In order to run in SID or SID Exhaustive mode, you will need to provide an options file to specify that behavior: $ localanalyze --options options.lst /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst Where options.lst is a text file with contents similar to: [region scoring] mode: SID_EXHAUSTIVE sad_threshold: 0.0 diarization_max_num_speakers: 2 The [region scoring] header is alerting the plugin that the options are being passed for scoring, and all of the parameters shown above (sad_threshold, diarization_max_num_speakers, mode) are optional parameters. The mode option controls the output behavior as described above, and the possible options are SID, SID_EXHAUSTIVE, and SPEAKER_DETECTION, and described directly below. The sad_threshold defaults to 2.0, and is used to fine tune the threshold for the internal speech activity detection plugin if necessary. The parameter diarization_max_num_speakers defaults to 4, and is the largest number of speakers the plugin will attempt to consider when clustering segments within the file. SDD Execution Modes To facilitate the understanding of each mode\u2019s output, consider that speech from an audio file is made up of clusters of speakers, and each cluster will have one or more contiguous segment of speech. SPEAKER_DETECTION The goal of Speaker Detection is to show the most probable speaker model for each segment of the input audio file. As output, this mode gives one line per segment within the file, along with the top scoring enrolled model for the cluster that segment belongs to, and that cluster's score for the given model. Note that many scores will be repeated in the output file, since each segment in the cluster shares the same score for a given speaker model. This mode is performed by default if no options file with a mode override is given. SID This mode is meant for triaging large amounts of audio files when the main goal is just finding which of these files may contain speech from one of the enrolled speakers. The output is the maximum score for each enrolled speaker within the audio file after scoring against each cluster in the file, as well as the timestamps for the beginning and end of the longest segment within the cluster that scored the highest for that model. This gives a specific segment to spot check and evaluate the plugin's decision if needed. SID_EXHAUSTIVE When using SID Exhaustive, each diarized cluster is scored against each enrolled model. The output is a complete listing for every speech segment of the input audio file, the score from testing every enrolled model against the cluster that the segment belongs to. Many scores will be repeated in the output file, since each segment in the cluster shares the same score. In this example, Chris and Jimmy are the only enrolled models, and 5 total segments were identified within the file. Language Identification (LID) LID plugins are global scorers that act very similar to SID with respect to scoring, except that each score corresponds to a language model rather than a speaker model. In most cases, LID plugins will be delivered from SRI with a set of languages already enrolled. Languages can be added to some plugins by the user if enough appropriate data is available, through the localtrain CLI call. Details on this will be added to this document in a later revision. Example localanalyze call: $ localanalyze /path/to/plugins/lid-embed-v2/domains/multi-v1/ testAudio.lst Output format: <audio_file_path> <language_id> <score> Output example: /data/lid/audio/file1.wav fre -0.5348 /data/lid/audio/file1.wav eng 3.2122 /data/lid/audio/file1.wav spa -5.5340 /data/lid/audio/file1.wav rus 0.5333 /data/lid/audio/file1.wav ara -4.9444 /data/lid/audio/file2.wav fre -2.6564 Keyword Spotting (KWS) KWS is an automatic speech recognition (ASR) based approach to detecting spoken keywords in audio. Rather than enrolling target keywords from audio, as you would with query-by-example, telling the plugin what keywords to search for is done by passing an IDs file to localanalyze . The format of the IDs file is: IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Keyword Spotting IDs example, search_list.lst: remote torpedo voice recognition Example KWS localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst /path/to/plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/testAudio.lst The output format for KWS plugins is identical to that of QBE. It is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/kws/testFile1.wav 7.170 7.570 remote 1.0 /data/kws/testFile1.wav 10.390 10.930 remote 0.693357 /data/kws/testFile1.wav 1.639 2.549 voice recognition 1.0 Automatic Speech Recognition (ASR) Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. ASR plugins do not require any enrollment of words (like QBE) or specification of words of interest (like traditional KWS), but instead rely on the vocabulary model built into the domain to define the list of available words. All that is necessary for scoring an audio file for ASR is a list of input files to be scored, which follows the format below. Generic input audio list format: <audioFile_1> <audioFile_2 (opt)> ... <audioFile_N (opt)> A specific example of this, called testAudio.lst, might look like: /data/asr/testFile1.wav /data/asr/testFile2.wav /data/asr/testFile3.wav Note that if the files are not contained within the directory that localanalyze is being run from, or if a relative path from that location is not provided, the full file path to each file is necessary. An example ASR localanalyze call: $ localanalyze /home/user/oliveAppData/plugins/asr-dynapy-v1/domains/eng-tdnnChain-tel-v1/ /data/asr/test/testAudio.lst The output format for KWS plugins is identical to that of QBE and other region-scoring OLIVE plugins. ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. Output is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <word> <score> An example in English: /data/asr/testEnglish1.wav 0.000 0.190 and 43.00000000 /data/asr/testEnglish1.wav 0.210 0.340 we're 44.00000000 /data/asr/testEnglish1.wav 0.330 0.460 going 97.00000000 /data/asr/testEnglish1.wav 0.450 0.520 to 97.00000000 /data/asr/testEnglish1.wav 0.510 0.940 fly 66.00000000 /data/asr/testEnglish1.wav 1.080 1.300 was 31.00000000 /data/asr/testEnglish1.wav 1.290 1.390 that 24.00000000 /data/asr/testEnglish1.wav 1.290 1.390 it 22.00000000 /data/asr/testEnglish1.wav 1.380 1.510 we're 27.00000000 /data/asr/testEnglish1.wav 1.500 1.660 going 97.00000000 /data/asr/testEnglish1.wav 1.650 1.720 to 98.00000000 /data/asr/testEnglish1.wav 1.710 1.930 fly 94.00000000 /data/asr/testEnglish1.wav 1.920 2.110 over 79.00000000 /data/asr/testEnglish1.wav 2.100 2.380 saint 93.00000000 /data/asr/testEnglish1.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: /data/asr/testMandarin1.wav 0.280 0.610 \u6218\u6597 99.00000000 /data/asr/testMandarin1.wav 0.600 0.880 \u7206\u53d1 98.00000000 /data/asr/testMandarin1.wav 0.870 0.970 \u7684 99.00000000 /data/asr/testMandarin1.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 /data/asr/testMandarin1.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 /data/asr/testMandarin1.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 /data/asr/testMandarin1.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 /data/asr/testMandarin1.wav 3.130 3.340 \u7684 100.00000000 /data/asr/testMandarin1.wav 3.330 3.720 \u6b66\u88c5 55.00000000 /data/asr/testMandarin1.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: /data/asr/testFarsi1.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 /data/asr/testFarsi1.wav 0.470 0.740 51.00000000 \u0627\u06cc /data/asr/testFarsi1.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 /data/asr/testFarsi1.wav 0.310 0.460 99.00000000 \u0645\u06cc /data/asr/testFarsi1.wav 0.450 0.680 99.00000000 \u06af\u0645 /data/asr/testFarsi1.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f /data/asr/testFarsi1.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Query by Example Keyword Spotting (QBE) Query by example is a specific type of keyword spotting plugin that searches for keywords matching a spoken word or phrase example, rather than from a text example like traditional KWS. This means that it is necessary to enroll keywords into the system from audio examples with localenroll before using QBE to search audio for these keywords or phrases. Enrollment follows the same format as enrolling speakers into a SID plugin, with the enrollment audio list following this format: <audio_file_path> <keyword_id> Example: /data/qbe/enroll/watermelon_example1.wav Watermelon /data/qbe/enroll/watermelon_example2.wav Watermelon /data/qbe/enroll/airplane_example.wav Airplane /data/qbe/enroll/keyword_example.wav Keyword Note that currently each enrollment audio file must contain ONLY the keyword that is desired to be enrolled. Also note that the text label in the second column of the enrollment file is only for user readability and is not used by the system when determining what to search the audio file for. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006 Topic Identification (TID) Topic Identification plugins attempt to detect and categorize the topic being discussed within an audio recording from a known and pre-enrolled set of available topics, and report this topic (if any are detected) to the user. Each domain of a TID plugin is language-dependent, and should display the target language as the first string of the domain\u2019s name. Some TID plugins may be delivered with pre-enrolled topics \u2013 please consult the documentation that accompanied your delivery if you are unsure if this is the case. If no topics are enrolled, however, or if you wish to enroll new topics, the format is very similar to enrolling new speakers into a SID plugin, and follows the same CLI call structure, with one caveat. $ localenroll --local $domain $enroll_list Due to limitations with the current TID technology, enrollment must be performed with the --local flag set. This limits OLIVE to serialized processing, which will process the enrollment slightly slower, but avoid resource-competition issues that may cause the system to become unstable and crash. Enrollment audio lists for TID follow the same format as SID, but substitute a topic name for a speaker label. Note: in the current version, we require the user to provide audio examples that are primarily about the topic of interest as enrollment examples. If there are significant portions of an audio file that are off-topic, we suggest the file be cut and fed as separate examples. Enroll list format: <audio_file_path> <topic_id> Example: /tid/enroll/topic_example_audio_5760.wav Travel /tid/enroll/topic_example_audio_5761.wav Travel /tid/enroll/topic_example_audio_5762.wav Travel /tid/enroll/topic_example_audio_5763.wav Travel To run TID, once target topics have been enrolled, the call to localanalyze is very similar to other plugin types, with the current plugin again requiring the --local flag. $ localanalyze --local /path/to/plugins/tid-svm-v2/domains/r-tel-v1/ testAudio.lst As with the SID and LID plugins, by default the plugin\u2019s output will be written to the file \u201coutput.txt\u201d in the directory localanalyze was called from. This can be overridden by passing localanalyze the -o flag, as well as an alternate file to save the results to. The TID results structure is very similar to KWS, with the following format for each line: <audio_file_path> <start_time_s> <end_time_s> <topic_id> <confidence_score> Example: /data/tid-example_wavs/20110511_GET-TOGETHER.wav 18.790 55.300 transportation 0.0222 /data/tid-example_wavs/20110511_TRANSPORTATION.wav 4.010 19.140 transportation 0.4532 The start and end timestamps above are provided in seconds. The will be displayed as it was supplied in the second column of the enrollment list. The will be between 0 and 1, and marks the confidence of the system in the decision of this topic. Please note that output.txt will be overwritten by each successive experiment. Please back it up or use the -o option to localanalyze if you would like to save the results. Also note that the start and end times for each topic refer to the chunk in each audio that has the highest probability of being about that topic. Currently, we report only ONE such segment per file in order to help the user locate the most useful part. The score associated with that segment is global, in that it represents the likelihood that this topic is present anywhere in the document. Important Background Example Information In order to train a Topic detector, we currently use an SVM classifier. This model uses \"positive\" examples of the topic as provided by the user during the enrollment phase, as well as \"negative\" examples to model what is not the topic. Those negative examples can be crucial to the performance of the final system. Currently, those \"negative\" examples come pre-processed as a python numpy archive and cannot be modified by the user explicitly. We do provide two different numpy archives that can be tried by a user: BG_RUS001-train-acc+neg-no-travel.npz (default) BG_RUS001-train-test-random-all-chunk-plugin.npz Archive 2) includes only data from the RUS001 conversational corpus, which didn't have very topic-specific prompts. Archive 1) includes a mix of RUS001 data as well as a subset of the RU_CTS conversational corpus which was topic annotated. We excluded examples pertaining to 'TRAVEL' in this archive, but this archive contains conversations about the following (loosely defined) topics: ACTIVITIES BIRTHDAY_WISHES CHILDREN ECONOMY EDUCATION ENTERTAINMENT FOOD_DRINK FRIENDS_RELATIVES GET-TOGETHER HEALTH HOME HOME_MAINTENANCE IMMIGRATION LANGUAGE_COMMUNICATION LEISURE LIFE_PHILOSOPHY_RELATIONSHIPS LOCATION_DESCRIPTION MARRIAGE MOOD_PHYSICAL MOVING_HOMES MUSIC PERFORMANCE_REHEARSAL PETS POLITICS PROJECT READING_WRITING RELIGION_HOLIDAY SPEECH_COLLECTION_PROJECT TECHNOLOGY TRANSPORTATION TV_MOVIES WEATHER_CLIMATE WORK If the topic you are training for is very similar to a topic listed above, it might be worth it trying archive (2) as well. In the future, we will provide the opportunity for the user to feed his own negative examples. Gender Identification (GID) Gender ID plugins allow for triage of audio files to identify only those containing speakers of a certain gender. For scoring files, gender identification plugins operate in the same manner as SID and LID plugins. GID plugins are delivered with two pre-enrolled classes; \u2018m\u2019 and \u2018f\u2019, for male and female, respectively, so user-side enrollment is not necessary. To score files with a GID plugin using localanalyze , use the following syntax: $ localanalyze /path/to/plugins/gid-gb-v1/domains/clean-v1/ testAudio.lst Where the output follows this format: <audio file 1> m <male likelihood score> <audio file 1> f <female likelihood score> \u2026 <audio file N> m <male likelihood score> <audio file N> f <female likelihood score> Example: /data/gender/m-testFile1.wav m 0.999999777927 /data/gender/m-testFile1.wav f -0.22073142865 Enhancement (ENH) Enhancement or AudioConverter plugins are audio-to-audio plugins that take an audio file as input and provide a second audio file as output. Currently they are used to enhance the input audio file, for listening comfort and/or intelligibility. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav Running speech enhancement is as simple as: $ localanalyze /path/to/plugins/enh-mmse-v1/domains/multi-v1/ inputAudio.lst In addition, you can pass an optional PEM file that specifies optional regions to the plugin \u2013 the current enhancement plugin uses this file to pass the \u2018noise regions\u2019 to OLIVE to allow the plugin to build a noise profile for more accurate characterization and removal of the noise present in that audio file. 6: Testing In this section you will find a description of benchmarking that is performed with each release of OLIVE to measure the performance of each plugin with respect to speed and memory usage. The hardware and software details used for these results are provided in the next section, with each of the current plugins\u2019 memory and speed results following. a. Benchmarking Setup (Hardware/Software) Each data point below was obtained by running the OLIVE 4.9.1 software with a 4.8.0 runtime that has been patched to 4.9.1 (contains libraries needed by the new Topic Identification and Enhancement plugins). Tests were performed on the CentOS 7 operating system, on a Gigabyte BRIX GB-BXI7-5500. This machine has 16GB RAM available, and an Intel i7-5500U processor, which is a dual core (quad-thread) processor that runs at 2.4 GHz base (3.0 GHz turbo). b. Plugin Memory Usage Results These results were generated by running several files through each plugin via the OLIVE CLI using localanalyze , one at a time, while measuring the memory used by the localanalyze utility. The files vary in length from 10 seconds through 4 hours, and this allows us to see how each plugin handles scaling the audio file length up, and also compare overall resource utilization between individual plugins. The values reported for each plugin and audio file are the peak memory usage, in MB \u2013 lower values are better. Note that this report is for processing a single file at a time and is representative of memory utilization that can be expected for serialized processing, or processing on a machine with a single processing core. Parallel processing can cause memory usage to rise. TODO: Need to put the charts/resuts in here! SAD SAD Memory Usage (MB) Plugin sad-dnn-v4 sad-dnn-v4 sad-dnn-v4 Domain digPtt-v1 ptt-v1 tel-v1 10s 142 142 142 1 min 158 158 158 10 min 221 221 221 30 min 408 408 408 2 hr 1,262 1,262 1,262 4 hr 2,377 2,377 2,377 c. Plugin Speed Analysis Results The following charts show the speed performance of the current release of each OLIVE plugin. Values are reported as the speed of the plugin in \u2018times faster than real time\u2019 and represent how fast the plugin is able to process the input audio data, with respect to the length of that data \u2013 higher is better. Each plugin is fed 10 hours of total data consisting of roughly 4-minute audio cuts to achieve this measurement. For this test, OLIVE has been limited to using a single core for processing, in order to keep measurements and results consistent. Note that enabling parallel processing if multiple CPU cores are available will improve performance. Plugin Speed Statistics Reported in Times Faster than Real Time Plugin Domain Speed (x RT) sad-dnn-v4 digPtt-v1 104.8 sad-dnn-v4 ptt-v1 111.5 sad-dnn-v4 tel-v1 117.0 sid-embed-v2 multi-v1 51.6 sid-embedDnnTbc-v1 tbc-v1 43.9 sid-embedDnnTbc-v1 tbcdnn-v1 64.5 sid-embedTbc-v1 tbc-v1 40.2 lid-embed-v2 multi-v1 39.7 kws-batch-v9 eng-tel-v1 1.11 kws-batch-v9 eng-tel-v2 1.09 kws-batch-v9 f-tel-v1 1.23 kws-batch-v9 r-tel-v1 1.61 kws-batch-v9 r-tel-v2 1.61 qbe-tdnn-v4 digPtt-v1 15.2 qbe-tdnn-v4 multi-v1 16.1","title":"Command Line Tools (Legacy)"},{"location":"cli.html#olive-command-line-interface-guide-legacy","text":"","title":"OLIVE Command Line Interface Guide (Legacy)"},{"location":"cli.html#disclaimer","text":"Note that the tools described below are legacy tools that are mostly used for internal testing and development. With docker-based deliveries, these utilities are difficult to access and have many performance tradeoffs versus using the OLIVE server through a client - they should not be used for integration, only for very basic experimentation. The functionality offered by these should instead be accessed through the provided Java example client (OliveAnalyze, OliveEnroll, etc.) or Python example client (olivepyanalyze, olivepyenroll, etc.). Documentation for these utilities is under construction and will be provided soon - but each utility has a help statement that provides instructions for running each.","title":"Disclaimer"},{"location":"cli.html#introduction","text":"This document describes running the OLIVE (formerly SCENIC) system from a command line. Our command line applications are geared toward a variety of specialized users such as researchers, system evaluators (i.e. Leidos for the DARPA RATS program), and testers. Casual users should consider using our graphical application. However, our command line applications can function as general-purpose tools, but may require specially formatted files such the RATS XML files for audio analysis and LDC-format TSV files for training annotations.","title":"Introduction"},{"location":"cli.html#1-overview","text":"OLIVE command line interface (CLI) tools include: localenroll \u2013 Used to enroll \u2018targets\u2019 into the system, such as a target speaker for speaker identification (SID), a topic of interest for topic identification (TID), or a keyword or phrase of interest for query-by-example keyword spotting (QBE). localanalyze \u2013 Used to query the OLIVE server to score audio to find speech with a speech activity detection (SAD) plugin, report scores for potential speakers or languages of interest for SID or language identification (LID) plugins, report likelihood and location(s) of conversation topics or keywords of interest (TID, QBE, KWS). localtrain \u2013 Used to train or adapt plugins that support the LearningTrait (SupervisedAdapter, SupervisedTrainer, or UnsupervisedAdapter) with examples of new audio conditions to improve performance in such conditions. Also used to add new language recognition capabilities to a LID plugin, and to retrain the background models of a SID plugin to prepare it for new audio conditions. Training and adaptation are not available in all plugins, please refer to individual plugin documentation or plugin capabilities matrix to verify availability of training or adaptation.","title":"1: Overview"},{"location":"cli.html#2-command-line-testing-and-analysis","text":"","title":"2: Command Line Testing and Analysis"},{"location":"cli.html#a-enrollment-with-localenroll","text":"The localenroll command is used to enroll audio for SID and TID. It can be invoked from a BASH or C-shell terminal. It takes a simply formatted text file as input and does not produce an output file. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TID, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in the Plugin Appendix. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7 The basic syntax for calling localenroll (more details and options below) is: $ ./localenroll <path_to_plugin_domain> <path_to_enrollment_file> Where an example of that may be: $ ./localenroll plugins/sid-embed-v1/domains/multi-v1/ /data/sid/smoke_enroll.lst The numerous options available in localenroll can be seen by executing localenroll --help, the output of which is shown below: usage : localenroll [- h ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path enrollment_file positional arguments : plugin_domain_path path to plugin domain used for analysis enrollment_file List of enrollments of the form < audio_path > < class_id > OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have a configuration section named enrollment . Only values from the enrollment section are read -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting","title":"A.  Enrollment with localenroll"},{"location":"cli.html#b-scoring-and-processing-with-localanalyze","text":"","title":"B.  Scoring and processing with localanalyze"},{"location":"cli.html#i-invoking-localanalyze","text":"The localanalyze utility is used to perform OLIVE scoring and analysis with most plugins (SAD, SID, SDD, LID, KWS, QBE, GID, TID), or processing with an ENH plugin, all on list-based input files. It can be invoked from a BASH or C-shell terminal. A path to a valid OLIVE plugin and domain as well as an audio paths input file are required for all tasks. For some plugins, like LID and SID, an optional IDs input file can be specified via the --class_ids argument to limit which languages or speakers are scored. This IDs input file is also how a keyword spotting plugin is informed what the keywords of interest are for a given analysis. The exact details for invoking localanalyze will depend upon the plugin technology being used, and may vary slightly depending upon the options available to each individual plugin, but the general format for running this utility is: $ localanalyze <path_to_plugin_domain> <list_of_files_to_analyze_or_process> With an example (SID): $ localanalyze plugins/sid-embed-v1/domains/multi-v1/ /data/sid/test_data.lst The format of the audio input file is simply a list of one or more newline-separated lines containing a path to an audio file: <audio_path> Example audio input file: /data/sid/test/unknownSpkr1.wav /data/sid/test/unknownSpkr27.wav As mentioned above, if you would only like to score a subset of the enrolled speakers or languages, you can optionally pass a list of these identifiers as a newline-separated list text file, with the --class_ids command line argument. This same argument is how you select keywords to search for when running localanalyze with a keyword spotting plugin (see KWS section in the Plugin Appendix for more information). IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Speaker Identification IDs example: Chris Billy Spkr3 A Keyword Spotting IDs example: turn left torpedo watermelon Example (KWS) of a localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/test-audio.lst Note that re-running localanalyze will overwrite the contents of the output.txt file or OUTPUT directory, depending on what type of plugin is being run. The OLIVE usage/help statement for localanalyze : usage : localanalyze [- h ] [-- output OUTPUT_PATH ] [-- thresholds THRESHOLDS ] [-- class_ids ID_LIST_PATH ] [-- options OPTIONS_PATH ] [-- regions REGION_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_domain_path audio_paths_file positional arguments : plugin_domain_path path to plugin domain used for analysis audio_paths_file List of audio files to analyze OR a PEM formatted file of the form < audio_path > < channel > < class_id > < start > < end > optional arguments : - h , -- help show this help message and exit -- output OUTPUT_PATH , - o OUTPUT_PATH path to output file or directory -- thresholds THRESHOLDS Optional comma - separated threshold values to apply to frame - level scores , e . g . 0.0 , 1.5 . Use syntax '-- thresholds=' for negative values , e . g -- thresholds =- 2.0 ,- 1.0 -- class_ids ID_LIST_PATH , - i ID_LIST_PATH Optional file that specifies class ids to be scored . E . g . limit the speakers that scored . -- options OPTIONS_PATH Optional file containing plugin specific name / value pairs . The option file may have more or more section headings s for each plugin type . Common section names are 'frame scoring' , , 'global scoring' or 'region scoring' -- regions REGION_PATH , - r REGION_PATH Optional flag indicating that the audio paths file should be supplemented with regions from a PEM formated file , it is up to the plugin to utilize these regions to supplement its scoring . This flag is ignored if the audio input list ( audio_paths_file ) is a PEM formatted file . -- timeout TIMEOUT timeout , in seconds , for all jobs regardless of the audio duration . otherwise the job will timeout based on the duration of audio to process and the domain 's timeout_weight --version show program' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting","title":"I. Invoking localanalyze"},{"location":"cli.html#ii-output","text":"","title":"II.    Output"},{"location":"cli.html#plugin-scoring-types","text":"In general, the output format and location of a call to localanalyze will depend on the type of \u2018scorer\u2019 the plugin being used is. There are currently four types of plugins in OLIVE: Global scorer Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Currently SID, LID, and GID are the only global scoring plugins. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Example \u2013 sid-embed-v1, lid-embed-v1 Region scorer Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. TID, SDD, QBE, and KWS are all region scorers. Example \u2013 sdd-embed-v1, qbe-tdnn-v4, kws-batch-v9 Frame scorer A frame scoring plugin provides a score for every \u2018frame\u2019 of audio within every test file passed to localanalyze . This allows OLIVE to find distinct regions of speech with high precision in recordings with noise and/or silence. SAD is a frame scoring plugin. It is also possible to apply a threshold to a frame scoring plugin at run-time to report regions of detection instead of frame scores. For a plugin like SAD, this allows OLIVE to provide output in the form of speech regions. A frame is a short segment of audio that typically consists of 10 milliseconds of audio (100 frames per second). Example \u2013 sad-dnn-v4 Audio to audio This plugin takes an audio file as input, and also returns an audio file as output. Currently the only plugins that fall into this category are speech/audio enhancement plugins, where the goal is removing noise and distortion from an audio file to improve the human listening experience and intelligibility. Example \u2013 enh-mmse-v1","title":"Plugin Scoring Types"},{"location":"cli.html#global-scorer-output","text":"In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> The name and location of the output file can be overridden by passing it as the argument to the -o or --output argument when calling localanalyze . To see specific examples for each plugin type, please refer to the appropriate section of the Plugin Appendix.","title":"Global Scorer Output"},{"location":"cli.html#region-scorer-output","text":"Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: < audio_path > < region_start_timestamp > < region_end_timestamp > < class_id > < score > Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. Specific examples can be found in the Plugin Appendix at the end of this document.","title":"Region Scorer Output"},{"location":"cli.html#frame-scorer-output","text":"In the case of frame scorers like SAD, an output file is generated for each audio input file, where each audio output file contains a score for each frame in the audio input. There is one frame score per line. Alternatively, an option exists to produce segmentation scores from SAD results by using the --threshold argument. When using the --threshold argument, the output file adheres to standard 5-column PEM format. Without supplying a threshold to localanalyze , the frame scorer output looks like this: <frame_1_score> <frame_2_score> \u2026 <frame_N_score> When a threshold is provided, the output file will resemble the following: < filename > , < channel > , < label ( \u201c speech \u201d ) > , < speech region start time ( seconds ) > , < end time ( seconds ) >","title":"Frame Scorer Output"},{"location":"cli.html#audio-to-audio-output","text":"An audio-to-audio plugin takes an audio file as input and returns a corresponding audio file as output. Currently, this plugin type is used to supply enhancement capabilities to OLIVE, to allow OLIVE to improve the quality, intelligibility, or just general human listening experience for an audio file. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav","title":"Audio to Audio Output"},{"location":"cli.html#3-command-line-field-adaptation","text":"","title":"3: Command Line Field Adaptation"},{"location":"cli.html#a-command-line-field-adaptation-overview","text":"In general, training and adaptation are very resource and time intensive operations. Very large amounts of RAM are used at certain steps in training. When attempting to train or adapt, the machine should be dedicated to that operation. If the plugin path contains a domain then adaptation is implied, otherwise training is implied. The high-level difference between training and adaptation is that adaptation will use the new data supplied during adaptation in addition to the data already used to train the model used by the plugin/domain. Training, on the other hand, ignores the data originally used for training a model and retrains from scratch using only the new data provided. When performing training, none of the data in the base plugin will be used, but the feature configs will. Check the plugin\u2019s traits to determine if full training and/or adaptation are supported.","title":"A.  Command Line Field Adaptation Overview"},{"location":"cli.html#b-invoking-localtrain","text":"Not to be confused with enrollment, the localtrain command line application is used to perform field adaptations for SAD, LID & SID. localtrain takes a plugin or plugin_domain path, and one or more data input files formatted for: Unsupervised data - a newline separated list of audio file paths Supervised data with file level annotations - a newline separated list of audio files paths with a class Id (i.e. \u201caudio_file1.flac fas\\n\u201d) Supervised data with region level annotations - a newline separated list of audio file paths, start time (seconds), end time (seconds), and class ID (i.e. \u201caudio_file1.flac 1.25 3.5 fas\\n\u201d) If multiple data files are specified then they must all use the same annotation format. The localtrain utility outputs a new domain in the plugin path. The details of the localtrain executable are below: usage : localtrain [- h ] -- domain - id DOMAIN_ID [-- overwrite ] [-- preprocess ] [-- finalize ] [-- unique ] [-- options OPTIONS_PATH ] [-- timeout TIMEOUT ] [-- version ] [-- log LOG_FILE ] [-- work_dir WORK_DIR ] [-- jobs JOBS ] [-- debug ] [-- nochildren ] [-- quiet ] [-- verbose ] [-- purge ] plugin_or_domain_path data [ data ...] Train or adapt OLIVE audio recognition systems positional arguments : plugin_or_domain_path Path to the plugin or domain . A plugin path implies full training . A domain path implies adaptation of the specified domain . data paths to data files for training / adapation . The files can have one of three forms . 1 : < audio_path >\\ n 2 : < audio_path > < class_id >\\ n 3 : < audio_path > < class_id > < start > < end > \\ n . The first form has no annotations and implies unsupervised . The second form provides for file - level annotations while the third form supports region - level annotations . Start and end times should be in seconds . If multiple files are specified , they must have the same form . optional arguments : - h , -- help show this help message and exit -- domain - id DOMAIN_ID The id of the new domain you 're creating through training or adaptation. Should be a string that is somewhat descriptive of the conditions --overwrite Forcefully overwite an existing domain --preprocess Pre-process audio only, do not finalize training/adaptation --finalize Pre-process audio only, do not finalize training/adaptation --unique gurantees log files are written to unique directoires/files. Helpful when running in SGE mode --options OPTIONS_PATH Optional file containing plugin specific name/value pairs. The option file must have one or more sections for each plugin type. Common section names are ' supervised trainer ', ,' supervised adapter ', ' unsupervised trainer ' or ' unsupervised adapter ' --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- version show program ' s version number and exit -- log LOG_FILE , - l LOG_FILE path to output log file -- work_dir WORK_DIR , - w WORK_DIR path to work dir -- jobs JOBS , - j JOBS specify number of parallel JOBS to run ; default is the number of local processors -- debug , - d debug mode prevents deletion of logs and intermediate files on success -- nochildren Perform sequential ( local ) processing without creating jobs as sub processes for ease of debugging ( IPython ) at the expense of speed / parallelization -- quiet , - q turn off progress bar display -- verbose , - v Print logging to the terminal . Overrides -- quiet -- purge , - p purge the work dir before starting In order to use an adapted system plugin, simply pass the full path of the domain generated by localtrain to localenroll or localanalyze as the plugin_domain_path argument. For training, do not include the domain in the plugin path. When running on a SGE, you may split the audio processing from the finalization step by using the --preprocess flag to first pre-process audio files, then use invoke localtrain with the --finalize argument to finalize training. Guidelines for the minimum amount of audio data required to successfully execute localtrain are listed in the table below. Task Operation Speech Duration SAD Adapt to new channel 1h LID Adapt to new channel 20m LID Train a new language 3h SID Adapt to new channel 1h","title":"B.  Invoking localtrain"},{"location":"cli.html#i-examples","text":"","title":"i. Examples"},{"location":"cli.html#sad-adaptation-example","text":"$ localtrain ./plugins/sad-dnn-v1/domains/ptt-v1/ adaptation-data.lst Where each line of adaptation-data.lst has the following format: /path/to/audio.wav label","title":"SAD Adaptation Example:"},{"location":"cli.html#c-lid-trainingadaptation","text":"When training new channel conditions, it is recommended to train all supported languages in the LID model to produce the best results. The out of set language is labeled as \u2018xxx\u2019. Use this language ID when training to add languages that you do not want to target in the LID task but are known to be in the test dataset.","title":"C.  LID Training/Adaptation"},{"location":"cli.html#4-log-files","text":"","title":"4: Log Files"},{"location":"cli.html#a-olive-command-line-logging","text":"When executing localtrain , localenroll , and localanalyze , here are three named log files that may be of interest should something go awry. The top-level log file: This log file corresponds to the -l option to the localtrain , localenroll , and localanalyze utilities. By default, it is named the same as the utility being used with \u201c.log\u201d appended (i.e. localanalyze.log when running localanalyze ) and will be written to the directory from which you executed the utility. The pool executor log file: This file will be written to [work_directory]/logs/pool_executor.log, where work_directory corresponds to the -w option to localtrain / localenroll / localanalyze and defaults to your current directory/WORK. The pool executor log file is the best log file to look at if unexpected errors occur. It corresponds to our internal job scheduler also known as the pool executor. The pool monitor log file: This file will be written to [work_directory]/logs/pool_monitor.log, where work_directory corresponds to the -w option to the localtrain , localenroll , localanalyze utilities and defaults to your current directory/WORK/. This log contains stats about memory and CPU utilization. All three of these log files will exhibit log rotation behavior. In the event of errors, [work_directory]/logs may also contain log files named [order_id].failed , where order_id generally corresponds to the file names of the audio files being used for adaptation/training, enrollment, or analysis. The id can be used to tie errors in the pool executor log file to the \u201c.failed\u201d log files. If you run the OLIVE CLI utilities in debug mode (-d), all log files will be maintained, even if they were successful.","title":"a.  OLIVE Command Line Logging"},{"location":"cli.html#b-rotating-log-files","text":"OLIVE employs rotating log files in many places. In this context, rotating refers to a log file that is rewritten each time the application is run. The old log file, if any, is renamed with an integer suffix denoting how many invocations in the past it corresponds to. For instance, if you run localanalyze and don\u2019t specify a -l option, you\u2019ll get the default localanalyze.log file. If localanalyze.log already exists, it is moved to localanalyze.log.1 . The system will keep the 10 most recent log files. A file named localanalyze.log.8 means that the file corresponds to eight invocations ago.","title":"b.  Rotating Log Files"},{"location":"cli.html#5-plugin-appendix","text":"","title":"5: Plugin Appendix"},{"location":"cli.html#plugin-types-and-acronyms","text":"Currently, OLIVE supports the plugin technologies listed in the following list. For operating instructions that apply to only a specific technology, refer to that section within this appendix. SAD \u2013 Speech activity detection. SID \u2013 Speaker identification. LID \u2013 Language identification. KWS \u2013 Keyword spotting. QBE \u2013 Query by example based keyword spotting. TID \u2013 Topic identification. SDD \u2013 Speaker diarization and detection. GID \u2013 Gender identification. ENH \u2013 Speech and audio enhancement.","title":"Plugin Types and Acronyms"},{"location":"cli.html#speech-activity-detection-sad","text":"SAD plugins are frame scorers that take an audio list file and annotate the presence and location of speech in each audio file in that list. In standard operation, SAD plugins produce a single output file for each input file, by default in a directory called OUTPUT in the location localanalyze was called from. Output files carry the name of the original input file, but with a new extension \u201c.scores\u201d \u2013 for example, audioFile1.wav will become audioFile1.wav.scores, saved inside OUTPUT/. The format of these results files is a newline separated list of numerical values representing the likelihood that each 10ms frame of the audio file contains speech. Typically, a score above 0 represents speech detection, and a score below 0 represents no speech. SAD analysis example: $ localanalyze /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst Output files: OUTPUT/audioFile1.wav.scores OUTPUT/audioFile2.wav.scores Example audioFile1.wav.scores contents: -0.22624 -0.10081 0.00925 0.12365 Alternatively, SAD plugins can be run with the --thresholds flag to have localanalyze automatically convert the frame scores to regions of speech, by applying the provided threshold. SAD analysis example using thresholds: $ localanalyze --thresholds = 0 .0 /plugins/sad-dnn-v4/domains/ptt-v1/ /data/sad/test/test-audio.lst This will provide a single output file in OUTPUT directory corresponding to the provided threshold: 0.0.pem. If more than one threshold is provided, there will be a PEM file placed into OUTPUT corresponding to each provided threshold. Example PEM output: /data/sad/test/audioFile1.wav 1 speech 63.110 66.060 /data/sad/test/audioFile1.wav 1 speech 66.510 69.230 /data/sad/test/audioFile1.wav 1 speech 93.480 96.090 /data/sad/test/audioFile1.wav 1 speech 96.570 100.760 Note that if negative thresholds are to be used, it is very important to specify the thresholds using an \u2018=\u2019 character. For example, this threshold specification is valid: --thresholds=-2.0,4.0 And this is not valid: --thresholds -2.0,4.0 If only thresholds of 0 or above are going to be used, it is acceptable to omit the equals sign.","title":"Speech Activity Detection (SAD)"},{"location":"cli.html#speaker-identification-sid","text":"SID plugins are global scorers that take an audio list file and return a score for each enrolled speaker model scored against the audio in each input audio file. Generally, a score above 0 for an enrolled speaker model represents that speaker being detected in the respective audio file. In order to perform analysis on a file with a SID plugin you must first enroll one or more target speakers. The enrollment list file for a SID plugin follows this format for each line: <audio_file_path> <speaker_id> An example enroll.lst: /data/spkr_example_audio_5760.wav UIM1 /data/spkr_example_audio_5761.wav UIM1 /data/spkr_example_audio_5762.wav John /data/spkr_example_audio_5763.wav John Enrolling these speakers with localenroll : $ localenroll /path/to/plugins/sid-embed-v2/domains/multi-v1/ enroll.lst Example localanalyze call: $ localanalyze ./plugins/sid-embed-v2/domains/multi-v1/ ./data/sid/test/testAudio.lst By default, the output of this call is written to output.txt in the directory the command was run. The format of output.txt contains one line for each enrolled speaker model, for each input audio file, and the corresponding score: <audio_file_path> <speaker_id> <score> Example output.txt: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Speaker Identification (SID)"},{"location":"cli.html#trial-based-calibration-speaker-identification-sid-tbc","text":"Trial-based Calibration SID plugins are identified by \u2018tbc\u2019 in the plugin name. They are used very similarly to a basic SID plugin, using localenroll and localanalyze just like the examples shown in the previous example. The benefit of TBC plugins is that they allow OLIVE to perform calibration at test-time, based on the actual data conditions being encountered, rather than being forced to use a single, global calibration model that has been trained a priori. The basics of TBC enrollment and testing follow the previous SID examples, but the additional options and outputs available to TBC are detailed below. The standard approach to calibration uses a \u201cone size fits all\u201d calibration model based on the developer\u2019s best understanding of potential operating conditions. This is problematic when the user either doesn\u2019t know ahead of time what likely conditions are, or when operating conditions may vary widely. Trial-based calibration was developed as a means of providing calibration that is responsive to the particular conditions of a trial, and adapts its calibration model based on the conditions encountered. There are two ways we have developed to do this. The first draws from a pool of available data (either provided by the developer or augmented with user-provided data) and uses measures of the conditions found within this data and the trial conditions to build an ideal calibration set on the fly. This is useful in that this approach can also determine when a trial CANNOT be calibrated, and to measure the success of calibration when it is possible. The clear downside of this approach is that it is quite slow. A second approach to TBC is to use a model that has used a DNN to learn to predict both calibration parameters and confidence from large sets of trials and available calibration data. This approach is very fast (about 5000 times faster than the first approach) but has the downside that expanding the calibration set by the user\u2019s data isn\u2019t possible. This plug-in provides both approaches in the TBC plug-in, as two separate domains. In addition to the output score file detailed in the SID section, TBC plugins have additional possible outputs.","title":"Trial-based Calibration Speaker Identification (SID TBC)"},{"location":"cli.html#speech-detection-output","text":"Segmentation files are used to label time regions in the speech signal. We use this format for voice activity detection (VAD). If an output_ivs_dump_path is provided as an option to localenroll or localanalyze , the system will produce this file in a folder corresponding to the wav_id for all registered waveforms. The format is the following: md5sum start end ( in seconds ) Example: b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.060 0.060 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.090 0.090 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.110 0.170 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.200 0.200 b5ae06002383da6c74a7b8424c5fb9282859cca36750565bfb80a13ab732fc57 0.560 3.550","title":"Speech Detection Output"},{"location":"cli.html#persistent-i-vectors","text":"For both localenroll and localanalyze , if the output_ivs_dump_path is defined via an options file with the --options flag, an i-vector from each audio file is saved in this directory for re-use. This reduces unnecessary computation when there exists overlap between lists of wave files to be processed. For instance, if the enroll and test wave file lists are identical (i.e., the case of an exhaustive comparison of a set of files), i-vector persistence will reduce overall computation by almost a factor of 2 since i-vector extraction consumes more of the computation required for an evaluation. I-vectors will be saved in a sub-directory of output_ivs_dump_path based on the base name of the wave file. In addition to this optional feature, the enrollment vectors are loaded in memory prior to verification and if the md5sum of a test audio file matches one used in the enrollment process, the corresponding vector will be used instead of re-processing the audio. This is because vector extraction is identical between enrollment and verification.","title":"Persistent I-Vectors"},{"location":"cli.html#trial-based-calibration","text":"Trial-based calibration (TBC) does not change the way calibration works but changes the way calibration is used. It relaxes the constraint on the system developers to train a calibration model that is ideally matched to the end use conditions. Rather than train a calibration model a priori, the system postpones this training until the conditions of the particular verification trial are known to the system; a trial consists of comparing test audio to an enrolled speaker model. The goal of trial-based calibration is to use information about the trial to generate an ideal calibration set for the trial conditions using the reservoir of possible calibration audio files available. Using this set, a calibration model tailored to the conditions of the trial can be trained and used to effectively calibrate the verification score. The TBC operation and output differs from traditional SID plugins; it may choose to reject a trial and NOT output a score if insufficient data is available for calibrating for those conditions. For instance, the output may look similar to the following: waves/T6_ACK2.sph T6 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00) waves/T6_ACK3.sph T1 0.0 -inf Unable to calibrate with only 12 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T2 0.0 -inf Unable to calibrate with only 3 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T3 0.0 -inf Unable to calibrate with only 0 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T4 0.0 -inf Unable to calibrate with only 2 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T6_ACK3.sph T6 0.0 -inf Unable to calibrate with only 9 relevant target trials (20 needed with similarity above 3.00)sed 0 segments in calibration waves/T4_ACK3.sph T5 0.96153318882 4.04821968079 Used 95 target trials in calibration waves/T4_RC.sph T4 3.46785068512 4.07499170303 Used 95 target trials in calibration waves/T5_Tip2.sph T5 8.90770149231 4.07352733612 Used 98 target trials in calibration waves/T5_RC.sph T5 10.2386112213 4.03855705261 Used 47 target trials in calibration waves/T4_Tip2.sph T4 10.8663234711 4.07404613495 Used 218 target trials in calibration waves/T4_Tip1.sph T4 11.793006897 3.98730397224 Used 164 target trials in calibration waves/T4_ACK2.sph T4 11.8091144562 3.90610170364 Used 119 target trials in calibration waves/T4_ACK1.sph T4 12.2115001678 4.16342687607 Used 208 target trials in calibration waves/T5_ACK1.sph T5 13.8099250793 3.99625587463 Used 99 target trials in calibration waves/T5_Tip1.sph T5 14.9411458969 3.96994686127 Used 83 target trials in calibration waves/T4_ACK3.sph T4 16.003446579 4.05554199219 Used 146 target trials in calibration The output follows the structure: <testwave> <modelid> <score> <confidence> <info> In the instance of insufficient calibration segments being located, a score of 0.0 and a calibration confidence of -inf is given. In contrast, when sufficient data is found for calibration, the number of segments used in calibration is reported. There exist two options for applying calibration with the current plugin: DNN-assisted TBC, normal TBC, or global calibration. Each of these options use duration information to reduce the impact of duration variation impacting calibration performance. Note that changing calibration domains does NOT require re-enrollment of models as these are done in a domain-independent way for any TBC-enabled plugin.","title":"Trial-based Calibration"},{"location":"cli.html#dnn-assisted-trial-based-calibration","text":"DNN-assisted TBC is invoked by passing the tbcdnn-v1 domain to localanalyze . This is a very fast and newly pioneered effort by SRI to reduce the computation needed to apply dynamic calibration methods to speaker recognition and operates with very low overhead compared to global calibration, and yet significantly benefits calibration performance in varying conditions or conditions that differ from the development conditions. localanalyze ... <plugin>/domains/tbcdnn-v1 test.lst","title":"DNN-assisted Trial-based Calibration"},{"location":"cli.html#normal-trial-based-calibration","text":"TBC is applied by default with the 'sid-embedDnnTbc-v1' plugin. The data within the domain (such as 'tbc-v1') is used as candidate calibration data. localanalyze ... <plugin>/domains/tbc-v1 test.lst TBC is applied to verification scores on a trial-by-trial basis. As such, verification using TBC will operate at a speed much slower than global or DNN-assisted TBC depending on the size and make-up of the TBC data. This should be considered when using TBC in a cluster environment where it is the number of trials (model vs test comparisons) that determine the running time instead of the number of test files.","title":"Normal Trial-based Calibration"},{"location":"cli.html#global-calibration","text":"Each domain can be used to invoke global calibration. This is particularly useful for user-defined data as it provides a rapid means of improving calibration performance without a dramatic increase in computation time. In this case, verification will operate at a much faster pace since TBC is essentially disabled and the global calibration model parameters are applied to all scores. In order to invoke global calibration, and optional parameter must be passed to localanalyze via an options file: echo \"[global scoring] global_calibration = True \" > options.lst localanalyze -- options options . lst ... < plugin >/ domains / tbc - v1 test . lst","title":"Global Calibration"},{"location":"cli.html#optional-parameters","text":"The TBC-based plugins offer several tunable parameters via the options parameter to localenroll or localanalyze . These can be passed to the enrollment phase or verification phase by preceding the options in an ascii text file as such: $ cat options . lst [ enrollment ] ... enrollment options per line ... [ global scoring ] ... verification options per line ... The optional parameters and their purpose are provided below. tbc_confidence_threshold = 3 . 0 , # Similarity threshold for processing a trial with TBC score_threshold = 0 . 0 , # Score offset subtracted from output LLRs to assist in making 0 threshold output tgt_max = 300 , # The maxmimum number of target trials used for TBC of a trial imp_max = 3000 , # The maxmimum number of impostor trials used for TBC of a trial tgt_imp_min = 20 , # The mimum number of relevant target and impostor calibration trials needed to use TBC ( rejected otherwise ) global_calibration = False , # Apply global calibration instead of TBC ivs_dump_path = None , # Output path for dumping vectors and meta information sad_threshold = 0 . 5 , # Threshold for speech activity detection ( higher results in less speech ) sad_filter = 1 , # Smoothing of LLRs from SAD DNN prior to thresholding sad_interpolate = 1 , # If > 1 , a speed up of SAD by interpolating values between frames ( 4 works well ) Utilizing these parameters in an options file may look like this: echo \"[enrollment] sad_threshold = 1.0 ivs_dump_path = . / embeddings [ global scoring ] sad_threshold = 1.0 ivs_dump_path = . / embeddings tgt_max = 100 tgt_imp_min = 50 \" > options.lst localenroll -- options options . lst ... < plugin >/ domains /< domain > enroll . lst localanalyze -- options options . lst ... < plugin >/ domains /< domain > test . lst","title":"Optional Parameters"},{"location":"cli.html#verification-trial-output","text":"The format for the verification trial is the following. Note that for global calibration, the optional parameters (calibration_confidence and calibration_remarks) are not output. Output format: wav_id speaker_id score [ calibration_confidence calibration_remarks ] Here is an example of score executed with Global Calibration: waves/T1_ACK1.sph T6 5.19274568558 waves/T1_ACK1.sph T4 1.204241395 waves/T1_ACK1.sph T5 1.69025540352 Here is an example of scores executed with DNN-assisted TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used DNN-assisted TBC with confidence 5.751 waves/T1_ACK1.sph T4 1.204241395 5.12 Used DNN-assisted TBC with confidence 3.122 waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with confidence above threshold (3.00) Here is an example of scores executed with normal TBC: waves/T1_ACK1.sph T6 5.19274568558 4.42 Used 67 target trials in calibration waves/T1_ACK1.sph T4 1.204241395 5.12 Used 73 target trials in calibration waves/T1_ACK1.sph T5 0.0 -inf Unable to calibrate with only 8 relevant target trials (20 needed with similarity above 3.00)","title":"Verification Trial Output"},{"location":"cli.html#speaker-diarization-and-detection-sdd","text":"The overall goal of SDD plugins is to detect regions of speech within an audio recording that are associated with different speakers, and then identify those speakers if possible. SDD plugins have three different modes of operation, as outlined below. Changing the execution mode for SDD is done by passing an options file to localanalyze as an argument to the --options flag. The main behavior and premise of the technology and plugin remain the same, but each mode changes the format and information contained in the output file. Running the SDD plugin is very similar to running SID plugins, with the same syntax for enrollment and testing. Currently, training or adaptation through localtrain is not supported, but enrolling new speakers and testing against enrolled speakers is as simple as: $ localenroll /path/to/plugins/sdd-embed-v1/domains/multi-v1/ enrollmentAudio.lst $ localanalyze /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst The enrollment and test audio file lists in this example follow the same format as the lists used by SID plugins, described above. By default, if run as above with no options, the plugin will run in Speaker Detection mode, and provide the output described above. In order to run in SID or SID Exhaustive mode, you will need to provide an options file to specify that behavior: $ localanalyze --options options.lst /path/to/plugins/sdd-embed-v1/domains/multi-v1/ testAudio.lst Where options.lst is a text file with contents similar to: [region scoring] mode: SID_EXHAUSTIVE sad_threshold: 0.0 diarization_max_num_speakers: 2 The [region scoring] header is alerting the plugin that the options are being passed for scoring, and all of the parameters shown above (sad_threshold, diarization_max_num_speakers, mode) are optional parameters. The mode option controls the output behavior as described above, and the possible options are SID, SID_EXHAUSTIVE, and SPEAKER_DETECTION, and described directly below. The sad_threshold defaults to 2.0, and is used to fine tune the threshold for the internal speech activity detection plugin if necessary. The parameter diarization_max_num_speakers defaults to 4, and is the largest number of speakers the plugin will attempt to consider when clustering segments within the file.","title":"Speaker Diarization and Detection (SDD)"},{"location":"cli.html#sdd-execution-modes","text":"To facilitate the understanding of each mode\u2019s output, consider that speech from an audio file is made up of clusters of speakers, and each cluster will have one or more contiguous segment of speech. SPEAKER_DETECTION The goal of Speaker Detection is to show the most probable speaker model for each segment of the input audio file. As output, this mode gives one line per segment within the file, along with the top scoring enrolled model for the cluster that segment belongs to, and that cluster's score for the given model. Note that many scores will be repeated in the output file, since each segment in the cluster shares the same score for a given speaker model. This mode is performed by default if no options file with a mode override is given. SID This mode is meant for triaging large amounts of audio files when the main goal is just finding which of these files may contain speech from one of the enrolled speakers. The output is the maximum score for each enrolled speaker within the audio file after scoring against each cluster in the file, as well as the timestamps for the beginning and end of the longest segment within the cluster that scored the highest for that model. This gives a specific segment to spot check and evaluate the plugin's decision if needed. SID_EXHAUSTIVE When using SID Exhaustive, each diarized cluster is scored against each enrolled model. The output is a complete listing for every speech segment of the input audio file, the score from testing every enrolled model against the cluster that the segment belongs to. Many scores will be repeated in the output file, since each segment in the cluster shares the same score. In this example, Chris and Jimmy are the only enrolled models, and 5 total segments were identified within the file.","title":"SDD Execution Modes"},{"location":"cli.html#language-identification-lid","text":"LID plugins are global scorers that act very similar to SID with respect to scoring, except that each score corresponds to a language model rather than a speaker model. In most cases, LID plugins will be delivered from SRI with a set of languages already enrolled. Languages can be added to some plugins by the user if enough appropriate data is available, through the localtrain CLI call. Details on this will be added to this document in a later revision. Example localanalyze call: $ localanalyze /path/to/plugins/lid-embed-v2/domains/multi-v1/ testAudio.lst Output format: <audio_file_path> <language_id> <score> Output example: /data/lid/audio/file1.wav fre -0.5348 /data/lid/audio/file1.wav eng 3.2122 /data/lid/audio/file1.wav spa -5.5340 /data/lid/audio/file1.wav rus 0.5333 /data/lid/audio/file1.wav ara -4.9444 /data/lid/audio/file2.wav fre -2.6564","title":"Language Identification (LID)"},{"location":"cli.html#keyword-spotting-kws","text":"KWS is an automatic speech recognition (ASR) based approach to detecting spoken keywords in audio. Rather than enrolling target keywords from audio, as you would with query-by-example, telling the plugin what keywords to search for is done by passing an IDs file to localanalyze . The format of the IDs file is: IDs List Format: <id_1> <id_2 (opt)> <id_N (opt)> A Keyword Spotting IDs example, search_list.lst: remote torpedo voice recognition Example KWS localanalyze call with the --class_ids argument: $ localanalyze --class_ids search_list.lst /path/to/plugins/kws-batch-v9/domains/eng-tel-v1/ /data/kws/test/testAudio.lst The output format for KWS plugins is identical to that of QBE. It is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/kws/testFile1.wav 7.170 7.570 remote 1.0 /data/kws/testFile1.wav 10.390 10.930 remote 0.693357 /data/kws/testFile1.wav 1.639 2.549 voice recognition 1.0","title":"Keyword Spotting (KWS)"},{"location":"cli.html#automatic-speech-recognition-asr","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. ASR plugins do not require any enrollment of words (like QBE) or specification of words of interest (like traditional KWS), but instead rely on the vocabulary model built into the domain to define the list of available words. All that is necessary for scoring an audio file for ASR is a list of input files to be scored, which follows the format below. Generic input audio list format: <audioFile_1> <audioFile_2 (opt)> ... <audioFile_N (opt)> A specific example of this, called testAudio.lst, might look like: /data/asr/testFile1.wav /data/asr/testFile2.wav /data/asr/testFile3.wav Note that if the files are not contained within the directory that localanalyze is being run from, or if a relative path from that location is not provided, the full file path to each file is necessary. An example ASR localanalyze call: $ localanalyze /home/user/oliveAppData/plugins/asr-dynapy-v1/domains/eng-tdnnChain-tel-v1/ /data/asr/test/testAudio.lst The output format for KWS plugins is identical to that of QBE and other region-scoring OLIVE plugins. ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. Output is written to output.txt by default and follows this format: <audio_file_path> <start_time_s> <end_time_s> <word> <score> An example in English: /data/asr/testEnglish1.wav 0.000 0.190 and 43.00000000 /data/asr/testEnglish1.wav 0.210 0.340 we're 44.00000000 /data/asr/testEnglish1.wav 0.330 0.460 going 97.00000000 /data/asr/testEnglish1.wav 0.450 0.520 to 97.00000000 /data/asr/testEnglish1.wav 0.510 0.940 fly 66.00000000 /data/asr/testEnglish1.wav 1.080 1.300 was 31.00000000 /data/asr/testEnglish1.wav 1.290 1.390 that 24.00000000 /data/asr/testEnglish1.wav 1.290 1.390 it 22.00000000 /data/asr/testEnglish1.wav 1.380 1.510 we're 27.00000000 /data/asr/testEnglish1.wav 1.500 1.660 going 97.00000000 /data/asr/testEnglish1.wav 1.650 1.720 to 98.00000000 /data/asr/testEnglish1.wav 1.710 1.930 fly 94.00000000 /data/asr/testEnglish1.wav 1.920 2.110 over 79.00000000 /data/asr/testEnglish1.wav 2.100 2.380 saint 93.00000000 /data/asr/testEnglish1.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: /data/asr/testMandarin1.wav 0.280 0.610 \u6218\u6597 99.00000000 /data/asr/testMandarin1.wav 0.600 0.880 \u7206\u53d1 98.00000000 /data/asr/testMandarin1.wav 0.870 0.970 \u7684 99.00000000 /data/asr/testMandarin1.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 /data/asr/testMandarin1.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 /data/asr/testMandarin1.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 /data/asr/testMandarin1.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 /data/asr/testMandarin1.wav 3.130 3.340 \u7684 100.00000000 /data/asr/testMandarin1.wav 3.330 3.720 \u6b66\u88c5 55.00000000 /data/asr/testMandarin1.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: /data/asr/testFarsi1.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 /data/asr/testFarsi1.wav 0.470 0.740 51.00000000 \u0627\u06cc /data/asr/testFarsi1.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 /data/asr/testFarsi1.wav 0.310 0.460 99.00000000 \u0645\u06cc /data/asr/testFarsi1.wav 0.450 0.680 99.00000000 \u06af\u0645 /data/asr/testFarsi1.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f /data/asr/testFarsi1.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Automatic Speech Recognition (ASR)"},{"location":"cli.html#query-by-example-keyword-spotting-qbe","text":"Query by example is a specific type of keyword spotting plugin that searches for keywords matching a spoken word or phrase example, rather than from a text example like traditional KWS. This means that it is necessary to enroll keywords into the system from audio examples with localenroll before using QBE to search audio for these keywords or phrases. Enrollment follows the same format as enrolling speakers into a SID plugin, with the enrollment audio list following this format: <audio_file_path> <keyword_id> Example: /data/qbe/enroll/watermelon_example1.wav Watermelon /data/qbe/enroll/watermelon_example2.wav Watermelon /data/qbe/enroll/airplane_example.wav Airplane /data/qbe/enroll/keyword_example.wav Keyword Note that currently each enrollment audio file must contain ONLY the keyword that is desired to be enrolled. Also note that the text label in the second column of the enrollment file is only for user readability and is not used by the system when determining what to search the audio file for. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006","title":"Query by Example Keyword Spotting (QBE)"},{"location":"cli.html#topic-identification-tid","text":"Topic Identification plugins attempt to detect and categorize the topic being discussed within an audio recording from a known and pre-enrolled set of available topics, and report this topic (if any are detected) to the user. Each domain of a TID plugin is language-dependent, and should display the target language as the first string of the domain\u2019s name. Some TID plugins may be delivered with pre-enrolled topics \u2013 please consult the documentation that accompanied your delivery if you are unsure if this is the case. If no topics are enrolled, however, or if you wish to enroll new topics, the format is very similar to enrolling new speakers into a SID plugin, and follows the same CLI call structure, with one caveat. $ localenroll --local $domain $enroll_list Due to limitations with the current TID technology, enrollment must be performed with the --local flag set. This limits OLIVE to serialized processing, which will process the enrollment slightly slower, but avoid resource-competition issues that may cause the system to become unstable and crash. Enrollment audio lists for TID follow the same format as SID, but substitute a topic name for a speaker label. Note: in the current version, we require the user to provide audio examples that are primarily about the topic of interest as enrollment examples. If there are significant portions of an audio file that are off-topic, we suggest the file be cut and fed as separate examples. Enroll list format: <audio_file_path> <topic_id> Example: /tid/enroll/topic_example_audio_5760.wav Travel /tid/enroll/topic_example_audio_5761.wav Travel /tid/enroll/topic_example_audio_5762.wav Travel /tid/enroll/topic_example_audio_5763.wav Travel To run TID, once target topics have been enrolled, the call to localanalyze is very similar to other plugin types, with the current plugin again requiring the --local flag. $ localanalyze --local /path/to/plugins/tid-svm-v2/domains/r-tel-v1/ testAudio.lst As with the SID and LID plugins, by default the plugin\u2019s output will be written to the file \u201coutput.txt\u201d in the directory localanalyze was called from. This can be overridden by passing localanalyze the -o flag, as well as an alternate file to save the results to. The TID results structure is very similar to KWS, with the following format for each line: <audio_file_path> <start_time_s> <end_time_s> <topic_id> <confidence_score> Example: /data/tid-example_wavs/20110511_GET-TOGETHER.wav 18.790 55.300 transportation 0.0222 /data/tid-example_wavs/20110511_TRANSPORTATION.wav 4.010 19.140 transportation 0.4532 The start and end timestamps above are provided in seconds. The will be displayed as it was supplied in the second column of the enrollment list. The will be between 0 and 1, and marks the confidence of the system in the decision of this topic. Please note that output.txt will be overwritten by each successive experiment. Please back it up or use the -o option to localanalyze if you would like to save the results. Also note that the start and end times for each topic refer to the chunk in each audio that has the highest probability of being about that topic. Currently, we report only ONE such segment per file in order to help the user locate the most useful part. The score associated with that segment is global, in that it represents the likelihood that this topic is present anywhere in the document.","title":"Topic Identification (TID)"},{"location":"cli.html#important-background-example-information","text":"In order to train a Topic detector, we currently use an SVM classifier. This model uses \"positive\" examples of the topic as provided by the user during the enrollment phase, as well as \"negative\" examples to model what is not the topic. Those negative examples can be crucial to the performance of the final system. Currently, those \"negative\" examples come pre-processed as a python numpy archive and cannot be modified by the user explicitly. We do provide two different numpy archives that can be tried by a user: BG_RUS001-train-acc+neg-no-travel.npz (default) BG_RUS001-train-test-random-all-chunk-plugin.npz Archive 2) includes only data from the RUS001 conversational corpus, which didn't have very topic-specific prompts. Archive 1) includes a mix of RUS001 data as well as a subset of the RU_CTS conversational corpus which was topic annotated. We excluded examples pertaining to 'TRAVEL' in this archive, but this archive contains conversations about the following (loosely defined) topics: ACTIVITIES BIRTHDAY_WISHES CHILDREN ECONOMY EDUCATION ENTERTAINMENT FOOD_DRINK FRIENDS_RELATIVES GET-TOGETHER HEALTH HOME HOME_MAINTENANCE IMMIGRATION LANGUAGE_COMMUNICATION LEISURE LIFE_PHILOSOPHY_RELATIONSHIPS LOCATION_DESCRIPTION MARRIAGE MOOD_PHYSICAL MOVING_HOMES MUSIC PERFORMANCE_REHEARSAL PETS POLITICS PROJECT READING_WRITING RELIGION_HOLIDAY SPEECH_COLLECTION_PROJECT TECHNOLOGY TRANSPORTATION TV_MOVIES WEATHER_CLIMATE WORK If the topic you are training for is very similar to a topic listed above, it might be worth it trying archive (2) as well. In the future, we will provide the opportunity for the user to feed his own negative examples.","title":"Important Background Example Information"},{"location":"cli.html#gender-identification-gid","text":"Gender ID plugins allow for triage of audio files to identify only those containing speakers of a certain gender. For scoring files, gender identification plugins operate in the same manner as SID and LID plugins. GID plugins are delivered with two pre-enrolled classes; \u2018m\u2019 and \u2018f\u2019, for male and female, respectively, so user-side enrollment is not necessary. To score files with a GID plugin using localanalyze , use the following syntax: $ localanalyze /path/to/plugins/gid-gb-v1/domains/clean-v1/ testAudio.lst Where the output follows this format: <audio file 1> m <male likelihood score> <audio file 1> f <female likelihood score> \u2026 <audio file N> m <male likelihood score> <audio file N> f <female likelihood score> Example: /data/gender/m-testFile1.wav m 0.999999777927 /data/gender/m-testFile1.wav f -0.22073142865","title":"Gender Identification (GID)"},{"location":"cli.html#enhancement-enh","text":"Enhancement or AudioConverter plugins are audio-to-audio plugins that take an audio file as input and provide a second audio file as output. Currently they are used to enhance the input audio file, for listening comfort and/or intelligibility. By default, each output audio file is created in an OUTPUT directory in the location that localanalyze was invoked. Within the OUTPUT directory, the folder structure of the original input audio file is preserved. This means that if the input audio file was: /data/enhancement-test/test_file1.wav Then the enhanced output file will be found by default in: ./OUTPUT/data/enhancement-test/test_file1.wav Running speech enhancement is as simple as: $ localanalyze /path/to/plugins/enh-mmse-v1/domains/multi-v1/ inputAudio.lst In addition, you can pass an optional PEM file that specifies optional regions to the plugin \u2013 the current enhancement plugin uses this file to pass the \u2018noise regions\u2019 to OLIVE to allow the plugin to build a noise profile for more accurate characterization and removal of the noise present in that audio file.","title":"Enhancement (ENH)"},{"location":"cli.html#6-testing","text":"In this section you will find a description of benchmarking that is performed with each release of OLIVE to measure the performance of each plugin with respect to speed and memory usage. The hardware and software details used for these results are provided in the next section, with each of the current plugins\u2019 memory and speed results following.","title":"6: Testing"},{"location":"cli.html#a-benchmarking-setup-hardwaresoftware","text":"Each data point below was obtained by running the OLIVE 4.9.1 software with a 4.8.0 runtime that has been patched to 4.9.1 (contains libraries needed by the new Topic Identification and Enhancement plugins). Tests were performed on the CentOS 7 operating system, on a Gigabyte BRIX GB-BXI7-5500. This machine has 16GB RAM available, and an Intel i7-5500U processor, which is a dual core (quad-thread) processor that runs at 2.4 GHz base (3.0 GHz turbo).","title":"a.  Benchmarking Setup (Hardware/Software)"},{"location":"cli.html#b-plugin-memory-usage-results","text":"These results were generated by running several files through each plugin via the OLIVE CLI using localanalyze , one at a time, while measuring the memory used by the localanalyze utility. The files vary in length from 10 seconds through 4 hours, and this allows us to see how each plugin handles scaling the audio file length up, and also compare overall resource utilization between individual plugins. The values reported for each plugin and audio file are the peak memory usage, in MB \u2013 lower values are better. Note that this report is for processing a single file at a time and is representative of memory utilization that can be expected for serialized processing, or processing on a machine with a single processing core. Parallel processing can cause memory usage to rise. TODO: Need to put the charts/resuts in here!","title":"b.  Plugin Memory Usage Results"},{"location":"cli.html#sad","text":"","title":"SAD"},{"location":"cli.html#sad-memory-usage-mb","text":"Plugin sad-dnn-v4 sad-dnn-v4 sad-dnn-v4 Domain digPtt-v1 ptt-v1 tel-v1 10s 142 142 142 1 min 158 158 158 10 min 221 221 221 30 min 408 408 408 2 hr 1,262 1,262 1,262 4 hr 2,377 2,377 2,377","title":"SAD Memory Usage (MB)"},{"location":"cli.html#c-plugin-speed-analysis-results","text":"The following charts show the speed performance of the current release of each OLIVE plugin. Values are reported as the speed of the plugin in \u2018times faster than real time\u2019 and represent how fast the plugin is able to process the input audio data, with respect to the length of that data \u2013 higher is better. Each plugin is fed 10 hours of total data consisting of roughly 4-minute audio cuts to achieve this measurement. For this test, OLIVE has been limited to using a single core for processing, in order to keep measurements and results consistent. Note that enabling parallel processing if multiple CPU cores are available will improve performance.","title":"c.  Plugin Speed Analysis Results"},{"location":"cli.html#plugin-speed-statistics-reported-in-times-faster-than-real-time","text":"Plugin Domain Speed (x RT) sad-dnn-v4 digPtt-v1 104.8 sad-dnn-v4 ptt-v1 111.5 sad-dnn-v4 tel-v1 117.0 sid-embed-v2 multi-v1 51.6 sid-embedDnnTbc-v1 tbc-v1 43.9 sid-embedDnnTbc-v1 tbcdnn-v1 64.5 sid-embedTbc-v1 tbc-v1 40.2 lid-embed-v2 multi-v1 39.7 kws-batch-v9 eng-tel-v1 1.11 kws-batch-v9 eng-tel-v2 1.09 kws-batch-v9 f-tel-v1 1.23 kws-batch-v9 r-tel-v1 1.61 kws-batch-v9 r-tel-v2 1.61 qbe-tdnn-v4 digPtt-v1 15.2 qbe-tdnn-v4 multi-v1 16.1","title":"Plugin Speed Statistics Reported in Times Faster than Real Time"},{"location":"clients.html","text":"OLIVE Java and Python Clients Introduction Each OLIVE delivery includes two OLIVE client utilities - one written in Java, one written in Python. Out of the box, these tools allow a user to jump right in with running OLIVE if the GUI is not desired. These can also serve as code examples for integrating with OLIVE. This page primarily covers using these clients for processing audio, rather than integrating with the OLIVE API. For more information on integration, the nitty-gritty details of the OLIVE Enterprise API, and code examples, refer to these integration-focused pages instead: OLIVE Enterprise API Primer OLIVE Python Client API Documentation Integrating a Client API with OLIVE Building an OLIVE API Reference Implementation As far as the usage and capabilities of these tools, they were meant to mirror the Legacy CLI Tools as closely as possible, and shares many input/output formats and assumptions with those tools. As this document is still under construction, referring to this older guide may help fill in some useful information that may currently be missing from this page. Note that unlike the Legacy CLI tools, that are calling plugin code directly, these client tools require a running OLIVE Server. They are client utilities that are queueing and submitting job requests to the OLIVE server, which then manages the plugins themselves and actual audio processing. If you haven't already, please refer to the appropriate guide for setting up and starting an OLIVE server depending on your installation type: OLIVE Martini Docker-based Installation OLIVE Standalone Docker-based Installation Redhat/CentOS 7 Native Linux Installation OLIVE Server Guide Client Setup, Installation, Requirements As a quick review, the contents of an OLIVE package typically look like this: olive5.5.1/ api/ java/ python/ docs/ martini/ -or- docker/ -or- runtime/ OliveGUI/ - (Optional) The OLIVE Nightingale GUI (not included in all deliveries) oliveAppData/ The clients this page describes are contained in the bolded api/ directory above. Java (OliveAnalyze) The Java tools are the most full-featured with respect to tasking individual plugins. They are asynchronous, and better able to deal with large amounts of file submissions by parallelizing the submission of large lists of files. If the primary task is enrolling and scoring audio files with individual plugins, the Java tools, what we call the OliveAnalyze suite. The tools themselves do not need to be 'installed'. For convenience, their directory can be added to your $PATH environment variable, so that they can be called from anywhere: $ export PATH =$ PATH : < path >/ olive5 . 5.1 / api / java / bin / $ OliveAnalyze - h But they can also be left alone and called directly, as long as their full or relative path is present: # From inside olive5.5.1/api/java/bin: $ ./OliveAnalyze -h # From inside olive5.5.1/: $ ./api/java/bin/OliveAnalyze -h # From elsewhere: $ <path>/olive5.5.1/api/java/bin/OliveAnalyze -h These tools depend on OpenJDK 11 or newer being installed. Refer to OpenJDK for more information on downloading and installing this for your operating system. The full list of utilities in this suite are as follows: OliveAnalyze OliveAnalyzeText OliveEnroll OliveLearn (rarely used) But the most commonly used are OliveAnalyze for scoring requests, and OliveEnroll for enrollment requests. Examples are provided for each of these below, and for more advanced users that need different tools, each utility has its own help statement that can be accessed with the -h flag: $ OliveAnalyzeText -h The arguments and formatting for each tool is very similar, so familiarity with the OliveAnalyze and OliveEnroll examples below should allow use of most of these tools. Python (olivepyanalyze) The Python client, what we call the olivepyanalyze suite, is not as fully-featured with respect to batch-processing of audio files. It performs synchronous requests to the OLIVE server, and so it will sequentially score each provided audio file, rather than submitting jobs in parallel. For this reason, the Java OliveAnalyze tools are recommended for batch processing of individual plugin tasks. Only the python client currently has workflow support, however, and the python workflow client utility, olivepyworkflow is not limited by the synchronous restriction of olivepyanalyze - so when operating with workflows it is the clear choice. The python client tools require Python 3.8 or newer - please refer to Python for downloading and installing Python. Installing these tools has been simplified by providing them in the form of a Python wheel, that can be easily installed with pip . $ cd olive5.5.1/api/python $ ls olivepy-5.5.1-py3-none-any.whl olivepy-5.5.1.tar.gz requirements.txt $ python3 -m pip install -r requirements.txt olivepy-5.5.1-py3-none-any.whl This will fetch and install (if necessary) the olivepy dependencies, and install the olivepy tools. Those dependencies are: protobuf soundfile numpy zmq The olivepy utilities closely mirror the Java utilities, with the addition of the workflow tool, and are as follows: olivepyanalyze olivepyenroll olivepylearn (rarely used) olivepyworkflow olivepyworkflowenroll The olivepyworkflow tools are the most important, and examples are provided below for both scoring with olivepyworkflow and enrollment with olivepyworkflowenroll . We also provide examples for olivepyanalyze and olivepyenroll that mirror the Java examples. Scoring/Analysis Requests Plugin Scoring Types In general, the output format will depend on the type of \u2018scorer\u2019 the plugin being used is. For a deeper dive into OLIVE scoring types, please refer to the appropriate section in the OLIVE Plugin Traits Guide , but a brief overview follows. The most common types of plugins in OLIVE are: Global Scorer Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Speaker and Language Identification are examples of global scorers. OLIVE typically calls a global scoring plugin an \"Identification\" plugin, whereas a region scoring plugin to pinpoint the same class types would instead be called a \"Detection\" plugin. For example, Speaker Identification versus Speaker Detection; the former assumes the entire audio contains a single speaker, where the latter makes no such assumption, and attempts to localize any detected speakers of interest. Global Scorer Output In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> For example, a Speaker Identification analysis run, with three enrolled speakers (Alex, Taylor, Blake) might return: /data/sid/audio/file1.wav Alex -0.5348 /data/sid/audio/file1.wav Taylor 3.2122 /data/sid/audio/file1.wav Blake -5.5340 /data/sid/audio/file2.wav Alex 0.5333 /data/sid/audio/file2.wav Taylor -4.9444 /data/sid/audio/file2.wav Blake -2.6564 Note the actual meanings of the scores and available classes will vary from plugin-to-plugin. Please refer to individual plugin documentation for more guidance on what the scores mean and what ranges are acceptable. Also note that the output format described here is literally what will be returned when calling a plugin directly with OliveAnalyze or olivepyanalyze - but when performing a global-scoring task as part of analysis with a workflow,these same informational pieces (audio_path or object, class_id, score) are still provided, but packed into a json structure. Region scorer Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. Automatic Speech Recognition (ASR), Language Detection (LDD), and Speaker Detection (SDD) are all region scorers. OLIVE typically calls a global scoring plugin an \"Identification\" plugin, whereas a region scoring plugin to pinpoint the same class types would instead be called a \"Detection\" plugin. For example, Speaker Identification versus Speaker Detection; the former assumes the entire audio contains a single speaker, where the latter makes no such assumption, and attempts to localize any detected speakers of interest. Region Scorer Output Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: < audio_path > < region_start_timestamp > < region_end_timestamp > < class_id > < score > For example, a language detection plugin might output something like this: /data/mixed-language/testFile1.wav 2.170 9.570 Arabic 0.912 /data/mixed-language/testFile1.wav 10.390 15.930 French 0.693 /data/mixed-language/testFile1.wav 17.639 22.549 English 0.832 /data/mixed-language/testFile2.wav 0.142 35.223 Pashto 0.977 Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. More specific examples can be found in the respective plugin-specific documentation pages. As with global scoring, note that the output format described here is literally what will be returned when calling a plugin directly with OliveAnalyze or olivepyanalyze - but when performing a region-scoring task as part of analysis with a workflow,these same informational pieces (audio_path or object, region start and end timestamps, class_id, score) are still provided, but packed into a json structure. Plugin Direct (Analysis) Performing an analysis request with both tools is very similar, as the tools were designed to closely mirror each other so that familiarity with one would easily transfer to the other. The usage statements for each can be examined by invoking each with their -h or --help flag: OliveAnalyze (Java) $ ./OliveAnalyze -h usage: OliveAnalyze --align Perform audio alignment analysis. Must specify the two files to compare using an input list file via the--list argument --apply_update Request the plugin is update ( if supported ) --box Perform bounding box analysis. Must specify an image or video input --channel <arg> Process stereo files using channel NUMBER --class_ids <arg> Use Class ( s ) from FILE for scoring. Each line in the file contains a single class, including any white space --compare Perform audio compare analysis. Must specify the two files to compare using an input list file via the--list argument --decoded Send audio file as decoded PCM16 samples instead of sending as serialized buffer. Input file must be a wav file --domain <arg> Use Domain NAME --enhance Perform audio conversion ( enhancement ) --frame Perform frame scoring analysis --global Perform global scoring analysis -h Print this help message -i,--input <arg> NAME of the input file ( audio/video/image as required by the plugin --input_list <arg> Use an input list FILE having multiple filenames/regions or PEM formatted -l,--load load a plugin now, must use --plugin and --domain to specify the plugin/domain to preload --options <arg> options from FILE --output <arg> Write any output to DIR, default is ./ -p,--port <arg> Scenicserver port number. Defauls is 5588 --path Send audio file path instead of a buffer. Server and client must share a filesystem to use this option --plugin <arg> Use Plugin NAME --print <arg> Print all available plugins and domains. Optionally add 'verbose' as a print option to print full plugin details including traits and classes -r,--unload unload a loaded plugin now, must use --plugin and --domain to specify the plugin/domain to unload --region Perform region scoring analysis -s,--server <arg> Scenicserver hostname. Default is localhost --shutdown Request a clean shutdown of the server --status Print the current status of the server -t,--timeout <arg> timeout ( in seconds ) when waiting for server response. Default is 10 seconds --threshold <arg> Apply threshold NUMBER when scoring --update_status Get the plugin ' s update status -v,--vec <arg> PATH to a serialized AudioVector, for plugins that support audio vectors in addition to wav files --vector Perform audio vectorization --workflow Request a workflow olivepyanalyze (Python) $ olivepyanalyze ---h usage: olivepyanalyze [ -h ] [ -C CLIENT_ID ] [ -p PLUGIN ] [ -d DOMAIN ] [ -G ] [ -e ] [ -f ] [ -g ] [ -r ] [ -b ] [ -P PORT ] [ -s SERVER ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --text ] [ --options OPTIONS ] [ --class_ids CLASS_IDS ] [ --debug ] [ --path ] [ --print ] optional arguments: -h, --help show this help message and exit -C CLIENT_ID, --client-id CLIENT_ID Experimental: the client_id to use -p PLUGIN, --plugin PLUGIN The plugin to use. -d DOMAIN, --domain DOMAIN The domain to use -G, --guess Experimental: guess the type of analysis to use based on the plugin/domain. -e, --enhance Enhance the audio of a wave file, which must be passed in with the --wav option. -f, --frame Do frame based analysis of a wave file, which must be passed in with the --wav option. -g, --global Do global analysis of a wave file, which must be passed in with the --wav option. -r, --region Do region based analysis of a wave file, which must be passed in with the --wav option. -b, --box Do bounding box based analysis of an input file, which must be passed in with the --wav option. -P PORT, --port PORT The port to use. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -t TIMEOUT, --timeout TIMEOUT The timeout to use -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --text Indicates that input ( or input list ) is a literal text string to send in the analysis request. --options OPTIONS Optional file containing plugin properties ans name/value pairs. --class_ids CLASS_IDS Optional file containing plugin properties ans name/value pairs. --debug Debug mode --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option --print Print all available plugins and domains To perform a scoring request with these tools, you will need these essential pieces of information: Plugin name ( --plugin ) Domain name ( --domain ) Scoring type to perform ( --region for region-scoring, --global for global-scoring, others for less-common plugins) Input audio file or list of input audio files ( --input for a single file, --input_list for a list of files) The flag for providing each piece of information is the same for both tools, as shown in the list above. For more information on what the difference is between a plugin and a domain, refer to the Plugins Overview . For more information on the domains available for each plugin, refer to documentation page for that specific plugin. To see which plugins and domains you have installed and running in your specific OLIVE environment, refer to the server startup status message, that appears when you start the server: martini.sh start , then martini.sh log once the server is running for martini-based OLIVE packages (most common) ./run.sh for non-martini docker OLIVE packages oliveserver for native linux OLIVE packages Or exercise the --print option for each tool to query the server and print the available plugins and domains: OliveAnalyze (Java) $ ./OliveAnalyze --print olivepyanalyze (python) $ olivepyanalyze --print Example output: 2022-06-14 12:12:25.786 INFO com.sri.speech.olive.api.Server - Connected to localhost - request port: 5588 status_port: 5589 Found 8 plugin(s): Plugin: sad-dnn-v7.0.2 (SAD,Speech) v7.0.2 has 2 domain(s): Domain: fast-multi-v1, Description: Trained with Telephony, PTT and Music data Domain: multi-v1, Description: Trained with Telephony, PTT and Music data Plugin: asr-dynapy-v3.0.0 (ASR,Content) v3.0.0 has 9 domain(s): Domain: english-tdnnChain-tel-v1, Description: Large vocabulary English DNN model for 8K data Domain: farsi-tdnnChain-tel-v1, Description: Large vocabulary Farsi DNN model for 8K data Domain: french-tdnnChain-tel-v2, Description: Large vocabulary African French DNN Chain model for 8K data Domain: iraqiArabic-tdnnChain-tel-v1, Description: Large vocabulary Iraqi Arabic DNN Chain model for 8K data Domain: levantineArabic-tdnnChain-tel-v1, Description: Large vocabulary Levantine Arabic DNN Chain model for 8K data Domain: mandarin-tdnnChain-tel-v1, Description: Large vocabulary Mandarin DNN model for clean CTS 8K data Domain: pashto-tdnnChain-tel-v1, Description: Large vocabulary Pashto DNN Chain model for 8K data Domain: russian-tdnnChain-tel-v2, Description: Large vocabulary Russian DNN model for 8K data Domain: spanish-tdnnChain-tel-v1, Description: Large vocabulary Spanish DNN model for clean CTS 8K data Plugin: sdd-diarizeEmbedSmolive-v1.0.0 (SDD,Speaker) v1.0.0 has 1 domain(s): Domain: telClosetalk-int8-v1, Description: Speaker Embeddings Framework Plugin: tmt-neural-v1.0.0 (TMT,Content) v1.0.0 has 3 domain(s): Domain: cmn-eng-nmt-v1, Description: Mandarin Chinese to English NMT Domain: rus-eng-nmt-v1, Description: Russian to English NMT Domain: spa-eng-nmt-v3, Description: Spanish to English NMT Plugin: ldd-embedplda-v1.0.1 (LDD,Language) v1.0.1 has 1 domain(s): Domain: multi-v1, Description: PNCC bottleneck domain suitable for mixed conditions (tel/mic/compression) Plugin: sdd-diarizeEmbedSmolive-v1.0.2 (SDD,Speaker) v1.0.2 has 1 domain(s): Domain: telClosetalk-smart-v1, Description: Speaker Embeddings Framework Plugin: sid-dplda-v2.0.2 (SID,Speaker) v2.0.2 has 1 domain(s): Domain: multi-v1, Description: Speaker Embeddings DPLDA Plugin: lid-embedplda-v3.0.1 (LID,Language) v3.0.1 has 1 domain(s): Domain: multi-v1, Description: PNCC Bottleneck embeddings suitable for mixed conditions (tel/mic/compression) Examples To perform a global score analysis on a single file with the speaker identification plugin sid-dplda-v2.0.2 , using the multi-v1 domain, the calls for each would look like this: OliveAnalyze (Java) $ ./OliveAnalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav olivepyanalyze (python) $ olivepyanalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav Performing region scoring instead, using a transcription plugin, asr-dynapy-v3.0.0 , via the english domain english-tdnnChain-tel-v1 on a list of audio files would be performed with: OliveAnalyze (Java) $ ./OliveAnalyze --plugin asr-dynapy-v3.0.0 --domain english-tdnnChain-tel-v1 --region --input_list ~/path/to/list-of-audio-files.txt olivepyanalyze (python) $ olivepyanalyze --plugin asr-dynapy-v3.0.0 --domain english-tdnnChain-tel-v1 --region --input_list ~/path/to/list-of-audio-files.txt Where the format of the input list is simply a text file with a path to an audio file on each line. For example: /data/mixed-language/testFile1.wav /data/mixed-language/testFile2.wav /data/mixed-language/testFile3.wav /moreData/test-files/unknown1.wav Workflow (Analysis) OLIVE Workflows provide a simple way of creating a sort of 'recipe' that specifies how to deal with the input data and one or more OLIVE plugins. It allows complex operations to be requested and performed with a single, simple call to the system by allowing complexities and specific knowledge to be encapsulated within the workflow itself, rather than known and implemented by the user at run time. Due to off-loading this burden, operating with workflows is much simpler than calling the plugin(s) directly - typically all that is needed from the user to request an analysis from olivepyworkflow is the workflow itself, and one or more input files. There are more options available to the olivepyworkflow client, as shown in the usage statement: $ olivepyworkflow usage: olivepyworkflow [ -h ] [ --tasks ] [ --class_ids ] [ --print_actualized ] [ --print_workflow ] [ -s SERVER ] [ -P PORT ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --text ] [ --options OPTIONS ] [ --path ] [ --debug ] workflow Perform OLIVE analysis using a Workflow Definition file positional arguments: workflow The workflow definition to use. optional arguments: -h, --help show this help message and exit --tasks Print the workflow analysis tasks. --class_ids Print the class IDs available for analysis in the specified workflow. --print_actualized Print the actualized workflow info. --print_workflow Print the workflow definition file info ( before it is actualized, if requested ) -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. -t TIMEOUT, --timeout TIMEOUT The timeout ( in seconds ) to wait for a response from the server -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --text Indicates that input ( or input list ) is a literal text string to send in the analysis request. --options OPTIONS A JSON formatted string of workflow options such as [{ \"task\" : \"SAD\" , \"options\" : { \"filter_length\" :99, \"interpolate\" :1.0 }] or { \"filter_length\" :99, \"interpolate\" :1.0, \"name\" : \"midge\" } , where the former options are only applied to the SAD task, and the later are applied to all tasks --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option --debug Debug mode But their use rarely necessary, and is reserved for advanced users or specific system testing. Generically, calling olivepyworkflow will look like this: Single Input File $ olivepyworkflow --input ~/path/to/test-file1.wav <workflow> List of Input Files $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt <workflow> As an example of the power of workflows, this request calls the SmartTranscription workflow - that performs Speech Activity Detection (region scoring), Speaker Diarization and Detection (region scoring), Language Detection (region scoring), and then Automatic Speech Recognition (region scoring) on any sections of each input file that are detected to be a language that ASR currently has support for, and returns all of the appropriate results in a JSON structure. Performing this same task by calling plugins directly, this same functionality would be a minimum of 4 separate calls to OLIVE; significantly more if more than one language is detected being spoken in the file. Single Input File $ olivepyworkflow --input ~/path/to/test-file1.wav ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json List of Input Files $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json Output Format (Workflow) Workflows are generally customer/user-specific and can be quite specialized - the output format and structure will depend heavily on the individual workflow itself and the tasks being performed. All of the information pieces that define each scoring type are still reported for each result, but the results are organized into a single JSON structure for the workflow call. This means that the output of a region scoring plugin within the workflow is still one or more sets of: < region_start_timestamp > < region_end_timestamp > < class_id > < score > But the data is arranged into the JSON structure and will be nested depending on the structure of the workflow itself and how the audio is routed by the workflow. For more detailed information on the structure of this JSON message and the inner-workings of workflows, please refer to the OLIVE Workflow API documentation. A brief, simplified summary to jump start working with workflow output follows. The main skeleton structure of the results output is shown below, along with an actual example. The results are provided as a result for each input file, that lists the job name(s), some metadata about the input audio and how it was processed, and then the returned results (if any) for each task, which is generally a plugin. Simplified/Generic Output Example (single-job workflow) Work fl ow a nal ysis resul ts : [ { \"job_name\" : <work fl ow job na me> , \"data\" : [ { \"data_id\" : <da ta ID , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <da ta label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ], < tas k 2 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ], < tas k 3 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ] } } ] English Transcription Output Example (single-job workflow) Work fl ow a nal ysis resul ts : [ { \"job_name\" : \"SAD, SDD, and ASR English Workflow\" , \"data\" : [ { \"data_id\" : \"z_eng_englishdemo.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 5.932625 , \"number_channels\" : 1 , \"label\" : \"z_eng_englishdemo.wav\" , \"id\" : \"0b04c7497521d53a5d6939533a55c461795f9d685b1bd19fd9031fc6f3997a8f\" } ], \"tasks\" : { \"SAD_REGIONS\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v7.0.2\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 5.93 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"SDD\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SDD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sdd-diarizeEmbedSmolive-v1.0.2\" , \"domain\" : \"telClosetalk-smart-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.1 , \"end_t\" : 5.0 , \"class_id\" : \"unknownspk00\" , \"score\" : 1.4 } ] } } ], \"ASR\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"ASR\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"asr-dynapy-v3.0.0\" , \"domain\" : \"english-tdnnChain-tel-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.15 , \"end_t\" : 0.51 , \"class_id\" : \"hello\" , \"score\" : 100.0 }, { \"start_t\" : 0.54 , \"end_t\" : 0.69 , \"class_id\" : \"my\" , \"score\" : 100.0 }, { \"start_t\" : 0.69 , \"end_t\" : 0.87 , \"class_id\" : \"name\" , \"score\" : 99.0 }, { \"start_t\" : 0.87 , \"end_t\" : 1.05 , \"class_id\" : \"is\" , \"score\" : 99.0 }, { \"start_t\" : 1.05 , \"end_t\" : 1.35 , \"class_id\" : \"evan\" , \"score\" : 88.0 }, { \"start_t\" : 1.35 , \"end_t\" : 1.47 , \"class_id\" : \"this\" , \"score\" : 99.0 }, { \"start_t\" : 1.5 , \"end_t\" : 1.98 , \"class_id\" : \"audio\" , \"score\" : 95.0 }, { \"start_t\" : 1.98 , \"end_t\" : 2.16 , \"class_id\" : \"is\" , \"score\" : 74.0 }, { \"start_t\" : 2.16 , \"end_t\" : 2.31 , \"class_id\" : \"for\" , \"score\" : 99.0 }, { \"start_t\" : 2.31 , \"end_t\" : 2.4 , \"class_id\" : \"the\" , \"score\" : 99.0 }, { \"start_t\" : 2.4 , \"end_t\" : 2.91 , \"class_id\" : \"purposes\" , \"score\" : 100.0 }, { \"start_t\" : 2.91 , \"end_t\" : 3.06 , \"class_id\" : \"of\" , \"score\" : 99.0 }, { \"start_t\" : 3.12 , \"end_t\" : 3.81 , \"class_id\" : \"demonstrating\" , \"score\" : 100.0 }, { \"start_t\" : 3.81 , \"end_t\" : 3.96 , \"class_id\" : \"our\" , \"score\" : 78.0 }, { \"start_t\" : 4.05 , \"end_t\" : 4.44 , \"class_id\" : \"language\" , \"score\" : 100.0 }, { \"start_t\" : 4.44 , \"end_t\" : 4.53 , \"class_id\" : \"and\" , \"score\" : 93.0 }, { \"start_t\" : 4.53 , \"end_t\" : 4.89 , \"class_id\" : \"speaker\" , \"score\" : 100.0 }, { \"start_t\" : 4.89 , \"end_t\" : 5.01 , \"class_id\" : \"i.\" , \"score\" : 99.0 }, { \"start_t\" : 5.01 , \"end_t\" : 5.22 , \"class_id\" : \"d.\" , \"score\" : 99.0 }, { \"start_t\" : 5.22 , \"end_t\" : 5.85 , \"class_id\" : \"capabilities\" , \"score\" : 99.0 } ] } } ] } } ] Each task output will typically be for a single plugin, and will be outputting the information provided by a Region Scorer or Global Scorer or Text Transformer in the case of Machine Translation, depending on how the workflow is using the plugin. The format of each result sub part is: Global Score < tas k na me> : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : < tas k t ype , ge nerall y LID , SID , e t c.> , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"score\" : [ { \"class_id\" : <class 1 > , \"score\" : <class 1 score> }, { \"class_id\" : <class 2 > , \"score\" : <class 2 score> }, ... { \"class_id\" : <class N> , \"score\" : <class N score> } ] } } ] Region Score < tas k na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype , t ypically ASR , SDD , SAD , e t c.> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 de te c te d class> , \"score\" : <regio n 1 score> }, { \"start_t\" : <regio n 2 s tart t ime (s)> , \"end_t\" : <regio n 2 e n d t ime (s)> , \"class_id\" : <regio n 2 de te c te d class> , \"score\" : <regio n 2 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N de te c te d class> , \"score\" : <regio n N score> }, ] } } ] Text Transformer < tas k na me> : [ { \"task_trait\" : \"TEXT_TRANSFORMER\" , \"task_type\" : < tas k t ype , t ypically MT> , \"message_type\" : \"TEXT_TRANSFORM_RESULT\" , \"plugin\" : <plugi n na me> , \"domain\" : <domai n na me> , \"analysis\" : { \"transformation\" : [ { \"class_id\" : \"test_label\" , \"transformed_text\" : < t he translate d/ transf ormed te x t re turne d fr om t he plugi n > } ] } } ] Many workflows consist of a single job , and bundle all plugin tasks into this single job, as seen above. More complex workflows, or what OLIVE calls \"Conditional Workflows\" can pack multiple jobs into a single workflow. This happens when there are certain tasks in the workflow that depend on other tasks in the workflow - for example when OLIVE needs to choose the appropriate Speech Recognition (ASR) language to use, depending on what language is detected being spoken by Language Identification (LID) or Language Detection (LDD). In this case, the LID/LDD is separated into one job, and the ASR into another, that is triggered to run once LID/LDD's decision is known. In this case, the results from each job are grouped accordingly in the results output. Below shows a simplified output from a workflow that includes three jobs; \"job 1\", \"job 2\", \"job 3\", and a real-life example output from the SmartTranscription conditional workflow that also has three jobs; the first performs Speech Activity Detection and Language Identification (LID); Smart Translation SAD and LID Pre-processing the second uses the language decision from Language Identification to choose the appropriate (if any) language and domain for Automatic Speech Recognition (ASR) and runs that, Dynamic ASR the third takes the output transcript from ASR and the language decision from LID and choose the appropriate (if any) language and domain for Text Machine Translation, and runs that. Dynamic MT As you can see below, these jobs are listed separately in the JSON for each result: Simplified/Generic Output Example (multi-job workflow) Work fl ow a nal ysis resul ts : [ { \"job name\" : <job 1 na me> , \"data\" : [ { \"data_id\" : <da ta ide nt i f ier , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <audio label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me (job 1 )> : [ { < tas k 1 resul ts > } ], ... < tas k N na me (job 1 )> : [ { < tas k N resul ts > } ] } }, { \"job name\" : <job 2 na me> , \"data\" : [ { \"data_id\" : <da ta ide nt i f ier , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <audio label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me (job 2 )> : [ { < tas k 1 resul ts > } ], ... < tas k N na me (job 2 )> : [ { < tas k N resul ts > } ] } }, ... <repea t i f more jobs> ] SmartTranslation Output Example Work fl ow a nal ysis resul ts : [ { \"job_name\" : \"Smart Translation SAD and LID Pre-processing\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 8.0 , \"number_channels\" : 1 , \"label\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"id\" : \"68984a7356fa1ea05f8e985868eb93e066ce80a0f4bf848edf55d547cfcbab41\" } ], \"tasks\" : { \"SAD_REGIONS\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v7.0.2\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 8.0 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"LID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"lid-embedplda-v3.0.1\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"Mandarin\" , \"score\" : 3.5306692 }, { \"class_id\" : \"Korean\" , \"score\" : -1.9072952 }, { \"class_id\" : \"Japanese\" , \"score\" : -3.7805116 }, { \"class_id\" : \"Tagalog\" , \"score\" : -7.4819508 }, { \"class_id\" : \"Vietnamese\" , \"score\" : -8.094855 }, { \"class_id\" : \"Iraqi Arabic\" , \"score\" : -10.63325 }, { \"class_id\" : \"Levantine Arabic\" , \"score\" : -10.694491 }, { \"class_id\" : \"French\" , \"score\" : -11.542379 }, { \"class_id\" : \"Pashto\" , \"score\" : -12.11981 }, { \"class_id\" : \"English\" , \"score\" : -12.323014 }, { \"class_id\" : \"Modern Standard Arabic\" , \"score\" : -12.626052 }, { \"class_id\" : \"Spanish\" , \"score\" : -13.469315 }, { \"class_id\" : \"Iranian Persian\" , \"score\" : -13.763366 }, { \"class_id\" : \"Amharic\" , \"score\" : -17.129797 }, { \"class_id\" : \"Portuguese\" , \"score\" : -17.31257 }, { \"class_id\" : \"Russian\" , \"score\" : -18.770994 } ] } } ] } }, { \"job_name\" : \"Dynamic ASR\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 8.0 , \"number_channels\" : 1 , \"label\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"id\" : \"68984a7356fa1ea05f8e985868eb93e066ce80a0f4bf848edf55d547cfcbab41\" } ], \"tasks\" : { \"ASR\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"ASR\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"asr-dynapy-v3.0.0\" , \"domain\" : \"mandarin-tdnnChain-tel-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 0.18 , \"class_id\" : \"\u8ddf\" , \"score\" : 31.0 }, { \"start_t\" : 0.18 , \"end_t\" : 0.36 , \"class_id\" : \"\u4e00\u4e2a\" , \"score\" : 83.0 }, { \"start_t\" : 0.36 , \"end_t\" : 0.66 , \"class_id\" : \"\u80af\u5b9a\" , \"score\" : 100.0 }, { \"start_t\" : 0.66 , \"end_t\" : 0.81 , \"class_id\" : \"\u662f\" , \"score\" : 83.0 }, { \"start_t\" : 0.81 , \"end_t\" : 1.23 , \"class_id\" : \"\u5317\u4eac\" , \"score\" : 95.0 }, { \"start_t\" : 1.23 , \"end_t\" : 1.47 , \"class_id\" : \"\u554a\" , \"score\" : 96.0 }, { \"start_t\" : 2.07 , \"end_t\" : 2.49 , \"class_id\" : \"\u4ed6\u4fe9\" , \"score\" : 96.0 }, { \"start_t\" : 2.7 , \"end_t\" : 3.09 , \"class_id\" : \"\u4e0a\u6d77\" , \"score\" : 99.0 }, { \"start_t\" : 3.09 , \"end_t\" : 3.21 , \"class_id\" : \"\u7684\" , \"score\" : 99.0 }, { \"start_t\" : 3.21 , \"end_t\" : 3.57 , \"class_id\" : \"\u4eba\u53e3\" , \"score\" : 99.0 }, { \"start_t\" : 3.57 , \"end_t\" : 3.87 , \"class_id\" : \"\u597d\u50cf\" , \"score\" : 73.0 }, { \"start_t\" : 3.87 , \"end_t\" : 3.99 , \"class_id\" : \"\u6ca1\" , \"score\" : 54.0 }, { \"start_t\" : 3.99 , \"end_t\" : 4.32 , \"class_id\" : \"\u5317\u4eac\" , \"score\" : 74.0 }, { \"start_t\" : 4.32 , \"end_t\" : 4.68 , \"class_id\" : \"\u591a\" , \"score\" : 99.0 }, { \"start_t\" : 4.86 , \"end_t\" : 5.19 , \"class_id\" : \"\u4f46\u662f\" , \"score\" : 100.0 }, { \"start_t\" : 5.4 , \"end_t\" : 5.91 , \"class_id\" : \"\u4e0d\u77e5\u9053\" , \"score\" : 100.0 }, { \"start_t\" : 6.06 , \"end_t\" : 6.48 , \"class_id\" : \"@reject@\" , \"score\" : 62.0 }, { \"start_t\" : 6.69 , \"end_t\" : 7.05 , \"class_id\" : \"\u5176\u4ed6\" , \"score\" : 93.0 }, { \"start_t\" : 7.05 , \"end_t\" : 7.2 , \"class_id\" : \"\u7684\" , \"score\" : 97.0 }, { \"start_t\" : 7.65 , \"end_t\" : 7.89 , \"class_id\" : \"\u4e0a\" , \"score\" : 32.0 }, { \"start_t\" : 7.89 , \"end_t\" : 7.95 , \"class_id\" : \"\u554a\" , \"score\" : 67.0 } ] } } ] } }, { \"job_name\" : \"Dynamic MT\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"WORKFlOW_TEXT_RESULT\" , \"text\" : \"\u8ddf \u4e00\u4e2a \u80af\u5b9a \u662f \u5317\u4eac \u554a \u4ed6\u4fe9 \u4e0a\u6d77 \u7684 \u4eba\u53e3 \u597d\u50cf \u6ca1 \u5317\u4eac \u591a \u4f46\u662f \u4e0d\u77e5\u9053 @reject@ \u5176\u4ed6 \u7684 \u4e0a \u554a\" } ], \"tasks\" : { \"MT\" : [ { \"task_trait\" : \"TEXT_TRANSFORMER\" , \"task_type\" : \"MT\" , \"message_type\" : \"TEXT_TRANSFORM_RESULT\" , \"plugin\" : \"tmt-neural-v1.0.0\" , \"domain\" : \"cmn-eng-nmt-v1\" , \"analysis\" : { \"transformation\" : [ { \"class_id\" : \"test_label\" , \"transformed_text\" : \"with someone in beijing they don't seem to have a population in shanghai but we don't know what else to do\" } ] } } ] } } ] Enrollment Requests Enrollments are a sub-set of classes that the user can create and/or modify. These are used for classes that cannot be known ahead of time and therefore can't be pre-loaded into the system, such as specific speakers or keywords of interest. To determine if a plugin supports or requires enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page, linked from the navigation or the Release Plugins page. Enrollment list format As with analysis, both the Java and Python tools were designed to share as much of a common interface as possible, and as such share an input list format when providing exemplars for enrollment. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TPD, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in each plugin's documentation. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7 Plugin Direct (Enrollment) Performing an enrollment request is similar to an analysis request and is again very similar between the two tools. The usage statements for each can be examined by invoking each with their -h or --help flag: OliveEnroll (Java) $ ./OliveEnroll -h usage: OliveEnroll --channel <arg> Process stereo files using channel NUMBER --classes Print class names if also printing plugin/domain names. Must use with --print option. Default is to not print class IDs --decoded Sennd audio file as a decoded PCM16 sample buffer instead of a serialized buffer. The file must be a WAV file --domain <arg> Use Domain NAME --enroll <arg> Enroll speaker NAME. If no name specified then , the pem or list option must specify an input file --export <arg> Export speaker NAME to an EnrollmentModel ( enrollment.tar.gz ) -h Print this help message -i,--input <arg> NAME of the input file ( input varies by plugin: audio, image, or video ) --import <arg> Import speaker from EnrollmentModel FILE --input_list <arg> Batch enroll using this input list FILE having multiple filenames/class IDs or PEM formmated file --nobatch Disable batch enrollment when using pem or list input files, so that files are processed serially --options <arg> Enrollment options from FILE --output <arg> Write any output to DIR, default is ./ -p,--port <arg> Scenicserver port number. Defauls is 5588 --path Send the path to the audio file instead of a ( serialized ) buffer. The server must have access to this path. --plugin <arg> Use Plugin NAME --print Print all plugins and domains that suport enrollment and/or class import and export --remove <arg> Remove audio enrollment for NAME olivepyenroll (Python) $ olivepyenroll -h usage: olivepyenroll [ -h ] [ -C CLIENT_ID ] [ -D ] [ -p PLUGIN ] [ -d DOMAIN ] [ -e ENROLL ] [ -u UNENROLL ] [ -s SERVER ] [ -P PORT ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --path ] optional arguments: -h, --help show this help message and exit -C CLIENT_ID, --client-id CLIENT_ID Experimental: the client_id to use -D, --debug The domain to use -p PLUGIN, --plugin PLUGIN The plugin to use. -d DOMAIN, --domain DOMAIN The domain to use -e ENROLL, --enroll ENROLL Enroll with this name. -u UNENROLL, --unenroll UNENROLL Uneroll with this name. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. -t TIMEOUT, --timeout TIMEOUT The timeout to use -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option To perform an enrollment request with these tools, you will need these essential pieces of information: Plugin name ( --plugin ) Domain name ( --domain ) One of: A properly formatted enrollment list ( --input_list ), if providing multiple files at once (see below for formatting) An input audio file ( --input ) AND the name of the class you wish to enroll ( --enroll for OliveEnroll , -e or --enroll for olivepyanalyze ) Generically, this looks like this for a single file input: OliveEnroll (Java) $ ./OliveEnroll --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> olivepyenroll (python) $ olivepyenroll --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> A more specific example: OliveEnroll (Java) $ ./OliveEnroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" olivepyenroll (python) $ olivepyenroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" Or if providing the enrollment list format shown above, the call is even simpler. Generically: OliveEnroll (Java) $ ./OliveEnroll --plugin <plugin> --domain <domain> --input_list <path to enrollment text file> olivepyenroll (python) $ olivepyenroll --plugin <plugin> --domain <domain> --input_list <path to enrollment text file> Specific: OliveEnroll (Java) $ ./OliveEnroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input_list ~/path/to/enrollment_input.txt olivepyenroll (python) $ olivepyenroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input_list ~/path/to/enrollment_input.txt Where the enrollment_input.txt file might look like: /some/data/somewhere/inputFile1.wav Logan /some/other/data/somewhere/else/LoganPodcast.wav Logan /yet/another/data/directory/charlie-speaks.wav Charlie Workflow (Enrollment) In the most basic case, enrollment using a workflow is just as simple as scoring with a workflow. This is becuase most workflows will only have a single enrollment-capable job; a job is a subset of the the tasks a workflow is performing, typically linked to a single plugin. In the rare case that you're using a workflow with multiple supported enrollment jobs, you will need to specify which job to enroll to. See the Advanced Workflow Enrollment section below. Workflow enrollment is performed by using the olivepyworkflowenroll utility, whose help/usage statement is: olivepyworkflowenroll usage $ olivepyworkflowenroll -h usage: olivepyworkflowenroll [ -h ] [ --print_jobs ] [ --job JOB ] [ --enroll ENROLL ] [ --unenroll UNENROLL ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --path ] [ -s SERVER ] [ -P PORT ] [ -t TIMEOUT ] workflow Perform OLIVE enrollment using a Workflow Definition file positional arguments: workflow The workflow definition to use. optional arguments: -h, --help show this help message and exit --print_jobs Print the supported workflow enrollment jobs. --job JOB Enroll/Unenroll an Class ID for a job ( s ) in the specified workflow. If not specified enroll or unenroll for ALL enrollment/unenrollment jobs --enroll ENROLL Enroll using this ( class ) name. Should be used with the job argument to specify a target job to enroll with ( if there are more than one enrollment jobs ) --unenroll UNENROLL Enroll using this ( class ) name. Should be used with the job argument to specify a job to unenroll ( if there are more than one unenrollment jobs ) -i INPUT, --input INPUT The data input to enroll. Either a pathname to an audio/image/video file or a string for text input --input_list INPUT_LIST A list of files to enroll. One file per line plus the class id to enroll. --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. -t TIMEOUT, --timeout TIMEOUT The timeout ( in seconds ) to wait for a response from the server If there is only one supported enrollment job in the workflow, using this utility for enrollment is very similar to the enrollment utilities above; but a workflow is provided instead of a plugin and domain combination. As with the other enrollment utilities, olivepyworkflowenroll supports both single-file enrollment and batch enrollment using an enrollment-formatted text file. Generically, this looks like: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input_list <path to enrollment file> <workflow> olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input <path to audio file> --enroll <class name> <workflow> And a specific example of each: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input ~/path/to/enroll-file1.wav --enroll \"Logan\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list ~/path/to/enrollment_input.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json Important: Note that in OLIVE, when you enroll a class, you are enrolling to a plugin and domain , and enrollments are shared server-wide. Even when you enroll using a workflow and the olivepyworkflow utility, enrollments are associated with the specific plugin/domain that the workflow is using under the hood. Any enrollments made to a workflow will be available to anyone else who may be using that server instance, and will also be made available to anyone interacting with the individual plugin - whether directly or via a workflow. As a more concrete example of this, the \"SmartTranscription\" workflow that is sometimes provided with OLIVE, that performs Speech Activity Detection, Speaker Detection, Language Detection, and Speech Recognition on supported languages has a single plugin that supports enrollments; Speaker Detection. As a result, the workflow is set up to have a single enrollment job, to allow workflow users to enroll new speakers to be detected by this plugin. When enrollment is performed with this workflow, the newly created speaker model is created by and for the Speaker Detection plugin itself, and goes into the global OLIVE enrollment space. If a file is analyzed by directly calling this Speaker Detection plugin, the new enrollment will be part of the pool of target speakers the plugin will search for. More information on this concept of \"Workflow Enrollment Jobs\" is provided in the next section. Advanced Workflow Enrollment - Jobs It's rare, but possible for a workflow to bundle multiple enrollment-capable plugin capabilities into one. One example could be combining Speaker Detection in a workflow that also runs Query-by-Example Keyword Spotting , both of which rely on user enrollments to define their target classes. When this happens, if a user wishes to maintain the ability to enroll separate classes into each enrollable plugin, the workflow needs to expose these different enrollment tasks as separate jobs in the workflow enrollment capabilities. If this is necessary, the workflow will come from SRI configured appropriately - the user need only be concerned with how to specify which job to enroll with. To query which enrollment jobs are available to a workflow, use the olivepyworkflowenroll tool with the --print_jobs flag: $ olivepyworkflowenroll --print_jobs <workflow> Investigating the \"SmartTranscription\" workflow we briefly mentioned above: $ olivepyworkflowenroll --print_jobs SmartTranscriptionFull.workflow.json enrolling 0 files Enrollment jobs '[' SDD Enrollment ']' Un-Enrollment jobs '[' SDD Unenrollment ']' We see that there is only a single Enrollment job available; SDD Enrollment . If there were others, they would be listed in this output. Now that the desired job name is known, enrolling with the specified job is done by supplying that job name to the --job flag; in this case: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input ~/path/to/enroll-file1.wav --enroll \"Logan\" --job \"SDD Enrollment\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list ~/path/to/enrollment_input.txt --job \"SDD Enrollment\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json","title":"Java and Python CLI Client Utilities"},{"location":"clients.html#olive-java-and-python-clients","text":"","title":"OLIVE Java and Python Clients"},{"location":"clients.html#introduction","text":"Each OLIVE delivery includes two OLIVE client utilities - one written in Java, one written in Python. Out of the box, these tools allow a user to jump right in with running OLIVE if the GUI is not desired. These can also serve as code examples for integrating with OLIVE. This page primarily covers using these clients for processing audio, rather than integrating with the OLIVE API. For more information on integration, the nitty-gritty details of the OLIVE Enterprise API, and code examples, refer to these integration-focused pages instead: OLIVE Enterprise API Primer OLIVE Python Client API Documentation Integrating a Client API with OLIVE Building an OLIVE API Reference Implementation As far as the usage and capabilities of these tools, they were meant to mirror the Legacy CLI Tools as closely as possible, and shares many input/output formats and assumptions with those tools. As this document is still under construction, referring to this older guide may help fill in some useful information that may currently be missing from this page. Note that unlike the Legacy CLI tools, that are calling plugin code directly, these client tools require a running OLIVE Server. They are client utilities that are queueing and submitting job requests to the OLIVE server, which then manages the plugins themselves and actual audio processing. If you haven't already, please refer to the appropriate guide for setting up and starting an OLIVE server depending on your installation type: OLIVE Martini Docker-based Installation OLIVE Standalone Docker-based Installation Redhat/CentOS 7 Native Linux Installation OLIVE Server Guide","title":"Introduction"},{"location":"clients.html#client-setup-installation-requirements","text":"As a quick review, the contents of an OLIVE package typically look like this: olive5.5.1/ api/ java/ python/ docs/ martini/ -or- docker/ -or- runtime/ OliveGUI/ - (Optional) The OLIVE Nightingale GUI (not included in all deliveries) oliveAppData/ The clients this page describes are contained in the bolded api/ directory above.","title":"Client Setup, Installation, Requirements"},{"location":"clients.html#java-oliveanalyze","text":"The Java tools are the most full-featured with respect to tasking individual plugins. They are asynchronous, and better able to deal with large amounts of file submissions by parallelizing the submission of large lists of files. If the primary task is enrolling and scoring audio files with individual plugins, the Java tools, what we call the OliveAnalyze suite. The tools themselves do not need to be 'installed'. For convenience, their directory can be added to your $PATH environment variable, so that they can be called from anywhere: $ export PATH =$ PATH : < path >/ olive5 . 5.1 / api / java / bin / $ OliveAnalyze - h But they can also be left alone and called directly, as long as their full or relative path is present: # From inside olive5.5.1/api/java/bin: $ ./OliveAnalyze -h # From inside olive5.5.1/: $ ./api/java/bin/OliveAnalyze -h # From elsewhere: $ <path>/olive5.5.1/api/java/bin/OliveAnalyze -h These tools depend on OpenJDK 11 or newer being installed. Refer to OpenJDK for more information on downloading and installing this for your operating system. The full list of utilities in this suite are as follows: OliveAnalyze OliveAnalyzeText OliveEnroll OliveLearn (rarely used) But the most commonly used are OliveAnalyze for scoring requests, and OliveEnroll for enrollment requests. Examples are provided for each of these below, and for more advanced users that need different tools, each utility has its own help statement that can be accessed with the -h flag: $ OliveAnalyzeText -h The arguments and formatting for each tool is very similar, so familiarity with the OliveAnalyze and OliveEnroll examples below should allow use of most of these tools.","title":"Java (OliveAnalyze)"},{"location":"clients.html#python-olivepyanalyze","text":"The Python client, what we call the olivepyanalyze suite, is not as fully-featured with respect to batch-processing of audio files. It performs synchronous requests to the OLIVE server, and so it will sequentially score each provided audio file, rather than submitting jobs in parallel. For this reason, the Java OliveAnalyze tools are recommended for batch processing of individual plugin tasks. Only the python client currently has workflow support, however, and the python workflow client utility, olivepyworkflow is not limited by the synchronous restriction of olivepyanalyze - so when operating with workflows it is the clear choice. The python client tools require Python 3.8 or newer - please refer to Python for downloading and installing Python. Installing these tools has been simplified by providing them in the form of a Python wheel, that can be easily installed with pip . $ cd olive5.5.1/api/python $ ls olivepy-5.5.1-py3-none-any.whl olivepy-5.5.1.tar.gz requirements.txt $ python3 -m pip install -r requirements.txt olivepy-5.5.1-py3-none-any.whl This will fetch and install (if necessary) the olivepy dependencies, and install the olivepy tools. Those dependencies are: protobuf soundfile numpy zmq The olivepy utilities closely mirror the Java utilities, with the addition of the workflow tool, and are as follows: olivepyanalyze olivepyenroll olivepylearn (rarely used) olivepyworkflow olivepyworkflowenroll The olivepyworkflow tools are the most important, and examples are provided below for both scoring with olivepyworkflow and enrollment with olivepyworkflowenroll . We also provide examples for olivepyanalyze and olivepyenroll that mirror the Java examples.","title":"Python (olivepyanalyze)"},{"location":"clients.html#scoringanalysis-requests","text":"","title":"Scoring/Analysis Requests"},{"location":"clients.html#plugin-scoring-types","text":"In general, the output format will depend on the type of \u2018scorer\u2019 the plugin being used is. For a deeper dive into OLIVE scoring types, please refer to the appropriate section in the OLIVE Plugin Traits Guide , but a brief overview follows. The most common types of plugins in OLIVE are:","title":"Plugin Scoring Types"},{"location":"clients.html#global-scorer","text":"Any plugin that reports a single score for a given model over the entire test audio file is a global scoring plugin. Every input test audio file will be assigned a single score for each enrolled target model, as measured by looking at the entire file at once. Speaker and Language Identification are examples of global scorers. OLIVE typically calls a global scoring plugin an \"Identification\" plugin, whereas a region scoring plugin to pinpoint the same class types would instead be called a \"Detection\" plugin. For example, Speaker Identification versus Speaker Detection; the former assumes the entire audio contains a single speaker, where the latter makes no such assumption, and attempts to localize any detected speakers of interest.","title":"Global Scorer"},{"location":"clients.html#global-scorer-output","text":"In the case of global scorers like LID and SID, the output file, which by default is called output.txt, contains one or more lines containing the audio path, speaker/language ID (class id), and the score: <audio_path> <class_id> <score> For example, a Speaker Identification analysis run, with three enrolled speakers (Alex, Taylor, Blake) might return: /data/sid/audio/file1.wav Alex -0.5348 /data/sid/audio/file1.wav Taylor 3.2122 /data/sid/audio/file1.wav Blake -5.5340 /data/sid/audio/file2.wav Alex 0.5333 /data/sid/audio/file2.wav Taylor -4.9444 /data/sid/audio/file2.wav Blake -2.6564 Note the actual meanings of the scores and available classes will vary from plugin-to-plugin. Please refer to individual plugin documentation for more guidance on what the scores mean and what ranges are acceptable. Also note that the output format described here is literally what will be returned when calling a plugin directly with OliveAnalyze or olivepyanalyze - but when performing a global-scoring task as part of analysis with a workflow,these same informational pieces (audio_path or object, class_id, score) are still provided, but packed into a json structure.","title":"Global Scorer Output"},{"location":"clients.html#region-scorer","text":"Region scoring plugins are capable of considering each audio file in small pieces at a time. Scores are reported for enrolled target models along with the location within that audio file that they are thought to occur. This allows OLIVE to pinpoint individual keywords or phrases or pick out one specific speaker in a recording where several people may be talking. Automatic Speech Recognition (ASR), Language Detection (LDD), and Speaker Detection (SDD) are all region scorers. OLIVE typically calls a global scoring plugin an \"Identification\" plugin, whereas a region scoring plugin to pinpoint the same class types would instead be called a \"Detection\" plugin. For example, Speaker Identification versus Speaker Detection; the former assumes the entire audio contains a single speaker, where the latter makes no such assumption, and attempts to localize any detected speakers of interest.","title":"Region scorer"},{"location":"clients.html#region-scorer-output","text":"Region scoring plugins will generate a single output file, that is also called output.txt by default, just like global scorers. The file looks very similar to a global scorer\u2019s output, but includes a temporal component to each line that represents the start and end of each scored region. In practice, this looks like: < audio_path > < region_start_timestamp > < region_end_timestamp > < class_id > < score > For example, a language detection plugin might output something like this: /data/mixed-language/testFile1.wav 2.170 9.570 Arabic 0.912 /data/mixed-language/testFile1.wav 10.390 15.930 French 0.693 /data/mixed-language/testFile1.wav 17.639 22.549 English 0.832 /data/mixed-language/testFile2.wav 0.142 35.223 Pashto 0.977 Each test file can have multiple regions where scores are reported, depending on the individual plugin. The region boundary timestamps are in seconds. More specific examples can be found in the respective plugin-specific documentation pages. As with global scoring, note that the output format described here is literally what will be returned when calling a plugin directly with OliveAnalyze or olivepyanalyze - but when performing a region-scoring task as part of analysis with a workflow,these same informational pieces (audio_path or object, region start and end timestamps, class_id, score) are still provided, but packed into a json structure.","title":"Region Scorer Output"},{"location":"clients.html#plugin-direct-analysis","text":"Performing an analysis request with both tools is very similar, as the tools were designed to closely mirror each other so that familiarity with one would easily transfer to the other. The usage statements for each can be examined by invoking each with their -h or --help flag: OliveAnalyze (Java) $ ./OliveAnalyze -h usage: OliveAnalyze --align Perform audio alignment analysis. Must specify the two files to compare using an input list file via the--list argument --apply_update Request the plugin is update ( if supported ) --box Perform bounding box analysis. Must specify an image or video input --channel <arg> Process stereo files using channel NUMBER --class_ids <arg> Use Class ( s ) from FILE for scoring. Each line in the file contains a single class, including any white space --compare Perform audio compare analysis. Must specify the two files to compare using an input list file via the--list argument --decoded Send audio file as decoded PCM16 samples instead of sending as serialized buffer. Input file must be a wav file --domain <arg> Use Domain NAME --enhance Perform audio conversion ( enhancement ) --frame Perform frame scoring analysis --global Perform global scoring analysis -h Print this help message -i,--input <arg> NAME of the input file ( audio/video/image as required by the plugin --input_list <arg> Use an input list FILE having multiple filenames/regions or PEM formatted -l,--load load a plugin now, must use --plugin and --domain to specify the plugin/domain to preload --options <arg> options from FILE --output <arg> Write any output to DIR, default is ./ -p,--port <arg> Scenicserver port number. Defauls is 5588 --path Send audio file path instead of a buffer. Server and client must share a filesystem to use this option --plugin <arg> Use Plugin NAME --print <arg> Print all available plugins and domains. Optionally add 'verbose' as a print option to print full plugin details including traits and classes -r,--unload unload a loaded plugin now, must use --plugin and --domain to specify the plugin/domain to unload --region Perform region scoring analysis -s,--server <arg> Scenicserver hostname. Default is localhost --shutdown Request a clean shutdown of the server --status Print the current status of the server -t,--timeout <arg> timeout ( in seconds ) when waiting for server response. Default is 10 seconds --threshold <arg> Apply threshold NUMBER when scoring --update_status Get the plugin ' s update status -v,--vec <arg> PATH to a serialized AudioVector, for plugins that support audio vectors in addition to wav files --vector Perform audio vectorization --workflow Request a workflow olivepyanalyze (Python) $ olivepyanalyze ---h usage: olivepyanalyze [ -h ] [ -C CLIENT_ID ] [ -p PLUGIN ] [ -d DOMAIN ] [ -G ] [ -e ] [ -f ] [ -g ] [ -r ] [ -b ] [ -P PORT ] [ -s SERVER ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --text ] [ --options OPTIONS ] [ --class_ids CLASS_IDS ] [ --debug ] [ --path ] [ --print ] optional arguments: -h, --help show this help message and exit -C CLIENT_ID, --client-id CLIENT_ID Experimental: the client_id to use -p PLUGIN, --plugin PLUGIN The plugin to use. -d DOMAIN, --domain DOMAIN The domain to use -G, --guess Experimental: guess the type of analysis to use based on the plugin/domain. -e, --enhance Enhance the audio of a wave file, which must be passed in with the --wav option. -f, --frame Do frame based analysis of a wave file, which must be passed in with the --wav option. -g, --global Do global analysis of a wave file, which must be passed in with the --wav option. -r, --region Do region based analysis of a wave file, which must be passed in with the --wav option. -b, --box Do bounding box based analysis of an input file, which must be passed in with the --wav option. -P PORT, --port PORT The port to use. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -t TIMEOUT, --timeout TIMEOUT The timeout to use -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --text Indicates that input ( or input list ) is a literal text string to send in the analysis request. --options OPTIONS Optional file containing plugin properties ans name/value pairs. --class_ids CLASS_IDS Optional file containing plugin properties ans name/value pairs. --debug Debug mode --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option --print Print all available plugins and domains To perform a scoring request with these tools, you will need these essential pieces of information: Plugin name ( --plugin ) Domain name ( --domain ) Scoring type to perform ( --region for region-scoring, --global for global-scoring, others for less-common plugins) Input audio file or list of input audio files ( --input for a single file, --input_list for a list of files) The flag for providing each piece of information is the same for both tools, as shown in the list above. For more information on what the difference is between a plugin and a domain, refer to the Plugins Overview . For more information on the domains available for each plugin, refer to documentation page for that specific plugin. To see which plugins and domains you have installed and running in your specific OLIVE environment, refer to the server startup status message, that appears when you start the server: martini.sh start , then martini.sh log once the server is running for martini-based OLIVE packages (most common) ./run.sh for non-martini docker OLIVE packages oliveserver for native linux OLIVE packages Or exercise the --print option for each tool to query the server and print the available plugins and domains: OliveAnalyze (Java) $ ./OliveAnalyze --print olivepyanalyze (python) $ olivepyanalyze --print Example output: 2022-06-14 12:12:25.786 INFO com.sri.speech.olive.api.Server - Connected to localhost - request port: 5588 status_port: 5589 Found 8 plugin(s): Plugin: sad-dnn-v7.0.2 (SAD,Speech) v7.0.2 has 2 domain(s): Domain: fast-multi-v1, Description: Trained with Telephony, PTT and Music data Domain: multi-v1, Description: Trained with Telephony, PTT and Music data Plugin: asr-dynapy-v3.0.0 (ASR,Content) v3.0.0 has 9 domain(s): Domain: english-tdnnChain-tel-v1, Description: Large vocabulary English DNN model for 8K data Domain: farsi-tdnnChain-tel-v1, Description: Large vocabulary Farsi DNN model for 8K data Domain: french-tdnnChain-tel-v2, Description: Large vocabulary African French DNN Chain model for 8K data Domain: iraqiArabic-tdnnChain-tel-v1, Description: Large vocabulary Iraqi Arabic DNN Chain model for 8K data Domain: levantineArabic-tdnnChain-tel-v1, Description: Large vocabulary Levantine Arabic DNN Chain model for 8K data Domain: mandarin-tdnnChain-tel-v1, Description: Large vocabulary Mandarin DNN model for clean CTS 8K data Domain: pashto-tdnnChain-tel-v1, Description: Large vocabulary Pashto DNN Chain model for 8K data Domain: russian-tdnnChain-tel-v2, Description: Large vocabulary Russian DNN model for 8K data Domain: spanish-tdnnChain-tel-v1, Description: Large vocabulary Spanish DNN model for clean CTS 8K data Plugin: sdd-diarizeEmbedSmolive-v1.0.0 (SDD,Speaker) v1.0.0 has 1 domain(s): Domain: telClosetalk-int8-v1, Description: Speaker Embeddings Framework Plugin: tmt-neural-v1.0.0 (TMT,Content) v1.0.0 has 3 domain(s): Domain: cmn-eng-nmt-v1, Description: Mandarin Chinese to English NMT Domain: rus-eng-nmt-v1, Description: Russian to English NMT Domain: spa-eng-nmt-v3, Description: Spanish to English NMT Plugin: ldd-embedplda-v1.0.1 (LDD,Language) v1.0.1 has 1 domain(s): Domain: multi-v1, Description: PNCC bottleneck domain suitable for mixed conditions (tel/mic/compression) Plugin: sdd-diarizeEmbedSmolive-v1.0.2 (SDD,Speaker) v1.0.2 has 1 domain(s): Domain: telClosetalk-smart-v1, Description: Speaker Embeddings Framework Plugin: sid-dplda-v2.0.2 (SID,Speaker) v2.0.2 has 1 domain(s): Domain: multi-v1, Description: Speaker Embeddings DPLDA Plugin: lid-embedplda-v3.0.1 (LID,Language) v3.0.1 has 1 domain(s): Domain: multi-v1, Description: PNCC Bottleneck embeddings suitable for mixed conditions (tel/mic/compression)","title":"Plugin Direct (Analysis)"},{"location":"clients.html#examples","text":"To perform a global score analysis on a single file with the speaker identification plugin sid-dplda-v2.0.2 , using the multi-v1 domain, the calls for each would look like this: OliveAnalyze (Java) $ ./OliveAnalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav olivepyanalyze (python) $ olivepyanalyze --plugin sid-dplda-v2.0.2 --domain multi-v1 --global --input ~/path/to/test-file1.wav Performing region scoring instead, using a transcription plugin, asr-dynapy-v3.0.0 , via the english domain english-tdnnChain-tel-v1 on a list of audio files would be performed with: OliveAnalyze (Java) $ ./OliveAnalyze --plugin asr-dynapy-v3.0.0 --domain english-tdnnChain-tel-v1 --region --input_list ~/path/to/list-of-audio-files.txt olivepyanalyze (python) $ olivepyanalyze --plugin asr-dynapy-v3.0.0 --domain english-tdnnChain-tel-v1 --region --input_list ~/path/to/list-of-audio-files.txt Where the format of the input list is simply a text file with a path to an audio file on each line. For example: /data/mixed-language/testFile1.wav /data/mixed-language/testFile2.wav /data/mixed-language/testFile3.wav /moreData/test-files/unknown1.wav","title":"Examples"},{"location":"clients.html#workflow-analysis","text":"OLIVE Workflows provide a simple way of creating a sort of 'recipe' that specifies how to deal with the input data and one or more OLIVE plugins. It allows complex operations to be requested and performed with a single, simple call to the system by allowing complexities and specific knowledge to be encapsulated within the workflow itself, rather than known and implemented by the user at run time. Due to off-loading this burden, operating with workflows is much simpler than calling the plugin(s) directly - typically all that is needed from the user to request an analysis from olivepyworkflow is the workflow itself, and one or more input files. There are more options available to the olivepyworkflow client, as shown in the usage statement: $ olivepyworkflow usage: olivepyworkflow [ -h ] [ --tasks ] [ --class_ids ] [ --print_actualized ] [ --print_workflow ] [ -s SERVER ] [ -P PORT ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --text ] [ --options OPTIONS ] [ --path ] [ --debug ] workflow Perform OLIVE analysis using a Workflow Definition file positional arguments: workflow The workflow definition to use. optional arguments: -h, --help show this help message and exit --tasks Print the workflow analysis tasks. --class_ids Print the class IDs available for analysis in the specified workflow. --print_actualized Print the actualized workflow info. --print_workflow Print the workflow definition file info ( before it is actualized, if requested ) -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. -t TIMEOUT, --timeout TIMEOUT The timeout ( in seconds ) to wait for a response from the server -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --text Indicates that input ( or input list ) is a literal text string to send in the analysis request. --options OPTIONS A JSON formatted string of workflow options such as [{ \"task\" : \"SAD\" , \"options\" : { \"filter_length\" :99, \"interpolate\" :1.0 }] or { \"filter_length\" :99, \"interpolate\" :1.0, \"name\" : \"midge\" } , where the former options are only applied to the SAD task, and the later are applied to all tasks --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option --debug Debug mode But their use rarely necessary, and is reserved for advanced users or specific system testing. Generically, calling olivepyworkflow will look like this: Single Input File $ olivepyworkflow --input ~/path/to/test-file1.wav <workflow> List of Input Files $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt <workflow> As an example of the power of workflows, this request calls the SmartTranscription workflow - that performs Speech Activity Detection (region scoring), Speaker Diarization and Detection (region scoring), Language Detection (region scoring), and then Automatic Speech Recognition (region scoring) on any sections of each input file that are detected to be a language that ASR currently has support for, and returns all of the appropriate results in a JSON structure. Performing this same task by calling plugins directly, this same functionality would be a minimum of 4 separate calls to OLIVE; significantly more if more than one language is detected being spoken in the file. Single Input File $ olivepyworkflow --input ~/path/to/test-file1.wav ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json List of Input Files $ olivepyworkflow --input_list ~/path/to/list-of-audio-files.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json","title":"Workflow (Analysis)"},{"location":"clients.html#output-format-workflow","text":"Workflows are generally customer/user-specific and can be quite specialized - the output format and structure will depend heavily on the individual workflow itself and the tasks being performed. All of the information pieces that define each scoring type are still reported for each result, but the results are organized into a single JSON structure for the workflow call. This means that the output of a region scoring plugin within the workflow is still one or more sets of: < region_start_timestamp > < region_end_timestamp > < class_id > < score > But the data is arranged into the JSON structure and will be nested depending on the structure of the workflow itself and how the audio is routed by the workflow. For more detailed information on the structure of this JSON message and the inner-workings of workflows, please refer to the OLIVE Workflow API documentation. A brief, simplified summary to jump start working with workflow output follows. The main skeleton structure of the results output is shown below, along with an actual example. The results are provided as a result for each input file, that lists the job name(s), some metadata about the input audio and how it was processed, and then the returned results (if any) for each task, which is generally a plugin. Simplified/Generic Output Example (single-job workflow) Work fl ow a nal ysis resul ts : [ { \"job_name\" : <work fl ow job na me> , \"data\" : [ { \"data_id\" : <da ta ID , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <da ta label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ], < tas k 2 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ], < tas k 3 na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 class na me> , \"score\" : <regio n 1 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N class na me> , \"score\" : <regio n N score> } ] } } ] } } ] English Transcription Output Example (single-job workflow) Work fl ow a nal ysis resul ts : [ { \"job_name\" : \"SAD, SDD, and ASR English Workflow\" , \"data\" : [ { \"data_id\" : \"z_eng_englishdemo.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 5.932625 , \"number_channels\" : 1 , \"label\" : \"z_eng_englishdemo.wav\" , \"id\" : \"0b04c7497521d53a5d6939533a55c461795f9d685b1bd19fd9031fc6f3997a8f\" } ], \"tasks\" : { \"SAD_REGIONS\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v7.0.2\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 5.93 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"SDD\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SDD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sdd-diarizeEmbedSmolive-v1.0.2\" , \"domain\" : \"telClosetalk-smart-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.1 , \"end_t\" : 5.0 , \"class_id\" : \"unknownspk00\" , \"score\" : 1.4 } ] } } ], \"ASR\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"ASR\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"asr-dynapy-v3.0.0\" , \"domain\" : \"english-tdnnChain-tel-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.15 , \"end_t\" : 0.51 , \"class_id\" : \"hello\" , \"score\" : 100.0 }, { \"start_t\" : 0.54 , \"end_t\" : 0.69 , \"class_id\" : \"my\" , \"score\" : 100.0 }, { \"start_t\" : 0.69 , \"end_t\" : 0.87 , \"class_id\" : \"name\" , \"score\" : 99.0 }, { \"start_t\" : 0.87 , \"end_t\" : 1.05 , \"class_id\" : \"is\" , \"score\" : 99.0 }, { \"start_t\" : 1.05 , \"end_t\" : 1.35 , \"class_id\" : \"evan\" , \"score\" : 88.0 }, { \"start_t\" : 1.35 , \"end_t\" : 1.47 , \"class_id\" : \"this\" , \"score\" : 99.0 }, { \"start_t\" : 1.5 , \"end_t\" : 1.98 , \"class_id\" : \"audio\" , \"score\" : 95.0 }, { \"start_t\" : 1.98 , \"end_t\" : 2.16 , \"class_id\" : \"is\" , \"score\" : 74.0 }, { \"start_t\" : 2.16 , \"end_t\" : 2.31 , \"class_id\" : \"for\" , \"score\" : 99.0 }, { \"start_t\" : 2.31 , \"end_t\" : 2.4 , \"class_id\" : \"the\" , \"score\" : 99.0 }, { \"start_t\" : 2.4 , \"end_t\" : 2.91 , \"class_id\" : \"purposes\" , \"score\" : 100.0 }, { \"start_t\" : 2.91 , \"end_t\" : 3.06 , \"class_id\" : \"of\" , \"score\" : 99.0 }, { \"start_t\" : 3.12 , \"end_t\" : 3.81 , \"class_id\" : \"demonstrating\" , \"score\" : 100.0 }, { \"start_t\" : 3.81 , \"end_t\" : 3.96 , \"class_id\" : \"our\" , \"score\" : 78.0 }, { \"start_t\" : 4.05 , \"end_t\" : 4.44 , \"class_id\" : \"language\" , \"score\" : 100.0 }, { \"start_t\" : 4.44 , \"end_t\" : 4.53 , \"class_id\" : \"and\" , \"score\" : 93.0 }, { \"start_t\" : 4.53 , \"end_t\" : 4.89 , \"class_id\" : \"speaker\" , \"score\" : 100.0 }, { \"start_t\" : 4.89 , \"end_t\" : 5.01 , \"class_id\" : \"i.\" , \"score\" : 99.0 }, { \"start_t\" : 5.01 , \"end_t\" : 5.22 , \"class_id\" : \"d.\" , \"score\" : 99.0 }, { \"start_t\" : 5.22 , \"end_t\" : 5.85 , \"class_id\" : \"capabilities\" , \"score\" : 99.0 } ] } } ] } } ] Each task output will typically be for a single plugin, and will be outputting the information provided by a Region Scorer or Global Scorer or Text Transformer in the case of Machine Translation, depending on how the workflow is using the plugin. The format of each result sub part is: Global Score < tas k na me> : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : < tas k t ype , ge nerall y LID , SID , e t c.> , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"score\" : [ { \"class_id\" : <class 1 > , \"score\" : <class 1 score> }, { \"class_id\" : <class 2 > , \"score\" : <class 2 score> }, ... { \"class_id\" : <class N> , \"score\" : <class N score> } ] } } ] Region Score < tas k na me> : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : < tas k t ype , t ypically ASR , SDD , SAD , e t c.> , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : <plugi n > , \"domain\" : <domai n > , \"analysis\" : { \"region\" : [ { \"start_t\" : <regio n 1 s tart t ime (s)> , \"end_t\" : <regio n 1 e n d t ime (s)> , \"class_id\" : <regio n 1 de te c te d class> , \"score\" : <regio n 1 score> }, { \"start_t\" : <regio n 2 s tart t ime (s)> , \"end_t\" : <regio n 2 e n d t ime (s)> , \"class_id\" : <regio n 2 de te c te d class> , \"score\" : <regio n 2 score> }, ... { \"start_t\" : <regio n N s tart t ime (s)> , \"end_t\" : <regio n N e n d t ime (s)> , \"class_id\" : <regio n N de te c te d class> , \"score\" : <regio n N score> }, ] } } ] Text Transformer < tas k na me> : [ { \"task_trait\" : \"TEXT_TRANSFORMER\" , \"task_type\" : < tas k t ype , t ypically MT> , \"message_type\" : \"TEXT_TRANSFORM_RESULT\" , \"plugin\" : <plugi n na me> , \"domain\" : <domai n na me> , \"analysis\" : { \"transformation\" : [ { \"class_id\" : \"test_label\" , \"transformed_text\" : < t he translate d/ transf ormed te x t re turne d fr om t he plugi n > } ] } } ] Many workflows consist of a single job , and bundle all plugin tasks into this single job, as seen above. More complex workflows, or what OLIVE calls \"Conditional Workflows\" can pack multiple jobs into a single workflow. This happens when there are certain tasks in the workflow that depend on other tasks in the workflow - for example when OLIVE needs to choose the appropriate Speech Recognition (ASR) language to use, depending on what language is detected being spoken by Language Identification (LID) or Language Detection (LDD). In this case, the LID/LDD is separated into one job, and the ASR into another, that is triggered to run once LID/LDD's decision is known. In this case, the results from each job are grouped accordingly in the results output. Below shows a simplified output from a workflow that includes three jobs; \"job 1\", \"job 2\", \"job 3\", and a real-life example output from the SmartTranscription conditional workflow that also has three jobs; the first performs Speech Activity Detection and Language Identification (LID); Smart Translation SAD and LID Pre-processing the second uses the language decision from Language Identification to choose the appropriate (if any) language and domain for Automatic Speech Recognition (ASR) and runs that, Dynamic ASR the third takes the output transcript from ASR and the language decision from LID and choose the appropriate (if any) language and domain for Text Machine Translation, and runs that. Dynamic MT As you can see below, these jobs are listed separately in the JSON for each result: Simplified/Generic Output Example (multi-job workflow) Work fl ow a nal ysis resul ts : [ { \"job name\" : <job 1 na me> , \"data\" : [ { \"data_id\" : <da ta ide nt i f ier , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <audio label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me (job 1 )> : [ { < tas k 1 resul ts > } ], ... < tas k N na me (job 1 )> : [ { < tas k N resul ts > } ] } }, { \"job name\" : <job 2 na me> , \"data\" : [ { \"data_id\" : <da ta ide nt i f ier , t ypically audio f ile na me> , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : <sample ra te > , \"duration_seconds\" : <audio dura t io n > , \"number_channels\" : 1 , \"label\" : <audio label> , \"id\" : <i n pu t audio UUID> } ], \"tasks\" : { < tas k 1 na me (job 2 )> : [ { < tas k 1 resul ts > } ], ... < tas k N na me (job 2 )> : [ { < tas k N resul ts > } ] } }, ... <repea t i f more jobs> ] SmartTranslation Output Example Work fl ow a nal ysis resul ts : [ { \"job_name\" : \"Smart Translation SAD and LID Pre-processing\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 8.0 , \"number_channels\" : 1 , \"label\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"id\" : \"68984a7356fa1ea05f8e985868eb93e066ce80a0f4bf848edf55d547cfcbab41\" } ], \"tasks\" : { \"SAD_REGIONS\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"sad-dnn-v7.0.2\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 8.0 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] } } ], \"LID\" : [ { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"plugin\" : \"lid-embedplda-v3.0.1\" , \"domain\" : \"multi-v1\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"Mandarin\" , \"score\" : 3.5306692 }, { \"class_id\" : \"Korean\" , \"score\" : -1.9072952 }, { \"class_id\" : \"Japanese\" , \"score\" : -3.7805116 }, { \"class_id\" : \"Tagalog\" , \"score\" : -7.4819508 }, { \"class_id\" : \"Vietnamese\" , \"score\" : -8.094855 }, { \"class_id\" : \"Iraqi Arabic\" , \"score\" : -10.63325 }, { \"class_id\" : \"Levantine Arabic\" , \"score\" : -10.694491 }, { \"class_id\" : \"French\" , \"score\" : -11.542379 }, { \"class_id\" : \"Pashto\" , \"score\" : -12.11981 }, { \"class_id\" : \"English\" , \"score\" : -12.323014 }, { \"class_id\" : \"Modern Standard Arabic\" , \"score\" : -12.626052 }, { \"class_id\" : \"Spanish\" , \"score\" : -13.469315 }, { \"class_id\" : \"Iranian Persian\" , \"score\" : -13.763366 }, { \"class_id\" : \"Amharic\" , \"score\" : -17.129797 }, { \"class_id\" : \"Portuguese\" , \"score\" : -17.31257 }, { \"class_id\" : \"Russian\" , \"score\" : -18.770994 } ] } } ] } }, { \"job_name\" : \"Dynamic ASR\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 8.0 , \"number_channels\" : 1 , \"label\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"id\" : \"68984a7356fa1ea05f8e985868eb93e066ce80a0f4bf848edf55d547cfcbab41\" } ], \"tasks\" : { \"ASR\" : [ { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"ASR\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"plugin\" : \"asr-dynapy-v3.0.0\" , \"domain\" : \"mandarin-tdnnChain-tel-v1\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 0.0 , \"end_t\" : 0.18 , \"class_id\" : \"\u8ddf\" , \"score\" : 31.0 }, { \"start_t\" : 0.18 , \"end_t\" : 0.36 , \"class_id\" : \"\u4e00\u4e2a\" , \"score\" : 83.0 }, { \"start_t\" : 0.36 , \"end_t\" : 0.66 , \"class_id\" : \"\u80af\u5b9a\" , \"score\" : 100.0 }, { \"start_t\" : 0.66 , \"end_t\" : 0.81 , \"class_id\" : \"\u662f\" , \"score\" : 83.0 }, { \"start_t\" : 0.81 , \"end_t\" : 1.23 , \"class_id\" : \"\u5317\u4eac\" , \"score\" : 95.0 }, { \"start_t\" : 1.23 , \"end_t\" : 1.47 , \"class_id\" : \"\u554a\" , \"score\" : 96.0 }, { \"start_t\" : 2.07 , \"end_t\" : 2.49 , \"class_id\" : \"\u4ed6\u4fe9\" , \"score\" : 96.0 }, { \"start_t\" : 2.7 , \"end_t\" : 3.09 , \"class_id\" : \"\u4e0a\u6d77\" , \"score\" : 99.0 }, { \"start_t\" : 3.09 , \"end_t\" : 3.21 , \"class_id\" : \"\u7684\" , \"score\" : 99.0 }, { \"start_t\" : 3.21 , \"end_t\" : 3.57 , \"class_id\" : \"\u4eba\u53e3\" , \"score\" : 99.0 }, { \"start_t\" : 3.57 , \"end_t\" : 3.87 , \"class_id\" : \"\u597d\u50cf\" , \"score\" : 73.0 }, { \"start_t\" : 3.87 , \"end_t\" : 3.99 , \"class_id\" : \"\u6ca1\" , \"score\" : 54.0 }, { \"start_t\" : 3.99 , \"end_t\" : 4.32 , \"class_id\" : \"\u5317\u4eac\" , \"score\" : 74.0 }, { \"start_t\" : 4.32 , \"end_t\" : 4.68 , \"class_id\" : \"\u591a\" , \"score\" : 99.0 }, { \"start_t\" : 4.86 , \"end_t\" : 5.19 , \"class_id\" : \"\u4f46\u662f\" , \"score\" : 100.0 }, { \"start_t\" : 5.4 , \"end_t\" : 5.91 , \"class_id\" : \"\u4e0d\u77e5\u9053\" , \"score\" : 100.0 }, { \"start_t\" : 6.06 , \"end_t\" : 6.48 , \"class_id\" : \"@reject@\" , \"score\" : 62.0 }, { \"start_t\" : 6.69 , \"end_t\" : 7.05 , \"class_id\" : \"\u5176\u4ed6\" , \"score\" : 93.0 }, { \"start_t\" : 7.05 , \"end_t\" : 7.2 , \"class_id\" : \"\u7684\" , \"score\" : 97.0 }, { \"start_t\" : 7.65 , \"end_t\" : 7.89 , \"class_id\" : \"\u4e0a\" , \"score\" : 32.0 }, { \"start_t\" : 7.89 , \"end_t\" : 7.95 , \"class_id\" : \"\u554a\" , \"score\" : 67.0 } ] } } ] } }, { \"job_name\" : \"Dynamic MT\" , \"data\" : [ { \"data_id\" : \"fvccmn-2009-12-21_019_2_020_2_cnv_R-030s_2.wav\" , \"msg_type\" : \"WORKFlOW_TEXT_RESULT\" , \"text\" : \"\u8ddf \u4e00\u4e2a \u80af\u5b9a \u662f \u5317\u4eac \u554a \u4ed6\u4fe9 \u4e0a\u6d77 \u7684 \u4eba\u53e3 \u597d\u50cf \u6ca1 \u5317\u4eac \u591a \u4f46\u662f \u4e0d\u77e5\u9053 @reject@ \u5176\u4ed6 \u7684 \u4e0a \u554a\" } ], \"tasks\" : { \"MT\" : [ { \"task_trait\" : \"TEXT_TRANSFORMER\" , \"task_type\" : \"MT\" , \"message_type\" : \"TEXT_TRANSFORM_RESULT\" , \"plugin\" : \"tmt-neural-v1.0.0\" , \"domain\" : \"cmn-eng-nmt-v1\" , \"analysis\" : { \"transformation\" : [ { \"class_id\" : \"test_label\" , \"transformed_text\" : \"with someone in beijing they don't seem to have a population in shanghai but we don't know what else to do\" } ] } } ] } } ]","title":"Output Format (Workflow)"},{"location":"clients.html#enrollment-requests","text":"Enrollments are a sub-set of classes that the user can create and/or modify. These are used for classes that cannot be known ahead of time and therefore can't be pre-loaded into the system, such as specific speakers or keywords of interest. To determine if a plugin supports or requires enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page, linked from the navigation or the Release Plugins page.","title":"Enrollment Requests"},{"location":"clients.html#enrollment-list-format","text":"As with analysis, both the Java and Python tools were designed to share as much of a common interface as possible, and as such share an input list format when providing exemplars for enrollment. The audio enrollment list input file is formatted as one or more newline-separated lines containing a path to an audio file and a class or model ID, which can be a speaker name, topic name, or query name for SID, TPD, and QBE respectively. A general example is given below, and more details and plugin-specific enrollment information are provided in the appropriate section in each plugin's documentation. Format: <audio_path> <model_id> Example enrollment list file (SID): /data/speaker1/audiofile1.wav speaker1 /data/speaker1/audiofile2.wav speaker1 /data/speaker7/audiofile1.wav speaker7","title":"Enrollment list format"},{"location":"clients.html#plugin-direct-enrollment","text":"Performing an enrollment request is similar to an analysis request and is again very similar between the two tools. The usage statements for each can be examined by invoking each with their -h or --help flag: OliveEnroll (Java) $ ./OliveEnroll -h usage: OliveEnroll --channel <arg> Process stereo files using channel NUMBER --classes Print class names if also printing plugin/domain names. Must use with --print option. Default is to not print class IDs --decoded Sennd audio file as a decoded PCM16 sample buffer instead of a serialized buffer. The file must be a WAV file --domain <arg> Use Domain NAME --enroll <arg> Enroll speaker NAME. If no name specified then , the pem or list option must specify an input file --export <arg> Export speaker NAME to an EnrollmentModel ( enrollment.tar.gz ) -h Print this help message -i,--input <arg> NAME of the input file ( input varies by plugin: audio, image, or video ) --import <arg> Import speaker from EnrollmentModel FILE --input_list <arg> Batch enroll using this input list FILE having multiple filenames/class IDs or PEM formmated file --nobatch Disable batch enrollment when using pem or list input files, so that files are processed serially --options <arg> Enrollment options from FILE --output <arg> Write any output to DIR, default is ./ -p,--port <arg> Scenicserver port number. Defauls is 5588 --path Send the path to the audio file instead of a ( serialized ) buffer. The server must have access to this path. --plugin <arg> Use Plugin NAME --print Print all plugins and domains that suport enrollment and/or class import and export --remove <arg> Remove audio enrollment for NAME olivepyenroll (Python) $ olivepyenroll -h usage: olivepyenroll [ -h ] [ -C CLIENT_ID ] [ -D ] [ -p PLUGIN ] [ -d DOMAIN ] [ -e ENROLL ] [ -u UNENROLL ] [ -s SERVER ] [ -P PORT ] [ -t TIMEOUT ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --path ] optional arguments: -h, --help show this help message and exit -C CLIENT_ID, --client-id CLIENT_ID Experimental: the client_id to use -D, --debug The domain to use -p PLUGIN, --plugin PLUGIN The plugin to use. -d DOMAIN, --domain DOMAIN The domain to use -e ENROLL, --enroll ENROLL Enroll with this name. -u UNENROLL, --unenroll UNENROLL Uneroll with this name. -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. -t TIMEOUT, --timeout TIMEOUT The timeout to use -i INPUT, --input INPUT The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag --input_list INPUT_LIST A list of files to analyze. One file per line. --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option To perform an enrollment request with these tools, you will need these essential pieces of information: Plugin name ( --plugin ) Domain name ( --domain ) One of: A properly formatted enrollment list ( --input_list ), if providing multiple files at once (see below for formatting) An input audio file ( --input ) AND the name of the class you wish to enroll ( --enroll for OliveEnroll , -e or --enroll for olivepyanalyze ) Generically, this looks like this for a single file input: OliveEnroll (Java) $ ./OliveEnroll --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> olivepyenroll (python) $ olivepyenroll --plugin <plugin> --domain <domain> --input <path to audio file> --enroll <class name> A more specific example: OliveEnroll (Java) $ ./OliveEnroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" olivepyenroll (python) $ olivepyenroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input ~/path/to/enroll-file1.wav --enroll \"Logan\" Or if providing the enrollment list format shown above, the call is even simpler. Generically: OliveEnroll (Java) $ ./OliveEnroll --plugin <plugin> --domain <domain> --input_list <path to enrollment text file> olivepyenroll (python) $ olivepyenroll --plugin <plugin> --domain <domain> --input_list <path to enrollment text file> Specific: OliveEnroll (Java) $ ./OliveEnroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input_list ~/path/to/enrollment_input.txt olivepyenroll (python) $ olivepyenroll --plugin sid-dplda-v2.0.2 --domain multi-v1 --input_list ~/path/to/enrollment_input.txt Where the enrollment_input.txt file might look like: /some/data/somewhere/inputFile1.wav Logan /some/other/data/somewhere/else/LoganPodcast.wav Logan /yet/another/data/directory/charlie-speaks.wav Charlie","title":"Plugin Direct (Enrollment)"},{"location":"clients.html#workflow-enrollment","text":"In the most basic case, enrollment using a workflow is just as simple as scoring with a workflow. This is becuase most workflows will only have a single enrollment-capable job; a job is a subset of the the tasks a workflow is performing, typically linked to a single plugin. In the rare case that you're using a workflow with multiple supported enrollment jobs, you will need to specify which job to enroll to. See the Advanced Workflow Enrollment section below. Workflow enrollment is performed by using the olivepyworkflowenroll utility, whose help/usage statement is: olivepyworkflowenroll usage $ olivepyworkflowenroll -h usage: olivepyworkflowenroll [ -h ] [ --print_jobs ] [ --job JOB ] [ --enroll ENROLL ] [ --unenroll UNENROLL ] [ -i INPUT ] [ --input_list INPUT_LIST ] [ --path ] [ -s SERVER ] [ -P PORT ] [ -t TIMEOUT ] workflow Perform OLIVE enrollment using a Workflow Definition file positional arguments: workflow The workflow definition to use. optional arguments: -h, --help show this help message and exit --print_jobs Print the supported workflow enrollment jobs. --job JOB Enroll/Unenroll an Class ID for a job ( s ) in the specified workflow. If not specified enroll or unenroll for ALL enrollment/unenrollment jobs --enroll ENROLL Enroll using this ( class ) name. Should be used with the job argument to specify a target job to enroll with ( if there are more than one enrollment jobs ) --unenroll UNENROLL Enroll using this ( class ) name. Should be used with the job argument to specify a job to unenroll ( if there are more than one unenrollment jobs ) -i INPUT, --input INPUT The data input to enroll. Either a pathname to an audio/image/video file or a string for text input --input_list INPUT_LIST A list of files to enroll. One file per line plus the class id to enroll. --path Send the path of the audio instead of a buffer. Server and client must share a filesystem to use this option -s SERVER, --server SERVER The machine the server is running on. Defaults to localhost. -P PORT, --port PORT The port to use. -t TIMEOUT, --timeout TIMEOUT The timeout ( in seconds ) to wait for a response from the server If there is only one supported enrollment job in the workflow, using this utility for enrollment is very similar to the enrollment utilities above; but a workflow is provided instead of a plugin and domain combination. As with the other enrollment utilities, olivepyworkflowenroll supports both single-file enrollment and batch enrollment using an enrollment-formatted text file. Generically, this looks like: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input_list <path to enrollment file> <workflow> olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input <path to audio file> --enroll <class name> <workflow> And a specific example of each: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input ~/path/to/enroll-file1.wav --enroll \"Logan\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list ~/path/to/enrollment_input.txt ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json Important: Note that in OLIVE, when you enroll a class, you are enrolling to a plugin and domain , and enrollments are shared server-wide. Even when you enroll using a workflow and the olivepyworkflow utility, enrollments are associated with the specific plugin/domain that the workflow is using under the hood. Any enrollments made to a workflow will be available to anyone else who may be using that server instance, and will also be made available to anyone interacting with the individual plugin - whether directly or via a workflow. As a more concrete example of this, the \"SmartTranscription\" workflow that is sometimes provided with OLIVE, that performs Speech Activity Detection, Speaker Detection, Language Detection, and Speech Recognition on supported languages has a single plugin that supports enrollments; Speaker Detection. As a result, the workflow is set up to have a single enrollment job, to allow workflow users to enroll new speakers to be detected by this plugin. When enrollment is performed with this workflow, the newly created speaker model is created by and for the Speaker Detection plugin itself, and goes into the global OLIVE enrollment space. If a file is analyzed by directly calling this Speaker Detection plugin, the new enrollment will be part of the pool of target speakers the plugin will search for. More information on this concept of \"Workflow Enrollment Jobs\" is provided in the next section.","title":"Workflow (Enrollment)"},{"location":"clients.html#advanced-workflow-enrollment-jobs","text":"It's rare, but possible for a workflow to bundle multiple enrollment-capable plugin capabilities into one. One example could be combining Speaker Detection in a workflow that also runs Query-by-Example Keyword Spotting , both of which rely on user enrollments to define their target classes. When this happens, if a user wishes to maintain the ability to enroll separate classes into each enrollable plugin, the workflow needs to expose these different enrollment tasks as separate jobs in the workflow enrollment capabilities. If this is necessary, the workflow will come from SRI configured appropriately - the user need only be concerned with how to specify which job to enroll with. To query which enrollment jobs are available to a workflow, use the olivepyworkflowenroll tool with the --print_jobs flag: $ olivepyworkflowenroll --print_jobs <workflow> Investigating the \"SmartTranscription\" workflow we briefly mentioned above: $ olivepyworkflowenroll --print_jobs SmartTranscriptionFull.workflow.json enrolling 0 files Enrollment jobs '[' SDD Enrollment ']' Un-Enrollment jobs '[' SDD Unenrollment ']' We see that there is only a single Enrollment job available; SDD Enrollment . If there were others, they would be listed in this output. Now that the desired job name is known, enrolling with the specified job is done by supplying that job name to the --job flag; in this case: olivepyworkflowenroll - Single File Input $ olivepyworkflowenroll --input ~/path/to/enroll-file1.wav --enroll \"Logan\" --job \"SDD Enrollment\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json olivepyworkflowenroll - Enrollment List Input $ olivepyworkflowenroll --input_list ~/path/to/enrollment_input.txt --job \"SDD Enrollment\" ~/olive5.4.0/oliveAppData/workflows/SmartTranscription.workflow.json","title":"Advanced Workflow Enrollment - Jobs"},{"location":"contact.html","text":"The Team The OLIVE Software and Plugins are developed and maintained by the engineers, scientists, and linguists of the Speech Technology and Research (STAR) Laboratory at SRI International. OLIVE Page at SRI.com For more information about STAR Lab and current research or development, please visit the STAR Lab SRI International page, or email olive-support@sri.com .","title":"Contact"},{"location":"contact.html#the-team","text":"The OLIVE Software and Plugins are developed and maintained by the engineers, scientists, and linguists of the Speech Technology and Research (STAR) Laboratory at SRI International. OLIVE Page at SRI.com For more information about STAR Lab and current research or development, please visit the STAR Lab SRI International page, or email olive-support@sri.com .","title":"The Team"},{"location":"docker.html","text":"OLIVE Installation for container-based Deliveries These instructions cover the traditional Docker container based OLIVE delivery where the container only includes the OLIVE server alone. For the new, multi-server delivery packaging that includes the Raven Web GUI , please refer to the OLIVE Martini Setup Guide. Deploying OLIVE in an Existing Multi-Container Application If deploying the OLIVE Docker image as a single container in a conventional system then skip to the next section . Otherwise, for users deploying OLIVE in an existing multi-container application managed by Docker Compose or Kubernetes, be sure to create OLIVE containers that meet these requirements: The oliveAppData directory, distributed with an OLIVE release, must be mounted as /home/olive/olive inside the docker container. The oliveAppData is the directory that contains the 'plugins' folder with one or more OLIVE plugins. OLIVE will write logs and persistent enrollments to the oliveAppData directory. The plugins folder mounted in /home/olive/olive can vary by container if specific tasks need to be supported by a container. For example, an olive data directory can be mounted that contains only one plugin. The OLIVE server is executed as 'oliveserver' The OLIVE server is accessible via ports 5588 and 5589 (and internally uses port 5590). External clients must have access to these ports on your OLIVE container(s). You can change the default ports used by the OLIVE server via the --port argument. For example, this command will start the OLIVE server using client accessible ports 5591 and 5592: oliveserver --port 5591 Starting the server with different ports is only necessary if the default OLIVE ports are already used by an existing application, or multiple OLIVE containers are addressable at the same hostname. The number of concurrent jobs supported by an OLIVE server can be restricted using the -j argument. For example, to limit the server to one job at a time: oliveserver -j 1 Limiting the number jobs to one in necessary for the TextTransformer plugin, tmt-statistical-v1 If audio needs to be submitted to OLIVE as a path instead of sent as buffer then please contact SRI for some advanced configuration options. Download, Install, and Launch Docker First you will need to obtain and install Docker - this can be done in a number of ways, the easiest of which is likely through the Docker website. The Community Edition is free (though you now need to create an account with Docker to obtain it), should fulfill most or all OLIVE needs, and is available for several operating systems, including Windows 10. Docker is available here - https://www.docker.com/community-edition and up-to-date installation instructions can be found from Docker. Once Docker is installed, launch it to proceed. When running Docker for the first time, if you are using a Windows OS, you may be prompted to enable some Windows features that are not enabled by default, but are required by Docker to run; Hyper-V and Containers. Select \u201cOk\u201d and wait for the computer to restart to enable these features. Docker should automatically start up once the computer has restarted. Adjust Docker settings (RAM, Cores) If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS Download OLIVE Docker Package Now that Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: olive2.0-docker-19Sep2020.tar.gz You should find similar content to below unless told otherwise: olive5.5.1 docs/ - Directory containing the OLIVE documentation oliveDocker/ olive-5.5.1-docker.tar \u2013 Docker image containing OLIVE run.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and oliveserver. run.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and oliveserver. run-shell.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. run-shell.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ sad-dnn-v7.0.0 \u2013 Speech Activity Detection plugin Actual plugins included will depend on the customer, mission, and delivery --optional-- oliveAppDataMT/ plugins/ tmt-statistical-v1.0.0 You may optionally be provided with Machine Translation capabilities, as shown at the end of the list - this plugin has some special runtime constraints, and will be contained in a separate oliveAppData directory as a result. Load the OLIVE Docker Image The first setup step is to load the OLIVE Docker image. Loading the image is as simple as opening a command prompt like PowerShell in Windows or Terminal in MacOS, navigating to the directory containing the OLIVE Docker image, and following the examples below. Loading the docker image (only necessary once). Windows $ cd C : \\ Users \\ < username > \\ olive2 . 0 \\ oliveDocker $ docker load - i olive - 5.5 . 1 - docker . tar macOS / linux $ cd / home /< username >/ olive5 . 5.1 / oliveDocker $ docker load - i olive - 5.5 . 1 - docker . tar This operation can take some time; quite a few minutes, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed. Set up run and run-shell scripts Note that we include two (optionally three, if translation is provided) 'run' scripts in this release. run-shell.sh will open a shell within the container, with the OLIVE environment properly set up, allowing a user to run CLI tasks using localenroll and localanalyze. This script is mainly used for troubleshooting and debugging. The other script(s) will launch an OLIVE server using the olive-5.5.1 container, and vary only slightly. run.sh is a general-purpose script currently configured to launch an OLIVE server that will only restrict the number of jobs/workers based on the number of cores available on the host machine, and will allow parallel processing to scale based on this. This should be used for most purposes. (optional) run_mt.sh is a specialized script that is meant to be used to launch an OLIVE server that only points to a Machine Translation plugin. It is configured to limit the number of jobs/workers to one and does not allow parallel processing. It is also configured to have this server listen on different ports than a standard OLIVE server would, in case you need both OLIVE servers to run on the same host. Note that this distinction is the reason that the plugins for deliveries that include translation are split between oliveAppData and oliveAppDataMT - you should be able to support running two containers; one for all plugins that support parallel processing and one for MT - without having to shuffle plugin directories around. The run.cmd / run.sh and run-shell.cmd / run-shell.sh scripts do most of the heavy-lifting for starting the OLIVE docker and associated processes, but before you can run them, you will need to either set an environment variable, or for a more permanent change, open the desired script with a text editor and edit one or two lines to allow them to establish a shared file location that both the host file system (Windows, Linux, or MacOS) and the OLIVE Docker container can access. Both processes are shown below. Environment Variable The run and run-shell scripts are shipped configured to pull the location of the plugins from an environment variable, so that the scripts don't need to be edited out of the box. This does require these variables to be set each time you will need to run If you would like to permanently edit the scripts instead, refer to the next section, direct script editing . To set this location, you will need to set the OLIVE_APP_DATA environment variable, that will adjust what the OLIVE_DATA variable inside the script is set to. This variable needs to point to the path on the host machine that contains the 'plugins' directory you wish for it to use - for typical OLIVE deliveries, this will be [REPLACE-WITH-LOCAL-PATH]/olive2.0/oliveAppData/. An example of this setup step for the non-translation server: $ export OLIVE_APP_DATA =[ REPLACE-WITH-LOCAL-PATH ] /olive2.0/oliveAppData/ $ ./run.sh Docker will get access to directory: /Users/allen/oliveAppData/ as /home/olive/olive and as /olive-data. Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- ASR asr-dynapy-v2.0.2 2 .0.2 [ 'cmn-tdnnChain-tel-v1' , 'eng-tdnnChain-tel-v1' , 'fas-tdnn-tel-v1' , 'rus-tdnnChain-tel-v1' , 'spa-tdnnChain-tel-v1' ] LDD ldd-sbcEmbed-v1.0.1 1 .0.1 [ 'multi-v1' ] LID lid-embedplda-v2.0.1 2 .0.1 [ 'multi-v1' ] ... etc. --------- Server ready Tue Feb 23 04 :03:13 2021 --------- -- Optional step for Machine Translation Only -- This same step described above is necessary for the run-mt.sh script that will launch the job-limited server that is only running Machine Translation. The only difference is that the environment variable is instead $OLIVE_APP_DATA_MT , and the default location for most OLIVE deliveries will likely be [REPLACE-WITH-LOCAL-PATH]/olive5.5.1/oliveAppDataMT/. $ export OLIVE_APP_DATA_MT= [ REPLACE - WITH - LOCAL - PATH ] /olive5 . 5 . 1/oliveAppDataMT/ $ . /run - mt . sh Docker will get access to directory: /Users/allen/oliveAppDataMT/ as /home/olive/olive and as /olive - data . Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- TMT tmt - statistical - v1 . 0 . 1 1 . 0 . 1 [ 'spa - eng - generic - v2' , 'fre - eng - generic - v1' ] --------- Server ready Tue Feb 23 04:03:13 2021 --------- NOTE that it is still necessary to set the LOCAL_OLIVE_DATA variable within the run-shell script if you wish to have access to shared audio files. On top of setting OLIVE_APP_DATA as above (which will mount plugins inside the container /home/olive/olive/plugins ), you will also need to set the LOCAL_OLIVE_DATA variable within the script. LOCAL_OLIVE_DATA must be set to a location that ends with a directory called olive-data , and can contain scripts, text files, audio files, and any other utilities you might wish to use with the shell while inside your container. To set this variable, you will need to edit this line in side run-shell.sh : LOCAL_OLIVE_DATA=`cd \"$THISDIR/../../../../../../olive-data\" && pwd` So that the path matches where your audio and scripts are stored, for example: LOCAL_OLIVE_DATA=/home/user1/audio/olive-data/ Direct Script Editing The first of these shared file locations, stored as the variable OLIVE_DATA within the run/run-shell scripts, is where the container will write information such as model enrollments and log files, as well as where the server will find plugins. Plugins must be contained in a directory called plugins, located directly within the directory that LOCAL_OLIVE is assigned to. In the examples below, C:\\Users\\<username>\\olive5.5.1\\oliveAppData for the Windows example, and /home/<username>/olive2.0/oliveAppData for the macOS/linux example has a child directory called plugins: C:\\Users\\<username>\\olive5.5.1\\oliveAppData\\plugins or /home/<username>/olive5.5.1/oliveAppData/plugins respectively. The second location is saved into the LOCAL_OLIVE_DATA variable, and should be assigned to a folder that contains audio you wish to process with OLIVE, or where you plan on placing this audio. It can be any directory on the host file system (Windows or MacOS) as long as the current user has access to it and the last directory in the path is a directory named \u2018olive-data\u2019. The files and folders contained within the directory assigned to LOCAL_OLIVE_DATA will be mounted to /olive-data/ within the OLIVE Docker container, and accessible at that location when operating within the container using the run or run-shell scripts. run-shell.sh and run.sh (macOS and linux) # MUST set OLIVE_DATA - For convenience we set it to the value of $OLIVE_APP_DATA , but one could also manually edit if # you don't normally set OLIVE_APP_DATA. The path at OLIVE_DATA must include your OLIVE 'plugins' folder : \" ${ OLIVE_APP_DATA : ? \"ERROR: OLIVE_APP_DATA is not set. Set this to the location of your OLIVE plugins folder\" } \" OLIVE_DATA= $OLIVE_APP_DATA # Optionally set LOCAL_OLIVE_DATA to a path on the host that ends with a directory named 'olive-data' LOCAL_OLIVE_DATA=\"/home/ <username> /olive-data\" run-shell.cmd and run.cmd (Windows) REM You must set LOCAL_OLIVE below to the directory on your local host that contains the OLIVE 'plugins' directory set OLIVE_DATA=C:\\Users\\<username>\\olive2.0\\oliveAppData set LOCAL_OLIVE_DATA=C:\\Users\\<username>\\olive-data Note that for macOS and linux hosts, the OLIVE_DATA variable is automatically assigned to the value of the $OLIVE_APP_DATA environment variable, if it is preferable to set this before calling the run scripts, instead of editing the scripts themselves. Your delivered run.sh and/or run-shell.sh scripts may have had the line that sets OLIVE_DATA modified to match your specific software installation package more closely and make the necessary changes clearer, especially if there are special cases to consider for your delivery, and may look more similar to this: OLIVE_DATA=[REPLACE-WITH-LOCAL-PATH]/olive5.5.1/oliveAppDataMT/ An example of what this might look like after adapting it to your local environment: OLIVE_DATA=/home/users/allen/olive5.5.1/oliveAppDataMT/ or OLIVE_DATA=/Users/allen/olive5.5.1/oliveAppDataMT/ Note that if your run.sh or run-shell.sh scripts look like this, OLIVE_DATA will not be set automatically based on your OLIVE_APP_DATA environment variable, but must be manually changed within the script before running. Run OLIVE scripts Now that all of the prep work has been done, you are ready to launch OLIVE and start running jobs. Windows hosts Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd C:\\Users\\<username>\\olive5.5.1\\oliveDocker $ .\\run.cmd -or- $ .\\run-shell.cmd macOS and linux hosts Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd / home /< username >/ olive5 .5.1 / oliveDocker $ export OLIVE_APP_DATA =/ home /< username >/ olive5 .5.1 / oliveAppData / ( Optional step if you haven 't edited the specified line in the `run` scripts to point to your plugins) $ ./run.sh -or- $ ./run-shell.sh Which script you run will depend on what task you are hoping to complete. The run.cmd or run.sh script will fire up the OLIVE Docker container and immediately load the oliveserver process. This is to be used alongside the OLIVE GUI or another tool that interacts with oliveserver through the OLIVE API. For command line processing, the run-shell.cmd or run-shell.sh scripts will launch the OLIVE Docker container and open a command line prompt with the OLIVE environment properly set up for running localenroll , localanalyze , and other command-line based tasks. Unload OLIVE Docker Container Image To remove/unload a docker image, whether to upgrade OLIVE containers or for any other reason, first, check that the container is actually still loaded. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE olive-5.5.1-docker latest d00396687de1 5 days ago 3.56GB And then unload it. $ docker rmi olive-5.5.1-docker [Optional] Install, set up, and launch OLIVE GUI An installation of OpenJDK 11 is required for running the OLIVE GUI. Instructions for obtaining and installing OpenJDK 11 will depend on your host's OS and can be found here Once you have installed Open JDK 11, you may need to set the JAVA_HOME environment variable, so that the host operating system knows where to find the appropriate JDK. On macOS and linux, this should be done automatically, or can be done as simply as exporting the JAVA_HOME environment variable to the appropriate location for your machine and OS, and/or with the correct settings: $ export JAVA_HOME =/ usr / libexec / java_home - v 11 You should then be free to launch the OLIVE GUI by simply executing the launcher script included in On Windows this process is slightly more complicated, but can be done by right-clicking This PC and selecting Properties. Before continuing, please refer to any additional instructions you may have been given when provided with access to the OLIVE software delivery. This process outlined below is currently being revised to be simpler, and may not be necessary for you to perform, especially for macOS or Windows 10 hosts. From there, select Advanced system settings in the left-side navigation menu. Navigate to the Advanced tab and select the Environment Variables button on the lower right portion of the dialog. From the Environment Variables window, select New from the lower System variables section, and create a new variable named JAVA_HOME that points to the full path of the JDK installation path. Typically, this is C:\\Program Files\\Java\\jdk11\\bin. Finally, you are ready to launch the GUI, either by navigating in File Explorer to the location where the package was uncompressed, then OliveGui/olive-ui/bin/, and double-clicking Nightingale.bat. You can create a shortcut to this file that you can then place on your desktop for more convenient access. Alternatively, it can be launched from the Windows PowerShell by navigating to the same location, then invoking Nightingale.bat. When launching this way, it is possible to provide a config file if desired. $ cd C:\\Users\\<username>\\olive5.5.1\\oliveGui $ .\\Nightingale.bat -c nightingale_config.xml [Optional] If you wish to use the Batch mode of the OLIVE GUI, an additional step is required to allow the Batch GUI to access audio files stored on the host operating system. Once you\u2019ve installed and run the GUI at least once, open and edit the .scenic-properties file that was created in the user\u2019s home folder in the host OS. For example, for a user named \u2018olive\u2019, this file will be created at the location C:\\Users\\olive.scenic-properties. Once you\u2019ve located and opened this file (it is a text file), add the following lines: use_scenic_data=true olive_data_directory=C:\\\\Users\\\\<username>\\\\olive-data Note that it is very important that the directory assigned to olive_data_directory matches the directory assigned to LOCAL_OLIVE_DATA in the step above. Note also that it is necessary to include double backslashes. If you don\u2019t, the Java properties reader strips them from the variable, and the path will not be valid.","title":"OLIVE Installation for container-based Deliveries"},{"location":"docker.html#olive-installation-for-container-based-deliveries","text":"These instructions cover the traditional Docker container based OLIVE delivery where the container only includes the OLIVE server alone. For the new, multi-server delivery packaging that includes the Raven Web GUI , please refer to the OLIVE Martini Setup Guide.","title":"OLIVE Installation for container-based Deliveries"},{"location":"docker.html#deploying-olive-in-an-existing-multi-container-application","text":"If deploying the OLIVE Docker image as a single container in a conventional system then skip to the next section . Otherwise, for users deploying OLIVE in an existing multi-container application managed by Docker Compose or Kubernetes, be sure to create OLIVE containers that meet these requirements: The oliveAppData directory, distributed with an OLIVE release, must be mounted as /home/olive/olive inside the docker container. The oliveAppData is the directory that contains the 'plugins' folder with one or more OLIVE plugins. OLIVE will write logs and persistent enrollments to the oliveAppData directory. The plugins folder mounted in /home/olive/olive can vary by container if specific tasks need to be supported by a container. For example, an olive data directory can be mounted that contains only one plugin. The OLIVE server is executed as 'oliveserver' The OLIVE server is accessible via ports 5588 and 5589 (and internally uses port 5590). External clients must have access to these ports on your OLIVE container(s). You can change the default ports used by the OLIVE server via the --port argument. For example, this command will start the OLIVE server using client accessible ports 5591 and 5592: oliveserver --port 5591 Starting the server with different ports is only necessary if the default OLIVE ports are already used by an existing application, or multiple OLIVE containers are addressable at the same hostname. The number of concurrent jobs supported by an OLIVE server can be restricted using the -j argument. For example, to limit the server to one job at a time: oliveserver -j 1 Limiting the number jobs to one in necessary for the TextTransformer plugin, tmt-statistical-v1 If audio needs to be submitted to OLIVE as a path instead of sent as buffer then please contact SRI for some advanced configuration options.","title":"Deploying OLIVE in an Existing Multi-Container Application"},{"location":"docker.html#download-install-and-launch-docker","text":"First you will need to obtain and install Docker - this can be done in a number of ways, the easiest of which is likely through the Docker website. The Community Edition is free (though you now need to create an account with Docker to obtain it), should fulfill most or all OLIVE needs, and is available for several operating systems, including Windows 10. Docker is available here - https://www.docker.com/community-edition and up-to-date installation instructions can be found from Docker. Once Docker is installed, launch it to proceed. When running Docker for the first time, if you are using a Windows OS, you may be prompted to enable some Windows features that are not enabled by default, but are required by Docker to run; Hyper-V and Containers. Select \u201cOk\u201d and wait for the computer to restart to enable these features. Docker should automatically start up once the computer has restarted.","title":"Download, Install, and Launch Docker"},{"location":"docker.html#adjust-docker-settings-ram-cores","text":"If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS","title":"Adjust Docker settings (RAM, Cores)"},{"location":"docker.html#download-olive-docker-package","text":"Now that Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: olive2.0-docker-19Sep2020.tar.gz You should find similar content to below unless told otherwise: olive5.5.1 docs/ - Directory containing the OLIVE documentation oliveDocker/ olive-5.5.1-docker.tar \u2013 Docker image containing OLIVE run.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and oliveserver. run.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and oliveserver. run-shell.sh - Linux/MacOS version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. run-shell.cmd \u2013 Windows version of script that will launch the OLIVE Docker image and open a command line prompt within the Docker container, for running command-line experiments and tasks. OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ sad-dnn-v7.0.0 \u2013 Speech Activity Detection plugin Actual plugins included will depend on the customer, mission, and delivery --optional-- oliveAppDataMT/ plugins/ tmt-statistical-v1.0.0 You may optionally be provided with Machine Translation capabilities, as shown at the end of the list - this plugin has some special runtime constraints, and will be contained in a separate oliveAppData directory as a result.","title":"Download OLIVE Docker Package"},{"location":"docker.html#load-the-olive-docker-image","text":"The first setup step is to load the OLIVE Docker image. Loading the image is as simple as opening a command prompt like PowerShell in Windows or Terminal in MacOS, navigating to the directory containing the OLIVE Docker image, and following the examples below. Loading the docker image (only necessary once). Windows $ cd C : \\ Users \\ < username > \\ olive2 . 0 \\ oliveDocker $ docker load - i olive - 5.5 . 1 - docker . tar macOS / linux $ cd / home /< username >/ olive5 . 5.1 / oliveDocker $ docker load - i olive - 5.5 . 1 - docker . tar This operation can take some time; quite a few minutes, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed.","title":"Load the OLIVE Docker Image"},{"location":"docker.html#set-up-run-and-run-shell-scripts","text":"Note that we include two (optionally three, if translation is provided) 'run' scripts in this release. run-shell.sh will open a shell within the container, with the OLIVE environment properly set up, allowing a user to run CLI tasks using localenroll and localanalyze. This script is mainly used for troubleshooting and debugging. The other script(s) will launch an OLIVE server using the olive-5.5.1 container, and vary only slightly. run.sh is a general-purpose script currently configured to launch an OLIVE server that will only restrict the number of jobs/workers based on the number of cores available on the host machine, and will allow parallel processing to scale based on this. This should be used for most purposes. (optional) run_mt.sh is a specialized script that is meant to be used to launch an OLIVE server that only points to a Machine Translation plugin. It is configured to limit the number of jobs/workers to one and does not allow parallel processing. It is also configured to have this server listen on different ports than a standard OLIVE server would, in case you need both OLIVE servers to run on the same host. Note that this distinction is the reason that the plugins for deliveries that include translation are split between oliveAppData and oliveAppDataMT - you should be able to support running two containers; one for all plugins that support parallel processing and one for MT - without having to shuffle plugin directories around. The run.cmd / run.sh and run-shell.cmd / run-shell.sh scripts do most of the heavy-lifting for starting the OLIVE docker and associated processes, but before you can run them, you will need to either set an environment variable, or for a more permanent change, open the desired script with a text editor and edit one or two lines to allow them to establish a shared file location that both the host file system (Windows, Linux, or MacOS) and the OLIVE Docker container can access. Both processes are shown below.","title":"Set up run and run-shell scripts"},{"location":"docker.html#environment-variable","text":"The run and run-shell scripts are shipped configured to pull the location of the plugins from an environment variable, so that the scripts don't need to be edited out of the box. This does require these variables to be set each time you will need to run If you would like to permanently edit the scripts instead, refer to the next section, direct script editing . To set this location, you will need to set the OLIVE_APP_DATA environment variable, that will adjust what the OLIVE_DATA variable inside the script is set to. This variable needs to point to the path on the host machine that contains the 'plugins' directory you wish for it to use - for typical OLIVE deliveries, this will be [REPLACE-WITH-LOCAL-PATH]/olive2.0/oliveAppData/. An example of this setup step for the non-translation server: $ export OLIVE_APP_DATA =[ REPLACE-WITH-LOCAL-PATH ] /olive2.0/oliveAppData/ $ ./run.sh Docker will get access to directory: /Users/allen/oliveAppData/ as /home/olive/olive and as /olive-data. Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- ASR asr-dynapy-v2.0.2 2 .0.2 [ 'cmn-tdnnChain-tel-v1' , 'eng-tdnnChain-tel-v1' , 'fas-tdnn-tel-v1' , 'rus-tdnnChain-tel-v1' , 'spa-tdnnChain-tel-v1' ] LDD ldd-sbcEmbed-v1.0.1 1 .0.1 [ 'multi-v1' ] LID lid-embedplda-v2.0.1 2 .0.1 [ 'multi-v1' ] ... etc. --------- Server ready Tue Feb 23 04 :03:13 2021 --------- -- Optional step for Machine Translation Only -- This same step described above is necessary for the run-mt.sh script that will launch the job-limited server that is only running Machine Translation. The only difference is that the environment variable is instead $OLIVE_APP_DATA_MT , and the default location for most OLIVE deliveries will likely be [REPLACE-WITH-LOCAL-PATH]/olive5.5.1/oliveAppDataMT/. $ export OLIVE_APP_DATA_MT= [ REPLACE - WITH - LOCAL - PATH ] /olive5 . 5 . 1/oliveAppDataMT/ $ . /run - mt . sh Docker will get access to directory: /Users/allen/oliveAppDataMT/ as /home/olive/olive and as /olive - data . Plugins should be located at /Users/allen/oliveAppData/plugins TASK PLUGIN VERSION DOMAINS ------- -------------------------------- --------- ------------------------------------------------------------------------------------------------------------------- TMT tmt - statistical - v1 . 0 . 1 1 . 0 . 1 [ 'spa - eng - generic - v2' , 'fre - eng - generic - v1' ] --------- Server ready Tue Feb 23 04:03:13 2021 --------- NOTE that it is still necessary to set the LOCAL_OLIVE_DATA variable within the run-shell script if you wish to have access to shared audio files. On top of setting OLIVE_APP_DATA as above (which will mount plugins inside the container /home/olive/olive/plugins ), you will also need to set the LOCAL_OLIVE_DATA variable within the script. LOCAL_OLIVE_DATA must be set to a location that ends with a directory called olive-data , and can contain scripts, text files, audio files, and any other utilities you might wish to use with the shell while inside your container. To set this variable, you will need to edit this line in side run-shell.sh : LOCAL_OLIVE_DATA=`cd \"$THISDIR/../../../../../../olive-data\" && pwd` So that the path matches where your audio and scripts are stored, for example: LOCAL_OLIVE_DATA=/home/user1/audio/olive-data/","title":"Environment Variable"},{"location":"docker.html#direct-script-editing","text":"The first of these shared file locations, stored as the variable OLIVE_DATA within the run/run-shell scripts, is where the container will write information such as model enrollments and log files, as well as where the server will find plugins. Plugins must be contained in a directory called plugins, located directly within the directory that LOCAL_OLIVE is assigned to. In the examples below, C:\\Users\\<username>\\olive5.5.1\\oliveAppData for the Windows example, and /home/<username>/olive2.0/oliveAppData for the macOS/linux example has a child directory called plugins: C:\\Users\\<username>\\olive5.5.1\\oliveAppData\\plugins or /home/<username>/olive5.5.1/oliveAppData/plugins respectively. The second location is saved into the LOCAL_OLIVE_DATA variable, and should be assigned to a folder that contains audio you wish to process with OLIVE, or where you plan on placing this audio. It can be any directory on the host file system (Windows or MacOS) as long as the current user has access to it and the last directory in the path is a directory named \u2018olive-data\u2019. The files and folders contained within the directory assigned to LOCAL_OLIVE_DATA will be mounted to /olive-data/ within the OLIVE Docker container, and accessible at that location when operating within the container using the run or run-shell scripts. run-shell.sh and run.sh (macOS and linux) # MUST set OLIVE_DATA - For convenience we set it to the value of $OLIVE_APP_DATA , but one could also manually edit if # you don't normally set OLIVE_APP_DATA. The path at OLIVE_DATA must include your OLIVE 'plugins' folder : \" ${ OLIVE_APP_DATA : ? \"ERROR: OLIVE_APP_DATA is not set. Set this to the location of your OLIVE plugins folder\" } \" OLIVE_DATA= $OLIVE_APP_DATA # Optionally set LOCAL_OLIVE_DATA to a path on the host that ends with a directory named 'olive-data' LOCAL_OLIVE_DATA=\"/home/ <username> /olive-data\" run-shell.cmd and run.cmd (Windows) REM You must set LOCAL_OLIVE below to the directory on your local host that contains the OLIVE 'plugins' directory set OLIVE_DATA=C:\\Users\\<username>\\olive2.0\\oliveAppData set LOCAL_OLIVE_DATA=C:\\Users\\<username>\\olive-data Note that for macOS and linux hosts, the OLIVE_DATA variable is automatically assigned to the value of the $OLIVE_APP_DATA environment variable, if it is preferable to set this before calling the run scripts, instead of editing the scripts themselves. Your delivered run.sh and/or run-shell.sh scripts may have had the line that sets OLIVE_DATA modified to match your specific software installation package more closely and make the necessary changes clearer, especially if there are special cases to consider for your delivery, and may look more similar to this: OLIVE_DATA=[REPLACE-WITH-LOCAL-PATH]/olive5.5.1/oliveAppDataMT/ An example of what this might look like after adapting it to your local environment: OLIVE_DATA=/home/users/allen/olive5.5.1/oliveAppDataMT/ or OLIVE_DATA=/Users/allen/olive5.5.1/oliveAppDataMT/ Note that if your run.sh or run-shell.sh scripts look like this, OLIVE_DATA will not be set automatically based on your OLIVE_APP_DATA environment variable, but must be manually changed within the script before running.","title":"Direct Script Editing"},{"location":"docker.html#run-olive-scripts","text":"Now that all of the prep work has been done, you are ready to launch OLIVE and start running jobs.","title":"Run OLIVE scripts"},{"location":"docker.html#windows-hosts","text":"Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd C:\\Users\\<username>\\olive5.5.1\\oliveDocker $ .\\run.cmd -or- $ .\\run-shell.cmd","title":"Windows hosts"},{"location":"docker.html#macos-and-linux-hosts","text":"Launching the OLIVE server inside the container, or opening a shell with the OLIVE environment within the container. $ cd / home /< username >/ olive5 .5.1 / oliveDocker $ export OLIVE_APP_DATA =/ home /< username >/ olive5 .5.1 / oliveAppData / ( Optional step if you haven 't edited the specified line in the `run` scripts to point to your plugins) $ ./run.sh -or- $ ./run-shell.sh Which script you run will depend on what task you are hoping to complete. The run.cmd or run.sh script will fire up the OLIVE Docker container and immediately load the oliveserver process. This is to be used alongside the OLIVE GUI or another tool that interacts with oliveserver through the OLIVE API. For command line processing, the run-shell.cmd or run-shell.sh scripts will launch the OLIVE Docker container and open a command line prompt with the OLIVE environment properly set up for running localenroll , localanalyze , and other command-line based tasks.","title":"macOS and linux hosts"},{"location":"docker.html#unload-olive-docker-container-image","text":"To remove/unload a docker image, whether to upgrade OLIVE containers or for any other reason, first, check that the container is actually still loaded. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE olive-5.5.1-docker latest d00396687de1 5 days ago 3.56GB And then unload it. $ docker rmi olive-5.5.1-docker","title":"Unload OLIVE Docker Container Image"},{"location":"docker.html#optional-install-set-up-and-launch-olive-gui","text":"An installation of OpenJDK 11 is required for running the OLIVE GUI. Instructions for obtaining and installing OpenJDK 11 will depend on your host's OS and can be found here Once you have installed Open JDK 11, you may need to set the JAVA_HOME environment variable, so that the host operating system knows where to find the appropriate JDK. On macOS and linux, this should be done automatically, or can be done as simply as exporting the JAVA_HOME environment variable to the appropriate location for your machine and OS, and/or with the correct settings: $ export JAVA_HOME =/ usr / libexec / java_home - v 11 You should then be free to launch the OLIVE GUI by simply executing the launcher script included in On Windows this process is slightly more complicated, but can be done by right-clicking This PC and selecting Properties. Before continuing, please refer to any additional instructions you may have been given when provided with access to the OLIVE software delivery. This process outlined below is currently being revised to be simpler, and may not be necessary for you to perform, especially for macOS or Windows 10 hosts. From there, select Advanced system settings in the left-side navigation menu. Navigate to the Advanced tab and select the Environment Variables button on the lower right portion of the dialog. From the Environment Variables window, select New from the lower System variables section, and create a new variable named JAVA_HOME that points to the full path of the JDK installation path. Typically, this is C:\\Program Files\\Java\\jdk11\\bin. Finally, you are ready to launch the GUI, either by navigating in File Explorer to the location where the package was uncompressed, then OliveGui/olive-ui/bin/, and double-clicking Nightingale.bat. You can create a shortcut to this file that you can then place on your desktop for more convenient access. Alternatively, it can be launched from the Windows PowerShell by navigating to the same location, then invoking Nightingale.bat. When launching this way, it is possible to provide a config file if desired. $ cd C:\\Users\\<username>\\olive5.5.1\\oliveGui $ .\\Nightingale.bat -c nightingale_config.xml [Optional] If you wish to use the Batch mode of the OLIVE GUI, an additional step is required to allow the Batch GUI to access audio files stored on the host operating system. Once you\u2019ve installed and run the GUI at least once, open and edit the .scenic-properties file that was created in the user\u2019s home folder in the host OS. For example, for a user named \u2018olive\u2019, this file will be created at the location C:\\Users\\olive.scenic-properties. Once you\u2019ve located and opened this file (it is a text file), add the following lines: use_scenic_data=true olive_data_directory=C:\\\\Users\\\\<username>\\\\olive-data Note that it is very important that the directory assigned to olive_data_directory matches the directory assigned to LOCAL_OLIVE_DATA in the step above. Note also that it is necessary to include double backslashes. If you don\u2019t, the Java properties reader strips them from the variable, and the path will not be valid.","title":"[Optional] Install, set up, and launch OLIVE GUI"},{"location":"glossary.html","text":"Glossary / Appendix Below you will find definitions of terms commonly used throughout this documentation. If anything is unclear, please reach out for clarification . General Terms Plugin A module that encapsulates a process designed to perform a specific task (detect speakers, identify languages, find keywords) in a specific way (using a deep neural network embeddings, i-vectors+DNN bottleneck, etc.). A plugin thus contains the plan that links together a series of components (acoustic front-end, representation, classifier, fusion, calibration) into a pipeline. This generally captures a specific approach (algorithm) or process that uses a data model in the domain to perform the task, though the algorithm is generally independent of the data or audio condition specialization in a plugin's domain(s). For more information on the different types and capabilities, see the Plugins information page and domain , below. Domain A domain always resides within a plugin. The domain contains specific information (trained models, parameters, etc.) needed to prepare the plugin for specific operating conditions. Every plugin must have at least one domain, but may have many. Examples include telephone (tel), analog and/or digital push-to-talk (ptt), or distant microphone. Some plugins have a general domain trained on many data types, commonly-called \"multi-condition\". Task In the context of OLIVE, Task typically refers to the goal of a plugin, or the problem it is designed to address. For example, the plugin sad-dnn-v6 has the Task of SAD, or speech activity detection. For more information on the range of Tasks we currently have plugins for, please refer to the Plugins documentation. Class A specific target category of interest to be identified or detected by the system. A class can refer to a variety of things, depending on the respective plugin type it belongs to. For example, a 'class' in the context of a speaker identification plugin is an individual speaker of interest; it is a language or dialect in a LID plugin, a keyword in a KWS or QBE plugin, or a topic when referring to a TPD plugin. Classes can be pre-enrolled within a plugin, as is often the case with language identification plugins, but it is often necessary for end users to enroll their own classes of interest, as in the case of speaker identification plugins. Frame A frame of audio is a very short, typically 10ms slice of audio. FrameScoring plugins will report a score for each frame of audio submitted. Plugin Traits (Common API Processes) A plugin's functionality is defined by the Traits that it implements. Each plugin trait is associated with the set of messages that it is allowed to send, and that must be implemented for proper functionality. Below, these traits and their associated messages are defined. FrameScorer A frame scorer provides a score output for every X ms of an audio file or buffer, generally 10 ms. SAD and VTD are currently the only frame scorers in OLIVE. RegionScorer A region scorer provides scores for audio sub-segments detected within an audio file or buffer. For example, a KWS plugin would provide a keyword detection, its boundaries in time and score. GlobalScorer A global scorer assumes that an audio file or buffer is all of the same class and scores it as a unit. Examples include language identification and speaker verification. Common API Processes Adaptable / Adaptation Adaptation typically uses in-domain data from a specific operational environment to alter the core behavior of the system such that it functions more effectively. Unlike adding data to a class, adaptation is altering the system as a whole and thus produces a new domain. Plugins that are adaptable support either supervised or unsupervised adaptation. Unsupervised adaptation improves performance without human input, using audio examples provided by users or accrued from use in a mission's audio conditions. This type of adaptation is \"triggered\" either when a user-specific amount of data is accrued or explicitly called by the end user application and applied to the plugin. Unsupervised adaptation does not create a new domain, it alters an existing domain, but it is reversible. Supervised adaptation, however, requires human input. Generally data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech/non-speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. Supervised adaptation creates a new domain in most cases. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate. Supervised Adaptation Human assisted improvement of the plugin, generally with feedback to the system in the form of annotations of target phenomena, or error corrections. Unsupervised Adaptation Autonomous adaptation using unlabelled data; requires no human labelling or feedback. This is not currently supported by any OLIVE plugins in the traditional sense, but some plugins do support the ability to perform an Update , which is a form of unsupervised adaptation. Enrollable / Enrollment Enrollment is the mechanism by which target classes are added to a plugin domain. An enroll able plugin allows users to add new classes and augment existing classes. Examples include speaker detection, language recognition and keyword spotting. Augmentable / Augmentation Plugins that support enrollment also support augmentation. Augmentation is simply the process of adding additional data to an existing class. Updateable / Update Updating occurs when a user invokes unsupervised adaptation on a plugin/domain by requesting that the plugin use the operational data examples it has accrued throughout normal usage to update the plugin's parameters and models to better fit the usage environment. Diarization Diarization is the process of automatically segmenting an audio file or stream based on a set of target phenomena. SAD is diarization based on speech and non-speech segments. Speaker diarization segments a files based on speaker changes. Audio Vector An audio vector is a representation of an audio file in a form pre-processed for a specific task. For example, an audio files stored as a speaker vector representation for a speaker detection plugin. This is useful since it is a very compact form of the file that is very small and quick to read into memory and very fast to score against versus reading in a file from disk.","title":"Glossary"},{"location":"glossary.html#glossary-appendix","text":"Below you will find definitions of terms commonly used throughout this documentation. If anything is unclear, please reach out for clarification .","title":"Glossary / Appendix"},{"location":"glossary.html#general-terms","text":"","title":"General Terms"},{"location":"glossary.html#plugin","text":"A module that encapsulates a process designed to perform a specific task (detect speakers, identify languages, find keywords) in a specific way (using a deep neural network embeddings, i-vectors+DNN bottleneck, etc.). A plugin thus contains the plan that links together a series of components (acoustic front-end, representation, classifier, fusion, calibration) into a pipeline. This generally captures a specific approach (algorithm) or process that uses a data model in the domain to perform the task, though the algorithm is generally independent of the data or audio condition specialization in a plugin's domain(s). For more information on the different types and capabilities, see the Plugins information page and domain , below.","title":"Plugin"},{"location":"glossary.html#domain","text":"A domain always resides within a plugin. The domain contains specific information (trained models, parameters, etc.) needed to prepare the plugin for specific operating conditions. Every plugin must have at least one domain, but may have many. Examples include telephone (tel), analog and/or digital push-to-talk (ptt), or distant microphone. Some plugins have a general domain trained on many data types, commonly-called \"multi-condition\".","title":"Domain"},{"location":"glossary.html#task","text":"In the context of OLIVE, Task typically refers to the goal of a plugin, or the problem it is designed to address. For example, the plugin sad-dnn-v6 has the Task of SAD, or speech activity detection. For more information on the range of Tasks we currently have plugins for, please refer to the Plugins documentation.","title":"Task"},{"location":"glossary.html#class","text":"A specific target category of interest to be identified or detected by the system. A class can refer to a variety of things, depending on the respective plugin type it belongs to. For example, a 'class' in the context of a speaker identification plugin is an individual speaker of interest; it is a language or dialect in a LID plugin, a keyword in a KWS or QBE plugin, or a topic when referring to a TPD plugin. Classes can be pre-enrolled within a plugin, as is often the case with language identification plugins, but it is often necessary for end users to enroll their own classes of interest, as in the case of speaker identification plugins.","title":"Class"},{"location":"glossary.html#frame","text":"A frame of audio is a very short, typically 10ms slice of audio. FrameScoring plugins will report a score for each frame of audio submitted.","title":"Frame"},{"location":"glossary.html#plugin-traits-common-api-processes","text":"A plugin's functionality is defined by the Traits that it implements. Each plugin trait is associated with the set of messages that it is allowed to send, and that must be implemented for proper functionality. Below, these traits and their associated messages are defined.","title":"Plugin Traits (Common API Processes)"},{"location":"glossary.html#framescorer","text":"A frame scorer provides a score output for every X ms of an audio file or buffer, generally 10 ms. SAD and VTD are currently the only frame scorers in OLIVE.","title":"FrameScorer"},{"location":"glossary.html#regionscorer","text":"A region scorer provides scores for audio sub-segments detected within an audio file or buffer. For example, a KWS plugin would provide a keyword detection, its boundaries in time and score.","title":"RegionScorer"},{"location":"glossary.html#globalscorer","text":"A global scorer assumes that an audio file or buffer is all of the same class and scores it as a unit. Examples include language identification and speaker verification.","title":"GlobalScorer"},{"location":"glossary.html#common-api-processes","text":"","title":"Common API Processes"},{"location":"glossary.html#adaptable-adaptation","text":"Adaptation typically uses in-domain data from a specific operational environment to alter the core behavior of the system such that it functions more effectively. Unlike adding data to a class, adaptation is altering the system as a whole and thus produces a new domain. Plugins that are adaptable support either supervised or unsupervised adaptation. Unsupervised adaptation improves performance without human input, using audio examples provided by users or accrued from use in a mission's audio conditions. This type of adaptation is \"triggered\" either when a user-specific amount of data is accrued or explicitly called by the end user application and applied to the plugin. Unsupervised adaptation does not create a new domain, it alters an existing domain, but it is reversible. Supervised adaptation, however, requires human input. Generally data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech/non-speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. Supervised adaptation creates a new domain in most cases. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate.","title":"Adaptable / Adaptation"},{"location":"glossary.html#supervised-adaptation","text":"Human assisted improvement of the plugin, generally with feedback to the system in the form of annotations of target phenomena, or error corrections.","title":"Supervised Adaptation"},{"location":"glossary.html#unsupervised-adaptation","text":"Autonomous adaptation using unlabelled data; requires no human labelling or feedback. This is not currently supported by any OLIVE plugins in the traditional sense, but some plugins do support the ability to perform an Update , which is a form of unsupervised adaptation.","title":"Unsupervised Adaptation"},{"location":"glossary.html#enrollable-enrollment","text":"Enrollment is the mechanism by which target classes are added to a plugin domain. An enroll able plugin allows users to add new classes and augment existing classes. Examples include speaker detection, language recognition and keyword spotting.","title":"Enrollable / Enrollment"},{"location":"glossary.html#augmentable-augmentation","text":"Plugins that support enrollment also support augmentation. Augmentation is simply the process of adding additional data to an existing class.","title":"Augmentable / Augmentation"},{"location":"glossary.html#updateable-update","text":"Updating occurs when a user invokes unsupervised adaptation on a plugin/domain by requesting that the plugin use the operational data examples it has accrued throughout normal usage to update the plugin's parameters and models to better fit the usage environment.","title":"Updateable / Update"},{"location":"glossary.html#diarization","text":"Diarization is the process of automatically segmenting an audio file or stream based on a set of target phenomena. SAD is diarization based on speech and non-speech segments. Speaker diarization segments a files based on speaker changes.","title":"Diarization"},{"location":"glossary.html#audio-vector","text":"An audio vector is a representation of an audio file in a form pre-processed for a specific task. For example, an audio files stored as a speaker vector representation for a speaker detection plugin. This is useful since it is a very compact form of the file that is very small and quick to read into memory and very fast to score against versus reading in a file from disk.","title":"Audio Vector"},{"location":"gpu-config.html","text":"GPU-Capable OLIVE Plugin / Domain Configuration Introduction With the release of OLIVE 5.5.0, certain plugin capabilities are now able to leverage GPU hardware to enhance the speed of certain operations and algorithms. This allows us to either use advanced technologies that were previously infeasible to deploy without the GPU speed bump, or to sometimes dramatically increase the speed performance of existing technologies. In order to use GPUs with an OLIVE software instance, three things need to happen; The hardware OLIVE is being installed and run on must have a compatible GPU or GPUs installed. Currently, OLIVE can only use Nvidia GPUs with CUDA cores and properly installed Nvidia drivers. If applicable, you will also need the Nvidia Docker toolkit. Refer to Nvidia's documentation for installation of these. OLIVE itself must be configured and/or launched so that it has access to these GPUs. Refer to the documentation specific to your delivery type for information on how to do this. The most likely appropriate reference for this is the Martini Startup Instructions . Each domain of each plugin that you wish to run on the GPU must be configured to specify this information. This is covered below . Plugin Configuration for GPU Use Enable GPU usage for a plugin's domain To allow a plugin to run on an available GPU, it is crucial that the plugin: Has each desired domain configured to choose a GPU device in its meta.conf file Only has domains configured to select GPU device(s) that are properly exposed to the server by configuring/launghing the server appropriately Configuring a plugin to use a GPU is done at the domain level, by changing the device variable assignment within the domain's meta.conf file. By default, most plugins have this variable assigned to cpu , and as such the domain will run on CPU only, even when running within a GPU-enabled OLIVE server. To assign a domain access to a GPU, change this device assignment from cpu to gpuN where N is the device ID of the GPU to run on, as reported by NVIDIA System Management Interface, or nvidia-smi (more info here ). As already discussed it is critical that the system's GPU device(s) assigned to the domain is exposed to the OLIVE server that will be running this domain. These device IDs can be verified by checking the top left entry for each GPU in the nvidia-smi output; an example can be expanded below. nvidia-smi Example Output (Click to expand) +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.46 Driver Version: 495.46 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:04:00.0 Off | N/A | | 20% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:05:00.0 Off | N/A | | 20% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce ... Off | 00000000:08:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 NVIDIA GeForce ... Off | 00000000:84:00.0 Off | N/A | | 18% 26C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 NVIDIA GeForce ... Off | 00000000:85:00.0 Off | N/A | | 19% 27C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 NVIDIA GeForce ... Off | 00000000:88:00.0 Off | N/A | | 18% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 NVIDIA GeForce ... Off | 00000000:89:00.0 Off | N/A | | 17% 27C P0 73W / 250W | 0MiB / 12212MiB | 1% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Most hardware setups will only have a single GPU available, so enabling this is simply replacing cpu with gpu0 . As an example, the meta.conf for the english-v1 domain of asr-end2end-v1.0.0 in its off-the-shelf format is shown below: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : cpu To instead configure this domain to run on the GPU with ID #0, as it is when delivered with OLIVE 5.5.1, this domain then becomes: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : gpu0 Each domain must be configured separately. If within a single plugin, one domain is configured for GPU, the others don't automatically start using a GPU. By default they will still run on CPU, which for most GPU-capable plugins will run significantly more slowly. By extension, it is not necessary for all domains of a plugin to run on the same GPU. Having this device assignment at the domain level allows the distribution of domains across multiple GPUs. For example, if multiple GPUs are available on a system, lower-memory-usage plugins like SAD, SID, and LID may share a single GPU, while heavier plugins like ASR or MT can be flexible enough to assign different language domains of each plugin across multiple GPUs, spreading the memory load to minimize the chance of exhausting GPU memory, while also saving lost time frequently loading and unloading models. Building off of the example above, if we wanted to run the russian-v1 domain of the same ASR plugin on GPU device #2, so that english-v1 and russian-v1 don't compete with respect to GPU memory, the russian-v1 domain would look like this: label : russian - v1 description : Large vocabulary Russian wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : Russian device : gpu2 The exact ideal configuration may vary greatly depending on specific customer use case and mission needs. Currently supported GPU plugins These plugins currently support GPU operation, when configured as outlined above: sad-dnn-v8.0.0+ (Speech Activity Detection) sid-dplda-v4.0.0+ (Speaker Identification) lid-embedplda-v4.0.0+ (Language Identification) tmt-neural-v1.1.0+ (Text Machine Translation) asr-end2end-v1.0.0+ (Speech Recognition) All are capable of running on CPU as well, though in some cases (notably asr-end2end-v1) at a drastically reduced speed. OLIVE GPU Restrictions and Configuration Notes GPU Compute Mode: \"Default\" vs \"Exclusive\" The OLIVE software currently assumes that any available GPUs are in the \"default\" mode. In testing, some configurations of number of OLIVE server workers have been found to cause unexpected issues if the GPUs are configured to be running in \"Exclusive\" mode. If possible, please configure GPUs that OLIVE will be using in \"Default\" mode, and if this is not possible, please ensure that the number of workers for the GPU-enabled server is specified to be 1. This is configured in the provided docker-compose.yml file. To check the mode of your GPU, you can view the Compute Mode field of the nvidia-smi command output. Please refer to Nvidia's instructions for Nvidia Control Panel or the Usage instructions for nvidia-smi for more information on how to set these modes.","title":"GPU Plugin Configuration"},{"location":"gpu-config.html#gpu-capable-olive-plugin-domain-configuration","text":"","title":"GPU-Capable OLIVE Plugin / Domain Configuration"},{"location":"gpu-config.html#introduction","text":"With the release of OLIVE 5.5.0, certain plugin capabilities are now able to leverage GPU hardware to enhance the speed of certain operations and algorithms. This allows us to either use advanced technologies that were previously infeasible to deploy without the GPU speed bump, or to sometimes dramatically increase the speed performance of existing technologies. In order to use GPUs with an OLIVE software instance, three things need to happen; The hardware OLIVE is being installed and run on must have a compatible GPU or GPUs installed. Currently, OLIVE can only use Nvidia GPUs with CUDA cores and properly installed Nvidia drivers. If applicable, you will also need the Nvidia Docker toolkit. Refer to Nvidia's documentation for installation of these. OLIVE itself must be configured and/or launched so that it has access to these GPUs. Refer to the documentation specific to your delivery type for information on how to do this. The most likely appropriate reference for this is the Martini Startup Instructions . Each domain of each plugin that you wish to run on the GPU must be configured to specify this information. This is covered below .","title":"Introduction"},{"location":"gpu-config.html#plugin-configuration-for-gpu-use","text":"","title":"Plugin Configuration for GPU Use"},{"location":"gpu-config.html#enable-gpu-usage-for-a-plugins-domain","text":"To allow a plugin to run on an available GPU, it is crucial that the plugin: Has each desired domain configured to choose a GPU device in its meta.conf file Only has domains configured to select GPU device(s) that are properly exposed to the server by configuring/launghing the server appropriately Configuring a plugin to use a GPU is done at the domain level, by changing the device variable assignment within the domain's meta.conf file. By default, most plugins have this variable assigned to cpu , and as such the domain will run on CPU only, even when running within a GPU-enabled OLIVE server. To assign a domain access to a GPU, change this device assignment from cpu to gpuN where N is the device ID of the GPU to run on, as reported by NVIDIA System Management Interface, or nvidia-smi (more info here ). As already discussed it is critical that the system's GPU device(s) assigned to the domain is exposed to the OLIVE server that will be running this domain. These device IDs can be verified by checking the top left entry for each GPU in the nvidia-smi output; an example can be expanded below. nvidia-smi Example Output (Click to expand) +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.46 Driver Version: 495.46 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:04:00.0 Off | N/A | | 20% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:05:00.0 Off | N/A | | 20% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce ... Off | 00000000:08:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 NVIDIA GeForce ... Off | 00000000:84:00.0 Off | N/A | | 18% 26C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 NVIDIA GeForce ... Off | 00000000:85:00.0 Off | N/A | | 19% 27C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 NVIDIA GeForce ... Off | 00000000:88:00.0 Off | N/A | | 18% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 NVIDIA GeForce ... Off | 00000000:89:00.0 Off | N/A | | 17% 27C P0 73W / 250W | 0MiB / 12212MiB | 1% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Most hardware setups will only have a single GPU available, so enabling this is simply replacing cpu with gpu0 . As an example, the meta.conf for the english-v1 domain of asr-end2end-v1.0.0 in its off-the-shelf format is shown below: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : cpu To instead configure this domain to run on the GPU with ID #0, as it is when delivered with OLIVE 5.5.1, this domain then becomes: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : gpu0 Each domain must be configured separately. If within a single plugin, one domain is configured for GPU, the others don't automatically start using a GPU. By default they will still run on CPU, which for most GPU-capable plugins will run significantly more slowly. By extension, it is not necessary for all domains of a plugin to run on the same GPU. Having this device assignment at the domain level allows the distribution of domains across multiple GPUs. For example, if multiple GPUs are available on a system, lower-memory-usage plugins like SAD, SID, and LID may share a single GPU, while heavier plugins like ASR or MT can be flexible enough to assign different language domains of each plugin across multiple GPUs, spreading the memory load to minimize the chance of exhausting GPU memory, while also saving lost time frequently loading and unloading models. Building off of the example above, if we wanted to run the russian-v1 domain of the same ASR plugin on GPU device #2, so that english-v1 and russian-v1 don't compete with respect to GPU memory, the russian-v1 domain would look like this: label : russian - v1 description : Large vocabulary Russian wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : Russian device : gpu2 The exact ideal configuration may vary greatly depending on specific customer use case and mission needs.","title":"Enable GPU usage for a plugin's domain"},{"location":"gpu-config.html#currently-supported-gpu-plugins","text":"These plugins currently support GPU operation, when configured as outlined above: sad-dnn-v8.0.0+ (Speech Activity Detection) sid-dplda-v4.0.0+ (Speaker Identification) lid-embedplda-v4.0.0+ (Language Identification) tmt-neural-v1.1.0+ (Text Machine Translation) asr-end2end-v1.0.0+ (Speech Recognition) All are capable of running on CPU as well, though in some cases (notably asr-end2end-v1) at a drastically reduced speed.","title":"Currently supported GPU plugins"},{"location":"gpu-config.html#olive-gpu-restrictions-and-configuration-notes","text":"","title":"OLIVE GPU Restrictions and Configuration Notes"},{"location":"gpu-config.html#gpu-compute-mode-default-vs-exclusive","text":"The OLIVE software currently assumes that any available GPUs are in the \"default\" mode. In testing, some configurations of number of OLIVE server workers have been found to cause unexpected issues if the GPUs are configured to be running in \"Exclusive\" mode. If possible, please configure GPUs that OLIVE will be using in \"Default\" mode, and if this is not possible, please ensure that the number of workers for the GPU-enabled server is specified to be 1. This is configured in the provided docker-compose.yml file. To check the mode of your GPU, you can view the Compute Mode field of the nvidia-smi command output. Please refer to Nvidia's instructions for Nvidia Control Panel or the Usage instructions for nvidia-smi for more information on how to set these modes.","title":"GPU Compute Mode: \"Default\" vs \"Exclusive\""},{"location":"gpu.html","text":"GPU-Capable OLIVE Docker Installation and Setup OLIVE Folder Structure Overview The initial release of GPU-enabled OLIVE follows the structure described below. Note that this is similar overall to previous deliveries but differs significantly from both native linux-based packages and docker-based packages in the past regarding how the server is started and managed. The important differences can be seen in the oliveDocker/ directory. If the OLIVE package you were provided does not match this formatting, please refer to the appropriate setup guide for your delivery. The OLIVE delivery typically comes in a single archive: olive5.5.1-DDMonthYYYY.tar.gz That unpacks into a structure resembling: - olive5 . 5 . 1 - api - Java and Python example client API implementation code and CLI client utilities - java - python - docs / - Directory containing the OLIVE documentation - index . html - Open this in a web browser to view - oliveDocker / - olive + runtime - 5 . 5 . 1 - Ubuntu - 20 . 04 - x86_64 . tar . gz - OLIVE Core Software and Runtime Bundle - Dockerfile - docker - compose . yml - OliveGUI / - The OLIVE Nightingale GUI ( not included in all deliveries ) - bin / - Nightingale - oliveAppData / - plugins / - sad - dnn - v7 . 0 . 0 ( example ) \u2013 Speech Activity Detection plugin - Actual plugins included will depend on the customer , mission , and delivery - oliveAppDataGPU / - plugins / - asr - end2end - v1 . 0 . 0 ( example ) - Speech Recognition ( end - to - end ) plugin configured to run on GPU - Actual plugins included will depend on the customer , mission , and delivery The actual plugins included will vary from customer to customer, and may even vary between use case configurations within a customer integration. Install and Start Docker Before you can get started installing and running OLIVE, you'll need to make sure you have fully installed and configured Docker. The proper installation steps vary depending on your host OS, so please refer to the appropriate official Docker installation instructions: Docker Desktop for Windows Docker Desktop for macOS Docker Engine for Ubuntu Docker for Ubuntu is especially important to follow the official steps closely, as there are additional important post-installation steps to perform to make sure docker runs smoothly on your system. Note that if installing into an Ubuntu instance running on WSL2, systemctl is not used on such systems. This means that some of the commands provided in the Docker for Ubuntu instructions above may not succeed as written; notably for starting and stopping the Docker service. Please use service for these commands instead: $ sudo service docker start In addition, if using Docker for Ubuntu, the Nvidia drivers must be installed separately. This doesn't seem to be necessary if using Docker Desktop. Instructions for this installation can be found here, from Nvidia . Before moving on, be sure that the docker service has been started. Build and Launch OLIVE Docker The core of the OLIVE software is contained within oliveDocker/olive+runtime-5.5.1-Ubuntu-20.04-x86_64.tar.gz archive. Each delivery includes a docker-compose.yml and Dockerfile that informs Docker how to build and launch this into a running OLIVE server. The process for building the OLIVE image is: $ cd olive5.5.0/oliveDocker/ $ docker compose build This only needs to be performed once per machine. Once this is complete, launching the server can be done with the following command, from the same location: $ docker compose up This call will start the OLIVE image and launch multiple OLIVE servers according to the configuration contained in docker-compose.yml ; the default configuration is described below. Default OLIVE Server Configuration As delivered, performing the steps above will launch two OLIVE servers. The first will only perform CPU processing, and has access to the plugins contained in: `olive5.5.1/oliveAppData/plugins/` This server is analogous to previous Docker-based OLIVE deliveries. It listens on the same ports as before, and is configured the same way, such that the number of workers (concurrent server jobs) is automatically limited based on the number of threads available on the CPU hardware. The second server has access to the GPU and can use this if the plugin domains have been properly configured, and assumes there is only one GPU available, device 0 according to nvidia-smi . It can run with the plugins contained in: `olive5.5.1/oliveAppDataGPU/plugins/` It listens on different ports (see next section), and is configured to have a single worker to stabilize GPU memory usage. This is a global setting for an OLIVE server, so separating the CPU and GPU plugins into separate OLIVE servers allows us to run the CPU server unthrottled, allowing a number of parallel jobs based on the number of cores or threads available on the host hardware, without being limited by the setting of the GPU server that is relying on a single (very fast) worker thread. By default, this initial delivery only includes a single plugin with domains configured and placed such that it will run on the GPU: asr-end2end-v1.0.0 See below for instructions on configuring others to run on the GPU, and a list of released GPU-capable plugins. Interacting with OLIVE GPU Server Once an OLIVE 5.5.1 GPU-capable server is running, tasking it from a client is largely identical to before; whether from the Java or python client APIs or one of our GUIs. The only salient difference is that the multi-server approach means that each server is listening on a different set of ports. By default, those ports are: Server Server Request Port Server Status Port CPU 5588 5589 GPU 6588 6589 These are the defaults, which can be configured in the docker-compose.yml file as shown below. Be sure to route requests to the appropriate server. If using most of SRI's client utilities or UIs, the default ports used (5588/5589) will contact the CPU-only server. Sending requests to the GPU server instead will require a flag (i.e. --port 6588 if using the Java and Python client utilities ) or other configuration changes. Docker Image Configuration Configuration of the number and setup of OLIVE servers is controlled by settings within the included docker-compose.yml file. The entire file is displayed below, with excerpts showing which section is configuring the CPU server and the GPU server. This is followed by a section for each of the most important and likely to be altered variables. Note that none of this needs to be touched if running the default configuration, on the assumed default hardware (single-GPU machine). docker-compose.yml Whole File version : \"3.9\" x-common-variables : &common-variables FONTCONFIG_PATH : /etc/fonts/ FONTCONFIG_FILE : /etc/fonts/fonts.conf XDG_CACHE_HOME : /tmp MPLCONFIGDIR : /tmp services : runtime-cpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" ports : - \"5588:5588\" - \"5589:5589\" - \"5590:5590\" - \"5591:5591\" volumes : - ../oliveAppData:/home/olive/olive environment : *common-variables shm_size : '4gb' runtime-gpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" - \"--workers=1\" ports : - \"6588:5588\" - \"6589:5589\" - \"6590:5590\" - \"6591:5591\" volumes : - ../oliveAppDataGPU:/home/olive/olive environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' shm_size : '4gb' deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] CPU Server Section ... services : runtime-cpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" ports : - \"5588:5588\" - \"5589:5589\" - \"5590:5590\" - \"5591:5591\" volumes : - ../oliveAppData:/home/olive/olive environment : *common-variables shm_size : '4gb' ... GPU Server Section ... services : ... runtime-gpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" - \"--workers=1\" ports : - \"6588:5588\" - \"6589:5589\" - \"6590:5590\" - \"6591:5591\" volumes : - ../oliveAppDataGPU:/home/olive/olive environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' shm_size : '4gb' deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] Exposing GPU(s) to an OLIVE Server (CUDA_VISIBLE_DEVICES) If your hardware contains multiple GPUs or if your GPU does not identify as device 0 for any reason, you may need to modify the CUDA_VISIBLE_DEVICES environment variable used by OLIVE GPU server and/or the device_ids argument in the resource allocation section of the docker-compose.yml . The former is controlled by: services: runtime-gpu: environment: CUDA_VISIBLE_DEVICES And the latter by: services: runtime-gpu: deploy: resources: reservations: devices: device_ids Both can be seen in the excerpt below: ... services : ... runtime-gpu : ... environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' ... deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] For more information on how and why to set CUDA_VISIBLE_DEVICES , see Nvidia's documentation . This variable is used to control what GPU(s) the server is able to see and lean on during processing. Exposing a GPU device here makes the GPU visible to OLIVE, but does not force anything to run on it. For configuring a plugin/domain to use an available GPU, see the relevant section below. For more information about device_ids , how it should be set, and how this may interact with other settings, see the Docker documentation on GPU support . This page also covers a possible, untested alternative, of using count instead of device_ids . Both of these arguments are controlling the container's GPU access; as such, these should likely match, and both pass the same list of GPUs and in the same order, but as each application and each customer's hardware setup and needs may vary, some testing and tweaking may be necessary on the client end to ensure the behavior is as desired. Note that these parameters identify a GPU by its device ID as reported by nvidia-smi . These can be verified by checking the top left entry for each GPU in the nvidia-smi output; an example is shown below. nvidia-smi Example Output (Click to expand) +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.46 Driver Version: 495.46 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:04:00.0 Off | N/A | | 20% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:05:00.0 Off | N/A | | 20% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce ... Off | 00000000:08:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 NVIDIA GeForce ... Off | 00000000:84:00.0 Off | N/A | | 18% 26C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 NVIDIA GeForce ... Off | 00000000:85:00.0 Off | N/A | | 19% 27C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 NVIDIA GeForce ... Off | 00000000:88:00.0 Off | N/A | | 18% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 NVIDIA GeForce ... Off | 00000000:89:00.0 Off | N/A | | 17% 27C P0 73W / 250W | 0MiB / 12212MiB | 1% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ Plugin Location (OLIVE_APP_DATA) As described above, the default configuration separates CPU-configured plugins into olive5.5.1/oliveAppData/plugins/ and GPU-enabled/configured plugins into olive5.5.1/oliveAppDataGPU/plugins/ . If maintaining a two-server setup similar to the delivered configuration, these do not need to be changed. To convert a GPU-capable plugin from CPU, first it must be moved or placed into the oliveAppDataGPU plugins directory. Note that if moving a plugin that has enrollment capability, any enrolled models will not be transferred. Re-enrollment must be performed once the plugin is moved and reconfigured, or for advanced users, the enrollments can be moved separately. If the name or location of the oliveAppData directories need to change for any reason, the docker-compose.yml must be updated to reflect this, specifically the volumes mounted for each server that are mapped to /home/olive/olive/ within the containers. This excerpt shows where this is set for the GPU server: ... services : ... runtime-gpu : ... volumes : - ../oliveAppDataGPU:/home/olive/olive ... GPU Server Concurrent Processing Jobs (--workers) [Experimental] If maximum batch throughput is important for your application, and it is known that memory requirements of your use case won't overwhelm the available memory of the GPU, the number of server workers for the GPU server can be increased. This is experimental and may affect overall server performance (speed) and stability. This setting controls the number of jobs the server can process simultaneously, and for GPU-enabled domains will increase the number of times the associated models are loaded into memory. As the percentage of used GPU memory increases, speed may decrease as less optimal pathways are used. This also increases the likelihood of ungracefully exhausting the available GPU memory, which may cause a system crash. The majority of the OLIVE 5.5.1 testing was performed with --workers set to 1 . To increase the number of server workers, increase the argument passed to the --workers flag in the docker-compose.yml , under services: runtime-gpu: command: ... services : ... runtime-gpu : ... command : ... - \"--workers=1\" Plugin Configuration for GPU Use Enable GPU usage for a plugin's domain To allow a plugin to run on an available GPU, it is crucial that the plugin: Is located in the plugins directory of a GPU enabled server (oliveAppDataGPU/plugins by default) Has each desired domain configured to choose a GPU device in its meta.conf file Only has domains configured to select GPU device(s) that are properly exposed to the server via CUDA_VISIBLE_DEVICES and Docker's device_ids More information for #1 and #3 can be found in the appropriate sections above. Be sure to move the plugin to oliveAppDataGPU/plugins/ when reconfiguring, and restarting any running servers. Be sure to double-check that the device(s) you are configuring each plugin domain to use are actually exposed to the server that will be using that plugin/domain. Configuring a plugin to use a GPU is done at the domain level, by changing the device variable assignment within the domain's meta.conf file. By default, most plugins have this variable assigned to cpu , and as such the domain will run on CPU only, even when running within a GPU-enabled OLIVE server. To assign a domain access to a GPU, change this device assignment from cpu to gpuN where N is the device ID of the GPU to run on, as reported by nvidia-smi (more info here ). As already discussed it is critical that the device assigned to the domain is exposed to the OLIVE server that will be running this domain. Most hardware setups will only have a single GPU available, so enabling this is simply replacing cpu with gpu0 . As an example, the meta.conf for the english-v1 domain of asr-end2end-v1.0.0 in its off-the-shelf format is shown below: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : cpu To instead configure this domain to run on the GPU with ID #0, as it is when delivered with OLIVE 5.5.1, this domain then becomes: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : gpu0 Each domain must be configured separately. If within a single plugin, one domain is configured for GPU, the others don't automatically start using a GPU. By default they will still run on CPU, which for most plugins will run significantly more slowly. By extension, it is not necessary for all domains of a plugin to run on the same GPU. Having this device assignment at the domain level allows the distribution of domains across multiple GPUs. For example, if multiple GPUs are available on a system, lower-memory-usage plugins like SAD, SID, and LID may share a single GPU, while heavier plugins like ASR or MT can be flexible enough to assign different language domains of each plugin across multiple GPUs, spreading the memory load to minimize the chance of exhausting GPU memory, while also saving lost time frequently loading and unloading models. Building off of the example above, if we wanted to run the russian-v1 domain of the same ASR plugin on GPU device #2, so that english-v1 and russian-v1 don't compete with respect to GPU memory, the russian-v1 domain would look like this: label : russian - v1 description : Large vocabulary Russian wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : Russian device : gpu2 Note that in these examples, GPU device 0 and GPU device 2 must both be listed in CUDA_VISIBLE_DEVICES and device_ids as outlined above . Example docker-compose.yml modification ``` yaml ... services: ... runtime-gpu: ... environment: <<: *common-variables CUDA_VISIBLE_DEVICES: '0,2' ... deploy: resources: reservations: devices: - driver: nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids: ['0', '2'] capabilities: [gpu] The exact ideal configuration may vary greatly depending on specific customer use case and mission needs. Currently supported GPU plugins These plugins currently support GPU operation, when configured as outlined above: sad-dnn-v8 (Speech Activity Detection) sid-dplda-v4 (Speaker Identification) lid-embedplda-v4 (Language Identification) tmt-neural-v1.1.0 (Text Machine Translation) asr-end2end-v1 (Speech Recognition) All are capable of running on CPU as well, though in some cases (notably asr-end2end-v1) at a drastically reduced speed. OLIVE GPU Restrictions and Configuration Notes GPU Compute Mode: \"Default\" vs \"Exclusive\" The OLIVE software currently assumes that any available GPUs are in the \"default\" mode. In testing, some configurations of number of OLIVE server workers have been found to cause unexpected issues if the GPUs are configured to be running in \"Exclusive\" mode. If possible, please configure GPUs that OLIVE will be using in \"Default\" mode, and if this is not possible, please ensure that the number of workers for the GPU-enabled server is specified to be 1. This is configured in the provided docker-compose.yml file. To check the mode of your GPU, you can view the Compute Mode field of the nvidia-smi command output. Please refer to Nvidia's instructions for Nvidia Control Panel or the Usage instructions for nvidia-smi for more information on how to set these modes. Workflow Restrictions Plugins can't currently pass jobs between separate OLIVE servers. This means that if you have a workflow that requires several plugins, each of those plugins must be visible to the OLIVE server that is receiving the workflow job request. If your current configuration separates any of these plugins, your plugins directories must be reconfigured so that all required plugins are colocated within the same server. There are two possible workarounds. The first would be disabling GPU capabilities in some plugins and moving them to the CPU oliveAppData/ directory, which may cause them to run much more slowly, but will allow full job parallelization. An alternative would be to move CPU-only plugins into the GPU-enabled server, leaving them configured to run on the CPU. This would allow the GPU-enabled plugins to run at full speed, but would limit parallel processing due to having a single worker thread. Future work in OLIVE hopes to address these first-release GPU limitations to reduce and eventually eliminate workarounds and restrictions like this.","title":"GPU-Capable OLIVE Docker Installation and Setup"},{"location":"gpu.html#gpu-capable-olive-docker-installation-and-setup","text":"","title":"GPU-Capable OLIVE Docker Installation and Setup"},{"location":"gpu.html#olive-folder-structure-overview","text":"The initial release of GPU-enabled OLIVE follows the structure described below. Note that this is similar overall to previous deliveries but differs significantly from both native linux-based packages and docker-based packages in the past regarding how the server is started and managed. The important differences can be seen in the oliveDocker/ directory. If the OLIVE package you were provided does not match this formatting, please refer to the appropriate setup guide for your delivery. The OLIVE delivery typically comes in a single archive: olive5.5.1-DDMonthYYYY.tar.gz That unpacks into a structure resembling: - olive5 . 5 . 1 - api - Java and Python example client API implementation code and CLI client utilities - java - python - docs / - Directory containing the OLIVE documentation - index . html - Open this in a web browser to view - oliveDocker / - olive + runtime - 5 . 5 . 1 - Ubuntu - 20 . 04 - x86_64 . tar . gz - OLIVE Core Software and Runtime Bundle - Dockerfile - docker - compose . yml - OliveGUI / - The OLIVE Nightingale GUI ( not included in all deliveries ) - bin / - Nightingale - oliveAppData / - plugins / - sad - dnn - v7 . 0 . 0 ( example ) \u2013 Speech Activity Detection plugin - Actual plugins included will depend on the customer , mission , and delivery - oliveAppDataGPU / - plugins / - asr - end2end - v1 . 0 . 0 ( example ) - Speech Recognition ( end - to - end ) plugin configured to run on GPU - Actual plugins included will depend on the customer , mission , and delivery The actual plugins included will vary from customer to customer, and may even vary between use case configurations within a customer integration.","title":"OLIVE Folder Structure Overview"},{"location":"gpu.html#install-and-start-docker","text":"Before you can get started installing and running OLIVE, you'll need to make sure you have fully installed and configured Docker. The proper installation steps vary depending on your host OS, so please refer to the appropriate official Docker installation instructions: Docker Desktop for Windows Docker Desktop for macOS Docker Engine for Ubuntu Docker for Ubuntu is especially important to follow the official steps closely, as there are additional important post-installation steps to perform to make sure docker runs smoothly on your system. Note that if installing into an Ubuntu instance running on WSL2, systemctl is not used on such systems. This means that some of the commands provided in the Docker for Ubuntu instructions above may not succeed as written; notably for starting and stopping the Docker service. Please use service for these commands instead: $ sudo service docker start In addition, if using Docker for Ubuntu, the Nvidia drivers must be installed separately. This doesn't seem to be necessary if using Docker Desktop. Instructions for this installation can be found here, from Nvidia . Before moving on, be sure that the docker service has been started.","title":"Install and Start Docker"},{"location":"gpu.html#build-and-launch-olive-docker","text":"The core of the OLIVE software is contained within oliveDocker/olive+runtime-5.5.1-Ubuntu-20.04-x86_64.tar.gz archive. Each delivery includes a docker-compose.yml and Dockerfile that informs Docker how to build and launch this into a running OLIVE server. The process for building the OLIVE image is: $ cd olive5.5.0/oliveDocker/ $ docker compose build This only needs to be performed once per machine. Once this is complete, launching the server can be done with the following command, from the same location: $ docker compose up This call will start the OLIVE image and launch multiple OLIVE servers according to the configuration contained in docker-compose.yml ; the default configuration is described below.","title":"Build and Launch OLIVE Docker"},{"location":"gpu.html#default-olive-server-configuration","text":"As delivered, performing the steps above will launch two OLIVE servers. The first will only perform CPU processing, and has access to the plugins contained in: `olive5.5.1/oliveAppData/plugins/` This server is analogous to previous Docker-based OLIVE deliveries. It listens on the same ports as before, and is configured the same way, such that the number of workers (concurrent server jobs) is automatically limited based on the number of threads available on the CPU hardware. The second server has access to the GPU and can use this if the plugin domains have been properly configured, and assumes there is only one GPU available, device 0 according to nvidia-smi . It can run with the plugins contained in: `olive5.5.1/oliveAppDataGPU/plugins/` It listens on different ports (see next section), and is configured to have a single worker to stabilize GPU memory usage. This is a global setting for an OLIVE server, so separating the CPU and GPU plugins into separate OLIVE servers allows us to run the CPU server unthrottled, allowing a number of parallel jobs based on the number of cores or threads available on the host hardware, without being limited by the setting of the GPU server that is relying on a single (very fast) worker thread. By default, this initial delivery only includes a single plugin with domains configured and placed such that it will run on the GPU: asr-end2end-v1.0.0 See below for instructions on configuring others to run on the GPU, and a list of released GPU-capable plugins.","title":"Default OLIVE Server Configuration"},{"location":"gpu.html#interacting-with-olive-gpu-server","text":"Once an OLIVE 5.5.1 GPU-capable server is running, tasking it from a client is largely identical to before; whether from the Java or python client APIs or one of our GUIs. The only salient difference is that the multi-server approach means that each server is listening on a different set of ports. By default, those ports are: Server Server Request Port Server Status Port CPU 5588 5589 GPU 6588 6589 These are the defaults, which can be configured in the docker-compose.yml file as shown below. Be sure to route requests to the appropriate server. If using most of SRI's client utilities or UIs, the default ports used (5588/5589) will contact the CPU-only server. Sending requests to the GPU server instead will require a flag (i.e. --port 6588 if using the Java and Python client utilities ) or other configuration changes.","title":"Interacting with OLIVE GPU Server"},{"location":"gpu.html#docker-image-configuration","text":"Configuration of the number and setup of OLIVE servers is controlled by settings within the included docker-compose.yml file. The entire file is displayed below, with excerpts showing which section is configuring the CPU server and the GPU server. This is followed by a section for each of the most important and likely to be altered variables. Note that none of this needs to be touched if running the default configuration, on the assumed default hardware (single-GPU machine).","title":"Docker Image Configuration"},{"location":"gpu.html#docker-composeyml","text":"Whole File version : \"3.9\" x-common-variables : &common-variables FONTCONFIG_PATH : /etc/fonts/ FONTCONFIG_FILE : /etc/fonts/fonts.conf XDG_CACHE_HOME : /tmp MPLCONFIGDIR : /tmp services : runtime-cpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" ports : - \"5588:5588\" - \"5589:5589\" - \"5590:5590\" - \"5591:5591\" volumes : - ../oliveAppData:/home/olive/olive environment : *common-variables shm_size : '4gb' runtime-gpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" - \"--workers=1\" ports : - \"6588:5588\" - \"6589:5589\" - \"6590:5590\" - \"6591:5591\" volumes : - ../oliveAppDataGPU:/home/olive/olive environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' shm_size : '4gb' deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] CPU Server Section ... services : runtime-cpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" ports : - \"5588:5588\" - \"5589:5589\" - \"5590:5590\" - \"5591:5591\" volumes : - ../oliveAppData:/home/olive/olive environment : *common-variables shm_size : '4gb' ... GPU Server Section ... services : ... runtime-gpu : build : context : . args : gpu_or_cpu : gpu user : 1000:1000 command : - \"--verbose\" - \"--debug\" - \"--workers=1\" ports : - \"6588:5588\" - \"6589:5589\" - \"6590:5590\" - \"6591:5591\" volumes : - ../oliveAppDataGPU:/home/olive/olive environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' shm_size : '4gb' deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ]","title":"docker-compose.yml"},{"location":"gpu.html#exposing-gpus-to-an-olive-server-cuda_visible_devices","text":"If your hardware contains multiple GPUs or if your GPU does not identify as device 0 for any reason, you may need to modify the CUDA_VISIBLE_DEVICES environment variable used by OLIVE GPU server and/or the device_ids argument in the resource allocation section of the docker-compose.yml . The former is controlled by: services: runtime-gpu: environment: CUDA_VISIBLE_DEVICES And the latter by: services: runtime-gpu: deploy: resources: reservations: devices: device_ids Both can be seen in the excerpt below: ... services : ... runtime-gpu : ... environment : << : *common-variables CUDA_VISIBLE_DEVICES : '0' ... deploy : resources : reservations : devices : - driver : nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids : [ \"0\" ] capabilities : [ gpu ] For more information on how and why to set CUDA_VISIBLE_DEVICES , see Nvidia's documentation . This variable is used to control what GPU(s) the server is able to see and lean on during processing. Exposing a GPU device here makes the GPU visible to OLIVE, but does not force anything to run on it. For configuring a plugin/domain to use an available GPU, see the relevant section below. For more information about device_ids , how it should be set, and how this may interact with other settings, see the Docker documentation on GPU support . This page also covers a possible, untested alternative, of using count instead of device_ids . Both of these arguments are controlling the container's GPU access; as such, these should likely match, and both pass the same list of GPUs and in the same order, but as each application and each customer's hardware setup and needs may vary, some testing and tweaking may be necessary on the client end to ensure the behavior is as desired. Note that these parameters identify a GPU by its device ID as reported by nvidia-smi . These can be verified by checking the top left entry for each GPU in the nvidia-smi output; an example is shown below. nvidia-smi Example Output (Click to expand) +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.46 Driver Version: 495.46 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:04:00.0 Off | N/A | | 20% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 1 NVIDIA GeForce ... Off | 00000000:05:00.0 Off | N/A | | 20% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 2 NVIDIA GeForce ... Off | 00000000:08:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 3 NVIDIA GeForce ... Off | 00000000:09:00.0 Off | N/A | | 19% 28C P0 73W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 4 NVIDIA GeForce ... Off | 00000000:84:00.0 Off | N/A | | 18% 26C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 5 NVIDIA GeForce ... Off | 00000000:85:00.0 Off | N/A | | 19% 27C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 6 NVIDIA GeForce ... Off | 00000000:88:00.0 Off | N/A | | 18% 28C P0 71W / 250W | 0MiB / 12212MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ | 7 NVIDIA GeForce ... Off | 00000000:89:00.0 Off | N/A | | 17% 27C P0 73W / 250W | 0MiB / 12212MiB | 1% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+","title":"Exposing GPU(s) to an OLIVE Server (CUDA_VISIBLE_DEVICES)"},{"location":"gpu.html#plugin-location-olive_app_data","text":"As described above, the default configuration separates CPU-configured plugins into olive5.5.1/oliveAppData/plugins/ and GPU-enabled/configured plugins into olive5.5.1/oliveAppDataGPU/plugins/ . If maintaining a two-server setup similar to the delivered configuration, these do not need to be changed. To convert a GPU-capable plugin from CPU, first it must be moved or placed into the oliveAppDataGPU plugins directory. Note that if moving a plugin that has enrollment capability, any enrolled models will not be transferred. Re-enrollment must be performed once the plugin is moved and reconfigured, or for advanced users, the enrollments can be moved separately. If the name or location of the oliveAppData directories need to change for any reason, the docker-compose.yml must be updated to reflect this, specifically the volumes mounted for each server that are mapped to /home/olive/olive/ within the containers. This excerpt shows where this is set for the GPU server: ... services : ... runtime-gpu : ... volumes : - ../oliveAppDataGPU:/home/olive/olive ...","title":"Plugin Location (OLIVE_APP_DATA)"},{"location":"gpu.html#gpu-server-concurrent-processing-jobs-workers-experimental","text":"If maximum batch throughput is important for your application, and it is known that memory requirements of your use case won't overwhelm the available memory of the GPU, the number of server workers for the GPU server can be increased. This is experimental and may affect overall server performance (speed) and stability. This setting controls the number of jobs the server can process simultaneously, and for GPU-enabled domains will increase the number of times the associated models are loaded into memory. As the percentage of used GPU memory increases, speed may decrease as less optimal pathways are used. This also increases the likelihood of ungracefully exhausting the available GPU memory, which may cause a system crash. The majority of the OLIVE 5.5.1 testing was performed with --workers set to 1 . To increase the number of server workers, increase the argument passed to the --workers flag in the docker-compose.yml , under services: runtime-gpu: command: ... services : ... runtime-gpu : ... command : ... - \"--workers=1\"","title":"GPU Server Concurrent Processing Jobs (--workers) [Experimental]"},{"location":"gpu.html#plugin-configuration-for-gpu-use","text":"","title":"Plugin Configuration for GPU Use"},{"location":"gpu.html#enable-gpu-usage-for-a-plugins-domain","text":"To allow a plugin to run on an available GPU, it is crucial that the plugin: Is located in the plugins directory of a GPU enabled server (oliveAppDataGPU/plugins by default) Has each desired domain configured to choose a GPU device in its meta.conf file Only has domains configured to select GPU device(s) that are properly exposed to the server via CUDA_VISIBLE_DEVICES and Docker's device_ids More information for #1 and #3 can be found in the appropriate sections above. Be sure to move the plugin to oliveAppDataGPU/plugins/ when reconfiguring, and restarting any running servers. Be sure to double-check that the device(s) you are configuring each plugin domain to use are actually exposed to the server that will be using that plugin/domain. Configuring a plugin to use a GPU is done at the domain level, by changing the device variable assignment within the domain's meta.conf file. By default, most plugins have this variable assigned to cpu , and as such the domain will run on CPU only, even when running within a GPU-enabled OLIVE server. To assign a domain access to a GPU, change this device assignment from cpu to gpuN where N is the device ID of the GPU to run on, as reported by nvidia-smi (more info here ). As already discussed it is critical that the device assigned to the domain is exposed to the OLIVE server that will be running this domain. Most hardware setups will only have a single GPU available, so enabling this is simply replacing cpu with gpu0 . As an example, the meta.conf for the english-v1 domain of asr-end2end-v1.0.0 in its off-the-shelf format is shown below: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : cpu To instead configure this domain to run on the GPU with ID #0, as it is when delivered with OLIVE 5.5.1, this domain then becomes: label : english - v1 description : Large vocabulary English wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : English device : gpu0 Each domain must be configured separately. If within a single plugin, one domain is configured for GPU, the others don't automatically start using a GPU. By default they will still run on CPU, which for most plugins will run significantly more slowly. By extension, it is not necessary for all domains of a plugin to run on the same GPU. Having this device assignment at the domain level allows the distribution of domains across multiple GPUs. For example, if multiple GPUs are available on a system, lower-memory-usage plugins like SAD, SID, and LID may share a single GPU, while heavier plugins like ASR or MT can be flexible enough to assign different language domains of each plugin across multiple GPUs, spreading the memory load to minimize the chance of exhausting GPU memory, while also saving lost time frequently loading and unloading models. Building off of the example above, if we wanted to run the russian-v1 domain of the same ASR plugin on GPU device #2, so that english-v1 and russian-v1 don't compete with respect to GPU memory, the russian-v1 domain would look like this: label : russian - v1 description : Large vocabulary Russian wav2vec2 model for both 8 K and 16 K data resample_rate : 8000 language : Russian device : gpu2 Note that in these examples, GPU device 0 and GPU device 2 must both be listed in CUDA_VISIBLE_DEVICES and device_ids as outlined above . Example docker-compose.yml modification ``` yaml ... services: ... runtime-gpu: ... environment: <<: *common-variables CUDA_VISIBLE_DEVICES: '0,2' ... deploy: resources: reservations: devices: - driver: nvidia # count: 1 # Can use 'count' or explicity list device_ids # How does device_ids and CUDA_VISIBLE_DEVICES match up? # Do the indexes inside the container always start at 0? device_ids: ['0', '2'] capabilities: [gpu] The exact ideal configuration may vary greatly depending on specific customer use case and mission needs.","title":"Enable GPU usage for a plugin's domain"},{"location":"gpu.html#currently-supported-gpu-plugins","text":"These plugins currently support GPU operation, when configured as outlined above: sad-dnn-v8 (Speech Activity Detection) sid-dplda-v4 (Speaker Identification) lid-embedplda-v4 (Language Identification) tmt-neural-v1.1.0 (Text Machine Translation) asr-end2end-v1 (Speech Recognition) All are capable of running on CPU as well, though in some cases (notably asr-end2end-v1) at a drastically reduced speed.","title":"Currently supported GPU plugins"},{"location":"gpu.html#olive-gpu-restrictions-and-configuration-notes","text":"","title":"OLIVE GPU Restrictions and Configuration Notes"},{"location":"gpu.html#gpu-compute-mode-default-vs-exclusive","text":"The OLIVE software currently assumes that any available GPUs are in the \"default\" mode. In testing, some configurations of number of OLIVE server workers have been found to cause unexpected issues if the GPUs are configured to be running in \"Exclusive\" mode. If possible, please configure GPUs that OLIVE will be using in \"Default\" mode, and if this is not possible, please ensure that the number of workers for the GPU-enabled server is specified to be 1. This is configured in the provided docker-compose.yml file. To check the mode of your GPU, you can view the Compute Mode field of the nvidia-smi command output. Please refer to Nvidia's instructions for Nvidia Control Panel or the Usage instructions for nvidia-smi for more information on how to set these modes.","title":"GPU Compute Mode: \"Default\" vs \"Exclusive\""},{"location":"gpu.html#workflow-restrictions","text":"Plugins can't currently pass jobs between separate OLIVE servers. This means that if you have a workflow that requires several plugins, each of those plugins must be visible to the OLIVE server that is receiving the workflow job request. If your current configuration separates any of these plugins, your plugins directories must be reconfigured so that all required plugins are colocated within the same server. There are two possible workarounds. The first would be disabling GPU capabilities in some plugins and moving them to the CPU oliveAppData/ directory, which may cause them to run much more slowly, but will allow full job parallelization. An alternative would be to move CPU-only plugins into the GPU-enabled server, leaving them configured to run on the CPU. This would allow the GPU-enabled plugins to run at full speed, but would limit parallel processing due to having a single worker thread. Future work in OLIVE hopes to address these first-release GPU limitations to reduce and eventually eliminate workarounds and restrictions like this.","title":"Workflow Restrictions"},{"location":"hardware.html","text":"OLIVE Hardware Requirements Processor Hardware Restrictions OLIVE is currently built, tested, and suported for running only on Intel x86_64 processor hardware. This includes consumer Core i-series processors like the i7, i9, etc., as well as the server-line Xeon processors. ARM processors are currently not supported; this includes the new M1 and M2 chips from Apple. Some plugins come with additional restrictions that have stricter requirements. The most notable at this point is for avx2 support from the CPU, this is required by our Neural Machine Translation plugin, as well as most of the low-resource targeted \"SmOlive\" plugins using quantized models. The \"SmOlive\" plugins typically have a \"smart\" domain that can back-off to a less-resource-streamlined set of models if the avx2 support is not discovered and loading the primary quantized model(s) has failed, but there is no back-off for Machine Translation. Typically, processors newer than roughly 2015 have this support, so it should only come into consideration in rare circumstances. Plugins currently requiring avx2 support: tmt-neural-v1 sad-dnnSmolive-v1 (Will back off to non-quantized models on unsupported hardware) sid-embedSmolive-v1 (Will back off to non-quantized models on unsupported hardware) lid-embedSmolive-v1 (Will back off to non-quantized models on unsupported hardware) sdd-diarizeEmbedSmolive-v1 (Will back off to non-quantized models on unsupported hardware) asr-end2end-v1 Speed and Memory Requirements Notes and Disclaimer for the Resource Requirement Estimates Provided A few performance-related things that might be important to note: These estimates provided below were recorded on a native linux installation of OLIVE 5.1.0 which may be slightly different than running the equivalent job on Windows or in a Docker based environment. Note that OLIVE 5.2.0 and 5.3.0 have memory improvements that are not yet reflected here; these statistics will be updated when new results are available. Any speed estimates we give are going to be hardware dependent. The numbers reported below should be pessimistic, as they are limited to running on a single core of a low-power computer. Stronger cores will be faster than what's reported, weaker CPU cores will run a bit slower. If you have more than 1 processor core(s) available, which is likely, OLIVE is able to parallelize and run more jobs simultaneously, so the speed should scale accordingly - but we're reporting single-core jobs just to keep everything on the same relative scale so that you can compare plugins to each other. Just as speed will increase/scale as the number of processor cores being used increases (i.e. number of simultaneous jobs), so will memory usage. The provided stats are for a process limited to one job at a time. Memory usage scales sometimes significantly depending on how large the input audio files are. For a plugin like SAD, that has a very small base memory footprint, which only barely increases even if processing many, many small files, can see a much larger memory utilization if you start running 1GB+ audio files through. ASR performance is largely domain-dependent - for example, the Russian domain currently has a much larger language model than other plugins because of how the language is structured, and priorities of the project that funded its development, so its memory usage is quite significant, \\~9+GB per processor core. Another thing to note for ASR speed performance is that because of how sizable the models can be, the overhead of the loading time of this model into memory can really come into play. It may take some time to get an initial response back from the server due to this overhead, but subsequent responses should be much faster as this 'heavy lifting' is already done. Note that if you're just running the CLI tools like localanalyze, this loading must be performed every time, so you won't realize this speedup unless you're running with the OLIVE server. If you are using the OLIVE server, it's possible to send a 'preload' request to load a plugin's models before any audio is submitted for scoring and avoid this initial delay. The models for each ASR/TPD domain/language are disjoint, and take up separate memory footprints. So if you would like to run data through both the Russian and English ASR domains when running the OLIVE server, the models, once loaded, are retained in memory for later processing and you may quickly run out of memory. For example, if you run a Russian job, a minimum of ~9GB of memory will be used. If you run an English job shortly after, this will load another ~6GB or so worth of models into memory. If you have insufficient memory, you will need to either explicity unload plugins/domains using API calls (not a feature currently offered by our GUI), or will need to restart the server to clear out the loaded models. QBE performance will depend on how many queries/keywords are currently enrolled - as more queries are enrolled and need to be considered during the search, the speed of the plugin will decrease. Some of the statistics below may be extra pessimistic because some of these readings will depend on how much of the input audio actually contains speech. If you have a 3 hour file, but only 5 minutes of it is speech, many of these plugins can be much, much faster and use less memory, because the task-specific processing (LID, for example) will only process audio that is identified as speech, and so will be operating on a much smaller piece of the audio than the whole file. The audio used to generate these numbers is pretty packed with speech, so should be close to a 'worst case.' Plugin Resource Requirement Estimates With that out of the way, here is a summary for most of the plugins: plugin / domain speed mem (1 min) mem (2 hr) sad-dnn-v7.0.1 / fast-multi-v1 214.1 105 MB 766 MB sad-dnn-v7.0.1 / multi-v1 90.6 127 MB 775 MB gid-gb-v2.0.0 / clean-v1 354.9 161 MB 1.56 GB ldd-sbcEmbed-v1.0.1 / multi-v1 18.5 582 MB 5.04 GB lid-embedplda-v2.0.1 / multi-v1 29.2 660 MB 3.15 GB qbe-tdnn-v5.0.0 / multi-v1* 28.3 198 MB 2.55 GB sdd-sbcEmbed-v2.0.2 / telClosetalk-v1 42.8 232 MB 1.12 GB sid-dplda-v2.0.1 / multi-v1 42.2 296 MB 2.38 GB asr-dynapy-v2.0.2 / rus-tdnnChain-tel-v1** 10.6 8.94 GB 10.19 GB tpd-dynapy-v3.0.0 / rus-cts-v1** 6.3 7.46 GB 8.66 GB * QBE Note: with 3 enrolled keywords ** ASR/TPD Note - the mem (2 hr) and speed statistics are generated from different data than the other plugins. That is because these plugins are language-dependent, and the data I used for the rest of the tests does not match the language of the domains I was running here. Feeding mismatched data into these plugins can cause both runtime and memory usage to balloon, as the plugin tries very hard to make sense of something that it's never seen before. Instead, 100 files adding to 2.5 hrs were used for the speed test and one 2-hr file was used for the mem (2 hr) test, but a different one than the rest of the plugins. Note also that the Russian models are by far the largest delivered - this will change depending on which language/domain you are using, but these should represent a 'worst case' for ASR/TPD for planning purposes. Speed numbers are reported in terms of \"times faster than real time\", and the numbers were reached by scoring 90 files adding up to approximately 5 hrs of data on a single core of a circa-2016 Gigabyte BRIX Compact PC (Intel i7-5500U 2.40 GHz processor). Higher is better, so for the slower SAD domain, which scores roughly 90 here, that means it can process a 90 second input file in 1 second. Two memory points are provided for each plugin - the memory used to score a single 1 minute file, which should show roughly the baseline usage of the plugin, as well as memory used to score a single 2 hour file, to give a sense of how the usage scales as files grow. Lower is better.","title":"OLIVE System Requirements"},{"location":"hardware.html#olive-hardware-requirements","text":"","title":"OLIVE Hardware Requirements"},{"location":"hardware.html#processor-hardware-restrictions","text":"OLIVE is currently built, tested, and suported for running only on Intel x86_64 processor hardware. This includes consumer Core i-series processors like the i7, i9, etc., as well as the server-line Xeon processors. ARM processors are currently not supported; this includes the new M1 and M2 chips from Apple. Some plugins come with additional restrictions that have stricter requirements. The most notable at this point is for avx2 support from the CPU, this is required by our Neural Machine Translation plugin, as well as most of the low-resource targeted \"SmOlive\" plugins using quantized models. The \"SmOlive\" plugins typically have a \"smart\" domain that can back-off to a less-resource-streamlined set of models if the avx2 support is not discovered and loading the primary quantized model(s) has failed, but there is no back-off for Machine Translation. Typically, processors newer than roughly 2015 have this support, so it should only come into consideration in rare circumstances. Plugins currently requiring avx2 support: tmt-neural-v1 sad-dnnSmolive-v1 (Will back off to non-quantized models on unsupported hardware) sid-embedSmolive-v1 (Will back off to non-quantized models on unsupported hardware) lid-embedSmolive-v1 (Will back off to non-quantized models on unsupported hardware) sdd-diarizeEmbedSmolive-v1 (Will back off to non-quantized models on unsupported hardware) asr-end2end-v1","title":"Processor Hardware Restrictions"},{"location":"hardware.html#speed-and-memory-requirements","text":"","title":"Speed and Memory Requirements"},{"location":"hardware.html#notes-and-disclaimer-for-the-resource-requirement-estimates-provided","text":"A few performance-related things that might be important to note: These estimates provided below were recorded on a native linux installation of OLIVE 5.1.0 which may be slightly different than running the equivalent job on Windows or in a Docker based environment. Note that OLIVE 5.2.0 and 5.3.0 have memory improvements that are not yet reflected here; these statistics will be updated when new results are available. Any speed estimates we give are going to be hardware dependent. The numbers reported below should be pessimistic, as they are limited to running on a single core of a low-power computer. Stronger cores will be faster than what's reported, weaker CPU cores will run a bit slower. If you have more than 1 processor core(s) available, which is likely, OLIVE is able to parallelize and run more jobs simultaneously, so the speed should scale accordingly - but we're reporting single-core jobs just to keep everything on the same relative scale so that you can compare plugins to each other. Just as speed will increase/scale as the number of processor cores being used increases (i.e. number of simultaneous jobs), so will memory usage. The provided stats are for a process limited to one job at a time. Memory usage scales sometimes significantly depending on how large the input audio files are. For a plugin like SAD, that has a very small base memory footprint, which only barely increases even if processing many, many small files, can see a much larger memory utilization if you start running 1GB+ audio files through. ASR performance is largely domain-dependent - for example, the Russian domain currently has a much larger language model than other plugins because of how the language is structured, and priorities of the project that funded its development, so its memory usage is quite significant, \\~9+GB per processor core. Another thing to note for ASR speed performance is that because of how sizable the models can be, the overhead of the loading time of this model into memory can really come into play. It may take some time to get an initial response back from the server due to this overhead, but subsequent responses should be much faster as this 'heavy lifting' is already done. Note that if you're just running the CLI tools like localanalyze, this loading must be performed every time, so you won't realize this speedup unless you're running with the OLIVE server. If you are using the OLIVE server, it's possible to send a 'preload' request to load a plugin's models before any audio is submitted for scoring and avoid this initial delay. The models for each ASR/TPD domain/language are disjoint, and take up separate memory footprints. So if you would like to run data through both the Russian and English ASR domains when running the OLIVE server, the models, once loaded, are retained in memory for later processing and you may quickly run out of memory. For example, if you run a Russian job, a minimum of ~9GB of memory will be used. If you run an English job shortly after, this will load another ~6GB or so worth of models into memory. If you have insufficient memory, you will need to either explicity unload plugins/domains using API calls (not a feature currently offered by our GUI), or will need to restart the server to clear out the loaded models. QBE performance will depend on how many queries/keywords are currently enrolled - as more queries are enrolled and need to be considered during the search, the speed of the plugin will decrease. Some of the statistics below may be extra pessimistic because some of these readings will depend on how much of the input audio actually contains speech. If you have a 3 hour file, but only 5 minutes of it is speech, many of these plugins can be much, much faster and use less memory, because the task-specific processing (LID, for example) will only process audio that is identified as speech, and so will be operating on a much smaller piece of the audio than the whole file. The audio used to generate these numbers is pretty packed with speech, so should be close to a 'worst case.'","title":"Notes and Disclaimer for the Resource Requirement Estimates Provided"},{"location":"hardware.html#plugin-resource-requirement-estimates","text":"With that out of the way, here is a summary for most of the plugins: plugin / domain speed mem (1 min) mem (2 hr) sad-dnn-v7.0.1 / fast-multi-v1 214.1 105 MB 766 MB sad-dnn-v7.0.1 / multi-v1 90.6 127 MB 775 MB gid-gb-v2.0.0 / clean-v1 354.9 161 MB 1.56 GB ldd-sbcEmbed-v1.0.1 / multi-v1 18.5 582 MB 5.04 GB lid-embedplda-v2.0.1 / multi-v1 29.2 660 MB 3.15 GB qbe-tdnn-v5.0.0 / multi-v1* 28.3 198 MB 2.55 GB sdd-sbcEmbed-v2.0.2 / telClosetalk-v1 42.8 232 MB 1.12 GB sid-dplda-v2.0.1 / multi-v1 42.2 296 MB 2.38 GB asr-dynapy-v2.0.2 / rus-tdnnChain-tel-v1** 10.6 8.94 GB 10.19 GB tpd-dynapy-v3.0.0 / rus-cts-v1** 6.3 7.46 GB 8.66 GB * QBE Note: with 3 enrolled keywords ** ASR/TPD Note - the mem (2 hr) and speed statistics are generated from different data than the other plugins. That is because these plugins are language-dependent, and the data I used for the rest of the tests does not match the language of the domains I was running here. Feeding mismatched data into these plugins can cause both runtime and memory usage to balloon, as the plugin tries very hard to make sense of something that it's never seen before. Instead, 100 files adding to 2.5 hrs were used for the speed test and one 2-hr file was used for the mem (2 hr) test, but a different one than the rest of the plugins. Note also that the Russian models are by far the largest delivered - this will change depending on which language/domain you are using, but these should represent a 'worst case' for ASR/TPD for planning purposes. Speed numbers are reported in terms of \"times faster than real time\", and the numbers were reached by scoring 90 files adding up to approximately 5 hrs of data on a single core of a circa-2016 Gigabyte BRIX Compact PC (Intel i7-5500U 2.40 GHz processor). Higher is better, so for the slower SAD domain, which scores roughly 90 here, that means it can process a 90 second input file in 1 second. Two memory points are provided for each plugin - the memory used to score a single 1 minute file, which should show roughly the baseline usage of the plugin, as well as memory used to score a single 2 hour file, to give a sense of how the usage scales as files grow. Lower is better.","title":"Plugin Resource Requirement Estimates"},{"location":"install.html","text":"OLIVE Installation OLIVE Folder Structure Overview Typical OLIVE installations for native, linux-based operating systems follow the general formats shown below. If your delivery is based on a docker container, please refer to the OLIVE Docker Container Setup Guide . We recommend installing the OLIVE Runtime and OLIVE itself within a directory in an easy-to-find location. Typically olive5.x.y/ within $HOME is a reasonable starting place. Plugins we generally store outside of this location, in a directory we name oliveAppData to allow for retention of plugins and audio enrollments if/when a new version of OLIVE is installed. $HOME is again often a reasonable location for this folder. The configuration or relationship between the runtime and the OLIVE package itself generally matches one of the following two examples. In either case, you will need to properly set up the OLIVE environment before performing any operations with OLIVE, and SRI provides a script to make this as straightforward as possible. olive_env.sh setup script This script is included with every release and is generally the recommended and easiest way to properly set up the OLIVE environment, as shown above. It should be possible to use this script to cover environmental setup in even advanced OLIVE deployments. This script will properly set a number of necessary environment variables and paths essential for OLIVE operation all in one go. Depending on how you've chosen to lay out your OLIVE package on disk, you may need to source two of these scripts. As a quick primer, this is the README included at the top of the olive_env.sh script, that will be explained in more detail below: # Sets the Olive / Scenic environment when running from a distribution . # Assumes this will be sourced in the Olive directory , as : # # % source olive_env . sh # # If this script is sourced in the runtime distribution , it will only set # runtime-related values . # # If this script is sourced in the Olive distribution , it will set both # OLIVE ( SCENIC ) and runtime-related values if : # ( 1 ) the Olive directory is a sub-directory of the runtime directory # or ( 2 ) the Olive and runtime directories have been combined . # # If the Olive directory is a subdirectory of the runtime , then you # should only source olive_env . sh from the Olive directory . # # If the runtime and Olive directories are completely separate , then # do the following ( in this order , so that $ OLIVE paths come before # $ OLIVE_RUNTIME paths ): # # ( 1 ) cd / path / to / runtime ; source olive_env . sh # ( 2 ) cd / path / to / olive ; source olive_env . sh One important thing to note is that the olive_env.sh scripts do not cover setting the OLIVE_APP_DATA environment variable, which informs the server of the location of the OLIVE plugins, among other things. More details available here . This is a very important step to make sure the Server can find and utilize the appropriate plugins. Setting this variable is included in each of the examples below. OLIVE/OLIVE-Runtime Layouts Often, the OLIVE Runtime and OLIVE itself are placed side-by-side in the same parent directory, for easier readability. If this configuration is used, it is necessary to source two separate olive_env.sh files to properly establish the OLIVE environment, as outlined in the olive_env.sh README shown above. You should always start by sourcing the olive_env.sh file for the runtime. An example file structure of side-by-side OLIVE and OLIVE runtime directories: $HOME olive5.5.1/ runtime-5.5.1-CentOS_Linux-7-x86_64/ olive-5.5.1-CentOS_Linux-7-x86_64/ documentation/ oliveAppData/ plugins/ sad-dnn-v7.0.0 sid-dplda-v2.0.0/ lid-embedplda-v2.0.0/ Example environment setup with this configuration: $ cd $ HOME / olive5 . 5.1 / runtime - 5.5 . 1 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ cd $ HOME / olive5 . 5.1 / olive - 5.5 . 1 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / Alternately, the OLIVE package can be nested within the runtime directory, a structure shown in the example below. If this arrangement is used, it is only necessary to source the olive_env.sh script within the OLIVE package, as it is aware that its parent directory is the OLIVE runtime, and appropriately establishes the environment with this information. $HOME olive5.5.1/ runtime-5.5.1-CentOS_Linux-7-x86_64/ olive-5.5.1-CentOS_Linux-7-x86_64/ documentation/ oliveAppData/ plugins/ lid-embed-v2/ sad-dnn-v4a/ sid-embed-v2/ Example environment setup with this configuration: $ cd $ HOME / olive5 . 5.1 / runtime - 5.5 . 1 - CentOS_Linux - 7 - x86_64 / olive - 5.5 . 1 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / After performing these installation and setup steps, you should be ready to get started and run your own instance of the OLIVE Server For more information on what variables are being established by this environment setup script and what is needed by OLIVE to run, continue on to the Environment Variables section of the OLIVE Server Page . For more information on getting started with running the OLIVE server, continue on to the Running The Server section. Or skip to the OLIVE Runtime for more details about the purpose and contents of the OLIVE Runtime.","title":"OLIVE CentOS/RedHat 7 Native Setup"},{"location":"install.html#olive-installation","text":"","title":"OLIVE Installation"},{"location":"install.html#olive-folder-structure-overview","text":"Typical OLIVE installations for native, linux-based operating systems follow the general formats shown below. If your delivery is based on a docker container, please refer to the OLIVE Docker Container Setup Guide . We recommend installing the OLIVE Runtime and OLIVE itself within a directory in an easy-to-find location. Typically olive5.x.y/ within $HOME is a reasonable starting place. Plugins we generally store outside of this location, in a directory we name oliveAppData to allow for retention of plugins and audio enrollments if/when a new version of OLIVE is installed. $HOME is again often a reasonable location for this folder. The configuration or relationship between the runtime and the OLIVE package itself generally matches one of the following two examples. In either case, you will need to properly set up the OLIVE environment before performing any operations with OLIVE, and SRI provides a script to make this as straightforward as possible.","title":"OLIVE Folder Structure Overview"},{"location":"install.html#olive_envsh-setup-script","text":"This script is included with every release and is generally the recommended and easiest way to properly set up the OLIVE environment, as shown above. It should be possible to use this script to cover environmental setup in even advanced OLIVE deployments. This script will properly set a number of necessary environment variables and paths essential for OLIVE operation all in one go. Depending on how you've chosen to lay out your OLIVE package on disk, you may need to source two of these scripts. As a quick primer, this is the README included at the top of the olive_env.sh script, that will be explained in more detail below: # Sets the Olive / Scenic environment when running from a distribution . # Assumes this will be sourced in the Olive directory , as : # # % source olive_env . sh # # If this script is sourced in the runtime distribution , it will only set # runtime-related values . # # If this script is sourced in the Olive distribution , it will set both # OLIVE ( SCENIC ) and runtime-related values if : # ( 1 ) the Olive directory is a sub-directory of the runtime directory # or ( 2 ) the Olive and runtime directories have been combined . # # If the Olive directory is a subdirectory of the runtime , then you # should only source olive_env . sh from the Olive directory . # # If the runtime and Olive directories are completely separate , then # do the following ( in this order , so that $ OLIVE paths come before # $ OLIVE_RUNTIME paths ): # # ( 1 ) cd / path / to / runtime ; source olive_env . sh # ( 2 ) cd / path / to / olive ; source olive_env . sh One important thing to note is that the olive_env.sh scripts do not cover setting the OLIVE_APP_DATA environment variable, which informs the server of the location of the OLIVE plugins, among other things. More details available here . This is a very important step to make sure the Server can find and utilize the appropriate plugins. Setting this variable is included in each of the examples below.","title":"olive_env.sh setup script"},{"location":"install.html#oliveolive-runtime-layouts","text":"Often, the OLIVE Runtime and OLIVE itself are placed side-by-side in the same parent directory, for easier readability. If this configuration is used, it is necessary to source two separate olive_env.sh files to properly establish the OLIVE environment, as outlined in the olive_env.sh README shown above. You should always start by sourcing the olive_env.sh file for the runtime. An example file structure of side-by-side OLIVE and OLIVE runtime directories: $HOME olive5.5.1/ runtime-5.5.1-CentOS_Linux-7-x86_64/ olive-5.5.1-CentOS_Linux-7-x86_64/ documentation/ oliveAppData/ plugins/ sad-dnn-v7.0.0 sid-dplda-v2.0.0/ lid-embedplda-v2.0.0/ Example environment setup with this configuration: $ cd $ HOME / olive5 . 5.1 / runtime - 5.5 . 1 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ cd $ HOME / olive5 . 5.1 / olive - 5.5 . 1 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / Alternately, the OLIVE package can be nested within the runtime directory, a structure shown in the example below. If this arrangement is used, it is only necessary to source the olive_env.sh script within the OLIVE package, as it is aware that its parent directory is the OLIVE runtime, and appropriately establishes the environment with this information. $HOME olive5.5.1/ runtime-5.5.1-CentOS_Linux-7-x86_64/ olive-5.5.1-CentOS_Linux-7-x86_64/ documentation/ oliveAppData/ plugins/ lid-embed-v2/ sad-dnn-v4a/ sid-embed-v2/ Example environment setup with this configuration: $ cd $ HOME / olive5 . 5.1 / runtime - 5.5 . 1 - CentOS_Linux - 7 - x86_64 / olive - 5.5 . 1 - CentOS_Linux - 7 - x86_64 / $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / After performing these installation and setup steps, you should be ready to get started and run your own instance of the OLIVE Server For more information on what variables are being established by this environment setup script and what is needed by OLIVE to run, continue on to the Environment Variables section of the OLIVE Server Page . For more information on getting started with running the OLIVE server, continue on to the Running The Server section. Or skip to the OLIVE Runtime for more details about the purpose and contents of the OLIVE Runtime.","title":"OLIVE/OLIVE-Runtime Layouts"},{"location":"javaClientSetup.html","text":"Setting up IDE for Java Client Development with OLIVE Reference Implementation Before you do integration with the OLIVE Java API, make sure you have the OLIVE server running. For more details, see Run OLIVE Server . Also, make sure you have Java installed, since you will be writing Java client programs. Source code and examples of the OLIVE Java API are included in your OLIVE delivery package, in a folder named OLIVE-API-example. At the top level, you should see the following items: Expand the src folder a few levels down and you should see a folder named client. This folder contains examples of Java client programs which make use of the OLIVE Java API. All examples shown in Example Client Code Using the OLIVE API are included in the folder. The easiest way to set up your environment to write Java client programs to use the API to access OLIVE services is to use a Java IDE, for example, IntelliJ, which you can download and install a free version of the community edition from here . There are other Java IDE available, we will use IntelliJ to illustrate the integration steps. Next is bring the scenic-example Java project into your IDE. At the top level of scenic-example, there is a gradlew script. Use it to create a Java project compatible with IntelliJ as follows: ./gradlew idea This creates a file named OLIVE-API-examples.ipr. Double click the file to launch the IntelliJ IDE window. Then click the \"Import Gradle Project\" link at the bottom right of the IntelliJ window to import the project into IntelliJ. If the IntelliJ system gives you a message to upgrade your gradle build to 2.6, as shown below, click the upgrade link to do the upgrade. Furthermore, if the IntelliJ system gives you a message to upgrade the gradle wrapper to be compatible with the version of Java you have on your local system, click the upgrade link to do the gradle wrapper upgrade. When the upgrades are done, you should see an IntelliJ window with a \"CONFIGURE SUCCESSFUL\" message. Close the gradle-wrapper.properties file. Click the \"Project\" folder icon all the way to the top left. This should bring the OLIVE-API-examples project tree into your IDE. Right click on \"OLIVE-API-examples\" and select \"BUILD Module OLIVE-API-examples\". This should compile the OLIVE API package and make its resources available. Expand OLIVE-API-examples and you should see this: Expand OLIVE-API-examples and navigate a few levels down into src, until you see the Java programs in the client folder. If there are incompatibilites between any source file and the version of gradle being used, the source file will have a little orange icon to its bottom left. To fix the incompatibilities, re-import the project by clicking the \"2 arrow\" icon in the gradle window. Once the reimport is done, the orange incompatible icons should change into blue compatible icons, as shown below. To read or edit MySADFrameScorer.java, double click the file to bring it into the edit window. Before you run the program, add a run-time configuration parameter by using the \"Add Configuration...\" pull-down menu. Click \"+\" to add a new configuration. Name this new configuration \"MySADFrameScorer\". Choose \"MySADFrameScorer (com.sri.scenic.api.client)\" for \"Main Class\", and \"OLIVE-API-examples.main\" for \"Use classpath of module\". Enter the location of an audio file on your local system in \"Program arguments\", as MySADFrameScorer.java takes one argument which is the path to the file for SAD scoring. Click \"OK\" to save the configuration. To run the program, right click on the file and choose \"Run MySADFrameScorer\". The SAD scores returned from the OLIVE server are shown in the bottom right window of the IDE, as seen below. Once you have brought the OLIVE API project into your Java development IDE, a convenient location to place and run your custom built client programs is the same client folder where you find the sample client programs such as SAD scoring request . You can start with any of the example programs in the client folder, modify it to suit your needs, and run it the same way as you would run the SAD scoring request program .","title":"javaClientSetup"},{"location":"javaClientSetup.html#setting-up-ide-for-java-client-development-with-olive-reference-implementation","text":"Before you do integration with the OLIVE Java API, make sure you have the OLIVE server running. For more details, see Run OLIVE Server . Also, make sure you have Java installed, since you will be writing Java client programs. Source code and examples of the OLIVE Java API are included in your OLIVE delivery package, in a folder named OLIVE-API-example. At the top level, you should see the following items: Expand the src folder a few levels down and you should see a folder named client. This folder contains examples of Java client programs which make use of the OLIVE Java API. All examples shown in Example Client Code Using the OLIVE API are included in the folder. The easiest way to set up your environment to write Java client programs to use the API to access OLIVE services is to use a Java IDE, for example, IntelliJ, which you can download and install a free version of the community edition from here . There are other Java IDE available, we will use IntelliJ to illustrate the integration steps. Next is bring the scenic-example Java project into your IDE. At the top level of scenic-example, there is a gradlew script. Use it to create a Java project compatible with IntelliJ as follows: ./gradlew idea This creates a file named OLIVE-API-examples.ipr. Double click the file to launch the IntelliJ IDE window. Then click the \"Import Gradle Project\" link at the bottom right of the IntelliJ window to import the project into IntelliJ. If the IntelliJ system gives you a message to upgrade your gradle build to 2.6, as shown below, click the upgrade link to do the upgrade. Furthermore, if the IntelliJ system gives you a message to upgrade the gradle wrapper to be compatible with the version of Java you have on your local system, click the upgrade link to do the gradle wrapper upgrade. When the upgrades are done, you should see an IntelliJ window with a \"CONFIGURE SUCCESSFUL\" message. Close the gradle-wrapper.properties file. Click the \"Project\" folder icon all the way to the top left. This should bring the OLIVE-API-examples project tree into your IDE. Right click on \"OLIVE-API-examples\" and select \"BUILD Module OLIVE-API-examples\". This should compile the OLIVE API package and make its resources available. Expand OLIVE-API-examples and you should see this: Expand OLIVE-API-examples and navigate a few levels down into src, until you see the Java programs in the client folder. If there are incompatibilites between any source file and the version of gradle being used, the source file will have a little orange icon to its bottom left. To fix the incompatibilities, re-import the project by clicking the \"2 arrow\" icon in the gradle window. Once the reimport is done, the orange incompatible icons should change into blue compatible icons, as shown below. To read or edit MySADFrameScorer.java, double click the file to bring it into the edit window. Before you run the program, add a run-time configuration parameter by using the \"Add Configuration...\" pull-down menu. Click \"+\" to add a new configuration. Name this new configuration \"MySADFrameScorer\". Choose \"MySADFrameScorer (com.sri.scenic.api.client)\" for \"Main Class\", and \"OLIVE-API-examples.main\" for \"Use classpath of module\". Enter the location of an audio file on your local system in \"Program arguments\", as MySADFrameScorer.java takes one argument which is the path to the file for SAD scoring. Click \"OK\" to save the configuration. To run the program, right click on the file and choose \"Run MySADFrameScorer\". The SAD scores returned from the OLIVE server are shown in the bottom right window of the IDE, as seen below. Once you have brought the OLIVE API project into your Java development IDE, a convenient location to place and run your custom built client programs is the same client folder where you find the sample client programs such as SAD scoring request . You can start with any of the example programs in the client folder, modify it to suit your needs, and run it the same way as you would run the SAD scoring request program .","title":"Setting up IDE for Java Client Development with OLIVE Reference Implementation"},{"location":"martini.html","text":"OLIVE Martini Docker Container Setup Introduction This page guides through the installation, setup, and launching of a docker-container based OLIVE software package featuring the new \"OLIVE Martini\" container setup. This new delivery method includes the usual OLIVE server and plugin capabilities delivered in the past, but includes a new addition - Batch GUI capabilities powered by the new Raven GUI that can be accessed through your web browser. For instructions on how to use this new GUI once you have the software properly installed, check out the Raven Web GUI page. The overall installation process is very similar to previous Docker container based OLIVE deliveries in the past, with the main changes affecting how the OLIVE container is managed for startup, shutdown, etc. This page assumes you already have Docker installed and configured, or that you are already familiar with it. Some sections are collapsed by default to shift the OLIVE-specific info up, but they can be easily expanded for more details if necessary. OLIVE Martini Quick Start This briefly covers the steps necessary to get up and running with an OLIVE Martini to serve as a quick refresher for returning users or those simply wishing to start as quickly as possible. Please refer to the respective linked sections for each step if more information is needed, or if any trouble is encountered. Make sure Docker is installed Unpack the delivery archive Using $ tar -xzf olive5.5.1-martini-2Jun2022.tar.gz Load the OLIVE Martini image $ cd olive5.5.1/martini/ docker load -i olive-martini-container-5.5.1.tar Start the container $ ./martini.sh start or $ .\\martini.ps1 start Use the container When finished, stop the container $ ./martini.sh stop or $ .\\martini.ps1 stop !!! info \"Note that if you wish for OLIVE to use GPUs for supported plugins, you must launch Martini with the --gpu / -gpu flag , and also be sure to configure compatible plugins to use the GPU as outlined in the GPU Plugin/Domain Configuration documentation before launching Martini. Download, Install, and Launch Docker Docker Installation Instructions (Click to expand) Install and Start Docker Before you can get started installing and running OLIVE, you'll need to make sure you have fully installed and configured Docker. The proper installation steps vary depending on your host OS, so please refer to the appropriate official Docker installation instructions: Docker Desktop for Windows Docker Desktop for macOS Docker Engine for Ubuntu Docker for Ubuntu is especially important to follow the official steps closely, as there are additional important post-installation steps to perform to make sure docker runs smoothly on your system. Note that if installing into an Ubuntu instance running on WSL2, systemctl is not used on such systems. This means that some of the commands provided in the Docker for Ubuntu instructions above may not succeed as written; notably for starting and stopping the Docker service. Please use service for these commands instead: $ sudo service docker start In addition, if using Docker for Ubuntu, the Nvidia drivers must be installed separately. This doesn't seem to be necessary if using Docker Desktop. Instructions for this installation can be found here, from Nvidia . Before moving on, be sure that the docker service has been started. Adjust Docker settings (RAM, Cores) Configure Docker-allocated Resources (Windows and macOS only) (Click to expand) If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS Anatomy of an OLIVE Martini Package Once Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: $ tar -xzf olive5.5.1-martini-2Jun2022.tar.gz You should find similar content to below unless told otherwise: olive5.5.1/ api/ - Directory containing the Python and Java OLIVE Client utilities docs/ - OLIVE documentation martini/ olive-martini-container-5.5.1.tar - OLIVE Martini Docker container martini.sh - Multi-purpose utility and management script for the OLIVE Martini Docker container on linux and macOS martini.ps1 / martini.bat - Same as above, but for Windows PowerShell OliveGUI/ - (Optional) The OLIVE Nightingale Forensic GUI oliveAppData/ plugins/ - OLIVE Plugins directory; Actual plugins included will depend on the customer, mission, and delivery workflows/ - (Optional) Directory containing OLIVE workflows; actual included worfklows will depend on the customer, mission, and delivery Expand the section below for more details on each component, or scroll to continue setup. OLIVE Martini Component Details (Click to Expand) api/ This directory contains the Python and Java OLIVE Client utilities that interact with and task a running OLIVE/Martini server. The client documentation covers how to get started with these. docs/ Contains the documentation you're currently browsing. Other versions can be viewed online at the OLIVE Software Documentation Page , but the version delivered with OLIVE should be the most appropriate for your specific delivery. To view the documentation, open this page in a web browser: olive5.5.1/docs/index.html Or follow the links given by the martini.sh startup output for the live-hosted local documentation. martini/ Contains the core OLIVE server and utility components for running the OLIVE software. This includes the OLIVE Martini image itself, and the the management scripts to help start, monitor, configure, and stop the container. olive-martini-container-.tar Docker image that includes OLIVE , the Raven Web GUI , the OLIVE Web Broker, and various other utilities, including an html server to host this documentation. martini.sh - martini.ps1 / martini.bat Multi-purpose utility and management script for the OLIVE Martini Docker container on linux and macOS (.sh) or Windows (.ps1 and .bat) respectively. OliveGUI/ The OLIVE Nightingale Forensic UI for close file analysis, live streaming tasks, and other OLIVE GUI tasks. oliveAppData/ This is the default location for the all important OLIVE plugins and workflows . Once you start using the system, it is also where the OLIVE server will start saving server logs, in a server/ directory created here. plugins/ Included plugins are installed here - for more information on OLIVE plugins, refer to the OLIVE Plugin Overview , the list of released plugins , or the individual documentation page of the plugin of interest from the navigation on the left. workflows/ If any workflows are included with your OLIVE package, they reside here. Workflows are powerful ways to combine different plugins and capabilities to perform multiple scoring requests at once, and/or perform complex routing operations like using Language ID results to choose whether or not to perform ASR, and which language to use if so. Load the OLIVE Martini Docker Image The first setup step is to load the OLIVE Martini Docker image. This is only necessary once. Open a command prompt (PowerShell in Windows or Terminal in linux/macOS), navigate to the directory containing the OLIVE Docker image, and follow the examples below. linux / macOS $ cd / home /< username >/ olive5 . 5.1 / martini $ docker load - i olive - martini - container - 5.5 . 1. tar Windows $ cd C : \\ Users \\ < username > \\ olive5 . 5.1 \\ martini $ docker load - i olive - martini - container - 5.5 . 1. tar This operation can take some time; quite a few minutes on some machines, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed. Operating Systems Note There are three provided martini management scripts: - martini.sh - martini.bat - martini.ps1 The functionality of each is intended to be identical, and these are provided as different options for different operating systems for convenience. The martini.sh script should be used for linux and for macOS. martini.ps1 should be used when managing the OLIVE Martini container from Windows PowerShell, and martini.bat , which calls martini.ps1 internally, should be used if martini.ps1 cannot execute for permission reasons in PowerShell, and when managing OLIVE Martini from Windows Command Prompt ( cmd ). The examples below attempt to show all three being used, but in the text, they are refered to as simply martini.sh for brevity. Their features and functionality are the same. Note that the one exception is the flag format. The Windows management use only a single '-' for flags, while the linux/macOS script uses two '--'. For example the '--gpu' optional flag for ./martini.sh start --gpu to launch with GPU connections enabled becomes '-gpu' on Windows: .\\martini.ps1 start -gpu Controlling the Container Controlling and managing the OLIVE Multi container is made much easier with the provided martini.sh management script. Using this script is as simple as running the script, and providing a command: macOS and linux $ ./martini.sh <command> Windows $ .\\martini.bat <command> The list of commands available to martini.sh are: cli - Starts a shell on the container for debugging or operating the OLIVE CLI tools. help - Display the martini.sh help statement (shown below) list - List the running container(s), if any. log - Display a snapshot of the OLIVE Server log in the terminal. Useful for experts for debugging. net - Show the ports on the host that the container is listening on. start - Start the container. stop - Stop the container. status - Show the status of the processes on the container. version - Display the version information for the installed Martini components. More details for each command, how to use it, and the designed functionality can be found below. martini.sh help Prints out the martini.sh help statement, reminding the user of the available commands: Supported commands include : cli , help , list , log , net , start , stop , status , and version . martini . sh cli : Starts a shell on the container for debugging . martini . sh help : Prints out help information . martini . sh list : List the running container . martini . sh log : Print out log files from the container . martini . sh net : Shows the ports on the host that the container is listening on . ( requires `netstat` OS utility be installed for proper function ) martini . sh start : Start the container . Optional flags : -- gpu : Enable Martini access to GPUs , if available . Be sure to properly configure any plugin / domains to be used with GPU devices as outlined in the documentation . -- tls_server_only : The server will be configured with a certificate and will only respond to HTTPS requests ; clients aren 't required to send certificate but must use HTTPS protocol (one-way TLS) --tls_server_and_client: Both the server and clients will need to communicate with certificates over HTTPS (two-way TLS) --debug: activate ' debug ' mode for the OLIVE server. This will cause OLIVE to maintain all OLIVE server logs to aid in troubleshooting and debugging. martini.sh status: Shows status of the processes on the container. martini.sh stop: Stop the container. martini.sh version: Prints out version information. martini.sh start Starts up a previously built and loaded container. Note that it can take several seconds for all the servers to start on the container. Introduced with OLIVE 5.5.1, the start command now accepts four different optional startup flags. martini.sh start --gpu This grants the OLIVE Martini access to GPUs, if available. Be sure to properly configure any plugin/domains to be used with GPU devices as outlined in the GPU Plugin/Domain Configuration documentation. Without explicit configuration to use a GPU, whether by SRI before delivery or by the end user, no plugin will take advantage of available GPUs. Note that if no GPUs are available on the machine, or the GPUs are not compatible with the OLIVE 5.5.1 software and this flag is used, startup of the Martini will fail! martini.sh start --tls_server_only Configures the server to use one-way TLS, relying on a server-side for encrypted communication. The server will only respond to HTTPS requests. Clients aren't required to send a certificate, but must use HTTPS protocol. Requires additional files for proper function, and for Martini to start at all if this option is enabled. All of these files must be placed in olive5.5.1/oliveAppData/nginx-files/ . The files are: passwords.txt A file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key. The file can be empty if passphrases are not required. server.crt A certificate in the PEM format. If intermediate certificates should be specified in addition to a primary certificate, they should be specified in the same file in the following order: the primary certificate comes first, then the intermediate certificates. NOTE : You need to rename a copy of your certificate and put it in the /nginx-files/ directory mentioned above. server.key The secret key in the PEM format for the server.crt . NOTE : You need to rename a copy of your certificate key and put it in the /nginx-files/ directory. Please work with your IT department to obtain valid certificates. martini.sh start --tls_server_and_client Similar to the --tls_server_only flag described above, but instead activates two-way TLS. Both the server and client are required to provide a certificate, and all communication occurs over HTTPS. Carries the same file and certificate requirements as --tls_server_only , but with the addition of the client providing their own valid certificate. The additional file requirement for twoway TLS: clientCA.crt A file with trusted CA certificates in the PEM format used to verify client certificates. NOTE : You need to rename a copy of your file and put it in the /nginx-files/ directory mentioned above. As always, please work with your IT department to obtain valid certificates. martini.sh start --debug Activates 'debug' mode for the OLIVE server. This causes OLIVE to mantain all OLIVE server logs to aid in troubleshooting and debugging. These logs are stored in olive5.5.1/oliveAppData/server/ by default. Without this flag, \"failed\" logs are retained, but logs for jobs deemed to be successful are deleted as tasks are completed to avoid clutter and confusion. martini.sh stop Stops a running container. martini.sh status Prints out some basic information on the processes running, network ports, workflows and plugins which are active on the container. $ martini.sh status Message brokers running: Processes: 1 Plugins: aed-enrollable-v1.0.1 aln-waveformAlignment-v1.0.0 env-indoorOutdoor-v1.0.0 env-multiClass-v2.0.0 env-powerSupplyHum-v1.0.0 env-speakerCount-v1.0.0 lid-embedplda-v2.0.1 nsd-sadInverter-v1.0.0 sad-dnn-v7.0.1 sed-rmsEnergy-v1.0.0 sid-dplda-v2.0.1 voi-speakingStyle-v1.0.0 voi-vocalEffort-v1.0.1 vtd-dnn-v7.0.1 Workflows: Acoustic-Event-Detection.workflow Background-Noise-Detection.workflow SAD_SID_LID.workflow Speech-Analysis.workflow tcp 0 0 0.0.0.0:5589 0.0.0.0:* LISTEN 0 7905040 72/python tcp 0 0 0.0.0.0:5590 0.0.0.0:* LISTEN 0 7915534 73/python tcp 0 0 0.0.0.0:8070 0.0.0.0:* LISTEN 0 7910329 9/python tcp 0 0 0.0.0.0:5004 0.0.0.0:* LISTEN 0 7917573 138/nginx: master p tcp 0 0 0.0.0.0:5005 0.0.0.0:* LISTEN 0 7914186 15/java tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 0 7917572 138/nginx: master p tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 0 7897930 26/httpd tcp 0 0 0.0.0.0:5588 0.0.0.0:* LISTEN 0 7915524 16/python unix 2 [ ACC ] STREAM LISTENING 7915525 16/python /tmp/executor_714acd0b-0d0d-4259-84c2-127b84b8d26c.pipe unix 2 [ ACC ] STREAM LISTENING 7905043 80/python /tmp/pymp-vu6okza8/listener-4e5_70ll Httpd (web-ui) servers running: 6 Nginx (reverse proxy) server running: 2 Olive servers running: 12 Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:5589 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5590 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8070 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5004 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5005 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5588 0.0.0.0:* LISTEN martini.sh cli Starts up a shell within the OLIVE Martini container. The container must already be running. This is typically used for internal testing and troubleshooting and is not meant to be used by end-users. You can use this shell to run Olive CLI commands, such as: $ martini.sh cli # Show the plugins installed on the container: $ ls /home/olive/olive/plugins/ # Show the audio files on the container, which you can use as examples: $ ls /olive-data/media/shared/ # Run an analysis $ localanalyze /home/olive/olive/plugins/lid-embedplda-v2.0.1/domains/multi-v1 /olive-data/media/shared/Komodo_dragon_en.wav.lst # See the results $ more output.txt This can also be used for troubleshooting. When running with the cli command, your shell starts in /opt/olive . This is where the runtime is installed, and where things are run within the container. In /opt/olive : Running martini.sh status will print out what is running, and what network connections are active. And looking at this script will tell you what should be running. Do not run martini-process.sh. That is run when the container starts. The /home/olive/olive directory is the other important directory. Here: The martini-process.log file is the output of the script which starts all servers. The env.log file (if it exists) are the shell variables set. The other log files here were started by each server. The plugins/ directory contains all plugins. The server/ directory contains server logs. martini.sh net Lists the project network ports that are active on the host machine, the machine running the container. martini.sh list Lists the project containers that are running. martini.sh log Displays the OLIVE Server log; useful for checking on the status of the OLIVE server in case of a malfunction, or troubleshooting unexpected behavior. martini.sh version Diplays the version information for the individual components of OLIVE: Martini v1.1. Olive v5.3.0. Broker v1.5. Raven v0.2.0. Using The Container Once you have started up the container using the martini.sh start command described above, you are ready to begin using it. Your container has four different applications within it, which you can use - the Raven Web UI , the OLIVE Server itself , the OLIVE Message Broker, and a web server hosting the OLIVE Documentation. In addition, most OLIVE deliveries typically ship with the interactive Nightingale Forensic GUI for performing close file analysis using OLIVE plugins and workflows, as well as both Java and Python OLIVE Client CLI example applications. Note that if you are attempting to connect from the same machine, you can access this feature at localhost:5580 . If you are accessing from a different machine, you must use the full hostname, or IP address. For convenience, the martini.sh script outputs the hostname and corresponding port for several activities: $ martini.sh start \\Started the container. From this machine: Run Nightingale (Olive GUI) using server localhost and port 5588. Use a web browser to localhost:5570 to see the documentation. Use a web browser to localhost:5580 to use the Raven Web UI. If using the REST API point to the server running on localhost:5004. From any other machine: Run Nightingale (Olive GUI) using server stauf-MBP16 and port 5588. Use a web browser to stauf-MBP16:5570 to see the documentation. Use a web browser to stauf-MBP16:5580 to use the Raven Web UI. If using the REST API point to the server running on stauf-MBP16:5004. Installed plugins (mounted from /Users/astauf/olive/olive5.3.0/oliveAppData/plugins) are: asr-dynapy-v3.0.0 map-routerGlobal-v1.0.0 dfa-cplda-v1.0.0 map-routerGlobal-v1.0.1 dfa-speakerSpecific-v1.0.0 map-routerRegion-v1.0.0 dfa-spoofnet-v1.0.0 pim-validateGlobal-v1.0.0 enh-mmse-v2.0.2 qbe-ftdnnSmolive-v1.0.0 env-audioQuality-v2.0.0 qua-analysis-v1.0.0 fdi-pyEmbed-v1.0.0 qua-filter-v1.0.0 fdv-pyEmbed-v1.0.0 red-transform-v1.0.0 fri-pyEmbed-v1.0.0 sad-dnn-v7.0.2 frv-pyEmbed-v1.0.0 sad-dnnSmolive-v1.0.0 gdd-embedplda-v1.0.0 sdd-diarizeembed-v1.0.0 gid-gb-v2.0.1 sdd-sbcEmbed-v2.0.3 ldd-embedplda-v1.0.0 shl-sbcEmbed-v1.0.2 ldd-embedplda-v1.0.1 sid-dplda-v2.0.2 lid-embedplda-v3.0.0 sid-embed-v6.0.2 lid-embedplda-v3.0.1 tpd-dynapy-v5.0.0 lid-embedpldaSmolive-v1.0.0 vtd-dnn-v7.0.2 Installed workflows (mounted from /Users/astauf/olive/olive5.3.0/oliveAppData/workflows) are: /opt/olive-broker/data/workflows/FaceDetection_Image_FDI.workflow.json /opt/olive-broker/data/workflows/FaceDetection_Video_FDV.workflow.json /opt/olive-broker/data/workflows/FaceRecognition_Image_FRI.workflow.json /opt/olive-broker/data/workflows/FaceRecognition_Video_FRV.workflow.json /opt/olive-broker/data/workflows/SID_quality-controlled.workflow.json /opt/olive-broker/data/workflows/SmartTranscriptionFull.workflow.json /opt/olive-broker/data/workflows/TPD-eng.workflow.json /opt/olive-broker/data/workflows/conditional_asr_v2.workflow.json /opt/olive-broker/data/workflows/quality-region-analysis-for-SID.workflow.json Please choose the appropriate hostname and port number for your desired activity and host situation. Raven Web UI To connect to the Raven Web/Batch GUI , open a web browser and navigate to URL host :5580. Once there, follow the on-screen prompts to drag-and-drop one or more audio files to the \"local media\" dropbox section, select one of the available workflows, and click on the action button in the lower right hand corner. More details for the Raven GUI can be found on its dedicated documentation page: Raven Web UI . The OLIVE Documentation When the container is started, it launches a web server serving this documentation. To view the hosted version, navigate a web browser to URL host :5570. The OLIVE Server This is used internally by the Olive Web UI and the Olive Message Broker. You can also write Python programs which make calls directly to the server. See seperate documentation on olivepy API for details. The OLIVE Message Broker This is used internally by the Olive Web UI. Nightingale Forensic UI This GUI provides access to the full suite of available OLIVE plugins and workflows, for performing close file analysis. It offers powerful audio visualization tools, live streaming capabilities, and a number of other useful features. For more information on what Nightingale offers and how to get started using it, please refer to the Nightingale GUI dedicated documentation page. As a quick primer, the Nightingale UI is typically provided in: <...>/olive5.5.1/OliveGUI Nightingale requires OpenJDK Java 11. Once this is installed, you can run it by navigating to: <...>/olive5.5.1/OliveGUI/bin/ And either double-clicking or running the Nightingale (macOS, linux) or Nightingale.bat (Windows) script appropriate for your OS. macOS / linux $ ./Nightingale Windows $ . \\N ightingale.bat NOTE that the Nightingale launcher scripts are designed to be launched from this bin/ directory in order to properly link to other folders that it uses within this package, like the included workflows directory. OLIVE Example API Client Implementations OLIVE is also generally shipped with example API Client implementations in both Java and python. They are usually included in: <...>/olive5.5.1/api/[java, python] These can provide both a command-line-based method of interacting with the OLIVE server, and as integration examples providing templates or code bases for some integration teams. Advanced and Optional Usage and Features Click to expand for a deeper dive on some Martini capabilities Configuring Ports (Optional) By default the container exposes seven ports on the host machine running the container: 5588 5589 5004 5005 5570 5580 5888 If you want to change them, there are two ways to do this: You can directly edit the martini.sh or martini.ps1 file to change these seven variables near the top of the file: OLIVE_HOST_PORT_OLIVESERVER OLIVE_HOST_PORT_OLIVESECOND OLIVE_HOST_PORT_TEST OLIVE_HOST_PORT_BROKER OLIVE_HOST_PORT_DOCSERVER OLIVE_HOST_PORT_WEBSERVER OLIVE_HOST_PORT_REVERSE_PROXY Note that if you are using Windows Command Prompt (cmd) or Windows PowerShell via martini.bat , edit the martini.ps1 file, as it is called by martini.bat during operation. Doing this will use the new ports every time the container starts. Alternatively, you can set shell variables, with the same names as above, to the port numbers you want to use. This method will temporarily override the ports used to what you have set, but subsequent container starts will revert to the original, if the shell/environment variables are no longer set. Configuring Plugins, Workflows, and Documentation (Optional) If you are using the default installation, then no configuration is required. Your workflows must be in a directory called oliveAppData/workflows/ , your documentation in a directory called docs/ , and your plugins in a directory called oliveAppData/plugins/ , and the martini.sh script in an oliveMartini directory that is adjacent to docs/ and oliveAppData/. By default, OLIVE Martini will store logs, enrollments and other overhead-related files to oliveAppData/server/ If your data is not located in these default locations, then you have two ways to configure the OLIVE Martini container (similar to configuring ports described above): You can directly edit the martini.sh file to change these three variables near the top of the file: OLIVE_HOST_PLUGINS OLIVE_HOST_SERVER OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE Doing this will use the new locations every time the container starts. You can set shell variables to the full path names of the locations of your plugins, workflows, and documentation. Set these shell variables: OLIVE_HOST_PLUGINS OLIVE_HOST_SERVER OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE But remember, neither of these is required, if your plugins, workflows, and documentation are all in the default locations. Alternatively, changing MY_OLIVE_APP_DATA in the martini.sh script or setting the OLIVE_APP_DATA environment variable (by default set to oliveAppData/ ) will change most of these settings at once (excluding documentation) if it is desired to keep these resources together but in a different location. You can also (optionally) edit martini.sh to change the DELAY count. It can take a few to 10 seconds or so for all the servers to start on the container, depending on the hardware performing the operation. Normally the martini.sh returns immediately, even if the servers have not had time to start. If you don't like that behavior, you can set the DELAY variable near the top of the file to the number of seconds it takes to start all the servers, usually 7 or 10 seconds. This would mean that the command will not return until the servers are actually up and running. Testing The Installation There are a few ways to quickly sanity check that the installation is properly configured, up and running. Use a web browser to see what workflows are available. Go to URL host :5004/api/workflows. You should see json text describing the available workflows. Use a web browser to test the Olive Web UI. Go to URL host :5580. You should see a page with \"SRI International\" in the upper left corner, and a section for \"Media\" and a section for \"Workflows\". (See below for getting started with the Olive Web UI.) Use a web browser to look at the documentation. Go to to URL host :5570. You should see a web page titled \"Open Language Interface for Voice Exploitation (OLIVE)\". Final Notes / Troubleshooting There are a couple of known minor issues that may occur the first time you are starting the OLIVE Martini container on a new device. Since Docker will prompt asking for permission to share a few locations on the machine, in order to access the included plugins, workflows, and documentation, the container cannot initially read these and may report no plugins or workflows found at first. Once access is granted for Docker to share these locations, you should be able to check the plugins that are loaded by checking martini log , or in the extreme case, by stopping and restarting the container: macOS / linux $ ./martini.sh stop $ ./martini.sh start Windows $ . \\m artini.bat stop $ . \\m artini.bat start It can also take a moment for the Raven UI to populate the available Workflows on the bottom of the interface screen, especially the first time you are loading the web page. If this space is blank, please wait a moment (may take up to a minute on some systems) and/or refresh the page.","title":"OLIVE Martini Docker Setup"},{"location":"martini.html#olive-martini-docker-container-setup","text":"","title":"OLIVE Martini Docker Container Setup"},{"location":"martini.html#introduction","text":"This page guides through the installation, setup, and launching of a docker-container based OLIVE software package featuring the new \"OLIVE Martini\" container setup. This new delivery method includes the usual OLIVE server and plugin capabilities delivered in the past, but includes a new addition - Batch GUI capabilities powered by the new Raven GUI that can be accessed through your web browser. For instructions on how to use this new GUI once you have the software properly installed, check out the Raven Web GUI page. The overall installation process is very similar to previous Docker container based OLIVE deliveries in the past, with the main changes affecting how the OLIVE container is managed for startup, shutdown, etc. This page assumes you already have Docker installed and configured, or that you are already familiar with it. Some sections are collapsed by default to shift the OLIVE-specific info up, but they can be easily expanded for more details if necessary.","title":"Introduction"},{"location":"martini.html#olive-martini-quick-start","text":"This briefly covers the steps necessary to get up and running with an OLIVE Martini to serve as a quick refresher for returning users or those simply wishing to start as quickly as possible. Please refer to the respective linked sections for each step if more information is needed, or if any trouble is encountered. Make sure Docker is installed Unpack the delivery archive Using $ tar -xzf olive5.5.1-martini-2Jun2022.tar.gz Load the OLIVE Martini image $ cd olive5.5.1/martini/ docker load -i olive-martini-container-5.5.1.tar Start the container $ ./martini.sh start or $ .\\martini.ps1 start Use the container When finished, stop the container $ ./martini.sh stop or $ .\\martini.ps1 stop !!! info \"Note that if you wish for OLIVE to use GPUs for supported plugins, you must launch Martini with the --gpu / -gpu flag , and also be sure to configure compatible plugins to use the GPU as outlined in the GPU Plugin/Domain Configuration documentation before launching Martini.","title":"OLIVE Martini Quick Start"},{"location":"martini.html#download-install-and-launch-docker","text":"Docker Installation Instructions (Click to expand)","title":"Download, Install, and Launch Docker"},{"location":"martini.html#install-and-start-docker","text":"Before you can get started installing and running OLIVE, you'll need to make sure you have fully installed and configured Docker. The proper installation steps vary depending on your host OS, so please refer to the appropriate official Docker installation instructions: Docker Desktop for Windows Docker Desktop for macOS Docker Engine for Ubuntu Docker for Ubuntu is especially important to follow the official steps closely, as there are additional important post-installation steps to perform to make sure docker runs smoothly on your system. Note that if installing into an Ubuntu instance running on WSL2, systemctl is not used on such systems. This means that some of the commands provided in the Docker for Ubuntu instructions above may not succeed as written; notably for starting and stopping the Docker service. Please use service for these commands instead: $ sudo service docker start In addition, if using Docker for Ubuntu, the Nvidia drivers must be installed separately. This doesn't seem to be necessary if using Docker Desktop. Instructions for this installation can be found here, from Nvidia . Before moving on, be sure that the docker service has been started.","title":"Install and Start Docker"},{"location":"martini.html#adjust-docker-settings-ram-cores","text":"Configure Docker-allocated Resources (Windows and macOS only) (Click to expand) If you are running Docker on Windows or MacOS, there may be some extra required configuration. By default, Docker is assigned one or two CPU cores, and only 2GB of memory. In order to effectively run the OLIVE docker container, you will need to increase the Memory allocated to Docker to at least 8 GB (or 16 GB for some plugins/tasks), depending on which plugins will be run. If you have cores available, you can obtain greater performance by also increasing the CPU allocation. Allocating more cores and more memory will almost always improve OLIVE performance. Windows macOS These settings are accessed by right clicking the Docker icon in the notification tray on Windows (notification bar if using MacOS), selecting \u2018Settings\u2019 and navigating to the \u2018Advanced\u2019 tab, then adjusting the CPU and Memory sliders as desired. Windows macOS","title":"Adjust Docker settings (RAM, Cores)"},{"location":"martini.html#anatomy-of-an-olive-martini-package","text":"Once Docker is installed and properly configured, you can download and uncompress the OLIVE Docker package from the SRI-provided share link. The link, delivery method, and exact filenames will vary from delivery to delivery, but once you have downloaded and unpacked the software archive: $ tar -xzf olive5.5.1-martini-2Jun2022.tar.gz You should find similar content to below unless told otherwise: olive5.5.1/ api/ - Directory containing the Python and Java OLIVE Client utilities docs/ - OLIVE documentation martini/ olive-martini-container-5.5.1.tar - OLIVE Martini Docker container martini.sh - Multi-purpose utility and management script for the OLIVE Martini Docker container on linux and macOS martini.ps1 / martini.bat - Same as above, but for Windows PowerShell OliveGUI/ - (Optional) The OLIVE Nightingale Forensic GUI oliveAppData/ plugins/ - OLIVE Plugins directory; Actual plugins included will depend on the customer, mission, and delivery workflows/ - (Optional) Directory containing OLIVE workflows; actual included worfklows will depend on the customer, mission, and delivery Expand the section below for more details on each component, or scroll to continue setup. OLIVE Martini Component Details (Click to Expand)","title":"Anatomy of an OLIVE Martini Package"},{"location":"martini.html#api","text":"This directory contains the Python and Java OLIVE Client utilities that interact with and task a running OLIVE/Martini server. The client documentation covers how to get started with these.","title":"api/"},{"location":"martini.html#docs","text":"Contains the documentation you're currently browsing. Other versions can be viewed online at the OLIVE Software Documentation Page , but the version delivered with OLIVE should be the most appropriate for your specific delivery. To view the documentation, open this page in a web browser: olive5.5.1/docs/index.html Or follow the links given by the martini.sh startup output for the live-hosted local documentation.","title":"docs/"},{"location":"martini.html#martini","text":"Contains the core OLIVE server and utility components for running the OLIVE software. This includes the OLIVE Martini image itself, and the the management scripts to help start, monitor, configure, and stop the container.","title":"martini/"},{"location":"martini.html#olive-martini-container-tar","text":"Docker image that includes OLIVE , the Raven Web GUI , the OLIVE Web Broker, and various other utilities, including an html server to host this documentation.","title":"olive-martini-container-.tar"},{"location":"martini.html#martinish-martinips1-martinibat","text":"Multi-purpose utility and management script for the OLIVE Martini Docker container on linux and macOS (.sh) or Windows (.ps1 and .bat) respectively.","title":"martini.sh - martini.ps1 / martini.bat"},{"location":"martini.html#olivegui","text":"The OLIVE Nightingale Forensic UI for close file analysis, live streaming tasks, and other OLIVE GUI tasks.","title":"OliveGUI/"},{"location":"martini.html#oliveappdata","text":"This is the default location for the all important OLIVE plugins and workflows . Once you start using the system, it is also where the OLIVE server will start saving server logs, in a server/ directory created here.","title":"oliveAppData/"},{"location":"martini.html#plugins","text":"Included plugins are installed here - for more information on OLIVE plugins, refer to the OLIVE Plugin Overview , the list of released plugins , or the individual documentation page of the plugin of interest from the navigation on the left.","title":"plugins/"},{"location":"martini.html#workflows","text":"If any workflows are included with your OLIVE package, they reside here. Workflows are powerful ways to combine different plugins and capabilities to perform multiple scoring requests at once, and/or perform complex routing operations like using Language ID results to choose whether or not to perform ASR, and which language to use if so.","title":"workflows/"},{"location":"martini.html#load-the-olive-martini-docker-image","text":"The first setup step is to load the OLIVE Martini Docker image. This is only necessary once. Open a command prompt (PowerShell in Windows or Terminal in linux/macOS), navigate to the directory containing the OLIVE Docker image, and follow the examples below. linux / macOS $ cd / home /< username >/ olive5 . 5.1 / martini $ docker load - i olive - martini - container - 5.5 . 1. tar Windows $ cd C : \\ Users \\ < username > \\ olive5 . 5.1 \\ martini $ docker load - i olive - martini - container - 5.5 . 1. tar This operation can take some time; quite a few minutes on some machines, and you may not see feedback right away. Once you are returned to the command prompt, if there are no error messages, loading is complete and you can proceed.","title":"Load the OLIVE Martini Docker Image"},{"location":"martini.html#operating-systems-note","text":"There are three provided martini management scripts: - martini.sh - martini.bat - martini.ps1 The functionality of each is intended to be identical, and these are provided as different options for different operating systems for convenience. The martini.sh script should be used for linux and for macOS. martini.ps1 should be used when managing the OLIVE Martini container from Windows PowerShell, and martini.bat , which calls martini.ps1 internally, should be used if martini.ps1 cannot execute for permission reasons in PowerShell, and when managing OLIVE Martini from Windows Command Prompt ( cmd ). The examples below attempt to show all three being used, but in the text, they are refered to as simply martini.sh for brevity. Their features and functionality are the same. Note that the one exception is the flag format. The Windows management use only a single '-' for flags, while the linux/macOS script uses two '--'. For example the '--gpu' optional flag for ./martini.sh start --gpu to launch with GPU connections enabled becomes '-gpu' on Windows: .\\martini.ps1 start -gpu","title":"Operating Systems Note"},{"location":"martini.html#controlling-the-container","text":"Controlling and managing the OLIVE Multi container is made much easier with the provided martini.sh management script. Using this script is as simple as running the script, and providing a command: macOS and linux $ ./martini.sh <command> Windows $ .\\martini.bat <command> The list of commands available to martini.sh are: cli - Starts a shell on the container for debugging or operating the OLIVE CLI tools. help - Display the martini.sh help statement (shown below) list - List the running container(s), if any. log - Display a snapshot of the OLIVE Server log in the terminal. Useful for experts for debugging. net - Show the ports on the host that the container is listening on. start - Start the container. stop - Stop the container. status - Show the status of the processes on the container. version - Display the version information for the installed Martini components. More details for each command, how to use it, and the designed functionality can be found below.","title":"Controlling the Container"},{"location":"martini.html#martinish-help","text":"Prints out the martini.sh help statement, reminding the user of the available commands: Supported commands include : cli , help , list , log , net , start , stop , status , and version . martini . sh cli : Starts a shell on the container for debugging . martini . sh help : Prints out help information . martini . sh list : List the running container . martini . sh log : Print out log files from the container . martini . sh net : Shows the ports on the host that the container is listening on . ( requires `netstat` OS utility be installed for proper function ) martini . sh start : Start the container . Optional flags : -- gpu : Enable Martini access to GPUs , if available . Be sure to properly configure any plugin / domains to be used with GPU devices as outlined in the documentation . -- tls_server_only : The server will be configured with a certificate and will only respond to HTTPS requests ; clients aren 't required to send certificate but must use HTTPS protocol (one-way TLS) --tls_server_and_client: Both the server and clients will need to communicate with certificates over HTTPS (two-way TLS) --debug: activate ' debug ' mode for the OLIVE server. This will cause OLIVE to maintain all OLIVE server logs to aid in troubleshooting and debugging. martini.sh status: Shows status of the processes on the container. martini.sh stop: Stop the container. martini.sh version: Prints out version information.","title":"martini.sh help"},{"location":"martini.html#martinish-start","text":"Starts up a previously built and loaded container. Note that it can take several seconds for all the servers to start on the container. Introduced with OLIVE 5.5.1, the start command now accepts four different optional startup flags.","title":"martini.sh start"},{"location":"martini.html#martinish-start-gpu","text":"This grants the OLIVE Martini access to GPUs, if available. Be sure to properly configure any plugin/domains to be used with GPU devices as outlined in the GPU Plugin/Domain Configuration documentation. Without explicit configuration to use a GPU, whether by SRI before delivery or by the end user, no plugin will take advantage of available GPUs. Note that if no GPUs are available on the machine, or the GPUs are not compatible with the OLIVE 5.5.1 software and this flag is used, startup of the Martini will fail!","title":"martini.sh start --gpu"},{"location":"martini.html#martinish-start-tls_server_only","text":"Configures the server to use one-way TLS, relying on a server-side for encrypted communication. The server will only respond to HTTPS requests. Clients aren't required to send a certificate, but must use HTTPS protocol. Requires additional files for proper function, and for Martini to start at all if this option is enabled. All of these files must be placed in olive5.5.1/oliveAppData/nginx-files/ . The files are: passwords.txt A file with passphrases for secret keys where each passphrase is specified on a separate line. Passphrases are tried in turn when loading the key. The file can be empty if passphrases are not required. server.crt A certificate in the PEM format. If intermediate certificates should be specified in addition to a primary certificate, they should be specified in the same file in the following order: the primary certificate comes first, then the intermediate certificates. NOTE : You need to rename a copy of your certificate and put it in the /nginx-files/ directory mentioned above. server.key The secret key in the PEM format for the server.crt . NOTE : You need to rename a copy of your certificate key and put it in the /nginx-files/ directory. Please work with your IT department to obtain valid certificates.","title":"martini.sh start --tls_server_only"},{"location":"martini.html#martinish-start-tls_server_and_client","text":"Similar to the --tls_server_only flag described above, but instead activates two-way TLS. Both the server and client are required to provide a certificate, and all communication occurs over HTTPS. Carries the same file and certificate requirements as --tls_server_only , but with the addition of the client providing their own valid certificate. The additional file requirement for twoway TLS: clientCA.crt A file with trusted CA certificates in the PEM format used to verify client certificates. NOTE : You need to rename a copy of your file and put it in the /nginx-files/ directory mentioned above. As always, please work with your IT department to obtain valid certificates.","title":"martini.sh start --tls_server_and_client"},{"location":"martini.html#martinish-start-debug","text":"Activates 'debug' mode for the OLIVE server. This causes OLIVE to mantain all OLIVE server logs to aid in troubleshooting and debugging. These logs are stored in olive5.5.1/oliveAppData/server/ by default. Without this flag, \"failed\" logs are retained, but logs for jobs deemed to be successful are deleted as tasks are completed to avoid clutter and confusion.","title":"martini.sh start --debug"},{"location":"martini.html#martinish-stop","text":"Stops a running container.","title":"martini.sh stop"},{"location":"martini.html#martinish-status","text":"Prints out some basic information on the processes running, network ports, workflows and plugins which are active on the container. $ martini.sh status Message brokers running: Processes: 1 Plugins: aed-enrollable-v1.0.1 aln-waveformAlignment-v1.0.0 env-indoorOutdoor-v1.0.0 env-multiClass-v2.0.0 env-powerSupplyHum-v1.0.0 env-speakerCount-v1.0.0 lid-embedplda-v2.0.1 nsd-sadInverter-v1.0.0 sad-dnn-v7.0.1 sed-rmsEnergy-v1.0.0 sid-dplda-v2.0.1 voi-speakingStyle-v1.0.0 voi-vocalEffort-v1.0.1 vtd-dnn-v7.0.1 Workflows: Acoustic-Event-Detection.workflow Background-Noise-Detection.workflow SAD_SID_LID.workflow Speech-Analysis.workflow tcp 0 0 0.0.0.0:5589 0.0.0.0:* LISTEN 0 7905040 72/python tcp 0 0 0.0.0.0:5590 0.0.0.0:* LISTEN 0 7915534 73/python tcp 0 0 0.0.0.0:8070 0.0.0.0:* LISTEN 0 7910329 9/python tcp 0 0 0.0.0.0:5004 0.0.0.0:* LISTEN 0 7917573 138/nginx: master p tcp 0 0 0.0.0.0:5005 0.0.0.0:* LISTEN 0 7914186 15/java tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 0 7917572 138/nginx: master p tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 0 7897930 26/httpd tcp 0 0 0.0.0.0:5588 0.0.0.0:* LISTEN 0 7915524 16/python unix 2 [ ACC ] STREAM LISTENING 7915525 16/python /tmp/executor_714acd0b-0d0d-4259-84c2-127b84b8d26c.pipe unix 2 [ ACC ] STREAM LISTENING 7905043 80/python /tmp/pymp-vu6okza8/listener-4e5_70ll Httpd (web-ui) servers running: 6 Nginx (reverse proxy) server running: 2 Olive servers running: 12 Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:5589 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5590 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8070 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5004 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5005 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:5588 0.0.0.0:* LISTEN","title":"martini.sh status"},{"location":"martini.html#martinish-cli","text":"Starts up a shell within the OLIVE Martini container. The container must already be running. This is typically used for internal testing and troubleshooting and is not meant to be used by end-users. You can use this shell to run Olive CLI commands, such as: $ martini.sh cli # Show the plugins installed on the container: $ ls /home/olive/olive/plugins/ # Show the audio files on the container, which you can use as examples: $ ls /olive-data/media/shared/ # Run an analysis $ localanalyze /home/olive/olive/plugins/lid-embedplda-v2.0.1/domains/multi-v1 /olive-data/media/shared/Komodo_dragon_en.wav.lst # See the results $ more output.txt This can also be used for troubleshooting. When running with the cli command, your shell starts in /opt/olive . This is where the runtime is installed, and where things are run within the container. In /opt/olive : Running martini.sh status will print out what is running, and what network connections are active. And looking at this script will tell you what should be running. Do not run martini-process.sh. That is run when the container starts. The /home/olive/olive directory is the other important directory. Here: The martini-process.log file is the output of the script which starts all servers. The env.log file (if it exists) are the shell variables set. The other log files here were started by each server. The plugins/ directory contains all plugins. The server/ directory contains server logs.","title":"martini.sh cli"},{"location":"martini.html#martinish-net","text":"Lists the project network ports that are active on the host machine, the machine running the container.","title":"martini.sh net"},{"location":"martini.html#martinish-list","text":"Lists the project containers that are running.","title":"martini.sh list"},{"location":"martini.html#martinish-log","text":"Displays the OLIVE Server log; useful for checking on the status of the OLIVE server in case of a malfunction, or troubleshooting unexpected behavior.","title":"martini.sh log"},{"location":"martini.html#martinish-version","text":"Diplays the version information for the individual components of OLIVE: Martini v1.1. Olive v5.3.0. Broker v1.5. Raven v0.2.0.","title":"martini.sh version"},{"location":"martini.html#using-the-container","text":"Once you have started up the container using the martini.sh start command described above, you are ready to begin using it. Your container has four different applications within it, which you can use - the Raven Web UI , the OLIVE Server itself , the OLIVE Message Broker, and a web server hosting the OLIVE Documentation. In addition, most OLIVE deliveries typically ship with the interactive Nightingale Forensic GUI for performing close file analysis using OLIVE plugins and workflows, as well as both Java and Python OLIVE Client CLI example applications. Note that if you are attempting to connect from the same machine, you can access this feature at localhost:5580 . If you are accessing from a different machine, you must use the full hostname, or IP address. For convenience, the martini.sh script outputs the hostname and corresponding port for several activities: $ martini.sh start \\Started the container. From this machine: Run Nightingale (Olive GUI) using server localhost and port 5588. Use a web browser to localhost:5570 to see the documentation. Use a web browser to localhost:5580 to use the Raven Web UI. If using the REST API point to the server running on localhost:5004. From any other machine: Run Nightingale (Olive GUI) using server stauf-MBP16 and port 5588. Use a web browser to stauf-MBP16:5570 to see the documentation. Use a web browser to stauf-MBP16:5580 to use the Raven Web UI. If using the REST API point to the server running on stauf-MBP16:5004. Installed plugins (mounted from /Users/astauf/olive/olive5.3.0/oliveAppData/plugins) are: asr-dynapy-v3.0.0 map-routerGlobal-v1.0.0 dfa-cplda-v1.0.0 map-routerGlobal-v1.0.1 dfa-speakerSpecific-v1.0.0 map-routerRegion-v1.0.0 dfa-spoofnet-v1.0.0 pim-validateGlobal-v1.0.0 enh-mmse-v2.0.2 qbe-ftdnnSmolive-v1.0.0 env-audioQuality-v2.0.0 qua-analysis-v1.0.0 fdi-pyEmbed-v1.0.0 qua-filter-v1.0.0 fdv-pyEmbed-v1.0.0 red-transform-v1.0.0 fri-pyEmbed-v1.0.0 sad-dnn-v7.0.2 frv-pyEmbed-v1.0.0 sad-dnnSmolive-v1.0.0 gdd-embedplda-v1.0.0 sdd-diarizeembed-v1.0.0 gid-gb-v2.0.1 sdd-sbcEmbed-v2.0.3 ldd-embedplda-v1.0.0 shl-sbcEmbed-v1.0.2 ldd-embedplda-v1.0.1 sid-dplda-v2.0.2 lid-embedplda-v3.0.0 sid-embed-v6.0.2 lid-embedplda-v3.0.1 tpd-dynapy-v5.0.0 lid-embedpldaSmolive-v1.0.0 vtd-dnn-v7.0.2 Installed workflows (mounted from /Users/astauf/olive/olive5.3.0/oliveAppData/workflows) are: /opt/olive-broker/data/workflows/FaceDetection_Image_FDI.workflow.json /opt/olive-broker/data/workflows/FaceDetection_Video_FDV.workflow.json /opt/olive-broker/data/workflows/FaceRecognition_Image_FRI.workflow.json /opt/olive-broker/data/workflows/FaceRecognition_Video_FRV.workflow.json /opt/olive-broker/data/workflows/SID_quality-controlled.workflow.json /opt/olive-broker/data/workflows/SmartTranscriptionFull.workflow.json /opt/olive-broker/data/workflows/TPD-eng.workflow.json /opt/olive-broker/data/workflows/conditional_asr_v2.workflow.json /opt/olive-broker/data/workflows/quality-region-analysis-for-SID.workflow.json Please choose the appropriate hostname and port number for your desired activity and host situation.","title":"Using The Container"},{"location":"martini.html#raven-web-ui","text":"To connect to the Raven Web/Batch GUI , open a web browser and navigate to URL host :5580. Once there, follow the on-screen prompts to drag-and-drop one or more audio files to the \"local media\" dropbox section, select one of the available workflows, and click on the action button in the lower right hand corner. More details for the Raven GUI can be found on its dedicated documentation page: Raven Web UI .","title":"Raven Web UI"},{"location":"martini.html#the-olive-documentation","text":"When the container is started, it launches a web server serving this documentation. To view the hosted version, navigate a web browser to URL host :5570.","title":"The OLIVE Documentation"},{"location":"martini.html#the-olive-server","text":"This is used internally by the Olive Web UI and the Olive Message Broker. You can also write Python programs which make calls directly to the server. See seperate documentation on olivepy API for details.","title":"The OLIVE Server"},{"location":"martini.html#the-olive-message-broker","text":"This is used internally by the Olive Web UI.","title":"The OLIVE Message Broker"},{"location":"martini.html#nightingale-forensic-ui","text":"This GUI provides access to the full suite of available OLIVE plugins and workflows, for performing close file analysis. It offers powerful audio visualization tools, live streaming capabilities, and a number of other useful features. For more information on what Nightingale offers and how to get started using it, please refer to the Nightingale GUI dedicated documentation page. As a quick primer, the Nightingale UI is typically provided in: <...>/olive5.5.1/OliveGUI Nightingale requires OpenJDK Java 11. Once this is installed, you can run it by navigating to: <...>/olive5.5.1/OliveGUI/bin/ And either double-clicking or running the Nightingale (macOS, linux) or Nightingale.bat (Windows) script appropriate for your OS. macOS / linux $ ./Nightingale Windows $ . \\N ightingale.bat NOTE that the Nightingale launcher scripts are designed to be launched from this bin/ directory in order to properly link to other folders that it uses within this package, like the included workflows directory.","title":"Nightingale Forensic UI"},{"location":"martini.html#olive-example-api-client-implementations","text":"OLIVE is also generally shipped with example API Client implementations in both Java and python. They are usually included in: <...>/olive5.5.1/api/[java, python] These can provide both a command-line-based method of interacting with the OLIVE server, and as integration examples providing templates or code bases for some integration teams.","title":"OLIVE Example API Client Implementations"},{"location":"martini.html#advanced-and-optional-usage-and-features","text":"Click to expand for a deeper dive on some Martini capabilities","title":"Advanced and Optional Usage and Features"},{"location":"martini.html#configuring-ports-optional","text":"By default the container exposes seven ports on the host machine running the container: 5588 5589 5004 5005 5570 5580 5888 If you want to change them, there are two ways to do this: You can directly edit the martini.sh or martini.ps1 file to change these seven variables near the top of the file: OLIVE_HOST_PORT_OLIVESERVER OLIVE_HOST_PORT_OLIVESECOND OLIVE_HOST_PORT_TEST OLIVE_HOST_PORT_BROKER OLIVE_HOST_PORT_DOCSERVER OLIVE_HOST_PORT_WEBSERVER OLIVE_HOST_PORT_REVERSE_PROXY Note that if you are using Windows Command Prompt (cmd) or Windows PowerShell via martini.bat , edit the martini.ps1 file, as it is called by martini.bat during operation. Doing this will use the new ports every time the container starts. Alternatively, you can set shell variables, with the same names as above, to the port numbers you want to use. This method will temporarily override the ports used to what you have set, but subsequent container starts will revert to the original, if the shell/environment variables are no longer set.","title":"Configuring Ports (Optional)"},{"location":"martini.html#configuring-plugins-workflows-and-documentation-optional","text":"If you are using the default installation, then no configuration is required. Your workflows must be in a directory called oliveAppData/workflows/ , your documentation in a directory called docs/ , and your plugins in a directory called oliveAppData/plugins/ , and the martini.sh script in an oliveMartini directory that is adjacent to docs/ and oliveAppData/. By default, OLIVE Martini will store logs, enrollments and other overhead-related files to oliveAppData/server/ If your data is not located in these default locations, then you have two ways to configure the OLIVE Martini container (similar to configuring ports described above): You can directly edit the martini.sh file to change these three variables near the top of the file: OLIVE_HOST_PLUGINS OLIVE_HOST_SERVER OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE Doing this will use the new locations every time the container starts. You can set shell variables to the full path names of the locations of your plugins, workflows, and documentation. Set these shell variables: OLIVE_HOST_PLUGINS OLIVE_HOST_SERVER OLIVE_HOST_WORKFLOWS OLIVE_HOST_DOCSITE But remember, neither of these is required, if your plugins, workflows, and documentation are all in the default locations. Alternatively, changing MY_OLIVE_APP_DATA in the martini.sh script or setting the OLIVE_APP_DATA environment variable (by default set to oliveAppData/ ) will change most of these settings at once (excluding documentation) if it is desired to keep these resources together but in a different location. You can also (optionally) edit martini.sh to change the DELAY count. It can take a few to 10 seconds or so for all the servers to start on the container, depending on the hardware performing the operation. Normally the martini.sh returns immediately, even if the servers have not had time to start. If you don't like that behavior, you can set the DELAY variable near the top of the file to the number of seconds it takes to start all the servers, usually 7 or 10 seconds. This would mean that the command will not return until the servers are actually up and running.","title":"Configuring Plugins, Workflows, and Documentation (Optional)"},{"location":"martini.html#testing-the-installation","text":"There are a few ways to quickly sanity check that the installation is properly configured, up and running. Use a web browser to see what workflows are available. Go to URL host :5004/api/workflows. You should see json text describing the available workflows. Use a web browser to test the Olive Web UI. Go to URL host :5580. You should see a page with \"SRI International\" in the upper left corner, and a section for \"Media\" and a section for \"Workflows\". (See below for getting started with the Olive Web UI.) Use a web browser to look at the documentation. Go to to URL host :5570. You should see a web page titled \"Open Language Interface for Voice Exploitation (OLIVE)\".","title":"Testing The Installation"},{"location":"martini.html#final-notes-troubleshooting","text":"There are a couple of known minor issues that may occur the first time you are starting the OLIVE Martini container on a new device. Since Docker will prompt asking for permission to share a few locations on the machine, in order to access the included plugins, workflows, and documentation, the container cannot initially read these and may report no plugins or workflows found at first. Once access is granted for Docker to share these locations, you should be able to check the plugins that are loaded by checking martini log , or in the extreme case, by stopping and restarting the container: macOS / linux $ ./martini.sh stop $ ./martini.sh start Windows $ . \\m artini.bat stop $ . \\m artini.bat start It can also take a moment for the Raven UI to populate the available Workflows on the bottom of the interface screen, especially the first time you are loading the web page. If this space is blank, please wait a moment (may take up to a minute on some systems) and/or refresh the page.","title":"Final Notes / Troubleshooting"},{"location":"nightingale.html","text":"OLIVE Nightingale GUI Installation and Startup Getting started with the OLIVE Nightingale GUI is a pretty straightforward process. The only prerequisite for running the GUI, apart from a functional and running OLIVE server, is openJDK Java 11, that can be sourced here: OpenJDK 11 The GUI is configured to automatically detect and connect to an OLIVE server instance running on the same machine as the GUI, as long as the default server ports are still being used. It is possible to connect this GUI to an OLIVE server running on a remote machine, so long as firewall and other settings permit. For details on how to do this if it's needed, get in touch with us for assistance. These instructions assume a typical OLIVE delivery layout as follows: olive5.5.1 docs/ oliveDocker or runtime-5.5.1*/olive-5.5.1* OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ workflows/ To launch the GUI, once you've installed the version of Java linked above, fire up the OLIVE server in your favorite manner, then you can launch the GUI by hopping into the appropriate directory, and starting the Nightingale launcher: $ cd olive5.5.1/OliveGUI/bin $ ./Nightingale This should start up the GUI that should automatically connect to the running server. The GUI should be pointing to your 'workflows' directory by default, so it should already know about the SAD_LID_SID.workflow that's included - but if for some reason this isn't showing up, you can add it using the '+' symbol in the GUI's Workflows panel. Nightingale GUI User Guide Overview Below is a basic diagram of Nightingale with all the major panels highlighted: Panels: Live Stream Playback Layout New Tier Server Status Workflows Plugins Channel List Spectrogram Spectrogram Control Global Score Results Region Score Results Enrollment What's an Enrollment? Enrollment is the process of adding a model to a plugin and is required for several types of plugins. In the case of Speaker Identification, and enrollment involves submitting a portion of audio that contains only the speaker that you want to look for in the future. The enrollment process turns this portion of audio into a model which the speaker identification plugin will use to score future audio against in order to detect additional instances of the speaker you enrolled. How to Enroll Enrollment in the Nightingale User Interface is fast and simple. You need to do the following steps: Select the audio you would like to enroll Select the plugin you would like to enroll it in Give the enrollment a name, or add it to an existing enrollment (this is called augmentation) Click enroll and then wait. Below are images that can be used as reference for each step. In these examples the enrollments are being done using the Speaker Identification plugin on the left side, and the Query By Example plugin on the right side. 1. Select Audio 2a. Select Plugin 2b. Select Plugin contd 3. Give Enrollment Name 4. Enroll Augmentation Augmentation is almost the exact same process as enrollment, but in augmentation instead of creating a new class (eg. speaker, keyword, etc) you'll be adding additional data to an existing class. This is useful for improving models, especially when the amount of audio, or the quality of audio for the initial enrollment is low. To augment a model simple follow the same steps for enrollment, but instead of giving the enrollment a name in Step 3, select an existing class from the following dropdown: Removing an Enrollment Removing an enrollment can be useful to remove clutter if you no longer need a class or classes. Removing an enrolled class removes the entirety of the class, if you had augmented the class with several audio clips, unenrolling will remove everything. There is no way through the Nightingale GUI to remove individual augmentation segments from an enrollment. To remove a class from a plugin follow the steps below: Select the 'Enroll' button to the right of the plugin you want to remove an enrolled model from. Select 'Remove ______'. Select which class you want to remove from the dropdown list. Click 'Submit' Analysis Analysis is the basic operation of most plugins, and mostly falls under 3 general types of scoring; Global Scoring and Region Scoring. Region scorers return score results for one or more classes, and associate them with a specific region (or regions) in time, whereas global scorers return a single score for each class for the entire portion of the audio that was submitted. Some examples of global scorers and region scorers are below; typically \"Identification\" plugins are global scorers, and \"Detection\" plugins are region scorers. Global Scorers: Speaker Identification (SID) Language Identification (LID) Gender Identification etc. Region Scorers: Language Detection (LDD) Speaker Detection (SDD) Query by Example Keyword Spotting (QBE) Acoustic Event Detection (AED) Automatic Speech Recognition (ASR) etc. The procedure for scoring audio is largely the same for both types of plugins, and entails following these steps: First select audio (if you are analyzing a specific region, otherwise just make sure the appropriate file is selected), then do the following steps to run analysis on that audio: Select plugin type from panel (eg. SID, LID, etc) Select from 'Entire File', 'Selection', and 'Tier' (If you are analyzing a specific Tier, you will also need to make sure the correct tier is selected from the dropdown) If you have multiple versions or domains of the plugin you will need to specify which version of the plugin to use from the '___ Plugin:' dropdown menu. Click 'Submit' Viewing Results Global Scoring After you've performed analysis with a global scoring plugin the results of the analysis will appear in the global scoring section in the top right of the Nightingale GUI. Depending on the plugin there will be a list of each of that plugins classes and a corresponding score displayed in this area. Scores that are above 0 are considered a detection and will be shown at the top of the list in large green text, scores that are below 0 are displayed below in grey text. The higher a positive score is the more confident the system is that that particular class was detected in the audio that was submitted, whereas conversely, the lower the score below zero the more confident the system is that that class was NOT detected in the submitted audio. Some plugins that have many classes enrolled will not be able to display all of the scores in the global score results region, instead they will display the top X number of results and will also display a small button that says \"more\" at the bottom of the list. Clicking \"more\" will display the rest of the class scores for that analysis. Global scoring results appear on the right side of the GUI in panel \"11.\" on the Nightingale Overview Region Scoring After you've performed analysis with a region scoring plugin the results will be displayed as a new tier beneath the audio file that was analyzed. If there were detections in the analysis regions with the name of the class that was detected will be displayed in this tier. Region Scoring results will populate below the file that they were run on in panel \"12.\" in the Nightingale Overview Areas in the region score tiers that are marked by a dot/hash pattern indicate that these areas were not included in the region scoring submission. This happens when only a selection or tier is submitted for analysis instead of the entire file. Workflows Workflows are prebuilt complex tasks that run a segment of audio through a predefined series of HLT algorithms. This can be very useful and timesaving when there is a well-defined use case or mission that requires the same tasks or procedure to be run on all audio. If you have been provided with one or multiple .workflow files you can load them into the Nightingale GUI by following the steps below: Once you've successfully loaded one or multiple workflows into the GUI you will see them appear as new buttons in the workflow panel. You can submit audio to a workflow in the same way as submitting audio for analysis, by highlighting a selection/tier/file and clicking the appropriate workflow button as shown below: Depending on how each workflow is configured and which plugins it consists of, the results will be displayed around the waveform as they are returned, and will be displayed either on the Region Scoring Results area below the waveform, or the Global Scoring Results area to the right of the waveform, as is appropriate for each plugin the workflow is running. Nightingale Basic Tools In addition to HLT capabilities Nightingale includes several built-in tools to help users navigate and manipulate audio files for analysis and submission to algorithms. Below are the descriptions of these basic tools and how to use them. Audio Playback Controls Nightingale has two 'Playback Control Panels' that can be used to play, stop, pause, or speed up audio. These panels will play the audio in the active panel, and will default to playing from the start of the active file if no audio is selected, otherwise they will cause playback to start at the beginning of the selected audio. The playback panels are the panels marked \"2.\" in the Nightingale Overview Layout Panel The layout panel, \"3.\" on the Nightingale Overview can be used to customize the number and layout of different channels the user would like to display for simultaneous viewing. This can be a usefull tool to scale Nightingale to the needs of the user. Typically a user focused on close analysis of a audio files might want to use 1-3 panels to display different files, a user focusing on monitoring many live audio streams may wish to use this panel to configure Nightingale to display up to 12 simultaneous audio files or streams. Tier Manipulation and Annotation The 'New Tier' panel marked \"4.\" in the Nightingale Overview allows users to create their own tiers. Additionally it allows for the creation of creating inverse tiers, and creating new tiers from the difference and sum of multiple existing tiers. This is a tool that can be used by advanced users to fine tune the portions of audio that they wish to run tasks on. The 'New Tier' panel also allows users to create an annotation tier, in which they can add comments and notes at different timestamps in the audio. Channel/File List The file list on the left side of the Nightingale GUI marked \"8.\" in the Nightingale Overview can be used to switch between any files (or channels if you are livestreaming) that have been loaded into Nightingale. If there are more files/streams loaded then there are Layout Panels allocated, then you can allocate one of the undisplayed files to one of the layout panels by hovering over it in the Channel/File list, and then clicking the panel on the grid where you would like the file to be displayed. Spectrogram Nightingale's spectrogram, displayed in panel \"9.\" and controlled from panel \"10.\" in the Nightingale Overview can be a powerful tool to analyze audio and pick out signal from the background noise. The spectrogram in panel 9. is the main interface panel for users, and is where the majority of audio selection can be done. The spectrogram control panel in panel 10. allows the user to customize the color schemes, sensitivity, and a host of more advance settings that can be accessed on panel 10. by selecting the \"more...\" button. Spectrogram 1 Spectrogram 2 Changes made here are temporary and will be reset when the GUI is closed.","title":"OLIVE Nightingale GUI"},{"location":"nightingale.html#olive-nightingale-gui","text":"","title":"OLIVE Nightingale GUI"},{"location":"nightingale.html#installation-and-startup","text":"Getting started with the OLIVE Nightingale GUI is a pretty straightforward process. The only prerequisite for running the GUI, apart from a functional and running OLIVE server, is openJDK Java 11, that can be sourced here: OpenJDK 11 The GUI is configured to automatically detect and connect to an OLIVE server instance running on the same machine as the GUI, as long as the default server ports are still being used. It is possible to connect this GUI to an OLIVE server running on a remote machine, so long as firewall and other settings permit. For details on how to do this if it's needed, get in touch with us for assistance. These instructions assume a typical OLIVE delivery layout as follows: olive5.5.1 docs/ oliveDocker or runtime-5.5.1*/olive-5.5.1* OliveGUI/ - The OLIVE Nightingale GUI (not included in all deliveries) bin/ Nightingale oliveAppData/ plugins/ workflows/ To launch the GUI, once you've installed the version of Java linked above, fire up the OLIVE server in your favorite manner, then you can launch the GUI by hopping into the appropriate directory, and starting the Nightingale launcher: $ cd olive5.5.1/OliveGUI/bin $ ./Nightingale This should start up the GUI that should automatically connect to the running server. The GUI should be pointing to your 'workflows' directory by default, so it should already know about the SAD_LID_SID.workflow that's included - but if for some reason this isn't showing up, you can add it using the '+' symbol in the GUI's Workflows panel.","title":"Installation and Startup"},{"location":"nightingale.html#nightingale-gui-user-guide","text":"","title":"Nightingale GUI User Guide"},{"location":"nightingale.html#overview","text":"Below is a basic diagram of Nightingale with all the major panels highlighted: Panels: Live Stream Playback Layout New Tier Server Status Workflows Plugins Channel List Spectrogram Spectrogram Control Global Score Results Region Score Results","title":"Overview"},{"location":"nightingale.html#enrollment","text":"","title":"Enrollment"},{"location":"nightingale.html#whats-an-enrollment","text":"Enrollment is the process of adding a model to a plugin and is required for several types of plugins. In the case of Speaker Identification, and enrollment involves submitting a portion of audio that contains only the speaker that you want to look for in the future. The enrollment process turns this portion of audio into a model which the speaker identification plugin will use to score future audio against in order to detect additional instances of the speaker you enrolled.","title":"What's an Enrollment?"},{"location":"nightingale.html#how-to-enroll","text":"Enrollment in the Nightingale User Interface is fast and simple. You need to do the following steps: Select the audio you would like to enroll Select the plugin you would like to enroll it in Give the enrollment a name, or add it to an existing enrollment (this is called augmentation) Click enroll and then wait. Below are images that can be used as reference for each step. In these examples the enrollments are being done using the Speaker Identification plugin on the left side, and the Query By Example plugin on the right side. 1. Select Audio 2a. Select Plugin 2b. Select Plugin contd 3. Give Enrollment Name 4. Enroll","title":"How to Enroll"},{"location":"nightingale.html#augmentation","text":"Augmentation is almost the exact same process as enrollment, but in augmentation instead of creating a new class (eg. speaker, keyword, etc) you'll be adding additional data to an existing class. This is useful for improving models, especially when the amount of audio, or the quality of audio for the initial enrollment is low. To augment a model simple follow the same steps for enrollment, but instead of giving the enrollment a name in Step 3, select an existing class from the following dropdown:","title":"Augmentation"},{"location":"nightingale.html#removing-an-enrollment","text":"Removing an enrollment can be useful to remove clutter if you no longer need a class or classes. Removing an enrolled class removes the entirety of the class, if you had augmented the class with several audio clips, unenrolling will remove everything. There is no way through the Nightingale GUI to remove individual augmentation segments from an enrollment. To remove a class from a plugin follow the steps below: Select the 'Enroll' button to the right of the plugin you want to remove an enrolled model from. Select 'Remove ______'. Select which class you want to remove from the dropdown list. Click 'Submit'","title":"Removing an Enrollment"},{"location":"nightingale.html#analysis","text":"Analysis is the basic operation of most plugins, and mostly falls under 3 general types of scoring; Global Scoring and Region Scoring. Region scorers return score results for one or more classes, and associate them with a specific region (or regions) in time, whereas global scorers return a single score for each class for the entire portion of the audio that was submitted. Some examples of global scorers and region scorers are below; typically \"Identification\" plugins are global scorers, and \"Detection\" plugins are region scorers. Global Scorers: Speaker Identification (SID) Language Identification (LID) Gender Identification etc. Region Scorers: Language Detection (LDD) Speaker Detection (SDD) Query by Example Keyword Spotting (QBE) Acoustic Event Detection (AED) Automatic Speech Recognition (ASR) etc. The procedure for scoring audio is largely the same for both types of plugins, and entails following these steps: First select audio (if you are analyzing a specific region, otherwise just make sure the appropriate file is selected), then do the following steps to run analysis on that audio: Select plugin type from panel (eg. SID, LID, etc) Select from 'Entire File', 'Selection', and 'Tier' (If you are analyzing a specific Tier, you will also need to make sure the correct tier is selected from the dropdown) If you have multiple versions or domains of the plugin you will need to specify which version of the plugin to use from the '___ Plugin:' dropdown menu. Click 'Submit'","title":"Analysis"},{"location":"nightingale.html#viewing-results","text":"","title":"Viewing Results"},{"location":"nightingale.html#global-scoring","text":"After you've performed analysis with a global scoring plugin the results of the analysis will appear in the global scoring section in the top right of the Nightingale GUI. Depending on the plugin there will be a list of each of that plugins classes and a corresponding score displayed in this area. Scores that are above 0 are considered a detection and will be shown at the top of the list in large green text, scores that are below 0 are displayed below in grey text. The higher a positive score is the more confident the system is that that particular class was detected in the audio that was submitted, whereas conversely, the lower the score below zero the more confident the system is that that class was NOT detected in the submitted audio. Some plugins that have many classes enrolled will not be able to display all of the scores in the global score results region, instead they will display the top X number of results and will also display a small button that says \"more\" at the bottom of the list. Clicking \"more\" will display the rest of the class scores for that analysis. Global scoring results appear on the right side of the GUI in panel \"11.\" on the Nightingale Overview","title":"Global Scoring"},{"location":"nightingale.html#region-scoring","text":"After you've performed analysis with a region scoring plugin the results will be displayed as a new tier beneath the audio file that was analyzed. If there were detections in the analysis regions with the name of the class that was detected will be displayed in this tier. Region Scoring results will populate below the file that they were run on in panel \"12.\" in the Nightingale Overview Areas in the region score tiers that are marked by a dot/hash pattern indicate that these areas were not included in the region scoring submission. This happens when only a selection or tier is submitted for analysis instead of the entire file.","title":"Region Scoring"},{"location":"nightingale.html#workflows","text":"Workflows are prebuilt complex tasks that run a segment of audio through a predefined series of HLT algorithms. This can be very useful and timesaving when there is a well-defined use case or mission that requires the same tasks or procedure to be run on all audio. If you have been provided with one or multiple .workflow files you can load them into the Nightingale GUI by following the steps below: Once you've successfully loaded one or multiple workflows into the GUI you will see them appear as new buttons in the workflow panel. You can submit audio to a workflow in the same way as submitting audio for analysis, by highlighting a selection/tier/file and clicking the appropriate workflow button as shown below: Depending on how each workflow is configured and which plugins it consists of, the results will be displayed around the waveform as they are returned, and will be displayed either on the Region Scoring Results area below the waveform, or the Global Scoring Results area to the right of the waveform, as is appropriate for each plugin the workflow is running.","title":"Workflows"},{"location":"nightingale.html#nightingale-basic-tools","text":"In addition to HLT capabilities Nightingale includes several built-in tools to help users navigate and manipulate audio files for analysis and submission to algorithms. Below are the descriptions of these basic tools and how to use them.","title":"Nightingale Basic Tools"},{"location":"nightingale.html#audio-playback-controls","text":"Nightingale has two 'Playback Control Panels' that can be used to play, stop, pause, or speed up audio. These panels will play the audio in the active panel, and will default to playing from the start of the active file if no audio is selected, otherwise they will cause playback to start at the beginning of the selected audio. The playback panels are the panels marked \"2.\" in the Nightingale Overview","title":"Audio Playback Controls"},{"location":"nightingale.html#layout-panel","text":"The layout panel, \"3.\" on the Nightingale Overview can be used to customize the number and layout of different channels the user would like to display for simultaneous viewing. This can be a usefull tool to scale Nightingale to the needs of the user. Typically a user focused on close analysis of a audio files might want to use 1-3 panels to display different files, a user focusing on monitoring many live audio streams may wish to use this panel to configure Nightingale to display up to 12 simultaneous audio files or streams.","title":"Layout Panel"},{"location":"nightingale.html#tier-manipulation-and-annotation","text":"The 'New Tier' panel marked \"4.\" in the Nightingale Overview allows users to create their own tiers. Additionally it allows for the creation of creating inverse tiers, and creating new tiers from the difference and sum of multiple existing tiers. This is a tool that can be used by advanced users to fine tune the portions of audio that they wish to run tasks on. The 'New Tier' panel also allows users to create an annotation tier, in which they can add comments and notes at different timestamps in the audio.","title":"Tier Manipulation and Annotation"},{"location":"nightingale.html#channelfile-list","text":"The file list on the left side of the Nightingale GUI marked \"8.\" in the Nightingale Overview can be used to switch between any files (or channels if you are livestreaming) that have been loaded into Nightingale. If there are more files/streams loaded then there are Layout Panels allocated, then you can allocate one of the undisplayed files to one of the layout panels by hovering over it in the Channel/File list, and then clicking the panel on the grid where you would like the file to be displayed.","title":"Channel/File List"},{"location":"nightingale.html#spectrogram","text":"Nightingale's spectrogram, displayed in panel \"9.\" and controlled from panel \"10.\" in the Nightingale Overview can be a powerful tool to analyze audio and pick out signal from the background noise. The spectrogram in panel 9. is the main interface panel for users, and is where the majority of audio selection can be done. The spectrogram control panel in panel 10. allows the user to customize the color schemes, sensitivity, and a host of more advance settings that can be accessed on panel 10. by selecting the \"more...\" button. Spectrogram 1 Spectrogram 2 Changes made here are temporary and will be reset when the GUI is closed.","title":"Spectrogram"},{"location":"olivepy-api.html","text":"OLIVE Python Client API Documentation The olivepy Python Client API Implementation is provided as an example/reference implementation for how to interact with the OLIVE server through or as a tool for integration. The olivepy package consists of four main modules, which can be found below: api module client module messaging module utils module","title":"OlivePy Python Client API PyDocs"},{"location":"olivepy-api.html#olive-python-client-api-documentation","text":"The olivepy Python Client API Implementation is provided as an example/reference implementation for how to interact with the OLIVE server through or as a tool for integration. The olivepy package consists of four main modules, which can be found below: api module client module messaging module utils module","title":"OLIVE Python Client API Documentation"},{"location":"plugins.html","text":"Plugin Documentation This page contains high level information about OLIVE plugins and their associated concepts - for more detailed information about integrating with plugins, please refer to the more low-level focused Plugin API Integration Details pages. For more information about a specific plugin, please find its info page link in the list at the bottom of this page . Anatomy of a Plugin OLIVE Plugins encapsulate the actual audio processing technologies and capabilities of the OLIVE system into modular pieces that facilitate system upgrades, capability additions, incremental updates, and provide other benefits such as allowing for tuning models to improve processing in targeted audio conditions or allowing for multiple tools or options to choose from to accomplish a given task. Each plugin has a specific type, which defines the task that it is capable of performing (see Plugin Types below). They consist of two parts, the plugin proper, which contains the recipe or algorithm information on how to perform the task, and one or more domains, which contain the data models used to run the algorithm and perform the function. Since plugins are generally machine learning-based the same algorithm may have multiple strengths based on the specific data used to train it (e.g. telephone audio, push-to-talk audio, high effort vocalization, conversational speech). This separation between the algorithm and the data model allows us to deliver new functionality that is based on training data independently of the algorithm by delivering a new domain. Classes and enrollments (described below) are associated with a domain. Plugin Types Plugin function types refer to the core task the plugin performs and have standard abbreviations. Most plugins are designed to perform one specific function (for example, language identification, keyword spotting). We refer to plugins as working on audio segments, since OLIVE can process both audio files (on the file system) and audio buffers (sent as data through the API). For more information regarding each plugin type, but not a specific plugin or domain, including plugin type definitions, general output formats, and use cases, please click on the name of the plugin in the 'Function Type' column of the table below to visit that plugin type's information page: Table of Plugin Function Types Function type Abbreviation Scoring type Classes Description Speech Activity Detection SAD Frame / Region speech Identifies speech regions in an audio segment Speaker Identification SID Global Enrolled speakers Identifies whether a single-talker audio segment contains a target speaker Speaker Detection SDD Region Enrolled speakers Detects a target speaker in audio with multiple talkers Speaker Diarization DIA Region Detect each unlabeled speaker Segments audio into clusters of unique speakers Language Identification LID Global Languages in training data and/or enrolled languages in some plug-ins Language and dialect detection Keyword Detection KWS Region Enrolled text keywords Language-specific approach to keyword detection using speech recognition and text Query by Example Keyword Spotting QBE Region Enrolled audio keywords Language independent approach to word spotting using one or more audio examples Gender Identification GID Global male, female Determines whether the audio segment was spoken by a male or female voice Topic Detection TPD Region Enrolled topics Detects topic regions in an audio segment Speech Enhancement ENH N/A N/A Reduces noise in an audio segment Voice Type Discrimination VTD Frame live-speech Detects presence of live-produced human speech, differentiating from silence, noise, speech coming from electronic device Scoring Types Different function types score audio segments on different levels of granularity. Some plugin functionality differences are essentially differences is how an audio segment is treated -- as a single unit or potentially multiple units. For example, the main difference between speaker identification and speaker detection is how a segment is scored, in that speaker identification assumes that the audio segment sent to it for scoring is homogenous and comes from a single speaker, where speaker detection will instead allow for the possibility of the presence of multiple speakers in a given audio segment. There are three major scoring types: Frame - Assigns a score for each 10ms frame of the audio segment submitted for scoring. Region - Assigns and reports time boundaries defining region(s) within the audio segment, and for each region, an accompanying score for each detected class . Global - Assigns a single score for the entire audio segment for each of the plugin's classes . For more information on these scoring types, refer to the Plugin Traits page. Classes Certain plugin types have classes as an attribute. These can be common, cross-mission categories that are often pre-trained - like speech, languages, or dialects - or they can be ad-hoc mission-specific classes like speakers or topics. A plugin\u2019s classes may be completely fixed as in gender identification (male, female) or speech activity detection (speech) or an open set as in language identification (English, Spanish, Mandarin, etc.), topics, or speakers. Some plugins allow the user to add new classes or modify existing classes. Some class sets are inherently closed, like SAD and GID, where the plugin is complete and covers the world of possible classes. Others, like LID/SID/TPD plugin will probably never be complete in covering all classes and thus will always need to be able to treat a segment as though it may not be from among the classes the plugin recognizes (i.e. \u2018out of set\u2019). Enrollments Enrollments are a sub-set of classes that the user can create and/or modify. Both creation of a class and modification of an existing class are class modification requests, where the first class modification request for a given class also has the effect of creating the new class if it does not yet exist. Enrollments may be generated by end users with examples from their own data and can be learned from a single or small number of examples (SID, QBE) to a relatively large number of examples (LID, TID). Speakers are typically enrollments, as are query-based keywords and topics. Languages can also be enrolled and augmented with certain plugins. Since enrollments are dynamic, they may be incrementally updated \u201con the fly\u201d with new examples. For integration details regarding enrollments, refer to the Enrollments section of the API Integration page. To determine if a plugin supports enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page from the Specific Plugins list below. Online Updates Considerable improvements to system accuracy and calibration can be found by updating a plugin post-deployment to better align with the conditions observed in recent history. Several plugins are able to perform unsupervised updates to certain submodules of the plugin. The updates do not require labels or human input and are based on automatically collected information during normal system use. In most cases, a system update must be invoked by the user via the API, and an option to determine if an update is ready to be applied is also provided in the API. For integration details regarding the update functionality, refer to the Update section of the API Integration page. To check if a plugin supports online updates, refer to its detailed information page from the Specific Plugins list below. Adaptation Similarly to online updates, it can be possible to achieve even larger boosts in performance by updating a plugin by exposing it to the mission's audio conditions, or similarly representative audio conditions. Adaptation, however, requires human input, and in some cases, data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate. Naming Conventions Plugin Names Each plugin proper is given a three part name in the following form: function-attribute-version: function - three-letter abbreviation from column two of the Plugin Types Table above attribute - string that identifies the key attribute of the plugin, generally the algorithm name version - tracks the iteration of the plug-in in development in the form of v<digit> For example, sid-embed-v2 is a speaker identification plugin using speaker embed dings algorithm and is the second release or update of the approach. An additonal alphanumeric character may be appended to the version number if a plugin is re-released with bug fixes, but the performance is expected to be the same. For example, sad-dnn-v6a is a modified version of sad-dnn-v6, but the changes were meant to address errors or shortcomings in the plugin, not to change the algorithm or data used. Domain Names Domain names typically have two or three parts: condition-version or language-condition-version for plugins that have language-dependent domain components. Keyword spotting domains also contain the language for which the domain was trained. language - the language for which the domain was trained if language-dependent, or a representation of the set of languages contained within a LID plugin's domain condition - the specific audio environment for which the domain was trained, or \u201cmulti\u201d if the domain was developed to be condition independent version - tracks the iteration of the plug-in in development in the form of v<digit> Specific Plugins For additional information about specific plugins, their options, implementation details and other, please refer to the specific plugin pages, accessible from each of the individual Plugin Task pages.","title":"Overview"},{"location":"plugins.html#plugin-documentation","text":"This page contains high level information about OLIVE plugins and their associated concepts - for more detailed information about integrating with plugins, please refer to the more low-level focused Plugin API Integration Details pages. For more information about a specific plugin, please find its info page link in the list at the bottom of this page .","title":"Plugin Documentation"},{"location":"plugins.html#anatomy-of-a-plugin","text":"OLIVE Plugins encapsulate the actual audio processing technologies and capabilities of the OLIVE system into modular pieces that facilitate system upgrades, capability additions, incremental updates, and provide other benefits such as allowing for tuning models to improve processing in targeted audio conditions or allowing for multiple tools or options to choose from to accomplish a given task. Each plugin has a specific type, which defines the task that it is capable of performing (see Plugin Types below). They consist of two parts, the plugin proper, which contains the recipe or algorithm information on how to perform the task, and one or more domains, which contain the data models used to run the algorithm and perform the function. Since plugins are generally machine learning-based the same algorithm may have multiple strengths based on the specific data used to train it (e.g. telephone audio, push-to-talk audio, high effort vocalization, conversational speech). This separation between the algorithm and the data model allows us to deliver new functionality that is based on training data independently of the algorithm by delivering a new domain. Classes and enrollments (described below) are associated with a domain.","title":"Anatomy of a Plugin"},{"location":"plugins.html#plugin-types","text":"Plugin function types refer to the core task the plugin performs and have standard abbreviations. Most plugins are designed to perform one specific function (for example, language identification, keyword spotting). We refer to plugins as working on audio segments, since OLIVE can process both audio files (on the file system) and audio buffers (sent as data through the API). For more information regarding each plugin type, but not a specific plugin or domain, including plugin type definitions, general output formats, and use cases, please click on the name of the plugin in the 'Function Type' column of the table below to visit that plugin type's information page:","title":"Plugin Types"},{"location":"plugins.html#table-of-plugin-function-types","text":"Function type Abbreviation Scoring type Classes Description Speech Activity Detection SAD Frame / Region speech Identifies speech regions in an audio segment Speaker Identification SID Global Enrolled speakers Identifies whether a single-talker audio segment contains a target speaker Speaker Detection SDD Region Enrolled speakers Detects a target speaker in audio with multiple talkers Speaker Diarization DIA Region Detect each unlabeled speaker Segments audio into clusters of unique speakers Language Identification LID Global Languages in training data and/or enrolled languages in some plug-ins Language and dialect detection Keyword Detection KWS Region Enrolled text keywords Language-specific approach to keyword detection using speech recognition and text Query by Example Keyword Spotting QBE Region Enrolled audio keywords Language independent approach to word spotting using one or more audio examples Gender Identification GID Global male, female Determines whether the audio segment was spoken by a male or female voice Topic Detection TPD Region Enrolled topics Detects topic regions in an audio segment Speech Enhancement ENH N/A N/A Reduces noise in an audio segment Voice Type Discrimination VTD Frame live-speech Detects presence of live-produced human speech, differentiating from silence, noise, speech coming from electronic device","title":"Table of Plugin Function Types"},{"location":"plugins.html#scoring-types","text":"Different function types score audio segments on different levels of granularity. Some plugin functionality differences are essentially differences is how an audio segment is treated -- as a single unit or potentially multiple units. For example, the main difference between speaker identification and speaker detection is how a segment is scored, in that speaker identification assumes that the audio segment sent to it for scoring is homogenous and comes from a single speaker, where speaker detection will instead allow for the possibility of the presence of multiple speakers in a given audio segment. There are three major scoring types: Frame - Assigns a score for each 10ms frame of the audio segment submitted for scoring. Region - Assigns and reports time boundaries defining region(s) within the audio segment, and for each region, an accompanying score for each detected class . Global - Assigns a single score for the entire audio segment for each of the plugin's classes . For more information on these scoring types, refer to the Plugin Traits page.","title":"Scoring Types"},{"location":"plugins.html#classes","text":"Certain plugin types have classes as an attribute. These can be common, cross-mission categories that are often pre-trained - like speech, languages, or dialects - or they can be ad-hoc mission-specific classes like speakers or topics. A plugin\u2019s classes may be completely fixed as in gender identification (male, female) or speech activity detection (speech) or an open set as in language identification (English, Spanish, Mandarin, etc.), topics, or speakers. Some plugins allow the user to add new classes or modify existing classes. Some class sets are inherently closed, like SAD and GID, where the plugin is complete and covers the world of possible classes. Others, like LID/SID/TPD plugin will probably never be complete in covering all classes and thus will always need to be able to treat a segment as though it may not be from among the classes the plugin recognizes (i.e. \u2018out of set\u2019).","title":"Classes"},{"location":"plugins.html#enrollments","text":"Enrollments are a sub-set of classes that the user can create and/or modify. Both creation of a class and modification of an existing class are class modification requests, where the first class modification request for a given class also has the effect of creating the new class if it does not yet exist. Enrollments may be generated by end users with examples from their own data and can be learned from a single or small number of examples (SID, QBE) to a relatively large number of examples (LID, TID). Speakers are typically enrollments, as are query-based keywords and topics. Languages can also be enrolled and augmented with certain plugins. Since enrollments are dynamic, they may be incrementally updated \u201con the fly\u201d with new examples. For integration details regarding enrollments, refer to the Enrollments section of the API Integration page. To determine if a plugin supports enrollments, or to check what its default enrolled classes are (if any), refer to that plugin's details page from the Specific Plugins list below.","title":"Enrollments"},{"location":"plugins.html#online-updates","text":"Considerable improvements to system accuracy and calibration can be found by updating a plugin post-deployment to better align with the conditions observed in recent history. Several plugins are able to perform unsupervised updates to certain submodules of the plugin. The updates do not require labels or human input and are based on automatically collected information during normal system use. In most cases, a system update must be invoked by the user via the API, and an option to determine if an update is ready to be applied is also provided in the API. For integration details regarding the update functionality, refer to the Update section of the API Integration page. To check if a plugin supports online updates, refer to its detailed information page from the Specific Plugins list below.","title":"Online Updates"},{"location":"plugins.html#adaptation","text":"Similarly to online updates, it can be possible to achieve even larger boosts in performance by updating a plugin by exposing it to the mission's audio conditions, or similarly representative audio conditions. Adaptation, however, requires human input, and in some cases, data that is properly annotated with respect to the target plugin. Language labels, for example, are necessary to perform LID adaptation, speech labels for SAD adaptation, speaker labels for SID adaptation, and so on. The benefits of adaptation vary between targeted applications, data conditions, and amount of labeled data that is available to run adaptation. More details about adaptation can be found in the Adaptation sections of the API Documentation or CLI User Guide , or within individual plugin sections, if appropriate.","title":"Adaptation"},{"location":"plugins.html#naming-conventions","text":"","title":"Naming Conventions"},{"location":"plugins.html#plugin-names","text":"Each plugin proper is given a three part name in the following form: function-attribute-version: function - three-letter abbreviation from column two of the Plugin Types Table above attribute - string that identifies the key attribute of the plugin, generally the algorithm name version - tracks the iteration of the plug-in in development in the form of v<digit> For example, sid-embed-v2 is a speaker identification plugin using speaker embed dings algorithm and is the second release or update of the approach. An additonal alphanumeric character may be appended to the version number if a plugin is re-released with bug fixes, but the performance is expected to be the same. For example, sad-dnn-v6a is a modified version of sad-dnn-v6, but the changes were meant to address errors or shortcomings in the plugin, not to change the algorithm or data used.","title":"Plugin Names"},{"location":"plugins.html#domain-names","text":"Domain names typically have two or three parts: condition-version or language-condition-version for plugins that have language-dependent domain components. Keyword spotting domains also contain the language for which the domain was trained. language - the language for which the domain was trained if language-dependent, or a representation of the set of languages contained within a LID plugin's domain condition - the specific audio environment for which the domain was trained, or \u201cmulti\u201d if the domain was developed to be condition independent version - tracks the iteration of the plug-in in development in the form of v<digit>","title":"Domain Names"},{"location":"plugins.html#specific-plugins","text":"For additional information about specific plugins, their options, implementation details and other, please refer to the specific plugin pages, accessible from each of the individual Plugin Task pages.","title":"Specific Plugins"},{"location":"raven.html","text":"Raven Web Graphical User Interface Raven Web GUI Overview The Raven Web GUI is a User Interface designed to primarily facilitate \"batch runs\" consisting of a large number of files through \"workflows\" consisting of multiple technologies. The Raven GUI can be used to run either local audio or serverside audio through one of several preconfigured HLT workflows. Below is a basic diagram of the Raven Web GUI with all the major panels highlighted: Panels: Media Menu Media File List Workflow List Submit Button Using the Raven Web GUI In order to submit audio to a workflow on the WebGUI you must: Select either \"Local Media\" or \"Remote Media\" from the Media Menu a. For \"Local Media\", upload all files you want to run via drag and drop or by clicking the Media File List to open a file browser b. For \"Remote Media\", navigate the files available on the server side and select all files you would like to submit Select a workflow to run from the \"Workflow List\" Click the submit button in the bottom right Below are images that can be used as reference for each step. 1. Select Local/Remote Media 2. Load Media 2a. Load Local Media 2b. Load Remote Media 3. Select Workflow 4. Submit Analyzing Results After running a set of data through a workflow you'll be directed to the results page which displays the HLT results for all plugins within the workflow that was run for each file that was submitted. The results page has a menu bar with several different ways to organize the files and scores on the page.","title":"OLIVE Raven Batch Web GUI"},{"location":"raven.html#raven-web-graphical-user-interface","text":"","title":"Raven Web Graphical User Interface"},{"location":"raven.html#raven-web-gui-overview","text":"The Raven Web GUI is a User Interface designed to primarily facilitate \"batch runs\" consisting of a large number of files through \"workflows\" consisting of multiple technologies. The Raven GUI can be used to run either local audio or serverside audio through one of several preconfigured HLT workflows. Below is a basic diagram of the Raven Web GUI with all the major panels highlighted: Panels: Media Menu Media File List Workflow List Submit Button","title":"Raven Web GUI Overview"},{"location":"raven.html#using-the-raven-web-gui","text":"In order to submit audio to a workflow on the WebGUI you must: Select either \"Local Media\" or \"Remote Media\" from the Media Menu a. For \"Local Media\", upload all files you want to run via drag and drop or by clicking the Media File List to open a file browser b. For \"Remote Media\", navigate the files available on the server side and select all files you would like to submit Select a workflow to run from the \"Workflow List\" Click the submit button in the bottom right Below are images that can be used as reference for each step. 1. Select Local/Remote Media 2. Load Media 2a. Load Local Media 2b. Load Remote Media 3. Select Workflow 4. Submit","title":"Using the Raven Web GUI"},{"location":"raven.html#analyzing-results","text":"After running a set of data through a workflow you'll be directed to the results page which displays the HLT results for all plugins within the workflow that was run for each file that was submitted. The results page has a menu bar with several different ways to organize the files and scores on the page.","title":"Analyzing Results"},{"location":"redaction.html","text":"Speaker Redaction Task This document covers how to use the features added to the OLIVE Nightingale GUI to support the task of \"Speaker Redaction\". This allows for the assisted labeling and discovery of all sections of speech within an audio recording that belong to a single speaker, whose audio is wished to be removed from the audio in order to protect his or her identity. Currently, the designated audio is replaced with a tone, and all content there is destroyed. In the future, different approaches may be available that disguise the identity of the voice while leaving the content of the speech audible. Speaker Redaction Task Overview Generally, the process of stepping through the Redaction task follows these summarized steps: Load the audio to be processed Find and select an example of the speaker whose voice is to be redacted Submit this audio to the system, so it can find and suggest additional regions where this speaker occurs Add additional selections from these candidates to the labeled regions Optionally iterate through steps 3 and 4 until all audio from the speaker to be redacted is appropriately labeled Submit this audio to the redaction system to replace the labeled regions with a tone Review the resulting audio to make sure all of the speaker's audio was removed or disguised Add regions and resubmit to redaction system if necessary This process is covered in more detail below. Speaker Redaction Walkthrough Getting Started To get started with this task, first open the OLIVE Nightingale GUI and load an audio file in by dragging and dropping a file into one of the GUI waveform slots, or by pressing ctrl+o or cmd+o while the GUI has focus to enter an 'Open File' dialog. Once the audio is loaded, select the \u20181. Set up tiers\u2019 button in the \u2018Redaction Task\u2019 panel. This will create two new tiers below the audio file: Suggested This tier is where the results from the speaker search algorithm will be displayed, providing suggested regions that the system believes belong to the speaker whose speech has been provided in the BLEEP regions of the tier below. BLEEP or keep This tier is where you provide 'seed' regions, as BLEEP labels to inform the speaker-search system what the speaker to redact sounds like. You can also use 'keep' labels to inform the system of regions that do not contain the speaker you're attempting to redact, so that those regions won't be suggested again in the future. All regions labeled as BLEEP in this tier are what will be removed by the redaction plugin during the final step of this process. All keep regions will be left alone, as will regions that have no labels. Only BLEEP regions will be affected. This initial setup step will also enable the \u20182. Find speaker\u2019 and \u20183. Create redacted audio\u2019 buttons on the \u2018Redaction task\u2019 panel. An in-app help pop-up with reminder instructions can be accessed by clicking the '2. Find speaker' button when no BLEEP regions have been added. Finding/Labeling Audio to Redact Start by selecting a portion of audio that contains only the speaker you wish to redact by clicking and dragging in the waveform portion of the audio after locating an appropriate speech segment. It is possible to perform this step with as little as 3-5 seconds of audio from the appropriate speaker, but providing more labeled audio at the start will make the system's suggestions for additional regions more accurate. Add this selection as a region to be 'bleeped' by clicking the '+' icon directly below your selection in the \" BLEEP or keep \" tier, and selecting BLEEP . Repeat this until you have selected at least 3-5 seconds of the speaker you wish to redact (more is better). Once you've selected some audio to get started, tell the system to find more speech from this speaker by clicking '2. Find speaker' in the 'Redaction Task' GUI panel. In the \u2018Suggested\u2019 tier you will see \u2018processing\u2019 followed by yellow boxes where the speaker of interest is likely to be speaking, this can be useful in finding additional areas with the speaker. Use the \u2018Suggested\u2019 tier to help review the rest of the file and add all other regions that you want to redact as BLEEP regions. You can perform the 'Find Speaker' step as many times as is desired to continue refining the provided suggestions. It will always use each BLEEP region together as a representation of the speaker you're attempting to find. Note that if the system suggests regions of speech that do not belong to the speaker to be redacted, you can stop future suggestions of this region by selecting the suggestion and labeling it as keep . The keep regions are not essential for this task, and can be used or not depending on user preference. Note that if you make a mistake and mark a region you wish to redact as keep , or a region you wish to be left alone as BLEEP , you can remove this selection by hovering over or clicking the respective label, and selecting the 'x' that appears in the top left corner. You will then be free to re-add this label as a different type, or adjust the boundaries and then do so. Redacting Selected Audio Once you have found and labeled all regions containing speech belonging to this speaker, you can proceed on to the final step in the process, of actually redacting the selected audio. This is done by selecting the '3. Create redacted audio' button from the 'Redactino Task' GUI panel. Once this is done, the GUI will prompt you with a file-save dialog to select where to save out the new audio file. Finally, a new slot is added below the BLEEP or keep tier that will display the redacted audio once it is created, and allow review of the redacted audio. Select which channel will be played back using the speaker icons to the left of each waveform slot, and review the final redacted audio. If you find while reviewing this audio that any audio from the speaker was missed and is still present in the final file, it is still possible to step through each part of this process again as necessary. A new BLEEP region (or regions) can be added at this point to catch the missed speech, and the audio can be resubmitted through '3. Create redacted audio'. At this point, the user can choose to save a new file, or overwrite the original redacted file. Cautions and Limitations Note that there is currently no mechanism for saving progress or state of the Redaction Task or BLEEP or keep tier. The final created audio file is saved in the location provided by the user, but it is not currently possible to save the GUI state while in the middle of labeling BLEEP regions and resume or complete the redaction task at a later time.","title":"Speaker Redaction"},{"location":"redaction.html#speaker-redaction-task","text":"This document covers how to use the features added to the OLIVE Nightingale GUI to support the task of \"Speaker Redaction\". This allows for the assisted labeling and discovery of all sections of speech within an audio recording that belong to a single speaker, whose audio is wished to be removed from the audio in order to protect his or her identity. Currently, the designated audio is replaced with a tone, and all content there is destroyed. In the future, different approaches may be available that disguise the identity of the voice while leaving the content of the speech audible.","title":"Speaker Redaction Task"},{"location":"redaction.html#speaker-redaction-task-overview","text":"Generally, the process of stepping through the Redaction task follows these summarized steps: Load the audio to be processed Find and select an example of the speaker whose voice is to be redacted Submit this audio to the system, so it can find and suggest additional regions where this speaker occurs Add additional selections from these candidates to the labeled regions Optionally iterate through steps 3 and 4 until all audio from the speaker to be redacted is appropriately labeled Submit this audio to the redaction system to replace the labeled regions with a tone Review the resulting audio to make sure all of the speaker's audio was removed or disguised Add regions and resubmit to redaction system if necessary This process is covered in more detail below.","title":"Speaker Redaction Task Overview"},{"location":"redaction.html#speaker-redaction-walkthrough","text":"","title":"Speaker Redaction Walkthrough"},{"location":"redaction.html#getting-started","text":"To get started with this task, first open the OLIVE Nightingale GUI and load an audio file in by dragging and dropping a file into one of the GUI waveform slots, or by pressing ctrl+o or cmd+o while the GUI has focus to enter an 'Open File' dialog. Once the audio is loaded, select the \u20181. Set up tiers\u2019 button in the \u2018Redaction Task\u2019 panel. This will create two new tiers below the audio file: Suggested This tier is where the results from the speaker search algorithm will be displayed, providing suggested regions that the system believes belong to the speaker whose speech has been provided in the BLEEP regions of the tier below. BLEEP or keep This tier is where you provide 'seed' regions, as BLEEP labels to inform the speaker-search system what the speaker to redact sounds like. You can also use 'keep' labels to inform the system of regions that do not contain the speaker you're attempting to redact, so that those regions won't be suggested again in the future. All regions labeled as BLEEP in this tier are what will be removed by the redaction plugin during the final step of this process. All keep regions will be left alone, as will regions that have no labels. Only BLEEP regions will be affected. This initial setup step will also enable the \u20182. Find speaker\u2019 and \u20183. Create redacted audio\u2019 buttons on the \u2018Redaction task\u2019 panel. An in-app help pop-up with reminder instructions can be accessed by clicking the '2. Find speaker' button when no BLEEP regions have been added.","title":"Getting Started"},{"location":"redaction.html#findinglabeling-audio-to-redact","text":"Start by selecting a portion of audio that contains only the speaker you wish to redact by clicking and dragging in the waveform portion of the audio after locating an appropriate speech segment. It is possible to perform this step with as little as 3-5 seconds of audio from the appropriate speaker, but providing more labeled audio at the start will make the system's suggestions for additional regions more accurate. Add this selection as a region to be 'bleeped' by clicking the '+' icon directly below your selection in the \" BLEEP or keep \" tier, and selecting BLEEP . Repeat this until you have selected at least 3-5 seconds of the speaker you wish to redact (more is better). Once you've selected some audio to get started, tell the system to find more speech from this speaker by clicking '2. Find speaker' in the 'Redaction Task' GUI panel. In the \u2018Suggested\u2019 tier you will see \u2018processing\u2019 followed by yellow boxes where the speaker of interest is likely to be speaking, this can be useful in finding additional areas with the speaker. Use the \u2018Suggested\u2019 tier to help review the rest of the file and add all other regions that you want to redact as BLEEP regions. You can perform the 'Find Speaker' step as many times as is desired to continue refining the provided suggestions. It will always use each BLEEP region together as a representation of the speaker you're attempting to find. Note that if the system suggests regions of speech that do not belong to the speaker to be redacted, you can stop future suggestions of this region by selecting the suggestion and labeling it as keep . The keep regions are not essential for this task, and can be used or not depending on user preference. Note that if you make a mistake and mark a region you wish to redact as keep , or a region you wish to be left alone as BLEEP , you can remove this selection by hovering over or clicking the respective label, and selecting the 'x' that appears in the top left corner. You will then be free to re-add this label as a different type, or adjust the boundaries and then do so.","title":"Finding/Labeling Audio to Redact"},{"location":"redaction.html#redacting-selected-audio","text":"Once you have found and labeled all regions containing speech belonging to this speaker, you can proceed on to the final step in the process, of actually redacting the selected audio. This is done by selecting the '3. Create redacted audio' button from the 'Redactino Task' GUI panel. Once this is done, the GUI will prompt you with a file-save dialog to select where to save out the new audio file. Finally, a new slot is added below the BLEEP or keep tier that will display the redacted audio once it is created, and allow review of the redacted audio. Select which channel will be played back using the speaker icons to the left of each waveform slot, and review the final redacted audio. If you find while reviewing this audio that any audio from the speaker was missed and is still present in the final file, it is still possible to step through each part of this process again as necessary. A new BLEEP region (or regions) can be added at this point to catch the missed speech, and the audio can be resubmitted through '3. Create redacted audio'. At this point, the user can choose to save a new file, or overwrite the original redacted file.","title":"Redacting Selected Audio"},{"location":"redaction.html#cautions-and-limitations","text":"Note that there is currently no mechanism for saving progress or state of the Redaction Task or BLEEP or keep tier. The final created audio file is saved in the location provided by the user, but it is not currently possible to save the GUI state while in the middle of labeling BLEEP regions and resume or complete the redaction task at a later time.","title":"Cautions and Limitations"},{"location":"releasePlugins.html","text":"OLIVE 5.5.1 Release Plugins The following plugins have been tested and certified for compatibility and release with the OLIVE 5.5.1 software package. Speech Speech Activity Detection (SAD) sad-dnnSmolive-v1.0.0 (low resource) sad-dnn-v8.0.0 (GPU capable) Voice Type Discrimination (VTD) vtd-dnn-v7.0.2 Deep Fake Audio Detection (DFA) dfa-spoofnet-v1.0.0 dfa-speakerSpecific-phonetic-v1.0.0 Speaker Speaker Identification (SID)] sid-dplda-v3.0.0 (GPU capable) sid-embedSmolive-v1.0.0 (lower resource) Speaker Detection (SDD) sdd-diarizeEmbedSmolive-v1.0.2 (lower resource) Speaker Diarization (DIA) dia-hybrid-v2.0.2 Speaker Highlighting (SHL) shl-sbcEmbed-v1.0.2 Language Language Identification (LID) lid-embedplda-v4.0.0 (GPU capable) lid-hdplda-v1.0.1 lid-embedpldaSmolive-v1.0.0 (lower resource) Language Detection (LDD) ldd-embedpldaSmolive-v1.0.0 (lower resource) ldd-embedplda-v1.0.1 Gender Gender Identification (GID) gid-gb-v2.0.1 Gender Detection (GDD) gdd-embedplda-v1.0.0 Keyword Query By Example (QBE) qbe-ftdnnSmolive-v1.0.0 (lower resource) Transcription Automatic Speech Recognition (ASR) asr-end2end-v1.0.0 (GPU capable) asr-dynapy-v4.0.0 Translation Note that this TMT plugin currently has additional server and resource restrictions. Contact SRI for details. Text Machine Translation (TMT) tmt-neural-v1.1.0 (GPU capable) Topic Topic Detection (TPD) tpd-dynapy-v5.0.1 tpd-fusion-v1.0.1 Manipulation Audio Redaction (RED) red-transform-v1.0.0 Imagery Face Detection Image (FDI) fdi-pyEmbed-v1.0.0 Face Detection Video (FDV) fdv-pyEmbed-v1.0.0 Face Recognition Image (FRI) fri-pyEmbed-v1.0.0 Face Recognition Video (FRV) frv-pyEmbed-v1.0.0 For further information on each individual plugin and their capabilities, please refer to the pages linked above. For more information about how to integrate with OLIVE and get started using these plugins, refer to the appropriate documentation linked from the landing page .","title":"Release Plugins"},{"location":"releasePlugins.html#olive-551-release-plugins","text":"The following plugins have been tested and certified for compatibility and release with the OLIVE 5.5.1 software package.","title":"OLIVE 5.5.1 Release Plugins"},{"location":"releasePlugins.html#speech","text":"Speech Activity Detection (SAD) sad-dnnSmolive-v1.0.0 (low resource) sad-dnn-v8.0.0 (GPU capable) Voice Type Discrimination (VTD) vtd-dnn-v7.0.2 Deep Fake Audio Detection (DFA) dfa-spoofnet-v1.0.0 dfa-speakerSpecific-phonetic-v1.0.0","title":"Speech"},{"location":"releasePlugins.html#speaker","text":"Speaker Identification (SID)] sid-dplda-v3.0.0 (GPU capable) sid-embedSmolive-v1.0.0 (lower resource) Speaker Detection (SDD) sdd-diarizeEmbedSmolive-v1.0.2 (lower resource) Speaker Diarization (DIA) dia-hybrid-v2.0.2 Speaker Highlighting (SHL) shl-sbcEmbed-v1.0.2","title":"Speaker"},{"location":"releasePlugins.html#language","text":"Language Identification (LID) lid-embedplda-v4.0.0 (GPU capable) lid-hdplda-v1.0.1 lid-embedpldaSmolive-v1.0.0 (lower resource) Language Detection (LDD) ldd-embedpldaSmolive-v1.0.0 (lower resource) ldd-embedplda-v1.0.1","title":"Language"},{"location":"releasePlugins.html#gender","text":"Gender Identification (GID) gid-gb-v2.0.1 Gender Detection (GDD) gdd-embedplda-v1.0.0","title":"Gender"},{"location":"releasePlugins.html#keyword","text":"Query By Example (QBE) qbe-ftdnnSmolive-v1.0.0 (lower resource)","title":"Keyword"},{"location":"releasePlugins.html#transcription","text":"Automatic Speech Recognition (ASR) asr-end2end-v1.0.0 (GPU capable) asr-dynapy-v4.0.0","title":"Transcription"},{"location":"releasePlugins.html#translation","text":"Note that this TMT plugin currently has additional server and resource restrictions. Contact SRI for details. Text Machine Translation (TMT) tmt-neural-v1.1.0 (GPU capable)","title":"Translation"},{"location":"releasePlugins.html#topic","text":"Topic Detection (TPD) tpd-dynapy-v5.0.1 tpd-fusion-v1.0.1","title":"Topic"},{"location":"releasePlugins.html#manipulation","text":"Audio Redaction (RED) red-transform-v1.0.0","title":"Manipulation"},{"location":"releasePlugins.html#imagery","text":"Face Detection Image (FDI) fdi-pyEmbed-v1.0.0 Face Detection Video (FDV) fdv-pyEmbed-v1.0.0 Face Recognition Image (FRI) fri-pyEmbed-v1.0.0 Face Recognition Video (FRV) frv-pyEmbed-v1.0.0 For further information on each individual plugin and their capabilities, please refer to the pages linked above. For more information about how to integrate with OLIVE and get started using these plugins, refer to the appropriate documentation linked from the landing page .","title":"Imagery"},{"location":"restApi.html","text":"OLIVE 5.5.1 REST API Swagger Documentation Below is the Swagger for the OLIVE REST API; Note that this appears to be interactive, but messages submitted from this page won't properly reach the OLIVE Martini. However, if you currently have an OLIVE Martini container running, you can check out a live/interactive version of this Swagger documentation here: http://localhost:5005/swagger Or if your Martini instance has TLS enabled: https://localhost:5005/swagger And if the OLIVE Martini is running remotely: <hostname or ip>:5005/swagger const ui = SwaggerUIBundle({ url: 'olive-message-broker-api.yaml', dom_id: '#swagger-ui', })","title":"OLIVE REST API"},{"location":"restApi.html#olive-551-rest-api-swagger-documentation","text":"Below is the Swagger for the OLIVE REST API; Note that this appears to be interactive, but messages submitted from this page won't properly reach the OLIVE Martini. However, if you currently have an OLIVE Martini container running, you can check out a live/interactive version of this Swagger documentation here: http://localhost:5005/swagger Or if your Martini instance has TLS enabled: https://localhost:5005/swagger And if the OLIVE Martini is running remotely: <hostname or ip>:5005/swagger const ui = SwaggerUIBundle({ url: 'olive-message-broker-api.yaml', dom_id: '#swagger-ui', })","title":"OLIVE 5.5.1 REST API Swagger Documentation"},{"location":"server.html","text":"OLIVE Server Overview In the OLIVE architecture, if the individual plugins are the muscles of the system, the Server serves as the brains. It provides coordination and tasking, and is responsible for properly receiving and interpreting messages from client applications, kicking off the appropriate plugin jobs that these messages may request, as well as routing the proper response or results from these jobs back to the requesting client. The OLIVE Enterprise API is client/server based. Therefore, you must run the OLIVE server and manage its lifecycle as part of your integration effort. The server is included as part of the primary system installation. The OLIVE server communicates with clients over two ports, 5588 and 5589 by default (configurable), using ZeroMQ and Google Protocol Buffers. It relies on other components to perform its duties; namely an assembly of third party and other libraries that are delivered with OLIVE as the OLIVE Runtime , two SRI-built libraries; Idento and dnn, that empower the final puzzle piece, the plugins themselves, to complete their assigned tasks. OLIVE is usually delivered with a simple startup script for ease of use and to facilitate a rapid deployment, so the actual details of the relationship between these pieces does not need to be considered too closely if sticking, but the next section outlines how to properly establish the environmental setup that the Server needs to properly function, and how to manually start and configure the server. Installation, Environment Setup and Startup This section outlines the steps needed to prepare the OLIVE environment for proper operation, describes the contents of the OLIVE runtime library, and covers starting the actual server. The final section of this page also outlines some common pitfalls encountered with running the OLIVE server, and offers troubleshooting advice. Installation If you haven't already installed the OLIVE software package, please jump over and refer to our OLIVE Installation Guide for information about the layout of the software and a quick-start setup guide before continuing on. Environment Variables As discussed in the Installation Guide , we recommend using the provided olive_env.sh scripts to do the heavy lifting and set up the OLIVE environment, but it may be helpful to know what some of the important environment variables being set are referring to. Below is a selection of some of the major variables, with a brief description of their function. OLIVE The OLIVE environment variable points to the actuall OLIVE installation location - If the software is installed as shown in the install page , this is typically something like $HOME/olive5.0.0/. From this, the OLIVE software is able to properly add the relevant bin/ and lib directories to the $PATH and $LD_LIBRARY_PATH environment variables, respectively, and find/set other important variables. OLIVE_RUNTIME OLIVE_RUNTIME points to the location of the OLIVE Runtime library outlined below. It's very important that this is set, as OLIVE will not function properly without access to the libraries and dependencies in the runtime. In the two examples in the install page , the OLIVE_RUNTIME variable would be set to: $HOME/olive5.0.0/runtime-5.0.0-centos-7.3.1611-x86_64/ OLIVE_APP_DATA This variable is very important if running the OLIVE Server, but not necessary when using the OLIVE command line tools. OLIVE_APP_DATA tells the Server where to find the OLIVE plugins, and also determines where it will store things like class enrollments (i.e. speakers for SID plugins, keyword examples for QBE plugins), as well as server logs and other files that must be written out by the system. OLIVE_APP_DATA must contain a directory called plugins that contains valid OLIVE plugins. In the two examples in the install page , the OLIVE_APP_DATA variable would be set to: $HOME/oliveAppData/ Starting the Server Once the OLIVE environment has been properly established, launching the server is as simple as entering this into the appropriate terminal: oliveserver Additional runtime/configuration options are available for advanced operation, and their details can be found in the oliveserver usage statement: usage : oliveserver [- h ] [-- version ] [-- verbose ] [-- interface INTERFACE ] [-- port REQUEST_PORT ] [-- workers WORKERS ] [-- work_dir WORK_DIR ] [-- enroll_dir ENROLL_DIR ] [-- debug ] [-- quiet ] [-- timeout TIMEOUT ] [-- options OPTIONS_PATH ] optional arguments : - h , -- help show this help message and exit -- version show program 's version number and exit --verbose increase server logging output --interface INTERFACE server binds to this address; default * (all) --port REQUEST_PORT ther first of three sequentially numbered ports used by the server. The request port is the first port in this range, followd by the status port, and then an interal port only used by the server; default value is 5588. --workers WORKERS, -j WORKERS specify number of parallel WORKERS to run; default is the number of local processors --work_dir WORK_DIR path to work dir to create. default value of environment variable $OLIVE_APP_DATA --enroll_dir ENROLL_DIR path for storage of enrollment data. default value of environment variable $OLIVE_APP_DATA --debug debug mode prevents deletion of logs and intermediate files on success --quiet, -q suppress sending log information to the console --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have more or more configuration sections . Currently only the server 'server' section is supported An example for setting up the OLIVE runtime environment using the first directory setup shown on the install page , and launching the OLIVE server: $ cd $ HOME / olive5 . 0.0 / runtime - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ cd $ HOME / olive5 . 0.0 / olive - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / $ oliveserver -- port 6678 -- work_dir / home / sysadmin / OLIVE In this example, the ports that the server will listen on and broadcast its status heartbeat messages on have been update, in addition to setting a custom WORK directory. For more information on the available server options, please refer to the OLIVE Server Options section below. Note that upon launching the server, if the operation was successful, the user will be greeted with a display of the plugins and domains that the server has access to, as well as a \u201cServer ready\u201d message. If a list of plugins does not appear, but the \u201cServer ready\u201d message does, the OLIVE_APP_DATA environment variable should be checked to ensure that it contains a valid plugins directory. An example output of a successful server startup: [ oliveuser@localhost olive5.0.0 ] $ oliveserver TASK PLUGIN DOMAINS ------ ------------ --------------------------------- LID lid - embed - v2 [ 'multi-v1' ] SID sid - embed - v2 [ 'multi-v1' ] SAD sad - dnn - v4a [ 'digPtt-v1', 'ptt-v1', 'tel-v1' ] --------- Server ready Fri Jan 11 12:43:26 2019 --------- The olive_env.sh file is included with OLIVE deliveries and does the bulk of the environment setup required for running the server. Important variables that are handled with this script and that are required for running the OLIVE Server include: OLIVE_RUNTIME Must be set to the path of the OLIVE runtime directory. Allows OLIVE to find the libraries it requires. Ex: /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/ OLIVE Contains the path to the OLIVE installation directory. Allows OLIVE to find the binaries and utilities it relies on to function. Ex: /home/user1/olive5.0.0-installation/olive-5.0.0-centos-7.3.1611-x86_64/ or /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/olive-5.0.0-centos-7.3.1611-x86_64/ OLIVE_APP_DATA This is the path used by the server to store class enrollments and log files. This is also where the plugins that the server will have access to must be stored, in a directory simply named plugins. If OLIVE_APP_DATA is set to a directory that does not contain a plugins folder, the server will launch with no plugins. Ex: /home/user1/oliveAppData/ Where the contents of oliveAppData might be: plugins/ sad-dnn-v4a sid-embed-v2 lid-embed-v2 server/ (generated by the server) enrollments/ processing/ If your server is protected by a firewall you must ensure that any ports you attempt to utilize for the server are able to send and receive network traffic. The path you provide to --work_dir will ideally point to a large and performant hard drive. The location will house the enrollments, log files and temporary workspaces utilized by the server. Server Options interface Use this option to specify a single interface for the server to use for communication, otherwise the ports used by the server (5588, 5589, 5590 by default ) are available to all interfaces on the host. For example, --interface 192.168.10.20 would restrict the server to using only that interface, and only available to clients on that interface port Use this option to change the set of ports used by the server for external and internal communication. For example --port 6000 instructs the server to use ports 6000 (requests), 6001 (heartbeats), 60002 (internal communication) workers Use this option to specify the number of current jobs the server can process, this value should be set between 1 and the number of processors on the host work_dir Use this option to specify an alternate location of the directory where the server stores log files, enrollments, and temporary data. Setting this does NOT change the location of the plugins, which should still be located at $OLIVE_APP_DATA/plugins enroll_dir Use this option to specify an alternate location of the enrollments directory, when specified the work directory and plugins location is not changed. debug Use this option to preserver all logs files and provide extra verbose Use this option with the --debug option to produce to maximize debug output quite Use this option to start the server without non-error output messages timeout Use this option adjust the timeout (in seconds) used for jobs, unless instructed by OLIVE technical support this value should not specified options Used to pass special options to the server, this option should only be used with guidance from OLIVE technical support Remote Access The OLIVE server uses three sequential ports for communication; by default these are ports are 5588, 5589, and 5590. An alternate set of ports can be specified using the --port argument, which instructs the server to use 3 ports starting at the given value, so if --port 6000 is specified then ports 6000 (request port), 6001 (status port), and 6002 (internal port) are used. Clients send and receive responses on the request port (5588 by default), while the server publishes health and status information on the status port (5589 by default). The internal IPC port (5590 by default) is only used for internal server communication. You can restrict these ports to a specific interface by using the --interface option, so that the server is only available on that network (by default the server should be available on all interfaces, which may not be desirable on a multi-homed server). For example, specifying an interface of --interface 192.168.10.99 would restrict the server to communication only on that port. You could also use the interface option to restrict the server to local processing, not networked, by setting interface as: --interface 127.0.0.1 (on some OSs you may be able to specify --interface localhost instead of the loopback address, 127.0.0.1). If using the localhost interface, the server is not accessible to remote clients. Important Information Audio Formats The OLIVE server can handle a wide range of input audio file formats, but does have some limitations. For more information on the details of these limitations, please refer to the OLIVE Audio Formats information page. Common Pitfalls One common mistake that's encountered is when informing the OLIVE Server where to find plugins. There is a directory within the OLIVE distribution, olive/plugins that is often mistaken for the directory where OLIVE plugins should oro do reside. This directory has a different purpose, however, and should not be used as such. Guidelines are provided above for general recommendations for where to place your OLIVE plugins. Runtime Description The OLIVE Runtime is a collection of third party server dependencies, which are typically Python packages and C libraries, that are distributed with OLIVE and necessary for proper/full functionality. A full list of the included packages/libraries included in the OLIVE 5.5.1 release can be found below, with links for more information, and license information. Python Components Version How Obtained License Link Python 3.8.5 (Anaconda) (Miniconda Distribution) 3.85 Open Source BSD-3 https://docs.anaconda.com/anaconda/eula cython 0.29.23 Open Source Apache-2.0 https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 Google Protocal Buffers (protobuf) 3.14.0 Open Source BSD https://raw.githubusercontent.com/google/protobuf/master/LICENSE numpy 1.20.2 Open Source BSD-3 https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 zmq 20.0.0 Open Source LGPL http://zeromq.org/area:licensing tabulate 0.8.9 Open Source MIT https://bitbucket.org/astanin/python-tabulate klepto 0.2.0 Open Source BSD http://www.cacr.caltech.edu/~mmckerns/klepto.htm fastcluster 1.1.26 Open Source BSD http://danifold.net nose-cprof 1.3.7 Open Source BSD https://github.com/msherry/nose-cprof PyTorch 1.8.1 Open Source BSD https://github.com/pytorch/pytorch/blob/master/LICENSE pycryptodome 3.10.1 Open Source BSD https://pycryptodome.readthedocs.io/en/latest/src/license.html rsa 4.7.2 Open Source Apache 2 https://stuvel.eu/software/rsa/ pysoundfile 0.10.3 Open Source BSD https://pysoundfile.readthedocs.io/en/latest/ ninja 1.10.2 Open Source BSD https://pypi.org/project/ninja/ pyyaml 5.4.1 Open Source MIT https://pyyaml.org/wiki/PyYAMLDocumentation cffi 1.14.5 Open Source MIT https://pypi.org/project/cffi/ future 0.18.2 Open Source MIT https://pypi.org/project/future/ six 1.15.0 Open Source MIT https://pypi.org/project/six/ requests 2.25.1 Open Source Apache 2 https://pypi.org/project/requests/ typing_extensions 3.7.4.3 Open Source PSF (Python Software Foundation) https://pypi.org/project/typing-extensions/ mkl-include 2021.2.0 Open Source Intel Simplified Software License https://pypi.org/project/mkl/ av 8.0.3 Open Source BSD https://pypi.org/project/av/ opencv 4.5.1 Open Source MIT https://pypi.org/project/opencv-python/ distro 1.5.0 Open Source Apache 2 https://pypi.org/project/distro/ mpi4py 3.0.3 Open Source BSD https://pypi.org/project/mpi4py/ pytest 6.2.3 Open Source MIT https://docs.pytest.org/en/6.2.x/#license scikit-learn 0.24.2 Open Source BSD https://scikit-learn.org/stable/ numexpr 2.7.3 Open Source MIT https://pypi.org/project/numexpr/ h5py 2.10.0 Open Source BSD https://pypi.org/project/h5py/ ipython 7.22.0 Open Source BSD https://ipython.org/ lxml 4.6.3 Open Source BSD https://pypi.org/project/lxml/ psutil 5.8.0 Open Source BSD https://pypi.org/project/lxml/ facenet-pytorch 1.0.1 Open Source MIT https://github.com/timesler/facenet-pytorch torchvggish 0.0.2 Open Source Apache 2 https://github.com/harritaylor/torchvggish transformers 4.6.1 Open Source Apache 2 https://github.com/huggingface/transformers pyworld 0.2.12 Open Source MIT https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder Non-Python Components Version How Obtained License Link libspeex 1.2 Open Source BSD https://www.speex.org/ libogg 1.3.2 Open Source BSD https://xiph.org/ogg/ libvorbis 1.3.5 Open Source BSD http://www.vorbis.com/ libflac 1.3.2 Open Source BSD https://xiph.org/flac/index.html libsndfile 1.0.28 Open Source LGPL http://www.mega-nerd.com/libsndfile/ liblinear 1.8 Open Source BSD https://www.csie.ntu.edu.tw/~cjlin/liblinear/ libsvm Open Source BSD http://www.csie.ntu.edu.tw/~cjlin/libsvm/ sox 14.3.2 Open Source LGPL http://sox.sourceforge.net/ dpwelib 5/16/13 Open Source Public Domain http://www1.icsi.berkeley.edu/~dpwe/dpwelib.html get_f0_snd n/a Open Source BSD https://github.com/MattShannon/HTS-demo_CMU-ARCTIC-SLT-STRAIGHT-AR-decision-tree/blob/master/License upfirdn 0.2.1 Open Source BSD https://github.com/telegraphic/upfirdn openfst 1.6.7 Open Source ASL http://www.openfst.org/twiki/bin/view/FST/WebHome OpenBlas 0.3.15 Open Source BSD https://raw.githubusercontent.com/xianyi/OpenBLAS/develop/LICENSE kaldi 5.2.125 Open Source 5.2.125 ASL 2.0 Marian NMT 3.16.0 Open Source MIT https://marian-nmt.github.io/","title":"Server Guide"},{"location":"server.html#olive-server","text":"","title":"OLIVE Server"},{"location":"server.html#overview","text":"In the OLIVE architecture, if the individual plugins are the muscles of the system, the Server serves as the brains. It provides coordination and tasking, and is responsible for properly receiving and interpreting messages from client applications, kicking off the appropriate plugin jobs that these messages may request, as well as routing the proper response or results from these jobs back to the requesting client. The OLIVE Enterprise API is client/server based. Therefore, you must run the OLIVE server and manage its lifecycle as part of your integration effort. The server is included as part of the primary system installation. The OLIVE server communicates with clients over two ports, 5588 and 5589 by default (configurable), using ZeroMQ and Google Protocol Buffers. It relies on other components to perform its duties; namely an assembly of third party and other libraries that are delivered with OLIVE as the OLIVE Runtime , two SRI-built libraries; Idento and dnn, that empower the final puzzle piece, the plugins themselves, to complete their assigned tasks. OLIVE is usually delivered with a simple startup script for ease of use and to facilitate a rapid deployment, so the actual details of the relationship between these pieces does not need to be considered too closely if sticking, but the next section outlines how to properly establish the environmental setup that the Server needs to properly function, and how to manually start and configure the server.","title":"Overview"},{"location":"server.html#installation-environment-setup-and-startup","text":"This section outlines the steps needed to prepare the OLIVE environment for proper operation, describes the contents of the OLIVE runtime library, and covers starting the actual server. The final section of this page also outlines some common pitfalls encountered with running the OLIVE server, and offers troubleshooting advice.","title":"Installation, Environment Setup and Startup"},{"location":"server.html#installation","text":"If you haven't already installed the OLIVE software package, please jump over and refer to our OLIVE Installation Guide for information about the layout of the software and a quick-start setup guide before continuing on.","title":"Installation"},{"location":"server.html#environment-variables","text":"As discussed in the Installation Guide , we recommend using the provided olive_env.sh scripts to do the heavy lifting and set up the OLIVE environment, but it may be helpful to know what some of the important environment variables being set are referring to. Below is a selection of some of the major variables, with a brief description of their function.","title":"Environment Variables"},{"location":"server.html#olive","text":"The OLIVE environment variable points to the actuall OLIVE installation location - If the software is installed as shown in the install page , this is typically something like $HOME/olive5.0.0/. From this, the OLIVE software is able to properly add the relevant bin/ and lib directories to the $PATH and $LD_LIBRARY_PATH environment variables, respectively, and find/set other important variables.","title":"OLIVE"},{"location":"server.html#olive_runtime","text":"OLIVE_RUNTIME points to the location of the OLIVE Runtime library outlined below. It's very important that this is set, as OLIVE will not function properly without access to the libraries and dependencies in the runtime. In the two examples in the install page , the OLIVE_RUNTIME variable would be set to: $HOME/olive5.0.0/runtime-5.0.0-centos-7.3.1611-x86_64/","title":"OLIVE_RUNTIME"},{"location":"server.html#olive_app_data","text":"This variable is very important if running the OLIVE Server, but not necessary when using the OLIVE command line tools. OLIVE_APP_DATA tells the Server where to find the OLIVE plugins, and also determines where it will store things like class enrollments (i.e. speakers for SID plugins, keyword examples for QBE plugins), as well as server logs and other files that must be written out by the system. OLIVE_APP_DATA must contain a directory called plugins that contains valid OLIVE plugins. In the two examples in the install page , the OLIVE_APP_DATA variable would be set to: $HOME/oliveAppData/","title":"OLIVE_APP_DATA"},{"location":"server.html#starting-the-server","text":"Once the OLIVE environment has been properly established, launching the server is as simple as entering this into the appropriate terminal: oliveserver Additional runtime/configuration options are available for advanced operation, and their details can be found in the oliveserver usage statement: usage : oliveserver [- h ] [-- version ] [-- verbose ] [-- interface INTERFACE ] [-- port REQUEST_PORT ] [-- workers WORKERS ] [-- work_dir WORK_DIR ] [-- enroll_dir ENROLL_DIR ] [-- debug ] [-- quiet ] [-- timeout TIMEOUT ] [-- options OPTIONS_PATH ] optional arguments : - h , -- help show this help message and exit -- version show program 's version number and exit --verbose increase server logging output --interface INTERFACE server binds to this address; default * (all) --port REQUEST_PORT ther first of three sequentially numbered ports used by the server. The request port is the first port in this range, followd by the status port, and then an interal port only used by the server; default value is 5588. --workers WORKERS, -j WORKERS specify number of parallel WORKERS to run; default is the number of local processors --work_dir WORK_DIR path to work dir to create. default value of environment variable $OLIVE_APP_DATA --enroll_dir ENROLL_DIR path for storage of enrollment data. default value of environment variable $OLIVE_APP_DATA --debug debug mode prevents deletion of logs and intermediate files on success --quiet, -q suppress sending log information to the console --timeout TIMEOUT timeout, in seconds, for all jobs regardless of the audio duration. otherwise the job will timeout based on the duration of audio to process and the domain' s timeout_weight -- options OPTIONS_PATH Optional file containing name / value pairs . The option file must have more or more configuration sections . Currently only the server 'server' section is supported An example for setting up the OLIVE runtime environment using the first directory setup shown on the install page , and launching the OLIVE server: $ cd $ HOME / olive5 . 0.0 / runtime - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ cd $ HOME / olive5 . 0.0 / olive - 5.0 . 0 - centos - 7.3 . 1611 - x86_64 $ source olive_env . sh $ export OLIVE_APP_DATA =$ HOME / oliveAppData / $ oliveserver -- port 6678 -- work_dir / home / sysadmin / OLIVE In this example, the ports that the server will listen on and broadcast its status heartbeat messages on have been update, in addition to setting a custom WORK directory. For more information on the available server options, please refer to the OLIVE Server Options section below. Note that upon launching the server, if the operation was successful, the user will be greeted with a display of the plugins and domains that the server has access to, as well as a \u201cServer ready\u201d message. If a list of plugins does not appear, but the \u201cServer ready\u201d message does, the OLIVE_APP_DATA environment variable should be checked to ensure that it contains a valid plugins directory. An example output of a successful server startup: [ oliveuser@localhost olive5.0.0 ] $ oliveserver TASK PLUGIN DOMAINS ------ ------------ --------------------------------- LID lid - embed - v2 [ 'multi-v1' ] SID sid - embed - v2 [ 'multi-v1' ] SAD sad - dnn - v4a [ 'digPtt-v1', 'ptt-v1', 'tel-v1' ] --------- Server ready Fri Jan 11 12:43:26 2019 --------- The olive_env.sh file is included with OLIVE deliveries and does the bulk of the environment setup required for running the server. Important variables that are handled with this script and that are required for running the OLIVE Server include: OLIVE_RUNTIME Must be set to the path of the OLIVE runtime directory. Allows OLIVE to find the libraries it requires. Ex: /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/ OLIVE Contains the path to the OLIVE installation directory. Allows OLIVE to find the binaries and utilities it relies on to function. Ex: /home/user1/olive5.0.0-installation/olive-5.0.0-centos-7.3.1611-x86_64/ or /home/user1/olive5.0.0-installation/runtime-5.0.0-centos-7.3.1611-x86_64/olive-5.0.0-centos-7.3.1611-x86_64/ OLIVE_APP_DATA This is the path used by the server to store class enrollments and log files. This is also where the plugins that the server will have access to must be stored, in a directory simply named plugins. If OLIVE_APP_DATA is set to a directory that does not contain a plugins folder, the server will launch with no plugins. Ex: /home/user1/oliveAppData/ Where the contents of oliveAppData might be: plugins/ sad-dnn-v4a sid-embed-v2 lid-embed-v2 server/ (generated by the server) enrollments/ processing/ If your server is protected by a firewall you must ensure that any ports you attempt to utilize for the server are able to send and receive network traffic. The path you provide to --work_dir will ideally point to a large and performant hard drive. The location will house the enrollments, log files and temporary workspaces utilized by the server.","title":"Starting the Server"},{"location":"server.html#server-options","text":"","title":"Server Options"},{"location":"server.html#interface","text":"Use this option to specify a single interface for the server to use for communication, otherwise the ports used by the server (5588, 5589, 5590 by default ) are available to all interfaces on the host. For example, --interface 192.168.10.20 would restrict the server to using only that interface, and only available to clients on that interface","title":"interface"},{"location":"server.html#port","text":"Use this option to change the set of ports used by the server for external and internal communication. For example --port 6000 instructs the server to use ports 6000 (requests), 6001 (heartbeats), 60002 (internal communication)","title":"port"},{"location":"server.html#workers","text":"Use this option to specify the number of current jobs the server can process, this value should be set between 1 and the number of processors on the host","title":"workers"},{"location":"server.html#work_dir","text":"Use this option to specify an alternate location of the directory where the server stores log files, enrollments, and temporary data. Setting this does NOT change the location of the plugins, which should still be located at $OLIVE_APP_DATA/plugins","title":"work_dir"},{"location":"server.html#enroll_dir","text":"Use this option to specify an alternate location of the enrollments directory, when specified the work directory and plugins location is not changed.","title":"enroll_dir"},{"location":"server.html#debug","text":"Use this option to preserver all logs files and provide extra","title":"debug"},{"location":"server.html#verbose","text":"Use this option with the --debug option to produce to maximize debug output","title":"verbose"},{"location":"server.html#quite","text":"Use this option to start the server without non-error output messages","title":"quite"},{"location":"server.html#timeout","text":"Use this option adjust the timeout (in seconds) used for jobs, unless instructed by OLIVE technical support this value should not specified","title":"timeout"},{"location":"server.html#options","text":"Used to pass special options to the server, this option should only be used with guidance from OLIVE technical support","title":"options"},{"location":"server.html#remote-access","text":"The OLIVE server uses three sequential ports for communication; by default these are ports are 5588, 5589, and 5590. An alternate set of ports can be specified using the --port argument, which instructs the server to use 3 ports starting at the given value, so if --port 6000 is specified then ports 6000 (request port), 6001 (status port), and 6002 (internal port) are used. Clients send and receive responses on the request port (5588 by default), while the server publishes health and status information on the status port (5589 by default). The internal IPC port (5590 by default) is only used for internal server communication. You can restrict these ports to a specific interface by using the --interface option, so that the server is only available on that network (by default the server should be available on all interfaces, which may not be desirable on a multi-homed server). For example, specifying an interface of --interface 192.168.10.99 would restrict the server to communication only on that port. You could also use the interface option to restrict the server to local processing, not networked, by setting interface as: --interface 127.0.0.1 (on some OSs you may be able to specify --interface localhost instead of the loopback address, 127.0.0.1). If using the localhost interface, the server is not accessible to remote clients.","title":"Remote Access"},{"location":"server.html#important-information","text":"","title":"Important Information"},{"location":"server.html#audio-formats","text":"The OLIVE server can handle a wide range of input audio file formats, but does have some limitations. For more information on the details of these limitations, please refer to the OLIVE Audio Formats information page.","title":"Audio Formats"},{"location":"server.html#common-pitfalls","text":"One common mistake that's encountered is when informing the OLIVE Server where to find plugins. There is a directory within the OLIVE distribution, olive/plugins that is often mistaken for the directory where OLIVE plugins should oro do reside. This directory has a different purpose, however, and should not be used as such. Guidelines are provided above for general recommendations for where to place your OLIVE plugins.","title":"Common Pitfalls"},{"location":"server.html#runtime-description","text":"The OLIVE Runtime is a collection of third party server dependencies, which are typically Python packages and C libraries, that are distributed with OLIVE and necessary for proper/full functionality. A full list of the included packages/libraries included in the OLIVE 5.5.1 release can be found below, with links for more information, and license information.","title":"Runtime Description"},{"location":"server.html#python","text":"Components Version How Obtained License Link Python 3.8.5 (Anaconda) (Miniconda Distribution) 3.85 Open Source BSD-3 https://docs.anaconda.com/anaconda/eula cython 0.29.23 Open Source Apache-2.0 https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 Google Protocal Buffers (protobuf) 3.14.0 Open Source BSD https://raw.githubusercontent.com/google/protobuf/master/LICENSE numpy 1.20.2 Open Source BSD-3 https://docs.anaconda.com/anaconda/packages/py2.7_linux-64 zmq 20.0.0 Open Source LGPL http://zeromq.org/area:licensing tabulate 0.8.9 Open Source MIT https://bitbucket.org/astanin/python-tabulate klepto 0.2.0 Open Source BSD http://www.cacr.caltech.edu/~mmckerns/klepto.htm fastcluster 1.1.26 Open Source BSD http://danifold.net nose-cprof 1.3.7 Open Source BSD https://github.com/msherry/nose-cprof PyTorch 1.8.1 Open Source BSD https://github.com/pytorch/pytorch/blob/master/LICENSE pycryptodome 3.10.1 Open Source BSD https://pycryptodome.readthedocs.io/en/latest/src/license.html rsa 4.7.2 Open Source Apache 2 https://stuvel.eu/software/rsa/ pysoundfile 0.10.3 Open Source BSD https://pysoundfile.readthedocs.io/en/latest/ ninja 1.10.2 Open Source BSD https://pypi.org/project/ninja/ pyyaml 5.4.1 Open Source MIT https://pyyaml.org/wiki/PyYAMLDocumentation cffi 1.14.5 Open Source MIT https://pypi.org/project/cffi/ future 0.18.2 Open Source MIT https://pypi.org/project/future/ six 1.15.0 Open Source MIT https://pypi.org/project/six/ requests 2.25.1 Open Source Apache 2 https://pypi.org/project/requests/ typing_extensions 3.7.4.3 Open Source PSF (Python Software Foundation) https://pypi.org/project/typing-extensions/ mkl-include 2021.2.0 Open Source Intel Simplified Software License https://pypi.org/project/mkl/ av 8.0.3 Open Source BSD https://pypi.org/project/av/ opencv 4.5.1 Open Source MIT https://pypi.org/project/opencv-python/ distro 1.5.0 Open Source Apache 2 https://pypi.org/project/distro/ mpi4py 3.0.3 Open Source BSD https://pypi.org/project/mpi4py/ pytest 6.2.3 Open Source MIT https://docs.pytest.org/en/6.2.x/#license scikit-learn 0.24.2 Open Source BSD https://scikit-learn.org/stable/ numexpr 2.7.3 Open Source MIT https://pypi.org/project/numexpr/ h5py 2.10.0 Open Source BSD https://pypi.org/project/h5py/ ipython 7.22.0 Open Source BSD https://ipython.org/ lxml 4.6.3 Open Source BSD https://pypi.org/project/lxml/ psutil 5.8.0 Open Source BSD https://pypi.org/project/lxml/ facenet-pytorch 1.0.1 Open Source MIT https://github.com/timesler/facenet-pytorch torchvggish 0.0.2 Open Source Apache 2 https://github.com/harritaylor/torchvggish transformers 4.6.1 Open Source Apache 2 https://github.com/huggingface/transformers pyworld 0.2.12 Open Source MIT https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder","title":"Python"},{"location":"server.html#non-python","text":"Components Version How Obtained License Link libspeex 1.2 Open Source BSD https://www.speex.org/ libogg 1.3.2 Open Source BSD https://xiph.org/ogg/ libvorbis 1.3.5 Open Source BSD http://www.vorbis.com/ libflac 1.3.2 Open Source BSD https://xiph.org/flac/index.html libsndfile 1.0.28 Open Source LGPL http://www.mega-nerd.com/libsndfile/ liblinear 1.8 Open Source BSD https://www.csie.ntu.edu.tw/~cjlin/liblinear/ libsvm Open Source BSD http://www.csie.ntu.edu.tw/~cjlin/libsvm/ sox 14.3.2 Open Source LGPL http://sox.sourceforge.net/ dpwelib 5/16/13 Open Source Public Domain http://www1.icsi.berkeley.edu/~dpwe/dpwelib.html get_f0_snd n/a Open Source BSD https://github.com/MattShannon/HTS-demo_CMU-ARCTIC-SLT-STRAIGHT-AR-decision-tree/blob/master/License upfirdn 0.2.1 Open Source BSD https://github.com/telegraphic/upfirdn openfst 1.6.7 Open Source ASL http://www.openfst.org/twiki/bin/view/FST/WebHome OpenBlas 0.3.15 Open Source BSD https://raw.githubusercontent.com/xianyi/OpenBLAS/develop/LICENSE kaldi 5.2.125 Open Source 5.2.125 ASL 2.0 Marian NMT 3.16.0 Open Source MIT https://marian-nmt.github.io/","title":"Non-Python"},{"location":"traits.html","text":"OLIVE Plugin Traits Traits Overview The functionality of each OLIVE plugin is defined by the API Traits that it implements. Each Trait defines a message or set of messages that the plugin must implement to perform an associated task. The available Traits are listed below, along with their associated implemented API messages, in the format: Trait Implemented Message 1 ... Implemented Message N Traits and their Corresponding Messages The Trait and Message List follows. Each Trait name links to the relevant section on this page . Each API message entry links to the relevant portion of the Protocol Buffers Message Definition Reference Page. GlobalScorer - ( Tutorial ) GlobalScorerRequest GlobalScorerStereoRequest RegionScorer - ( Tutorial ) RegionScorerRequest RegionScorerStereoRequest FrameScorer - ( Tutorial ) FrameScorerRequest FrameScorerStereoRequest ClassModifier - ( Tutorial ) ClassModificationRequest ClassRemovalRequest ClassExporter ClassExportRequest ClassImportRequest AudioConverter AudioModificationRequest AudioVectorizer PluginAudioVectorRequest LearningTrait (SupervisedAdapter, SupervisedTrainer, UnsupervisedAdapter) - ( Tutorial ) PreprocessAudioTrainRequest PreprocessAudioAdaptRequest SupervisedTrainingRequest UnsupervisedAdaptationRequest UpdateTrait GetUpdateStatusRequest ApplyUpdateRequest GlobalComparer GlobalComparerRequest TextTransformer TextTransformationRequest AudioAlignmentScorer AudioAlignmentScoreRequest In addition to the messages above, the following messages exist for interacting with the server itself and are independent of individual plugins or plugin Traits: GetStatusRequest PluginDirectoryRequest LoadPluginDomainRequest RemovePluginDomainRequest Full message definition details for all of the messages mentioned here Request/Result Message Pairs In general, each Request message is paired with a Result message, that defines the structure and contents of the server's reply to a given client Request message. The table below defines the pairs of the Result messages for the Request messages mentioned above; and the full message definition details can be found in the OLIVE API Message Protocol Documentation , which each entry below is linked to. Request Message Result (Response) Message GlobalScorerRequest GlobalScorerResult RegionScorerRequest RegionScorerResult FrameScorerRequest FrameScorerResult ClassModificationRequest ClassModificationResult ClassRemovalRequest ClassRemovalResult ClassExportRequest ClassExportResult ClassImportRequest ClassImportResult AudioModificationRequest AudioModificationResult PluginAudioVectorRequest PluginAudioVectorResult PreprocessAudioTrainRequest PreprocessAudioTrainResult PreprocessAudioAdaptRequest PreprocessAudioAdaptResult SupervisedTrainingRequest SupervisedTrainingResult UnsupervisedAdaptationRequest UnsupervisedAdaptationResult GetUpdateStatusRequest GetUpdateStatusResult ApplyUpdateRequest ApplyUpdateResult GlobalComparerRequest GlobalComparerResult TextTransformationRequest TextTransformationResult AudioAlignmentScoreRequest AudioAlignmentScoreResult or Task Request Message Result (Response) Message Global Score GlobalScorerRequest GlobalScorerResult Region Score RegionScorerRequest RegionScorerResult Frame Score FrameScorerRequest FrameScorerResult Create or modify a class enrollment ClassModificationRequest ClassModificationResult Remove an enrolled class ClassRemovalRequest ClassRemovalResult Export a class for later use or for model sharing ClassExportRequest ClassExportResult Import a previously exported class ClassImportRequest ClassImportResult Submit audio for modification (enhancement) AudioModificationRequest AudioModificationResult Request a vectorized representation of audio PluginAudioVectorRequest PluginAudioVectorResult Prepare audio for submission to training a new domain from scratch PreprocessAudioTrainRequest PreprocessAudioTrainResult Prepare audio for submission to adapt an existing domain PreprocessAudioAdaptRequest PreprocessAudioAdaptResult Query a plugin to see if it is ready to apply an \"Update\" GetUpdateStatusRequest GetUpdateStatusResult Instruct a plugin to perform an \"Update\" ApplyUpdateRequest ApplyUpdateResult Submit two audio files for forensic comparison GlobalComparerRequest GlobalComparerResult Submit text (string) for translation TextTransformationRequest TextTransformationResult Submit two or more audio files for alignment shift scores AudioAlignmentScoreRequest AudioAlignmentScoreResult Traits Deep Dive This section provides more details on what the \"real world\" usage and definition of each of the above Traits actually entails. If anything is unclear or could use more expansion, please contact us and we will provide updates and/or clarification. GlobalScorer A GlobalScorer plugin has the output of reporting a single score for each relevant plugin class representing the likelihood that the entire audio file or clip contains this class. For example, when audio is sent to a Speaker Identification (SID) plugin with the GlobalScorer Trait, if there are 3 enrolled speakers at that time, the plugin will return a single score for each enrolled speaker representing the likelihood that the audio is comprised of speech from each speaker, respectively. The output of such a request will contain information that looks something like this: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 Keep in mind that speakers and speaker identification are just examples in this case, and that this generalizes to any global scoring plugin and whatever classes it is meant to distinguish between. This same example could be represented more generically as: /data/sid/audio/file1.wav class1 -0.5348 /data/sid/audio/file1.wav class2 3.2122 /data/sid/audio/file1.wav class3 -5.5340 Due to the nature of global scoring plugins, they are best used when the attribute that's being estimated is known to be, or very likely to be static. For example, using global scoring SID on one side of a 4-wire telephone conversation, where the speaker is likely to remain the same throughout, or using global scoring LID on a TV broadcast that is expected to only contain a single language. These plugins can miss the nuances of or be confused by things like code-switching within recordings, or unexpected speaker changes. The benefits of global scoring plugins are that they are often very fast, since they treat the entirety of the submitted audio as a single unit, and do not worry about trying to subdivide or chunk it in any way. For a first-pass or quick triage approach, or when the data is known or suspected to be homogenous in the ways discussed above, these plugins are very effective. When finer grained results are necessary, though, a RegionScorer or FrameScorer may be more appropriate. RegionScorer For each audio file or recording submitted to a RegionScorer plugin, results are returned consisting of 0 or more regions with an associated score and plugin class. Typically, regions are returned in the case of a 'detection' of an instance of the respective class. A 'region' is a pair of timestamps, referring to the start time and end time of the detected class presence, and includes the name of the respective class, and an associated score. An example of this might be a keyword spotting plugin returning the keyword class name, as well as the location and likelihood of that keyword's presence. This might look something like: /data/test/testFile1.wav 0.630 1.170 Airplane 4.3725 /data/test/testFile1.wav 1.520 2.010 Watermelon -1.1978 Another example of output you may see from a region scoring plugin follows, showing what a region scoring speaker detection plugin might output. In this example, an enrolled speaker, speaker2 was detected in testFile1.wav from 0.630 s to 1.170 s with a likelihood score of 4.3725. Likewise for 1.520 s to 2.010 s in the same file for the enrolled speaker called speaker1 , this time with a likelihood score of -1.1978. /data/test/testFile1.wav 0.630 1.170 speaker2 4.3725 /data/test/testFile1.wav 1.520 2.010 speaker1 -1.1978 An even simpler output of this type may just label the regions within a file that the plugin determines contain speech. Again, these are just arbitrary examples using a specific plugin type to more easily describe the scoring type; a more generic output example could be: /data/test/testFile1.wav 0.630 1.170 class1 4.3725 /data/test/testFile1.wav 1.520 2.010 class2 -1.1978 RegionScorer plugins allow a finer resolution with respect to results granularity, and allow plugins to be more flexible and deal with transitions between languages or speakers or other classes of interest within a given audio file or recording. Sometimes this comes at a cost of increased processing complexity and/or slower runtime. FrameScorer A plugin with the FrameScorer Trait that is queried with a FrameScorerRequest will provide score results for each frame of whatever audio has been submitted. Unless otherwise noted, an audio frame is 10 milliseconds. The most common OLIVE plugin that has the FrameScorer Trait are speech activity detection (SAD) plugins, where the score for each frame represents the likelihood of speech being present in that 10 ms audio frame. The output in this case is simply a sequential list of numbers, corresponding to the output score for each frame, in order: 1.9047 1.8088 1.2382 - 0.8862 - 2.5509 In this example, these frames can then be processed to turn them into region scores, labeling the locations where speech has been detected as present within the file. Returning raw frame scores as a result allows more down-stream flexibility, allowing thresholds to be adjusted and regions re-labeled if desired, for example to allow for tuning for more difficult or unexpected audio conditions. ClassModifier Any plugin capable of adding or removing classes to or from its set of enrollments or target classes carries the ClassModifier Trait. This trait means the set of classes the plugin is interested in is mutable, and can be altered. Typically this is done by providing labeled data that belongs to the new class of interest to the server, which then enrolls a new model representing what it has learned about distinguishing this class from others. Existing class models can also be augmented by providing the system with additional data with this class label. In addition to adding new classes, and improving/augmenting existing ones, it is also possible to remove enrolled classes from domains of plugins carrying this trait, using the ClassRemovalRequest message. ClassExporter A plugin that implements the ClassExporter Trait is capable of exporting enrolled class models in a form that can be either imported back into the same system, as a way to save the model for preservation, or it can be imported into a different system, so long as that system has the same plugin and domain that was used to initially create the class model. This allows enrollments to be shared between systems. In general, exported models are specific to the plugin and domain that created them, so care must be taken to ensure models are not mixed in to other plugins. It is up to the client program or end user to keep tabs on where the exported models came from and what class they represent, and to manage these models once they've been exported. AudioConverter An AudioConverter plugin has audio as both its input and as its output. This Trait allows the system to take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. The only current plugin to implement the AudioConverter Trait is a speech enhancement plugin for enhancing the speech intelligibility of the submitted audio. AudioVectorizer This is a prototype feature that allows the system to preprocess an audio file or model to perform the compute-heavy steps of feature extraction and others, so that at a later time a user can either enroll a model or score a file very quickly, since the most time-consuming steps have already been performed. Like the ClassExporter Trait and exported classes/models, the vectorized audio representations are plugin/domain specific and cannot be used with plugins other than the one that created them. This is helpful if enrollments will be frequently rolled in or out of the system, or if the same audio files will be frequently re-tested, to avoid wasted repeat compute cycles. LearningTrait There are three total LearningTrait s: SupervisedAdapter SupervisedTrainer UnsupervisedAdapter Only SupervisedAdapter is currently supported by plugins in the OLIVE ecosystem; the others are deprecated or prototype features and should be ignored. SupervisedAdapter Plugins with the SupervisedAdapter Trait enable the ability to perform Supervised Adaptation of a domain. This is human assisted improvement of a plugin's domain, generally with feedback to the system in the form of annotations of target phenomena, or in some cases, error corrections. For more information on Adaptation, refer to this section . UpdateTrait Certain systems, such as recent speaker and language recognition plugins, sid-embed-v5 and lid-embedplda-v1], have the capability to adapt themselves based purely on raw data provided by the user in the normal use of the system (enrollment and test data). These systems collect statistics over from the data feed through the system that can be used to update system parameters in unsupervised (autonomous) adaptation, thereby improving the performance of the plugin in the conditions in which is has been deployed. Since the use of this data in adaptation changes the behavior of the plugin the system does not automatically update itself, but rather requires the end user to \"trigger\" the update and use the data the system has collected to adapt. Implementing and invoking the associated Update Request Messages to start the Update process will use the accrued data from the test and enroll conditions to pdate system parameters and apply the update. One can always revert to the plugin's original state by clearing out the data and statistics collected the learn directory from the server's storage. GlobalComparer A plugin that supports the GlobalComparer Trait has the capability of accepting two waveforms as input, performing some sort of analysis or comparison of the two, and returning a PDF report summarizing the analysis. TextTransformer A plugin with the TextTransformer Trait is used to translate a text input when queried with a TextTransformationRequest, providing translation results for the submitted string (it does not take an audio input, unlike other scoring traits). The output in this case is simply a string, which is the translation result AudioAlignmentScorer A plugin with the AudioAlignmentScorer Trait can be used to provide alignment shift scores for two or more audio inputs using an AudioAlignmentScoreRequest. The output is a set of shift scores betwen each combination of two audio inputs in the AudioAlignmentScoreRequest","title":"OLIVE Plugin Traits"},{"location":"traits.html#olive-plugin-traits","text":"","title":"OLIVE Plugin Traits"},{"location":"traits.html#traits-overview","text":"The functionality of each OLIVE plugin is defined by the API Traits that it implements. Each Trait defines a message or set of messages that the plugin must implement to perform an associated task. The available Traits are listed below, along with their associated implemented API messages, in the format: Trait Implemented Message 1 ... Implemented Message N","title":"Traits Overview"},{"location":"traits.html#traits-and-their-corresponding-messages","text":"The Trait and Message List follows. Each Trait name links to the relevant section on this page . Each API message entry links to the relevant portion of the Protocol Buffers Message Definition Reference Page. GlobalScorer - ( Tutorial ) GlobalScorerRequest GlobalScorerStereoRequest RegionScorer - ( Tutorial ) RegionScorerRequest RegionScorerStereoRequest FrameScorer - ( Tutorial ) FrameScorerRequest FrameScorerStereoRequest ClassModifier - ( Tutorial ) ClassModificationRequest ClassRemovalRequest ClassExporter ClassExportRequest ClassImportRequest AudioConverter AudioModificationRequest AudioVectorizer PluginAudioVectorRequest LearningTrait (SupervisedAdapter, SupervisedTrainer, UnsupervisedAdapter) - ( Tutorial ) PreprocessAudioTrainRequest PreprocessAudioAdaptRequest SupervisedTrainingRequest UnsupervisedAdaptationRequest UpdateTrait GetUpdateStatusRequest ApplyUpdateRequest GlobalComparer GlobalComparerRequest TextTransformer TextTransformationRequest AudioAlignmentScorer AudioAlignmentScoreRequest In addition to the messages above, the following messages exist for interacting with the server itself and are independent of individual plugins or plugin Traits: GetStatusRequest PluginDirectoryRequest LoadPluginDomainRequest RemovePluginDomainRequest Full message definition details for all of the messages mentioned here","title":"Traits and their Corresponding Messages"},{"location":"traits.html#requestresult-message-pairs","text":"In general, each Request message is paired with a Result message, that defines the structure and contents of the server's reply to a given client Request message. The table below defines the pairs of the Result messages for the Request messages mentioned above; and the full message definition details can be found in the OLIVE API Message Protocol Documentation , which each entry below is linked to. Request Message Result (Response) Message GlobalScorerRequest GlobalScorerResult RegionScorerRequest RegionScorerResult FrameScorerRequest FrameScorerResult ClassModificationRequest ClassModificationResult ClassRemovalRequest ClassRemovalResult ClassExportRequest ClassExportResult ClassImportRequest ClassImportResult AudioModificationRequest AudioModificationResult PluginAudioVectorRequest PluginAudioVectorResult PreprocessAudioTrainRequest PreprocessAudioTrainResult PreprocessAudioAdaptRequest PreprocessAudioAdaptResult SupervisedTrainingRequest SupervisedTrainingResult UnsupervisedAdaptationRequest UnsupervisedAdaptationResult GetUpdateStatusRequest GetUpdateStatusResult ApplyUpdateRequest ApplyUpdateResult GlobalComparerRequest GlobalComparerResult TextTransformationRequest TextTransformationResult AudioAlignmentScoreRequest AudioAlignmentScoreResult or Task Request Message Result (Response) Message Global Score GlobalScorerRequest GlobalScorerResult Region Score RegionScorerRequest RegionScorerResult Frame Score FrameScorerRequest FrameScorerResult Create or modify a class enrollment ClassModificationRequest ClassModificationResult Remove an enrolled class ClassRemovalRequest ClassRemovalResult Export a class for later use or for model sharing ClassExportRequest ClassExportResult Import a previously exported class ClassImportRequest ClassImportResult Submit audio for modification (enhancement) AudioModificationRequest AudioModificationResult Request a vectorized representation of audio PluginAudioVectorRequest PluginAudioVectorResult Prepare audio for submission to training a new domain from scratch PreprocessAudioTrainRequest PreprocessAudioTrainResult Prepare audio for submission to adapt an existing domain PreprocessAudioAdaptRequest PreprocessAudioAdaptResult Query a plugin to see if it is ready to apply an \"Update\" GetUpdateStatusRequest GetUpdateStatusResult Instruct a plugin to perform an \"Update\" ApplyUpdateRequest ApplyUpdateResult Submit two audio files for forensic comparison GlobalComparerRequest GlobalComparerResult Submit text (string) for translation TextTransformationRequest TextTransformationResult Submit two or more audio files for alignment shift scores AudioAlignmentScoreRequest AudioAlignmentScoreResult","title":"Request/Result Message Pairs"},{"location":"traits.html#traits-deep-dive","text":"This section provides more details on what the \"real world\" usage and definition of each of the above Traits actually entails. If anything is unclear or could use more expansion, please contact us and we will provide updates and/or clarification.","title":"Traits Deep Dive"},{"location":"traits.html#globalscorer","text":"A GlobalScorer plugin has the output of reporting a single score for each relevant plugin class representing the likelihood that the entire audio file or clip contains this class. For example, when audio is sent to a Speaker Identification (SID) plugin with the GlobalScorer Trait, if there are 3 enrolled speakers at that time, the plugin will return a single score for each enrolled speaker representing the likelihood that the audio is comprised of speech from each speaker, respectively. The output of such a request will contain information that looks something like this: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 Keep in mind that speakers and speaker identification are just examples in this case, and that this generalizes to any global scoring plugin and whatever classes it is meant to distinguish between. This same example could be represented more generically as: /data/sid/audio/file1.wav class1 -0.5348 /data/sid/audio/file1.wav class2 3.2122 /data/sid/audio/file1.wav class3 -5.5340 Due to the nature of global scoring plugins, they are best used when the attribute that's being estimated is known to be, or very likely to be static. For example, using global scoring SID on one side of a 4-wire telephone conversation, where the speaker is likely to remain the same throughout, or using global scoring LID on a TV broadcast that is expected to only contain a single language. These plugins can miss the nuances of or be confused by things like code-switching within recordings, or unexpected speaker changes. The benefits of global scoring plugins are that they are often very fast, since they treat the entirety of the submitted audio as a single unit, and do not worry about trying to subdivide or chunk it in any way. For a first-pass or quick triage approach, or when the data is known or suspected to be homogenous in the ways discussed above, these plugins are very effective. When finer grained results are necessary, though, a RegionScorer or FrameScorer may be more appropriate.","title":"GlobalScorer"},{"location":"traits.html#regionscorer","text":"For each audio file or recording submitted to a RegionScorer plugin, results are returned consisting of 0 or more regions with an associated score and plugin class. Typically, regions are returned in the case of a 'detection' of an instance of the respective class. A 'region' is a pair of timestamps, referring to the start time and end time of the detected class presence, and includes the name of the respective class, and an associated score. An example of this might be a keyword spotting plugin returning the keyword class name, as well as the location and likelihood of that keyword's presence. This might look something like: /data/test/testFile1.wav 0.630 1.170 Airplane 4.3725 /data/test/testFile1.wav 1.520 2.010 Watermelon -1.1978 Another example of output you may see from a region scoring plugin follows, showing what a region scoring speaker detection plugin might output. In this example, an enrolled speaker, speaker2 was detected in testFile1.wav from 0.630 s to 1.170 s with a likelihood score of 4.3725. Likewise for 1.520 s to 2.010 s in the same file for the enrolled speaker called speaker1 , this time with a likelihood score of -1.1978. /data/test/testFile1.wav 0.630 1.170 speaker2 4.3725 /data/test/testFile1.wav 1.520 2.010 speaker1 -1.1978 An even simpler output of this type may just label the regions within a file that the plugin determines contain speech. Again, these are just arbitrary examples using a specific plugin type to more easily describe the scoring type; a more generic output example could be: /data/test/testFile1.wav 0.630 1.170 class1 4.3725 /data/test/testFile1.wav 1.520 2.010 class2 -1.1978 RegionScorer plugins allow a finer resolution with respect to results granularity, and allow plugins to be more flexible and deal with transitions between languages or speakers or other classes of interest within a given audio file or recording. Sometimes this comes at a cost of increased processing complexity and/or slower runtime.","title":"RegionScorer"},{"location":"traits.html#framescorer","text":"A plugin with the FrameScorer Trait that is queried with a FrameScorerRequest will provide score results for each frame of whatever audio has been submitted. Unless otherwise noted, an audio frame is 10 milliseconds. The most common OLIVE plugin that has the FrameScorer Trait are speech activity detection (SAD) plugins, where the score for each frame represents the likelihood of speech being present in that 10 ms audio frame. The output in this case is simply a sequential list of numbers, corresponding to the output score for each frame, in order: 1.9047 1.8088 1.2382 - 0.8862 - 2.5509 In this example, these frames can then be processed to turn them into region scores, labeling the locations where speech has been detected as present within the file. Returning raw frame scores as a result allows more down-stream flexibility, allowing thresholds to be adjusted and regions re-labeled if desired, for example to allow for tuning for more difficult or unexpected audio conditions.","title":"FrameScorer"},{"location":"traits.html#classmodifier","text":"Any plugin capable of adding or removing classes to or from its set of enrollments or target classes carries the ClassModifier Trait. This trait means the set of classes the plugin is interested in is mutable, and can be altered. Typically this is done by providing labeled data that belongs to the new class of interest to the server, which then enrolls a new model representing what it has learned about distinguishing this class from others. Existing class models can also be augmented by providing the system with additional data with this class label. In addition to adding new classes, and improving/augmenting existing ones, it is also possible to remove enrolled classes from domains of plugins carrying this trait, using the ClassRemovalRequest message.","title":"ClassModifier"},{"location":"traits.html#classexporter","text":"A plugin that implements the ClassExporter Trait is capable of exporting enrolled class models in a form that can be either imported back into the same system, as a way to save the model for preservation, or it can be imported into a different system, so long as that system has the same plugin and domain that was used to initially create the class model. This allows enrollments to be shared between systems. In general, exported models are specific to the plugin and domain that created them, so care must be taken to ensure models are not mixed in to other plugins. It is up to the client program or end user to keep tabs on where the exported models came from and what class they represent, and to manage these models once they've been exported.","title":"ClassExporter"},{"location":"traits.html#audioconverter","text":"An AudioConverter plugin has audio as both its input and as its output. This Trait allows the system to take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. The only current plugin to implement the AudioConverter Trait is a speech enhancement plugin for enhancing the speech intelligibility of the submitted audio.","title":"AudioConverter"},{"location":"traits.html#audiovectorizer","text":"This is a prototype feature that allows the system to preprocess an audio file or model to perform the compute-heavy steps of feature extraction and others, so that at a later time a user can either enroll a model or score a file very quickly, since the most time-consuming steps have already been performed. Like the ClassExporter Trait and exported classes/models, the vectorized audio representations are plugin/domain specific and cannot be used with plugins other than the one that created them. This is helpful if enrollments will be frequently rolled in or out of the system, or if the same audio files will be frequently re-tested, to avoid wasted repeat compute cycles.","title":"AudioVectorizer"},{"location":"traits.html#learningtrait","text":"There are three total LearningTrait s: SupervisedAdapter SupervisedTrainer UnsupervisedAdapter Only SupervisedAdapter is currently supported by plugins in the OLIVE ecosystem; the others are deprecated or prototype features and should be ignored.","title":"LearningTrait"},{"location":"traits.html#supervisedadapter","text":"Plugins with the SupervisedAdapter Trait enable the ability to perform Supervised Adaptation of a domain. This is human assisted improvement of a plugin's domain, generally with feedback to the system in the form of annotations of target phenomena, or in some cases, error corrections. For more information on Adaptation, refer to this section .","title":"SupervisedAdapter"},{"location":"traits.html#updatetrait","text":"Certain systems, such as recent speaker and language recognition plugins, sid-embed-v5 and lid-embedplda-v1], have the capability to adapt themselves based purely on raw data provided by the user in the normal use of the system (enrollment and test data). These systems collect statistics over from the data feed through the system that can be used to update system parameters in unsupervised (autonomous) adaptation, thereby improving the performance of the plugin in the conditions in which is has been deployed. Since the use of this data in adaptation changes the behavior of the plugin the system does not automatically update itself, but rather requires the end user to \"trigger\" the update and use the data the system has collected to adapt. Implementing and invoking the associated Update Request Messages to start the Update process will use the accrued data from the test and enroll conditions to pdate system parameters and apply the update. One can always revert to the plugin's original state by clearing out the data and statistics collected the learn directory from the server's storage.","title":"UpdateTrait"},{"location":"traits.html#globalcomparer","text":"A plugin that supports the GlobalComparer Trait has the capability of accepting two waveforms as input, performing some sort of analysis or comparison of the two, and returning a PDF report summarizing the analysis.","title":"GlobalComparer"},{"location":"traits.html#texttransformer","text":"A plugin with the TextTransformer Trait is used to translate a text input when queried with a TextTransformationRequest, providing translation results for the submitted string (it does not take an audio input, unlike other scoring traits). The output in this case is simply a string, which is the translation result","title":"TextTransformer"},{"location":"traits.html#audioalignmentscorer","text":"A plugin with the AudioAlignmentScorer Trait can be used to provide alignment shift scores for two or more audio inputs using an AudioAlignmentScoreRequest. The output is a set of shift scores betwen each combination of two audio inputs in the AudioAlignmentScoreRequest","title":"AudioAlignmentScorer"},{"location":"workflows.html","text":"OLIVE Python Workflow API Introduction The OLIVE Workflow API extends the OLIVE Enterprise API to simplify working with an OLIVE server, allowing clients to request multiple OLIVE tasks with one API call. This eliminates the more verbose and complex calls necessary when using the Enterprise API. In particular, this framework will encapsulate 'feeding' functionality, to link together tasks, like speech activity detection and a plugin that uses speech regions or frames in its processing, or to 'bundle' multiple requests in a single call, rather than having every API call to a plugin be a separate action. The Workflow API is based around binary or text \"Workflow Definition\" files that SRI distributes to clients. These files contain a 'recipe' to be executed on an OLIVE server to perform analysis, enrollment, and eventually adaption using one or more audio files/inputs. This places the work of specifying how to execute a complex task to execute within the OLIVE server, instead of the client. To use this API, a client submits the SRI provided Workflow Definition file to an OLIVE server. The server verifies it can run the Workflow Definition through a process called \"actualization\". If successful, an activated Workflow is returned to the client. This activated Workflow is then ready for one or more analysis (or enrollment) requests. These requests can be made numerous times with one or more audio submissions. OLIVE still supports the original OLIVE Enterprise API that was based on creating and sending Google Protocol Buffers (Protobuf) message for each request, so if desired, clients can combine classic OLIVE API calls with the Workflow API to implement advanced functionality. For advanced users, see Creating a Workflow Definition for information on composing your own Workflow Definition files. For advanced users, see Creating a Workflow Definition for information on composing your own Workflow Definition files. For more information about working with OLIVE, please see: The OLIVE Server The OLIVE Plugins The OLIVE Enterprise API Useful Concepts to Know Workflow Definition - distributed as a file (text or binary) with these characteristics: Similar to Plugins, Workflows are independent of the OLIVE software and can be updated and/or distributed outside of an OLIVE software release. Can be submitted to multiple OLIVE servers for actualization in parallel, where actualization is the process of verifying that the server can perform the activities defined in a Workflow Definition. Each actualized Workflow is considered unique to the server where it was actualized, as it is possible the plugin/domain names could vary by each server that actualizes the same WorkflowDefinition Most Workflow Definitions are implemented with the specific names of plugins/domains delivered with the OLIVE system. Changing the name of a plugin or domain will cause workflows that use them to cease to function. Order: a set one or more jobs. There are 3 supported Workflow orders: analysis, enrollment, and unenrollment. Analysis orders require at least one data (audio) input, enrollment orders require one or more data (audio) inputs, plus a class ID, unenrollment inputs do not accept data, only a class ID. Job: A set of tasks, plus depending on the order (analysis, enrollment, unenrollment) may include audio (data) and/or as class ID. An analysis order typically only includes one job, with that job accepting one audio input. An enrollment order may have multiple jobs, where each jobs has it's own set of tasks, one or more data/audio(s), and a class id (i.e. speaker name). Only tasks that support the Class Enroller trait such as speaker enrollment for SID and language enrollment for LID can be part of an enrollment job. Each enrollment job should support enrollment for a single class enroller. This allows enrollments (such as for LID and SID) to be handled separately, since it is very unlikely the same audio and class id (speaker name/language name) would be used for both tasks. Unenrollment jobs are similar to enrollment jobs, except that do NOT consume audio/data. They only accept a class ID Tasks: at the lowest level, a Workflow is composed of one or more tasks, such as SAD, LID, SID, QBE, ASR, etc. A task typically maps to a traditional plugin/domain, but as we expand the capabilities of the Workflow API, 'tasks' are likely to include functionality that is not implemented by a traditional OLIVE plugin, but by private, helper, components that assist other tasks in the workflow but do not return values to the user. Limitations The Workflow API is a new OLIVE extension whose interfaces are subject to change. This release covers two types of behavior: bundling of disparate tasks together into a single call and restricted feeding processes, wherein the output of one plugin may be fed into one or more other \"downstream\" plugins that will use this information in their processing. The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the plugin must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the downstream plugin(s) must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). Further, the current implementation of workflows does not support tasks that create entirely new domains, like adaptation. This is much easier to accomplish with a direct call to the plugin, as bundling and feeding behavior is not relevant to this activity. Python Client Install The OLIVE Python API is distributed as a Python wheel package. To install, navigate into the 'api' folder distributed with your OLIVE delivery, and install it using your native pip3: $ pip3 install olivepy-5.3.0-py3-none-any.whl Or if installing on CentOS, the wheel package includes all 3rd party dependencies, so OlivePy can be installed on a standalone (no Internet) system: pip3 install -r requirements.txt --use-wheel --no-index --find-links wheelhouse olivepy-5.3.0-py3-none-any.whl This installs OLIVE and its dependencies into your local Python distribution. The client source is also available in the olivepy-5.2.0.tar.gz package. OLIVE client dependencies include: protobuf soundfile (suggested) numpy zmq python3 Integration Analysis Request With a Workflow Definition file, it only takes a few steps to make an analysis request. In the following example, a Workflow Definition file (sad_lid_sid.workflow) that supports SAD, LID, and SID analysis is used to make a request: import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow from olivepy.messaging.msgutil import InputTransferType import os client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) workflow = owd . create_workflow ( client ) # Send audio as a serialized buffer buffer = workflow . package_audio ( '~/olive/sad_smoke.wav' , InputTransferType . SERIALIZED , label = os . path . basename ( 'sad_smoke.wav' )) response = workflow . analyze ([ buffer ]) The analysis response can be pretty printed as JSON: print ( \"Workflow Results: {} \" . format ( response . to_json ( indent = 1 ))) Work fl ow Resul ts : [ { \"job_name\" : \"SAD, LID, and SID workflow\" , \"data\" : [ { \"data_id\" : \"sad_smoke.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 27.9325 , \"number_channels\" : 1 , \"label\" : \"sad_smoke.wav\" , \"id\" : \"ebc1dfa7502841216526768a3f94b095b9362f6a37cf903631494e8784471931\" } ], \"tasks\" : { \"SAD\" : { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 1.06 , \"end_t\" : 2.65 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 3.19 , \"end_t\" : 4.18 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 4.84 , \"end_t\" : 10.54 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 10.99 , \"end_t\" : 16.63 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 16.91 , \"end_t\" : 18.18 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 18.25 , \"end_t\" : 19.66 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 21.11 , \"end_t\" : 24.26 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] }, \"plugin\" : \"sad-dnn-v7.0.1\" , \"domain\" : \"multi-v1\" }, \"LID\" : { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"eng\" , \"score\" : 1.3094566 }, { \"class_id\" : \"tgl\" , \"score\" : -2.0286703 }, { \"class_id\" : \"apc\" , \"score\" : -2.3600318 }, { \"class_id\" : \"pus\" , \"score\" : -2.6724625 }, { \"class_id\" : \"arb\" , \"score\" : -2.8804097 }, { \"class_id\" : \"arz\" , \"score\" : -3.0850184 }, { \"class_id\" : \"fas\" , \"score\" : -3.1287756 }, { \"class_id\" : \"tur\" , \"score\" : -3.1409845 }, { \"class_id\" : \"spa\" , \"score\" : -4.291254 }, { \"class_id\" : \"yue\" , \"score\" : -4.695897 }, { \"class_id\" : \"urd\" , \"score\" : -5.4247284 }, { \"class_id\" : \"fre\" , \"score\" : -6.19234 }, { \"class_id\" : \"tha\" , \"score\" : -7.447339 }, { \"class_id\" : \"vie\" , \"score\" : -7.9231014 }, { \"class_id\" : \"kor\" , \"score\" : -8.1033945 }, { \"class_id\" : \"jpn\" , \"score\" : -9.810704 }, { \"class_id\" : \"cmn\" , \"score\" : -10.923913 }, { \"class_id\" : \"rus\" , \"score\" : -11.114039 }, { \"class_id\" : \"amh\" , \"score\" : -19.474201 } ] }, \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" }, \"SID\" : { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"SID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"test_speaker\" , \"score\" : -3.035223 } ] }, \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" } } } ] In the above example, results for SAD, LID, and SID analysis are output. Enrollment Request Some workflows support enrollment for one or more jobs. To list the jobs that support enrollment in a workflow, use the OliveWorkflow get_enrollment_job_names method: import olivepy.api.olive_async_client as oc , os import olivepy.api.workflow as ow from olivepy.messaging.msgutil import InputTransferType client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/sad-lid-sid_enroll-lid-sid_example.workflow\" ) workflow = owd . create_workflow ( client ) workflow . get_analysis_tasks () # ['SAD', 'LID', 'SID'] print ( \"Enrollment Jobs: {} \" . format ( workflow . get_enrollment_job_names ())) # Enrollment jobs '['SID Enrollment', 'LID Enrollment']' Shown above, a workflow that supports SAD, LID, and SID analysis, while also supporting SID and/or LID enrollment. To enroll a speaker (generically a class id) via this workflow, use the OliveWorkflow's enroll method: ... buffer = workflow . package_audio ( '~/olive/English.wav' , InputTransferType . SERIALIZED , label = os . path . basename ( 'English.wav' )) response = workflow . enroll ([ buffer ], 'example speaker' , [ 'SID Enrollment' ]) print ( response . to_json ( indent = 1 )) Which prints: [ { \"job_name\" : \"SID Enrollment\" , \"data\" : [ { \"data_id\" : \"English.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 5.0 , \"number_channels\" : 1 , \"label\" : \"English.wav\" , \"id\" : \"32b34b7a675e47c0a4eb7a6a6ff8cc5d470beafda6cf6e53c5a12b4f711ea37a\" } ], \"tasks\" : { \"SID Enrollment\" : { \"task_trait\" : \"CLASS_MODIFIER\" , \"task_type\" : \"SID\" , \"message_type\" : \"CLASS_MODIFICATION_RESULT\" , \"analysis\" : { \"addition_result\" : [ { \"successful\" : true , \"label\" : \"English.wav\" } ] }, \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"example speaker\" } } } ] NOTE: since there was only one enrollment job, the enrollment request could have been made without specifying the job name (as shown below). Not specifying the job name will result in enrolling for all jobs, so it is a best practice to explicitly request the enrollment job. ... response = workflow . enroll ([ buffer ], 'example speaker' ) ... To confirm the new class/speaker name was added: class_status_response = workflow . get_analysis_class_ids () print ( \"Analysis Class Info: {} \" . format ( class_status_response . to_json ( indent = 1 ))) A nal ysis Class I nf o : { \"job_class\" : [ { \"job_name\" : \"SAD, LID, SID analysis with LID and/or SID enrollment\" , \"task\" : [ { \"task_name\" : \"SAD\" , \"class_id\" : [ \"speech\" ] }, { \"task_name\" : \"LID\" , \"class_id\" : [ \"amh\" , \"apc\" , \"arb\" , \"arz\" , \"cmn\" , \"eng\" , \"fas\" , \"fre\" , \"jpn\" , \"kor\" , \"pus\" , \"rus\" , \"spa\" , \"tgl\" , \"tha\" , \"tur\" , \"urd\" , \"vie\" , \"yue\" ] }, { \"task_name\" : \"SID\" , \"class_id\" : [ \"test_speaker\" , \"example speaker\" ] } ] } ] } Workflow job that support enrollment, often support un-enrollment. Use the get_unenrollment_job_names() message to list jobs that support unenrollment: print ( \"Unenrollment jobs: {} \" . format ( workflow . get_unenrollment_job_names ())) # Unenrollment jobs: ['SID Unenrollment', 'LID Unenrollment'] response = workflow . unenroll ( 'example speaker' , [ 'SID Unenrollment' ]) print ( \"Workflow unenrollment: {} \" . format ( response . to_json ( indent = 1 ))) Work fl ow u nenr ollme nt : [ { \"job_name\" : \"SID Unenrollment\" , \"data\" : [], \"tasks\" : { \"SID Unenroll\" : { \"task_trait\" : \"CLASS_MODIFIER\" , \"task_type\" : \"SID\" , \"message_type\" : \"CLASS_MODIFICATION_RESULT\" , \"analysis\" : {}, \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"example speaker\" } } } ] Running workflow.get_analysis_class_ids() again would show that 'example speaker' was removed. Other Enrollment Options Enrollment can also be done via the Python CLI tool, olivepyenroll: olivepyenroll -p sid-embed-v6.0.1 --domain multicond-v1 -e Test_Speaker -w test.wav Or incorporate the code from OliveClient (or AsyncOliveClient if client your client needs non-blocking calls): import olivepy.api.oliveclient as oc client = oc . OliveClient ( 'test client' ) client . enroll ( 'sid-embed-v6.0.1' , 'multicond-v1' , 'Test_Speaker' , '/olive/data/test.wav' ) Advanced Workflow Features Audio Submission Options Audio can be submitted as a path name or as a buffer. If submitted as a path name, then the path must be accessible by the OLIVE server. If submitting audio as a buffer then that buffer can contain either: A 'serialized' file, where the file contents are read into a buffer. PCM-16 encoded samples. If the client has decoded a audio source, then those samples can be submitted directly in the audio buffer. The audio pathname or buffer is wrapped in a WorkflowDataRequest object before submitting to OLIVE. As of OLIVE 5.3, a serialized video file (that contains audio) can also be submitted as a serialized buffer or pathname for audio processing. Sending a local file as a serialized buffer: ... # Submit the serialized audio file into a buffer for submission to OLIVE audio_filename = \"~/audio/test_audio.wav\" # OliveWorkflow provides a helper method to serialize a file: # Wrap the serialized audio buffer in a WorkflowDataRequest: olive_data = workflow . package_audio ( audio_filename , InputTransferType . SERIALIZED , label = os . path . basename ( audio_filename )) Or to send the path to a local file (assuming the server and client are running on the same host or have access to a shared file system): ... # Submit the path of the audio file: audio_filename = \"~/audio/test_audio.wav\" olive_data = workflow . package_audio ( audio_filename , mode = msgutil . InputTransferType . PATH ) Once the data has been wrapped in a WorkflowDataRequest it can be submitted for analysis or enrollment. An example of the AUDIO_DECODED transfer is not provided since, decoded audio requires a 3rd party package to decode audio input. Audio Annotations The audio submitted for analysis (or enrollment) can be annotated with start/end regions when packaging audio using the OliveWorkflow.package_audio() method. For example, here is how to specify two regions within a file: ... filename = '/home/olive/test.wav' # Provide annotations for two regions: 0.3 to 1.7 seconds, and 2.4 to 3.3 seconds in filename: regions = [( 0.3 , 1.7 ), ( 2.4 , 3.3 )] audio = workflow . package_audio ( filename , InputTransferType . AUDIO_SERIALIZED , annotations = regions ) ... Binary Data New since OLIVE 5.3 is the ability to send audio or other data types such as Video or Images in a more generic container, BinaryMedia, which can contain audio, video, or image data (either as a serialzied file buffer or as filepath, decoded data is NOT supported with this message type). Replace calls to OliveWorkflow.package_audio() with OliveWorkflow.package_binary() to use this new data container. Handling A Workflow Analysis Response (Python) A successful Workflow analysis produces a response that includes information about the audio analyzed and the results of one or more tasks. The Workflow API includes the method, workflow.get_analysis_tasks(), to help clients identify and prepare for the types of tasks a Workflow will produce. The method, get_analysis_tasks(), returns the names of the tasks supported by this workflow. For example: print ( \"Analysis Tasks: {} \" . format ( workflow . get_analysis_tasks ())) # Analysis Tasks: ['SAD', 'LID'] For detailed information about the tasks(): workflow_def_json = owd . to_json ( indent = 1 ) # Pretty print the workflow definition: print ( workflow_def_json ) { \"order\" : [ { \"workflow_type\" : \"WORKFLOW_ANALYSIS_TYPE\" , \"job_definition\" : [ { \"job_name\" : \"SAD, LID, SID analysis with LID and/or SID enrollment\" , \"tasks\" : [ { \"message_type\" : \"REGION_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"sad-dnn-v7.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"REGION_SCORER\" , \"task\" : \"SAD\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SAD\" , \"return_result\" : true }, { \"message_type\" : \"GLOBAL_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"GLOBAL_SCORER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"LID\" , \"return_result\" : true }, { \"message_type\" : \"GLOBAL_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"GLOBAL_SCORER\" , \"task\" : \"SID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SID\" , \"return_result\" : true } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" } } ], \"order_name\" : \"Analysis Order\" }, { \"workflow_type\" : \"WORKFLOW_ENROLLMENT_TYPE\" , \"job_definition\" : [ { \"job_name\" : \"SID Enrollment\" , \"tasks\" : [ { \"message_type\" : \"CLASS_MODIFICATION_REQUEST\" , \"message_data\" : { \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"SID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SID Enrollment\" , \"return_result\" : true , \"allow_failure\" : false } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, \"description\" : \"SID Enrollment\" }, { \"job_name\" : \"LID Enrollment\" , \"tasks\" : [ { \"message_type\" : \"CLASS_MODIFICATION_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"LID Enrollment\" , \"return_result\" : true , \"allow_failure\" : false } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, \"description\" : \"LID Enrollment\" } ], \"order_name\" : \"Enrollment Order\" }, { \"workflow_type\" : \"WORKFLOW_UNENROLLMENT_TYPE\" , \"job_definition\" : [ { \"job_name\" : \"SID Unenrollment\" , \"tasks\" : [ { \"message_type\" : \"CLASS_REMOVAL_REQUEST\" , \"message_data\" : { \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"SID\" , \"consumer_data_label\" : \"\" , \"consumer_result_label\" : \"SID Unenroll\" , \"return_result\" : true , \"allow_failure\" : false } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, \"description\" : \"SID UNenrollment job \" }, { \"job_name\" : \"LID Unenrollment\" , \"tasks\" : [ { \"message_type\" : \"CLASS_REMOVAL_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"\" , \"consumer_result_label\" : \"LID Unenroll\" , \"return_result\" : true , \"allow_failure\" : false } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, \"description\" : \"LID UNenrollment job \" } ], \"order_name\" : \"Unenrollment Order\" } ], \"actualized\" : false , \"created\" : { \"year\" : 2021 , \"month\" : 6 , \"day\" : 29 , \"hour\" : 8 , \"min\" : 0 , \"sec\" : 0 } } In the above example, the 'trait_output' attribute, identifies the type of score out produced by the task. This attribute lets clients know the type of score produced by this task. Currently, Workflow tasks return one of these three score (analysis) types: FRAME_SCORER - these tasks create a set of scores for each frame in the submitted audio. Typically frame scoring is returned by SAD plugins to provide a set of frame scores for 'speech'. For users of the Enterprise API, this the output from a FrameScorerResult message. GLOBAL_SCORER - these tasks create a single set of scores (one per class) for an entire audio. In the Enterprise API this maps to a GlobalScorerResult message. REGION_SCORER - these tasks create a set of scores for regions in the audio submission. In the Enterprise API this maps to a RegionScorerResult message Parsing Analysis Output Analysis output is grouped into one or more 'jobs', where a job is the audio info plus the analysis task result(s). In the example above, a single audio file was serialized, then submitted to OLIVE for analysis by SAD and LID tasks. This resulted in an OLIVE response containing the SAD and LID task results, plus information about the audio used for analysis. Since only one file was submitted for analysis, only one 'job' was returned. Had two audio files been submitted, then OLIVE would have created two job results - one for each file submitted. Each job will have a data element that describes properties of the audio used for analysis and one or more 'task' elements that contain the analysis results which are either frame scores, global scores, or region scores. Frame Scorer Task Output Tasks that return frame scores, are based on the Enterprise API FrameScoreResult message: // The results from a FrameScorerRequest message FrameScorerResult { repeated FrameScores result = 1 ; // List of frame scores by class_id } // The basic unit of a frame score , returned in a FrameScorerRequest message FrameScores { required string class_id = 1 ; // The class id to which the frame scores pertain required int32 frame_rate = 2 ; // The number of frames per second required double frame_offset = 3 ; // The offset to the center of the frame 'window' repeated double score = 4 [ packed = true ]; // The frame - level scores for the class_id } The JSON output for frame score results has the form: resul t [ { classId : s tr , fra meRa te : i nt , fra meO ffset : double , score :[ double ]} ] Here is an example of SAD output as frame scores: \"tasks\" : { ... \"SAD\" : { \"task_trait\" : \"FRAME_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"FRAME_SCORER_RESULT\" , \"analysis\" : { \"result\" : [ { \"class_id\" : \"speech\" , \"frame_rate\" : 100 , \"frame_offset\" : 0.0 , \"score\" : [ -1.65412407898345 , -1.65412407898345 , -1.6976179167708825 , -1.7201831820448585 , -1.7218198748053783 , -1.7285406659981328 , -1.7403455556231222 , -1.7572345436803467 , -1.779207630169806 , -1.8052880586341415 , ... ] } ] }, \"plugin\" : \"sad-dnn\" , \"domain\" : \"multi-v1\" }, Global Scorer Tasks Tasks that return global scores, are based on the Enterprise API's GlobalScorerResult message: message GlobalScorerResult { repeated GlobalScore score = 1 ; // The class scores } // The global score for a class message GlobalScore { required string class_id = 1 ; // The class required float score = 2 ; // The score associated with the class optional float confidence = 3 ; // An optional confidence value when part of a calibration report optional string comment = 4 ; // An optional suggested action when part of a calibration report } The JSON output for global score results has the form: score [ { classId : s tr , score :[ fl oa t ]} ] Here is an example of LID output as global scores: \"tasks\" : { ... \"LID\" : { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"eng\" , \"score\" : 2.5195693969726562 }, { \"class_id\" : \"apc\" , \"score\" : -3.509812355041504 }, { \"class_id\" : \"tgl\" , \"score\" : -3.566483974456787 }, { \"class_id\" : \"arb\" , \"score\" : -3.994529962539673 }, { \"class_id\" : \"fre\" , \"score\" : -21.55596160888672 } ] }, \"plugin\" : \"lid-embedplda\" , \"domain\" : \"multi-v1\" } Region Scorer Tasks Tasks that return region scores, are based on the Enterprise API's RegionScorerResult message: // The region score result message RegionScorerResult { repeated RegionScore region = 1 ; // The scored regions } // The basic unit a region score . There may be multiple RegionScore values in a RegionScorerResult message RegionScore { required float start_t = 1 ; // Begin - time of the region ( in seconds ) required float end_t = 2 ; // End - time of t he region ( in seconds ) required string class_id = 3 ; // Class ID associated with region optional float score = 4 ; // Optional score associated with the class_id label } The JSON output for region score results has the form: score [ { s tart T : fl oa t , e n dT : fl oa t , classId : s tr , score : fl oa t } ] Here is an example of SAD output as region score: \"tasks\" : { \"SAD\" : { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 3.3399999141693115 , \"end_t\" : 10.5 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] }, \"plugin\" : \"sad-dnn\" , \"domain\" : \"vtd-v1\" }, Enrolled Classes To list the current classes (such as speakers for a SID task, or languages for LID) in an analysis workflow, use the OliveWorkflow get_analysis_class_ids method: import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) workflow = owd . create_workflow ( client ) class_status_response = workflow . get_analysis_class_ids () print ( \"Task Class IDs: {} \" . format ( class_status_response . to_json ( indent = 1 ))) E nr ollme nt Class IDs : [ { \"job_name\" : \"SAD LID, SID\" , \"tasks\" : { \"SAD\" : { \"class_id\" : [ \"speech\" ] }, \"LID\" : { \"class_id\" : [ \"amh\" , \"apc\" , \"arb\" , \"arz\" , \"cmn\" , \"eng\" , \"fas\" , \"fre\" , \"jpn\" , \"kor\" , \"pus\" , \"rus\" , \"spa\" , \"tgl\" , \"tha\" , \"tur\" , \"urd\" , \"vie\" , \"yue\" ] }, \"SID\" : { \"class_id\" : [ \"test speaker\" ] } } } ] Note that some tasks, such as SID, support enrollment, so the list of class IDs can can change over time as new enrollments are added. Multi-channel Audio The default workflow behavior is to merge multi-channel audio into a single channel, which is known as 'MONO' mode. To perform analysis on each channel instead of a merged channel, then the Workflow Definition must be authored with a data mode of 'SPLIT'. When using the split mode, each channel in a multi-channel audio input is \"split\" into a job. Here is a typical workflow that merges any multi-channel audio into a single channel audio input: import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) workflow = owd . create_workflow ( client ) print ( workflow . get_analysis_task_info ()) [ { \"Data Input\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, ... Here is a workflow that will split each channel in a multi-channel (stereo) audio input into separate jobs: import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/stereo_sid_example.workflow\" ) workflow = owd . create_workflow ( client ) print ( workflow . get_analysis_task_info ()) [ { \"Data Input\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"SPLIT\" }, The above Workflow Definition when applied to stereo file will produce a set of SID scores for each channel. Adaption Using a Workflow Not yet implemented via a Workflow. The adaption process is complicated, time-consuming, and plugin/domain specific. Until Adaptation via a workflow is implemented, please use the SRI provided Python client (olivepylearn) or Java client (OliveLearn) to perform adaptation. To adapt using the olivepylearn utility: olivepylearn --plugin sad-dnn --domain multi-v1 -a TEST_NEW_DOMAIN -i /olive/sadRegression/lists/adapt_s.lst Where that adapt_s.lst looks like this: /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav S 20.469 21.719 /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav NS 10.8000 10.8229 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav S 72.898 73.748 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav NS 42.754 43.010 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav S 79.437 80.427 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav NS 61.459 62.003 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav S 11.0438 111.638 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav NS 69.058 73.090 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav S 112.936 113.656 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav NS 83.046 83.114 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav S 16.940 20.050 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav NS 59.794 59.858 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav S 87.280 88.651 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav NS 82.229 82.461 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav S 111.346 111.936 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav NS 83.736 84.446 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav S 77.291 78.421 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav NS 0 4.951 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav S 30.349 32.429 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav NS 100.299 101.647 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav S 46.527 49.147 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav NS 44.747 46.148 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav S 24.551 25.471 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav NS 52.033 52.211 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav S 88.358 93.418 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav NS 46.564 46.788 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav S 10.507 11.077 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav NS 41.099 41.227 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav S 61.072 63.002 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav NS 19.108 19.460 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav S 97.182 97.789 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav NS 71.711 71.732 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav S 114.312 117.115 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav NS 31.065 31.154 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav S 90.346 91.608 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav NS 50.028 51.377 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav S 75.986 76.596 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav NS 12.485 12.709 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav S 72.719 73.046 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav NS 51.923 53.379 /olive//sadRegression/audio/adapt/20131213T104502UTC_18520_A.wav NS 11.1192 112.761 /olive//sadRegression/audio/adapt/20131213T121501UTC_18530_A.wav NS 81.277 82.766 /olive//sadRegression/audio/adapt/20131213T124501UTC_18533_A.wav NS 83.702 84.501 /olive//sadRegression/audio/adapt/20131213T134502UTC_18567_A.wav NS 69.379 72.258 /olive//sadRegression/audio/adapt/20131217T015001UTC_18707_A.wav NS 5.099 10.507 You may also incorporate the adapt code from olivepy.oliveclient directly into your client: # Setup processing variables (get this config or via command line options plugin = \"sad-dnn\" domain = \"multi-v1\" new_domain_name = \"python_adapted_multi-v2\" # Build the list of files plus the regions in the those files to adapt by parsing the input file: file_annotations = self . parse_annotation_file ( \"lists/adapt.lst\" ) return self . adapt_supervised_old ( plugin , domain , file_annotations , new_domain_name ) Python Workflow Example As previously mentioned, SRI distributes Workflow Definitions as files, which are preconfigured to perform tasks such as SAD, LID, SID, etc. The example below uses a Workflow Definition file with the SRI implemented Python Workflow API to make a SAD and LID request via a Workflow Definition file: import os , json , sys import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow import olivepy.messaging.msgutil as msgutil from olivepy.messaging.msgutil import InputTransferType # Create a connection to a local OLIVE server client = oc . AsyncOliveClient ( \"test olive client\" , 'localhost' ) client . connect () # Example workflow definition file: workflow_filename = \"~/olive/sad_lid_example.workflow\" workflow_def = ow . OliveWorkflowDefinition ( workflow_filename ) # Submit the workflow definition to the client for actualization (instantiation or sometimes called activation): workflow = workflow_def . create_workflow ( client ) # Prepare a audio file to submit (as a serialzied file) audio_filename = \"~/olive/sad_smoke.wav\" buffers = [] # read in the file as raw bytes so a buffer can be sent to the server as a buffer: data_wrapper = workflow . package_audio ( audio_filename , InputTransferType . SERIALIZED , label = audio_filename ) buffers . append ( data_wrapper ) # OR - if the server and client share a filesystem, uncomment the following to send the path to the audio file instead of a buffer: # data_wrapper = workflow.package_audio(audio_filename, mode=msgutil.InputTransferType.PATH) # Submit the workflow to OLIVE for analysis: response = workflow . analyze ( buffers ) # response will contain an error message if the workflow failed, or results for each task: print ( \"Workflow results:\" ) print ( \" {} \" . format ( response . to_json ( 1 ))) # Be sure to close the connection to the server when done client . disconnect () For the above code, one would see JSON formatted output like this (of course your output will vary by the audio submitted for analysis): Workflow results: [ { \"job_name\" : \"Basic SAD and LID workflow\" , \"data\" : [ { \"data_id\" : \"~/olive/sad_smoke.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 27.9325 , \"number_channels\" : 1 , \"label\" : \"~/olive/sad_smoke.wav\" , \"id\" : \"ebc1dfa7502841216526768a3f94b095b9362f6a37cf903631494e8784471931\" } ], \"tasks\" : { \"Speech Regions\" : { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 1.06 , \"end_t\" : 2.65 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 3.19 , \"end_t\" : 4.18 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 4.84 , \"end_t\" : 10.54 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 10.99 , \"end_t\" : 16.63 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 16.91 , \"end_t\" : 18.18 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 18.25 , \"end_t\" : 19.66 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 21.11 , \"end_t\" : 24.26 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] }, \"plugin\" : \"sad-dnn\" , \"domain\" : \"multi-v1\" }, \"LID\" : { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"eng\" , \"score\" : 1.3094566 }, { \"class_id\" : \"tgl\" , \"score\" : -2.0286703 }, { \"class_id\" : \"apc\" , \"score\" : -2.3600318 }, { \"class_id\" : \"pus\" , \"score\" : -2.6724625 }, { \"class_id\" : \"arb\" , \"score\" : -2.8804097 }, { \"class_id\" : \"arz\" , \"score\" : -3.0850184 }, { \"class_id\" : \"fas\" , \"score\" : -3.1287756 }, { \"class_id\" : \"tur\" , \"score\" : -3.1409845 }, { \"class_id\" : \"spa\" , \"score\" : -4.291254 }, { \"class_id\" : \"yue\" , \"score\" : -4.695897 }, { \"class_id\" : \"urd\" , \"score\" : -5.4247284 }, { \"class_id\" : \"fre\" , \"score\" : -6.19234 }, { \"class_id\" : \"tha\" , \"score\" : -7.447339 }, { \"class_id\" : \"vie\" , \"score\" : -7.9231014 }, { \"class_id\" : \"kor\" , \"score\" : -8.1033945 }, { \"class_id\" : \"jpn\" , \"score\" : -9.810704 }, { \"class_id\" : \"cmn\" , \"score\" : -10.923913 }, { \"class_id\" : \"rus\" , \"score\" : -11.114039 }, { \"class_id\" : \"amh\" , \"score\" : -19.474201 } ] }, \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" } } } ] Creating a Workflow Definition Internally, the OLIVE design uses a simple kitchen metaphor, which is expressed in the elements that compose an OLIVE workflow. If one thinks of OLIVE as a kitchen, orders are submitted to OLIVE, which are then extracted into jobs that are cooked/executed (as tasks). In this analogy, when authoring OLIVE workflows you are authoring both a menu and a recipe. Clients use the Workflow to request an order (analysis, enrollment, unenrollment), while the Workflow also includes the recipe to cook/execute those orders. The \"job\" portion of a Workflow is used to build a simple directed acyclic graph (DAG) within OLIVE. This DAG, which OLIVE refers to as a \"Job Graph\", allows data to be shared by multiple tasks, while also allowing connections between these tasks. When connected, the outputs of upstream tasks (tasks that have been executed) and provided to downstream tasks (tasks that have not yet been executed). For example the output of a SAD task can be supplied to one or more downstream tasks such as LID or SID, so those tasks do not have to internally run SAD. To enable this complexity, the workflow recipe defines a set of elements/objects: WorkflowDefinition - The outer container for all orders and their jobs (Job Graphs) WorkflowOrderDefinition - Used to group jobs into processing pipelines for analysis, enrollment, or unenrollment activities JobDefinition - Defines the set of tasks that are executed in a DAG and how task output is shared/returned. To simplify authoring, tasks in a job are executed sequentially. WorkflowTask - The task that is executed, which is usually implemented by a plugin A quick note on the structure of elements: The WorkflowDefinition, WorkflowOrderDefinition, JobDefinition, and WorkflowTask are all based on a Protobuf message of the same name in the Enterprise API. Should you have questions about these elements it maybe be helpful to refer to these protobuf messages; however, the relationship between these messages/elements is not clear when looking at the Enterprise API messages, so the tables below include a \"cardinality\" column to make it clearer about the relationship between these elements. WorkflowDefinition At the top-level, the Workflow Definition element includes these required and optional fields: Attribute Type Cardinality Description order WorkflowOrderDefinition 1 to 3 See WorkflowOrderDefinition below. There must be one or more of these elements actualized boolean 1 Always set to 'false' when creating a new Workflow. This is an internal field set by the OLIVE server when actualized version string 0 or 1 An optional user defined version number for this workflow description string 0 or 1 An optional description of this workflow created DateTime 0 or 1 An optional date this workflow was created updated DateTime 0 or 1 An optional date this workflow was updated An example Workflow Definition element (the WorkflowOrderDefinition element(s) within order defined later to avoid confusion ) { \"order\" : [], \"actualized\" : false , \"version\" : \"1.0\" , \"description\" : \"Example of a Workflow Definition Object\" , \"created\" : { \"year\" : 2021 , \"month\" : 6 , \"day\" : 29 , \"hour\" : 8 , \"min\" : 0 , \"sec\" : 0 } } WorkflowOrderDefinition Within a WorkflowDefinition element there can be 1 to 3 WorkflowOrderDefinition elements defined (one for analysis, one for enrollment, and one for unenrollment). The WorkflowOrderDefinition includes these required and optional fields: Attribute Type Cardinality Description workflow_type WorkflowType 1 One of: 'WORKFLOW_ANALYSIS_TYPE', 'WORKFLOW_ENROLLMENT_TYPE', or 'WORKFLOW_UNENROLLMENT_TYPE' job_definition JobDefinition 1 or more See JobDefinition element below. An order must have at least one job order_name JobDefinition 1 or more See JobDefinition element below. An order must have at least one job Below is an example of a WorkflowOrderDefinition (JobDefinition element(s) defined later). 1 to 3 of these elements can be added to a WorkflowDefintion: { \"workflow_type\" : \"WORKFLOW_ANALYSIS_TYPE\" , \"job_definition\" : [], \"order_name\" : \"Analysis Order\" } JobDefinition One or more JobDefinition elements must be added to WorkflowOrderDefinition element (see above). This element includes these required and optional fields: Attribute Type Cardinality Description job_name string 1 A unique name for this job tasks WorkflowTask 1 or more See the WorkflowTask below. There must be at least one task defined for a Job. Assume tasks are executed in the order they are defined in this list. data_properties DataHandlerProperty 1 See DataHandlerProperty below. This defines the data (normally audio) properties for all tasks in this job description string 0 or 1 An optional description processing_type WorkflowJobType 1 Optional, do not specify unless creating a conditional workflow. The default value is \"MERGE\", with allowable values: \"MERGE\", \"PARALLEL\", or \"SEQUENTIAL\". See section on conditional workflows conditional_job_output boolean 0 or 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows dynamic_job_name string 0 or 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows resolved bool 0 or 1 DO NOT SET. This value is assgiend by the server transfer_result_labels string 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows Here is an example of JobDefinition element (the WorkflowTask and DataHandlerProperty elements are defined below). One or more of these elements must be added to a WorkflowOrderDefinition. [ { \"job_name\" : \"SAD, LID, SID analysis with LID and SID enrollment\" , \"tasks\" : [], \"data_properties\" : {} } ] WorkflowTask One or more WorkflowTask elements must be defined for a JobDefinition element (see above). This element includes these required and optional fields: Attribute Type Cardinality Description message_type MessageType 1 The type of the task. One of: \"REGION_SCORER_REQUEST\", \"GLOBAL_SCORER_REQUEST\", \"FRAME_SCORER_REQUEST\", \"AUDIO_ALIGN_REQUEST\", \"CLASS_MODIFICATION_REQUEST\", \"CLASS_REMOVAL_REQUEST\", or \"PLUGIN_2_PLUGIN_REQUEST\" message_data varies 1 This element contains values based on the message type. For non-enrollment requests this is a dictionary that contains the keys 'plugin', and 'domain' For enrollment tasks, the dictionary also includes \"class id\": \"None\" trait_output TraitType 1 Classifies the type of output produced by this task. Must be one of: \"GLOBAL_SCORER\", 'REGION_SCORER', 'FRAME_SCORER', \"CLASS_MODIFIER\", \"PLUGIN_2_PLUGIN\", \"AUDIO_ALIGNMENT_SCORER\" task string 1 A label used to define the task type. By convention of one: \"SAD\", \"LID\", \"SID\", \"LDD\", \"SDD\", \"QBE\", \"ASR\", etc; however, one can define your own type name. For example if you prefere \"STT\" to \"ASR\" consumer_data_label string 1 This task consumes data data input having this name, which is 'audio' for almost all tasks. If using a non 'audio' lable, then this value must match a 'consumer_data_label' used in the workflow's DataHandlerProperty. consumer_result_label string 1 The unique name assigned to this task. Each task in a job must specify a unique name, which is often the same as task. One can also consider this 'task_id' return_result bool 0 or 1 If true, then output produced by this task is returned to the client. option_mappings OptionMap 0 or more This is used to connect the outputs from one or more upstream tasks to this (plugin) task. If one or more values are defined in this mapping, then any upstream (completed) tasks that produced output (defined by the tasks 'consumer_result_label') matching the value of 'workflow_keyword_name' are added to the option dictionary as 'plugin_keyword_name'. allow_failure bool zero or 1 If true, then this task can fail without stopping the execution of workflow. If false then a failure of this task will prevent downstream tasks/jobs from being ran supported_options OptionDefinition 0 or more DO NOT SET. This is set by the server when the workflow is actualized, letting clients know the options supported by the plugin/task available on the server class_id string 0 or 1 NOT YET SUPPORTED - class IDs can be added to the message_data section (if supported by the message type) description string 0 or 1 An optional description of this task Example of defining a SAD task: { \"message_type\" : \"REGION_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"sad-dnn-v7.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"REGION_SCORER\" , \"task\" : \"SAD\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SAD\" , \"return_result\" : true } Example of defining an enrollment task for a LID plugin (note the inclusion of class_id in the message_data element): { \"message_type\" : \"CLASS_MODIFICATION_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"LID_Enroll\" , \"return_result\" : true , \"allow_failure\" : false } DataHandlerProperty The JobDefinition element requires a DataHandlerProperty element that defines the type of data (currently only supporting 'audio') data handling properties used by tasks in the job. This element includes these required and optional fields: Attribute Type Cardinality Description min_number_inputs int 1 The minimum number of data inputs required for a job. This value can be 0, but almost all tasks require 1 audio input. An audio comparison task is one of the few tasks that will require 2 inputs max_number_inputs int 0 or 1 Optional value, for furture use of batch processing of tasks that consume more than one input. This specifies the max number of data inputs consumed by task(s) in the job when doing batch processing, but is not currently used by any tasks type InputDataType 1 For now use \"AUDIO\", but can be one of \"AUDIO\", \"VIDEO\", \"TEXT\", or \"IMAGE\" preprocessing_required boolean 1 Set to 'true'. Not configurable at this time resample_rate int 0 or 1 Do not specify. Currently a value of 8000 is used mode MultiChannelMode 0 or 1 One of \"MONO\", \"SPLIT\", or \"SELECTED\". This determines how multi channel audio is handled in a workflow, with \"MONO\" being the default. In \"MONO\" mode, any multi channel data/audio is converted to mono when processed by task(s) in this job. For \"SPLIT\" each channel is handled by the task(s) in this job, so there is a set of results for each channel. For \"SELECTED\" a channel number must be provided when packaing the audio for the workflow and that channel is used for the job task(s) consumer_data_label string 0 or 1 Data supplied for Analysis, Enrollment, or Unenrollment is labeled by this name when passed to tasks within the job. Bu default use a value of 'audio' { \"data_properties\" : { \"min_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"mode\" : \"MONO\" } } API Specification Auto generated PyDoc class olivepy.api.workflow.OliveWorkflowDefinition(filename) Used to load a Workflow Definition from a file. Parameters - filename: the path/filename of a workflow definition file to load get_json(indent=1) Create a JSON structure of the Workflow Returns A JSON (dictionary) representation of the Workflow Definition to_json(indent=None) Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document Returns the Workflow Definition as as JSON string: create_workflow(client) Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, enrollment, or adaptation requests Parameters client ( AsyncOliveClient ) \u2013 an open client connection to an OLIVE server Returns a new OliveWorkflow object, which has been actualized (activated) by the olive server exception olivepy.api.workflow.WorkflowException() This exception means that an error occurred handling a Workflow class olivepy..workflow.OliveWorkflow(olive_async_client: olivepy.api.olive_async_client.AsyncOliveClient, actualized_workflow: olivepy.messaging.response.OliveWorkflowActualizedResponse) An OliveWorkflow instance represents a Workflow Definition actualized by an OLIVE server. An OliveWorkflow instance is used to make analysis, or enrollment requests. An OliveWorkflow should be created using an OliveWorkflowDefinition's create_workflow() method. All calls to the server include an optional callback. When the callback is provided, the call does not block and the callback method is invoked when a response is received from the server. A callback method has 3 arguments: the original request, the response, and an error message if the request failed. Parameters filename : the name of the workflow definition file Raises WorkflowException \u2013 If the workflow was not actualized get_analysis_job_names() The names of analysis jobs in this workflow (usually only one analysis job) Return type `List` [ `str` ] Returns A list of analysis job names in this workflow get_enrollment_job_names() The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment Return type `List` [ `str` ] Returns A list of enrollment job names in this workflow get_unenrollment_job_names() The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment Return type List [ str ] Returns A list of un-enrollment job names in this workflow get_analysis_tasks(job_name=None) Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) Parameters job_name - ( Optional [ str ]) \u2013 filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. Return type List [ str ] Returns A list of task names get_enrollment_tasks(job_name=None, type=2) Return a list of tasks that support enrollment in this workflow. Parameters job_name ( Optional [ str ]) \u2013 optionally the name of the enrollment job. Optional since most workflows only support one job Return type List [ str ] Returns a list of task names get_unenrollment_tasks(job_name=None) Return a list of tasks that support UNenrollment in this workflow. Parameters job_name ( Optional [ str ]) \u2013 optionally the name of the enrollment job. Optional since most workflows only support one job Return type List [ str ] Returns a list of task names get_analysis_task_info() A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is not known until runtime) Return type List [ Dict [ str , Dict ]] Returns JSON structured detailed information of analysis tasks used in this workflow to_json(indent=None) Generate the workflow as a JSON string Parameters indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document Returns the Workflow Definition as as JSON string: serialize_audio(filename) Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters filename ( str ) \u2013 the local path to the file to serialize Return type AnyStr Returns the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples package_audio(audio_data, mode= , annotations=None, task_annotations=None, selected_channel=None, num_channels=None, sample_rate=None, num_samples=None, validate_local_path=True, label=None) Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. Parameters audio_data ( AnyStr ) \u2013 the input data is a string (file path) if mode is \u2018AUDIO_PATH\u2019, otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples mode \u2013 specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. annotations ( Optional [ List [ Tuple [ float , float ]]]) \u2013 optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) task_annotations ( Optional [ Dict [ str , Dict [ str , List [ Tuple [ float , float ]]]]]) \u2013 optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {\u2018SHL\u2019: {\u2018speaker\u2019\u2019:[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the \u2018SHL\u2019 task, which are labeled as class \u2018speaker\u2019 having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . selected_channel ( Optional [ int ]) \u2013 optional - the channel to process if using multi-channel audio num_channels ( Optional [ int ]) \u2013 The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored sample_rate ( Optional [ int ]) \u2013 The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored num_samples ( Optional [ int ]) \u2013 The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored validate_local_path ( bool ) \u2013 If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired label \u2013 an optional name to use with the audio Return type WorkflowDataRequest Returns A populated WorkflowDataRequest to use in a workflow activity package_text(text_input) Not yet supported Parameters text_input ( str ) \u2013 a text input Return type WorkflowDataRequest Returns TBD package_image(image_input) Not yet supported Parameters image_input \u2013 An image input Return type WorkflowDataRequest Returns TBD package_video(video_input) Not yet supported Parameters video_input \u2013 a video input Return type WorkflowDataRequest Returns TBD get_analysis_class_ids(callback=None) Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. Parameters callback \u2013 an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) Return type OliveClassStatusResponse Returns an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server analyze(data_inputs, callback=None, options=None) Perform a workflow analysis Parameters data_inputs ( List [ WorkflowDataRequest ]) \u2013 a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. callback \u2013 an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. options ( Optional [ str ]) \u2013 a JSON string of name/value options to include with the analysis request such as \u2018{\u201cfilter_length\u201d:99, \u201cinterpolate\u201d:1.0, \u201ctest_name\u201d:\u201dmidge\u201d}\u2019 Return type OliveWorkflowAnalysisResponse Returns an OliveWorkflowAnalysisResponse enroll(data_inputs, class_id, job_names, callback=None, options=None) Submit data for enrollment. Parameters data_inputs ( List [ WorkflowDataRequest ]) \u2013 a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. class_id ( str ) \u2013 the name of the enrollment job_names ( List [ str ]) \u2013 a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. callback \u2013 an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. options \u2013 a dictionary of name/value option pairs to include with the enrollment request Returns unenroll(class_id, job_names, callback=None, options=None) Submit a class id (speaker name, language name, etc) for un-enrollment. Parameters class_id ( str ) \u2013 the name of the enrollment class to remove job_names ( List [ str ]) \u2013 a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). callback \u2013 an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. options \u2013 a dictionary of name/value option pairs to include with the enrollment request Returns class olivepy.api.olive_async_client.AsyncOliveClient(client_id, address='localhost', request_port=5588, timeout_second=10) Bases: threading.Thread This class is used to make asynchronous requests to the OLIVE server connect(monitor_status=False) Connect this client to the server Parameters monitor_server \u2013 if true, starts a thread to monitor the server status connection for heartbeat messages add_heartbeat_listener(heartbeat_callback) Register a callback function to be notified when a heartbeat is received from the OLIVE server Parameters heartbeat_callback ( Callable [[ Heartbeat ], None ]) \u2013 The callback method that is notified each time a heartbeat message is received from the OLIVE server clear_heartbeat_listeners() Remove all heartbeat listeners enqueue_request(message, callback, wrapper=None) Add a message request to the outbound queue Parameters message \u2013 the request message to send callback \u2013 this is called when response message is received from the server wrapper \u2013 the message wrapper sync_request(message, wrapper=None) Send a request to the OLIVE server, but wait for a response from the server Parameters message \u2013 the request message to send to the OLIVE server Returns the response from the server run() Starts the thread to handle async messages disconnect() Closes the connection to the OLIVE server is_connected() Status of the connection to the OLIVE server Returns True if connected classmethod setup_multithreading() This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. request_plugins(callback=None) Used to make a PluginDirectoryRequest Parameters callback ( Optional [ Callable [[ OliveServerResponse ], None ]]) \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult) get_update_status(plugin, domain, callback=None) Used to make a GetUpdateStatusRequest Parameters callback ( Optional [ Callable [[ OliveServerResponse ], None ]]) \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult load_plugin_domain(plugin, domain, callback) Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest messge) :plugin: the name of the plugin to pre-load :domain: the name of hte domain to pre-load :param callback: optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the update status of the request (LoadPluginDomainResult) unload_plugin_domain(plugin, domain, callback) Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded plugin from server memory) Plugin the name of the plugin to unload Domain the name of hte domain to unload Parameters callback \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the status of the request (RemovePluginDomainResult) update_plugin_domain(plugin, domain, metadata, callback) Used to make a ApplyUpdateRequest plugin: the name of the plugin to update :domain: the name of hte domain to update :param callback: optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the status of the request (ApplyUpdateResult) get_active(callback) Used to make a GetActiveRequest Parameters callback \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the status of the request (GetActiveResult) get_status(callback=None) Used to make a GetStatusRequest and receive a GetStatusResult Parameters callback ( Optional [ Callable [[ OliveServerResponse ], None ]]) \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse that contains the most recent server status (GetStatusResult) analyze_frames(plugin, domain, audio_input, callback, mode= , opts=None) Request a analysis of \u2018filename\u2019, returning frame scores. Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain audio_input \u2013 the audio to score callback \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode opts \u2013 a dictionary of name/value pair options for this plugin request Returns a OliveServerResponse containing the status of the request (FrameScorerResult) analyze_regions(plugin, domain, filename, callback, mode= ) Request a analysis of \u2018filename\u2019, returning regions Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain filename \u2013 the name of the audio file to score callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode Returns a OliveServerResponse containing the status of the request (RegionScorerResult) analyze_global(plugin, domain, audio_input, callback, mode= ) Request a global score analysis of \u2018filename\u2019 Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain audio_input \u2013 the name of the audio file to score callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode Returns a OliveServerResponse containing the status of the request (GlobalScorerResult) enroll(plugin, domain, class_id, audio_input, callback, mode= ) Request a enrollment of \u2018audio\u2019 Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain class_id \u2013 the name of the class (i.e. speaker) to enroll audio_input \u2013 the Audio message to add as an enrollment addition callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode Returns a OliveServerResponse containing the status of the request (ClassModificationResult) unenroll(plugin, domain, class_id, callback) Unenroll class_id Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain class_id \u2013 the name of the class (i.e. speaker) to remove callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the status of the request (ClassRemovalResult) audio_modification(plugin, domain, audio_input, callback, mode= ) Used to make a AudioModificationRequest (enhancement). Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain audio_input \u2013 the audio path or buffer to submit for modification callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode Returns a OliveServerResponse containing the status of the request (AudioModificationResult)","title":"Workflows"},{"location":"workflows.html#olive-python-workflow-api","text":"","title":"OLIVE Python Workflow API"},{"location":"workflows.html#introduction","text":"The OLIVE Workflow API extends the OLIVE Enterprise API to simplify working with an OLIVE server, allowing clients to request multiple OLIVE tasks with one API call. This eliminates the more verbose and complex calls necessary when using the Enterprise API. In particular, this framework will encapsulate 'feeding' functionality, to link together tasks, like speech activity detection and a plugin that uses speech regions or frames in its processing, or to 'bundle' multiple requests in a single call, rather than having every API call to a plugin be a separate action. The Workflow API is based around binary or text \"Workflow Definition\" files that SRI distributes to clients. These files contain a 'recipe' to be executed on an OLIVE server to perform analysis, enrollment, and eventually adaption using one or more audio files/inputs. This places the work of specifying how to execute a complex task to execute within the OLIVE server, instead of the client. To use this API, a client submits the SRI provided Workflow Definition file to an OLIVE server. The server verifies it can run the Workflow Definition through a process called \"actualization\". If successful, an activated Workflow is returned to the client. This activated Workflow is then ready for one or more analysis (or enrollment) requests. These requests can be made numerous times with one or more audio submissions. OLIVE still supports the original OLIVE Enterprise API that was based on creating and sending Google Protocol Buffers (Protobuf) message for each request, so if desired, clients can combine classic OLIVE API calls with the Workflow API to implement advanced functionality. For advanced users, see Creating a Workflow Definition for information on composing your own Workflow Definition files. For advanced users, see Creating a Workflow Definition for information on composing your own Workflow Definition files. For more information about working with OLIVE, please see: The OLIVE Server The OLIVE Plugins The OLIVE Enterprise API","title":"Introduction"},{"location":"workflows.html#useful-concepts-to-know","text":"Workflow Definition - distributed as a file (text or binary) with these characteristics: Similar to Plugins, Workflows are independent of the OLIVE software and can be updated and/or distributed outside of an OLIVE software release. Can be submitted to multiple OLIVE servers for actualization in parallel, where actualization is the process of verifying that the server can perform the activities defined in a Workflow Definition. Each actualized Workflow is considered unique to the server where it was actualized, as it is possible the plugin/domain names could vary by each server that actualizes the same WorkflowDefinition Most Workflow Definitions are implemented with the specific names of plugins/domains delivered with the OLIVE system. Changing the name of a plugin or domain will cause workflows that use them to cease to function. Order: a set one or more jobs. There are 3 supported Workflow orders: analysis, enrollment, and unenrollment. Analysis orders require at least one data (audio) input, enrollment orders require one or more data (audio) inputs, plus a class ID, unenrollment inputs do not accept data, only a class ID. Job: A set of tasks, plus depending on the order (analysis, enrollment, unenrollment) may include audio (data) and/or as class ID. An analysis order typically only includes one job, with that job accepting one audio input. An enrollment order may have multiple jobs, where each jobs has it's own set of tasks, one or more data/audio(s), and a class id (i.e. speaker name). Only tasks that support the Class Enroller trait such as speaker enrollment for SID and language enrollment for LID can be part of an enrollment job. Each enrollment job should support enrollment for a single class enroller. This allows enrollments (such as for LID and SID) to be handled separately, since it is very unlikely the same audio and class id (speaker name/language name) would be used for both tasks. Unenrollment jobs are similar to enrollment jobs, except that do NOT consume audio/data. They only accept a class ID Tasks: at the lowest level, a Workflow is composed of one or more tasks, such as SAD, LID, SID, QBE, ASR, etc. A task typically maps to a traditional plugin/domain, but as we expand the capabilities of the Workflow API, 'tasks' are likely to include functionality that is not implemented by a traditional OLIVE plugin, but by private, helper, components that assist other tasks in the workflow but do not return values to the user.","title":"Useful Concepts to Know"},{"location":"workflows.html#limitations","text":"The Workflow API is a new OLIVE extension whose interfaces are subject to change. This release covers two types of behavior: bundling of disparate tasks together into a single call and restricted feeding processes, wherein the output of one plugin may be fed into one or more other \"downstream\" plugins that will use this information in their processing. The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the plugin must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). The most common example in this is SAD feeding other processes. The plugins that receive output from other plugins in a workflow must have an interface that accepts this input. I.e. to use SAD output the downstream plugin(s) must be designed to accept SAD input in addition to the usual audio inputs. Current workflows only allow for categorical feeding (e.g. SAD output always goes to SID input) but not conditional feeding behavior (audio goes to the ASR English domain if LID detects file is English). Further, the current implementation of workflows does not support tasks that create entirely new domains, like adaptation. This is much easier to accomplish with a direct call to the plugin, as bundling and feeding behavior is not relevant to this activity.","title":"Limitations"},{"location":"workflows.html#python-client-install","text":"The OLIVE Python API is distributed as a Python wheel package. To install, navigate into the 'api' folder distributed with your OLIVE delivery, and install it using your native pip3: $ pip3 install olivepy-5.3.0-py3-none-any.whl Or if installing on CentOS, the wheel package includes all 3rd party dependencies, so OlivePy can be installed on a standalone (no Internet) system: pip3 install -r requirements.txt --use-wheel --no-index --find-links wheelhouse olivepy-5.3.0-py3-none-any.whl This installs OLIVE and its dependencies into your local Python distribution. The client source is also available in the olivepy-5.2.0.tar.gz package. OLIVE client dependencies include: protobuf soundfile (suggested) numpy zmq python3","title":"Python Client Install"},{"location":"workflows.html#integration","text":"","title":"Integration"},{"location":"workflows.html#analysis-request","text":"With a Workflow Definition file, it only takes a few steps to make an analysis request. In the following example, a Workflow Definition file (sad_lid_sid.workflow) that supports SAD, LID, and SID analysis is used to make a request: import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow from olivepy.messaging.msgutil import InputTransferType import os client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) workflow = owd . create_workflow ( client ) # Send audio as a serialized buffer buffer = workflow . package_audio ( '~/olive/sad_smoke.wav' , InputTransferType . SERIALIZED , label = os . path . basename ( 'sad_smoke.wav' )) response = workflow . analyze ([ buffer ]) The analysis response can be pretty printed as JSON: print ( \"Workflow Results: {} \" . format ( response . to_json ( indent = 1 ))) Work fl ow Resul ts : [ { \"job_name\" : \"SAD, LID, and SID workflow\" , \"data\" : [ { \"data_id\" : \"sad_smoke.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 27.9325 , \"number_channels\" : 1 , \"label\" : \"sad_smoke.wav\" , \"id\" : \"ebc1dfa7502841216526768a3f94b095b9362f6a37cf903631494e8784471931\" } ], \"tasks\" : { \"SAD\" : { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 1.06 , \"end_t\" : 2.65 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 3.19 , \"end_t\" : 4.18 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 4.84 , \"end_t\" : 10.54 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 10.99 , \"end_t\" : 16.63 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 16.91 , \"end_t\" : 18.18 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 18.25 , \"end_t\" : 19.66 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 21.11 , \"end_t\" : 24.26 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] }, \"plugin\" : \"sad-dnn-v7.0.1\" , \"domain\" : \"multi-v1\" }, \"LID\" : { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"eng\" , \"score\" : 1.3094566 }, { \"class_id\" : \"tgl\" , \"score\" : -2.0286703 }, { \"class_id\" : \"apc\" , \"score\" : -2.3600318 }, { \"class_id\" : \"pus\" , \"score\" : -2.6724625 }, { \"class_id\" : \"arb\" , \"score\" : -2.8804097 }, { \"class_id\" : \"arz\" , \"score\" : -3.0850184 }, { \"class_id\" : \"fas\" , \"score\" : -3.1287756 }, { \"class_id\" : \"tur\" , \"score\" : -3.1409845 }, { \"class_id\" : \"spa\" , \"score\" : -4.291254 }, { \"class_id\" : \"yue\" , \"score\" : -4.695897 }, { \"class_id\" : \"urd\" , \"score\" : -5.4247284 }, { \"class_id\" : \"fre\" , \"score\" : -6.19234 }, { \"class_id\" : \"tha\" , \"score\" : -7.447339 }, { \"class_id\" : \"vie\" , \"score\" : -7.9231014 }, { \"class_id\" : \"kor\" , \"score\" : -8.1033945 }, { \"class_id\" : \"jpn\" , \"score\" : -9.810704 }, { \"class_id\" : \"cmn\" , \"score\" : -10.923913 }, { \"class_id\" : \"rus\" , \"score\" : -11.114039 }, { \"class_id\" : \"amh\" , \"score\" : -19.474201 } ] }, \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" }, \"SID\" : { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"SID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"test_speaker\" , \"score\" : -3.035223 } ] }, \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" } } } ] In the above example, results for SAD, LID, and SID analysis are output.","title":"Analysis Request"},{"location":"workflows.html#enrollment-request","text":"Some workflows support enrollment for one or more jobs. To list the jobs that support enrollment in a workflow, use the OliveWorkflow get_enrollment_job_names method: import olivepy.api.olive_async_client as oc , os import olivepy.api.workflow as ow from olivepy.messaging.msgutil import InputTransferType client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/sad-lid-sid_enroll-lid-sid_example.workflow\" ) workflow = owd . create_workflow ( client ) workflow . get_analysis_tasks () # ['SAD', 'LID', 'SID'] print ( \"Enrollment Jobs: {} \" . format ( workflow . get_enrollment_job_names ())) # Enrollment jobs '['SID Enrollment', 'LID Enrollment']' Shown above, a workflow that supports SAD, LID, and SID analysis, while also supporting SID and/or LID enrollment. To enroll a speaker (generically a class id) via this workflow, use the OliveWorkflow's enroll method: ... buffer = workflow . package_audio ( '~/olive/English.wav' , InputTransferType . SERIALIZED , label = os . path . basename ( 'English.wav' )) response = workflow . enroll ([ buffer ], 'example speaker' , [ 'SID Enrollment' ]) print ( response . to_json ( indent = 1 )) Which prints: [ { \"job_name\" : \"SID Enrollment\" , \"data\" : [ { \"data_id\" : \"English.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 5.0 , \"number_channels\" : 1 , \"label\" : \"English.wav\" , \"id\" : \"32b34b7a675e47c0a4eb7a6a6ff8cc5d470beafda6cf6e53c5a12b4f711ea37a\" } ], \"tasks\" : { \"SID Enrollment\" : { \"task_trait\" : \"CLASS_MODIFIER\" , \"task_type\" : \"SID\" , \"message_type\" : \"CLASS_MODIFICATION_RESULT\" , \"analysis\" : { \"addition_result\" : [ { \"successful\" : true , \"label\" : \"English.wav\" } ] }, \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"example speaker\" } } } ] NOTE: since there was only one enrollment job, the enrollment request could have been made without specifying the job name (as shown below). Not specifying the job name will result in enrolling for all jobs, so it is a best practice to explicitly request the enrollment job. ... response = workflow . enroll ([ buffer ], 'example speaker' ) ... To confirm the new class/speaker name was added: class_status_response = workflow . get_analysis_class_ids () print ( \"Analysis Class Info: {} \" . format ( class_status_response . to_json ( indent = 1 ))) A nal ysis Class I nf o : { \"job_class\" : [ { \"job_name\" : \"SAD, LID, SID analysis with LID and/or SID enrollment\" , \"task\" : [ { \"task_name\" : \"SAD\" , \"class_id\" : [ \"speech\" ] }, { \"task_name\" : \"LID\" , \"class_id\" : [ \"amh\" , \"apc\" , \"arb\" , \"arz\" , \"cmn\" , \"eng\" , \"fas\" , \"fre\" , \"jpn\" , \"kor\" , \"pus\" , \"rus\" , \"spa\" , \"tgl\" , \"tha\" , \"tur\" , \"urd\" , \"vie\" , \"yue\" ] }, { \"task_name\" : \"SID\" , \"class_id\" : [ \"test_speaker\" , \"example speaker\" ] } ] } ] } Workflow job that support enrollment, often support un-enrollment. Use the get_unenrollment_job_names() message to list jobs that support unenrollment: print ( \"Unenrollment jobs: {} \" . format ( workflow . get_unenrollment_job_names ())) # Unenrollment jobs: ['SID Unenrollment', 'LID Unenrollment'] response = workflow . unenroll ( 'example speaker' , [ 'SID Unenrollment' ]) print ( \"Workflow unenrollment: {} \" . format ( response . to_json ( indent = 1 ))) Work fl ow u nenr ollme nt : [ { \"job_name\" : \"SID Unenrollment\" , \"data\" : [], \"tasks\" : { \"SID Unenroll\" : { \"task_trait\" : \"CLASS_MODIFIER\" , \"task_type\" : \"SID\" , \"message_type\" : \"CLASS_MODIFICATION_RESULT\" , \"analysis\" : {}, \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"example speaker\" } } } ] Running workflow.get_analysis_class_ids() again would show that 'example speaker' was removed.","title":"Enrollment Request"},{"location":"workflows.html#other-enrollment-options","text":"Enrollment can also be done via the Python CLI tool, olivepyenroll: olivepyenroll -p sid-embed-v6.0.1 --domain multicond-v1 -e Test_Speaker -w test.wav Or incorporate the code from OliveClient (or AsyncOliveClient if client your client needs non-blocking calls): import olivepy.api.oliveclient as oc client = oc . OliveClient ( 'test client' ) client . enroll ( 'sid-embed-v6.0.1' , 'multicond-v1' , 'Test_Speaker' , '/olive/data/test.wav' )","title":"Other Enrollment Options"},{"location":"workflows.html#advanced-workflow-features","text":"","title":"Advanced Workflow Features"},{"location":"workflows.html#audio-submission-options","text":"Audio can be submitted as a path name or as a buffer. If submitted as a path name, then the path must be accessible by the OLIVE server. If submitting audio as a buffer then that buffer can contain either: A 'serialized' file, where the file contents are read into a buffer. PCM-16 encoded samples. If the client has decoded a audio source, then those samples can be submitted directly in the audio buffer. The audio pathname or buffer is wrapped in a WorkflowDataRequest object before submitting to OLIVE. As of OLIVE 5.3, a serialized video file (that contains audio) can also be submitted as a serialized buffer or pathname for audio processing. Sending a local file as a serialized buffer: ... # Submit the serialized audio file into a buffer for submission to OLIVE audio_filename = \"~/audio/test_audio.wav\" # OliveWorkflow provides a helper method to serialize a file: # Wrap the serialized audio buffer in a WorkflowDataRequest: olive_data = workflow . package_audio ( audio_filename , InputTransferType . SERIALIZED , label = os . path . basename ( audio_filename )) Or to send the path to a local file (assuming the server and client are running on the same host or have access to a shared file system): ... # Submit the path of the audio file: audio_filename = \"~/audio/test_audio.wav\" olive_data = workflow . package_audio ( audio_filename , mode = msgutil . InputTransferType . PATH ) Once the data has been wrapped in a WorkflowDataRequest it can be submitted for analysis or enrollment. An example of the AUDIO_DECODED transfer is not provided since, decoded audio requires a 3rd party package to decode audio input.","title":"Audio Submission Options"},{"location":"workflows.html#audio-annotations","text":"The audio submitted for analysis (or enrollment) can be annotated with start/end regions when packaging audio using the OliveWorkflow.package_audio() method. For example, here is how to specify two regions within a file: ... filename = '/home/olive/test.wav' # Provide annotations for two regions: 0.3 to 1.7 seconds, and 2.4 to 3.3 seconds in filename: regions = [( 0.3 , 1.7 ), ( 2.4 , 3.3 )] audio = workflow . package_audio ( filename , InputTransferType . AUDIO_SERIALIZED , annotations = regions ) ...","title":"Audio Annotations"},{"location":"workflows.html#binary-data","text":"New since OLIVE 5.3 is the ability to send audio or other data types such as Video or Images in a more generic container, BinaryMedia, which can contain audio, video, or image data (either as a serialzied file buffer or as filepath, decoded data is NOT supported with this message type). Replace calls to OliveWorkflow.package_audio() with OliveWorkflow.package_binary() to use this new data container.","title":"Binary Data"},{"location":"workflows.html#handling-a-workflow-analysis-response-python","text":"A successful Workflow analysis produces a response that includes information about the audio analyzed and the results of one or more tasks. The Workflow API includes the method, workflow.get_analysis_tasks(), to help clients identify and prepare for the types of tasks a Workflow will produce. The method, get_analysis_tasks(), returns the names of the tasks supported by this workflow. For example: print ( \"Analysis Tasks: {} \" . format ( workflow . get_analysis_tasks ())) # Analysis Tasks: ['SAD', 'LID'] For detailed information about the tasks(): workflow_def_json = owd . to_json ( indent = 1 ) # Pretty print the workflow definition: print ( workflow_def_json ) { \"order\" : [ { \"workflow_type\" : \"WORKFLOW_ANALYSIS_TYPE\" , \"job_definition\" : [ { \"job_name\" : \"SAD, LID, SID analysis with LID and/or SID enrollment\" , \"tasks\" : [ { \"message_type\" : \"REGION_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"sad-dnn-v7.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"REGION_SCORER\" , \"task\" : \"SAD\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SAD\" , \"return_result\" : true }, { \"message_type\" : \"GLOBAL_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"GLOBAL_SCORER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"LID\" , \"return_result\" : true }, { \"message_type\" : \"GLOBAL_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"GLOBAL_SCORER\" , \"task\" : \"SID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SID\" , \"return_result\" : true } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" } } ], \"order_name\" : \"Analysis Order\" }, { \"workflow_type\" : \"WORKFLOW_ENROLLMENT_TYPE\" , \"job_definition\" : [ { \"job_name\" : \"SID Enrollment\" , \"tasks\" : [ { \"message_type\" : \"CLASS_MODIFICATION_REQUEST\" , \"message_data\" : { \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"SID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SID Enrollment\" , \"return_result\" : true , \"allow_failure\" : false } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, \"description\" : \"SID Enrollment\" }, { \"job_name\" : \"LID Enrollment\" , \"tasks\" : [ { \"message_type\" : \"CLASS_MODIFICATION_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"LID Enrollment\" , \"return_result\" : true , \"allow_failure\" : false } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, \"description\" : \"LID Enrollment\" } ], \"order_name\" : \"Enrollment Order\" }, { \"workflow_type\" : \"WORKFLOW_UNENROLLMENT_TYPE\" , \"job_definition\" : [ { \"job_name\" : \"SID Unenrollment\" , \"tasks\" : [ { \"message_type\" : \"CLASS_REMOVAL_REQUEST\" , \"message_data\" : { \"plugin\" : \"sid-dplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"SID\" , \"consumer_data_label\" : \"\" , \"consumer_result_label\" : \"SID Unenroll\" , \"return_result\" : true , \"allow_failure\" : false } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, \"description\" : \"SID UNenrollment job \" }, { \"job_name\" : \"LID Unenrollment\" , \"tasks\" : [ { \"message_type\" : \"CLASS_REMOVAL_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"\" , \"consumer_result_label\" : \"LID Unenroll\" , \"return_result\" : true , \"allow_failure\" : false } ], \"data_properties\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, \"description\" : \"LID UNenrollment job \" } ], \"order_name\" : \"Unenrollment Order\" } ], \"actualized\" : false , \"created\" : { \"year\" : 2021 , \"month\" : 6 , \"day\" : 29 , \"hour\" : 8 , \"min\" : 0 , \"sec\" : 0 } } In the above example, the 'trait_output' attribute, identifies the type of score out produced by the task. This attribute lets clients know the type of score produced by this task. Currently, Workflow tasks return one of these three score (analysis) types: FRAME_SCORER - these tasks create a set of scores for each frame in the submitted audio. Typically frame scoring is returned by SAD plugins to provide a set of frame scores for 'speech'. For users of the Enterprise API, this the output from a FrameScorerResult message. GLOBAL_SCORER - these tasks create a single set of scores (one per class) for an entire audio. In the Enterprise API this maps to a GlobalScorerResult message. REGION_SCORER - these tasks create a set of scores for regions in the audio submission. In the Enterprise API this maps to a RegionScorerResult message","title":"Handling A Workflow Analysis Response (Python)"},{"location":"workflows.html#parsing-analysis-output","text":"Analysis output is grouped into one or more 'jobs', where a job is the audio info plus the analysis task result(s). In the example above, a single audio file was serialized, then submitted to OLIVE for analysis by SAD and LID tasks. This resulted in an OLIVE response containing the SAD and LID task results, plus information about the audio used for analysis. Since only one file was submitted for analysis, only one 'job' was returned. Had two audio files been submitted, then OLIVE would have created two job results - one for each file submitted. Each job will have a data element that describes properties of the audio used for analysis and one or more 'task' elements that contain the analysis results which are either frame scores, global scores, or region scores.","title":"Parsing Analysis Output"},{"location":"workflows.html#frame-scorer-task-output","text":"Tasks that return frame scores, are based on the Enterprise API FrameScoreResult message: // The results from a FrameScorerRequest message FrameScorerResult { repeated FrameScores result = 1 ; // List of frame scores by class_id } // The basic unit of a frame score , returned in a FrameScorerRequest message FrameScores { required string class_id = 1 ; // The class id to which the frame scores pertain required int32 frame_rate = 2 ; // The number of frames per second required double frame_offset = 3 ; // The offset to the center of the frame 'window' repeated double score = 4 [ packed = true ]; // The frame - level scores for the class_id } The JSON output for frame score results has the form: resul t [ { classId : s tr , fra meRa te : i nt , fra meO ffset : double , score :[ double ]} ] Here is an example of SAD output as frame scores: \"tasks\" : { ... \"SAD\" : { \"task_trait\" : \"FRAME_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"FRAME_SCORER_RESULT\" , \"analysis\" : { \"result\" : [ { \"class_id\" : \"speech\" , \"frame_rate\" : 100 , \"frame_offset\" : 0.0 , \"score\" : [ -1.65412407898345 , -1.65412407898345 , -1.6976179167708825 , -1.7201831820448585 , -1.7218198748053783 , -1.7285406659981328 , -1.7403455556231222 , -1.7572345436803467 , -1.779207630169806 , -1.8052880586341415 , ... ] } ] }, \"plugin\" : \"sad-dnn\" , \"domain\" : \"multi-v1\" },","title":"Frame Scorer Task Output"},{"location":"workflows.html#global-scorer-tasks","text":"Tasks that return global scores, are based on the Enterprise API's GlobalScorerResult message: message GlobalScorerResult { repeated GlobalScore score = 1 ; // The class scores } // The global score for a class message GlobalScore { required string class_id = 1 ; // The class required float score = 2 ; // The score associated with the class optional float confidence = 3 ; // An optional confidence value when part of a calibration report optional string comment = 4 ; // An optional suggested action when part of a calibration report } The JSON output for global score results has the form: score [ { classId : s tr , score :[ fl oa t ]} ] Here is an example of LID output as global scores: \"tasks\" : { ... \"LID\" : { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"eng\" , \"score\" : 2.5195693969726562 }, { \"class_id\" : \"apc\" , \"score\" : -3.509812355041504 }, { \"class_id\" : \"tgl\" , \"score\" : -3.566483974456787 }, { \"class_id\" : \"arb\" , \"score\" : -3.994529962539673 }, { \"class_id\" : \"fre\" , \"score\" : -21.55596160888672 } ] }, \"plugin\" : \"lid-embedplda\" , \"domain\" : \"multi-v1\" }","title":"Global Scorer Tasks"},{"location":"workflows.html#region-scorer-tasks","text":"Tasks that return region scores, are based on the Enterprise API's RegionScorerResult message: // The region score result message RegionScorerResult { repeated RegionScore region = 1 ; // The scored regions } // The basic unit a region score . There may be multiple RegionScore values in a RegionScorerResult message RegionScore { required float start_t = 1 ; // Begin - time of the region ( in seconds ) required float end_t = 2 ; // End - time of t he region ( in seconds ) required string class_id = 3 ; // Class ID associated with region optional float score = 4 ; // Optional score associated with the class_id label } The JSON output for region score results has the form: score [ { s tart T : fl oa t , e n dT : fl oa t , classId : s tr , score : fl oa t } ] Here is an example of SAD output as region score: \"tasks\" : { \"SAD\" : { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 3.3399999141693115 , \"end_t\" : 10.5 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] }, \"plugin\" : \"sad-dnn\" , \"domain\" : \"vtd-v1\" },","title":"Region Scorer Tasks"},{"location":"workflows.html#enrolled-classes","text":"To list the current classes (such as speakers for a SID task, or languages for LID) in an analysis workflow, use the OliveWorkflow get_analysis_class_ids method: import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) workflow = owd . create_workflow ( client ) class_status_response = workflow . get_analysis_class_ids () print ( \"Task Class IDs: {} \" . format ( class_status_response . to_json ( indent = 1 ))) E nr ollme nt Class IDs : [ { \"job_name\" : \"SAD LID, SID\" , \"tasks\" : { \"SAD\" : { \"class_id\" : [ \"speech\" ] }, \"LID\" : { \"class_id\" : [ \"amh\" , \"apc\" , \"arb\" , \"arz\" , \"cmn\" , \"eng\" , \"fas\" , \"fre\" , \"jpn\" , \"kor\" , \"pus\" , \"rus\" , \"spa\" , \"tgl\" , \"tha\" , \"tur\" , \"urd\" , \"vie\" , \"yue\" ] }, \"SID\" : { \"class_id\" : [ \"test speaker\" ] } } } ] Note that some tasks, such as SID, support enrollment, so the list of class IDs can can change over time as new enrollments are added.","title":"Enrolled Classes"},{"location":"workflows.html#multi-channel-audio","text":"The default workflow behavior is to merge multi-channel audio into a single channel, which is known as 'MONO' mode. To perform analysis on each channel instead of a merged channel, then the Workflow Definition must be authored with a data mode of 'SPLIT'. When using the split mode, each channel in a multi-channel audio input is \"split\" into a job. Here is a typical workflow that merges any multi-channel audio into a single channel audio input: import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/sad_lid_sid.workflow\" ) workflow = owd . create_workflow ( client ) print ( workflow . get_analysis_task_info ()) [ { \"Data Input\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"MONO\" }, ... Here is a workflow that will split each channel in a multi-channel (stereo) audio input into separate jobs: import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow client = oc . AsyncOliveClient ( \"example client\" ) client . connect () owd = ow . OliveWorkflowDefinition ( \"~/olive/stereo_sid_example.workflow\" ) workflow = owd . create_workflow ( client ) print ( workflow . get_analysis_task_info ()) [ { \"Data Input\" : { \"min_number_inputs\" : 1 , \"max_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"resample_rate\" : 8000 , \"mode\" : \"SPLIT\" }, The above Workflow Definition when applied to stereo file will produce a set of SID scores for each channel.","title":"Multi-channel Audio"},{"location":"workflows.html#adaption-using-a-workflow","text":"Not yet implemented via a Workflow. The adaption process is complicated, time-consuming, and plugin/domain specific. Until Adaptation via a workflow is implemented, please use the SRI provided Python client (olivepylearn) or Java client (OliveLearn) to perform adaptation. To adapt using the olivepylearn utility: olivepylearn --plugin sad-dnn --domain multi-v1 -a TEST_NEW_DOMAIN -i /olive/sadRegression/lists/adapt_s.lst Where that adapt_s.lst looks like this: /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav S 20.469 21.719 /olive/sadRegression/audio/adapt/20131209T225239UTC_10777_A.wav NS 10.8000 10.8229 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav S 72.898 73.748 /olive//sadRegression/audio/adapt/20131209T234551UTC_10782_A.wav NS 42.754 43.010 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav S 79.437 80.427 /olive//sadRegression/audio/adapt/20131210T184243UTC_10791_A.wav NS 61.459 62.003 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav S 11.0438 111.638 /olive//sadRegression/audio/adapt/20131212T030311UTC_10817_A.wav NS 69.058 73.090 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav S 112.936 113.656 /olive//sadRegression/audio/adapt/20131212T052052UTC_10823_A.wav NS 83.046 83.114 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav S 16.940 20.050 /olive//sadRegression/audio/adapt/20131212T064501UTC_10831_A.wav NS 59.794 59.858 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav S 87.280 88.651 /olive//sadRegression/audio/adapt/20131212T084501UTC_10856_A.wav NS 82.229 82.461 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav S 111.346 111.936 /olive//sadRegression/audio/adapt/20131212T101501UTC_10870_A.wav NS 83.736 84.446 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav S 77.291 78.421 /olive//sadRegression/audio/adapt/20131212T104501UTC_10876_A.wav NS 0 4.951 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav S 30.349 32.429 /olive//sadRegression/audio/adapt/20131212T111501UTC_10878_A.wav NS 100.299 101.647 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav S 46.527 49.147 /olive//sadRegression/audio/adapt/20131212T114501UTC_10880_A.wav NS 44.747 46.148 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav S 24.551 25.471 /olive//sadRegression/audio/adapt/20131212T134501UTC_10884_A.wav NS 52.033 52.211 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav S 88.358 93.418 /olive//sadRegression/audio/adapt/20131212T141502UTC_10887_A.wav NS 46.564 46.788 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav S 10.507 11.077 /olive//sadRegression/audio/adapt/20131212T151501UTC_10895_A.wav NS 41.099 41.227 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav S 61.072 63.002 /olive//sadRegression/audio/adapt/20131212T154502UTC_10906_A.wav NS 19.108 19.460 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav S 97.182 97.789 /olive//sadRegression/audio/adapt/20131213T023248UTC_10910_A.wav NS 71.711 71.732 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav S 114.312 117.115 /olive//sadRegression/audio/adapt/20131213T041143UTC_10913_A.wav NS 31.065 31.154 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav S 90.346 91.608 /olive//sadRegression/audio/adapt/20131213T044200UTC_10917_A.wav NS 50.028 51.377 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav S 75.986 76.596 /olive//sadRegression/audio/adapt/20131213T050721UTC_10921_A.wav NS 12.485 12.709 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav S 72.719 73.046 /olive//sadRegression/audio/adapt/20131213T071501UTC_11020_A.wav NS 51.923 53.379 /olive//sadRegression/audio/adapt/20131213T104502UTC_18520_A.wav NS 11.1192 112.761 /olive//sadRegression/audio/adapt/20131213T121501UTC_18530_A.wav NS 81.277 82.766 /olive//sadRegression/audio/adapt/20131213T124501UTC_18533_A.wav NS 83.702 84.501 /olive//sadRegression/audio/adapt/20131213T134502UTC_18567_A.wav NS 69.379 72.258 /olive//sadRegression/audio/adapt/20131217T015001UTC_18707_A.wav NS 5.099 10.507 You may also incorporate the adapt code from olivepy.oliveclient directly into your client: # Setup processing variables (get this config or via command line options plugin = \"sad-dnn\" domain = \"multi-v1\" new_domain_name = \"python_adapted_multi-v2\" # Build the list of files plus the regions in the those files to adapt by parsing the input file: file_annotations = self . parse_annotation_file ( \"lists/adapt.lst\" ) return self . adapt_supervised_old ( plugin , domain , file_annotations , new_domain_name )","title":"Adaption Using a Workflow"},{"location":"workflows.html#python-workflow-example","text":"As previously mentioned, SRI distributes Workflow Definitions as files, which are preconfigured to perform tasks such as SAD, LID, SID, etc. The example below uses a Workflow Definition file with the SRI implemented Python Workflow API to make a SAD and LID request via a Workflow Definition file: import os , json , sys import olivepy.api.olive_async_client as oc import olivepy.api.workflow as ow import olivepy.messaging.msgutil as msgutil from olivepy.messaging.msgutil import InputTransferType # Create a connection to a local OLIVE server client = oc . AsyncOliveClient ( \"test olive client\" , 'localhost' ) client . connect () # Example workflow definition file: workflow_filename = \"~/olive/sad_lid_example.workflow\" workflow_def = ow . OliveWorkflowDefinition ( workflow_filename ) # Submit the workflow definition to the client for actualization (instantiation or sometimes called activation): workflow = workflow_def . create_workflow ( client ) # Prepare a audio file to submit (as a serialzied file) audio_filename = \"~/olive/sad_smoke.wav\" buffers = [] # read in the file as raw bytes so a buffer can be sent to the server as a buffer: data_wrapper = workflow . package_audio ( audio_filename , InputTransferType . SERIALIZED , label = audio_filename ) buffers . append ( data_wrapper ) # OR - if the server and client share a filesystem, uncomment the following to send the path to the audio file instead of a buffer: # data_wrapper = workflow.package_audio(audio_filename, mode=msgutil.InputTransferType.PATH) # Submit the workflow to OLIVE for analysis: response = workflow . analyze ( buffers ) # response will contain an error message if the workflow failed, or results for each task: print ( \"Workflow results:\" ) print ( \" {} \" . format ( response . to_json ( 1 ))) # Be sure to close the connection to the server when done client . disconnect () For the above code, one would see JSON formatted output like this (of course your output will vary by the audio submitted for analysis): Workflow results: [ { \"job_name\" : \"Basic SAD and LID workflow\" , \"data\" : [ { \"data_id\" : \"~/olive/sad_smoke.wav\" , \"msg_type\" : \"PREPROCESSED_AUDIO_RESULT\" , \"mode\" : \"MONO\" , \"merged\" : false , \"sample_rate\" : 8000 , \"duration_seconds\" : 27.9325 , \"number_channels\" : 1 , \"label\" : \"~/olive/sad_smoke.wav\" , \"id\" : \"ebc1dfa7502841216526768a3f94b095b9362f6a37cf903631494e8784471931\" } ], \"tasks\" : { \"Speech Regions\" : { \"task_trait\" : \"REGION_SCORER\" , \"task_type\" : \"SAD\" , \"message_type\" : \"REGION_SCORER_RESULT\" , \"analysis\" : { \"region\" : [ { \"start_t\" : 1.06 , \"end_t\" : 2.65 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 3.19 , \"end_t\" : 4.18 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 4.84 , \"end_t\" : 10.54 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 10.99 , \"end_t\" : 16.63 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 16.91 , \"end_t\" : 18.18 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 18.25 , \"end_t\" : 19.66 , \"class_id\" : \"speech\" , \"score\" : 0.0 }, { \"start_t\" : 21.11 , \"end_t\" : 24.26 , \"class_id\" : \"speech\" , \"score\" : 0.0 } ] }, \"plugin\" : \"sad-dnn\" , \"domain\" : \"multi-v1\" }, \"LID\" : { \"task_trait\" : \"GLOBAL_SCORER\" , \"task_type\" : \"LID\" , \"message_type\" : \"GLOBAL_SCORER_RESULT\" , \"analysis\" : { \"score\" : [ { \"class_id\" : \"eng\" , \"score\" : 1.3094566 }, { \"class_id\" : \"tgl\" , \"score\" : -2.0286703 }, { \"class_id\" : \"apc\" , \"score\" : -2.3600318 }, { \"class_id\" : \"pus\" , \"score\" : -2.6724625 }, { \"class_id\" : \"arb\" , \"score\" : -2.8804097 }, { \"class_id\" : \"arz\" , \"score\" : -3.0850184 }, { \"class_id\" : \"fas\" , \"score\" : -3.1287756 }, { \"class_id\" : \"tur\" , \"score\" : -3.1409845 }, { \"class_id\" : \"spa\" , \"score\" : -4.291254 }, { \"class_id\" : \"yue\" , \"score\" : -4.695897 }, { \"class_id\" : \"urd\" , \"score\" : -5.4247284 }, { \"class_id\" : \"fre\" , \"score\" : -6.19234 }, { \"class_id\" : \"tha\" , \"score\" : -7.447339 }, { \"class_id\" : \"vie\" , \"score\" : -7.9231014 }, { \"class_id\" : \"kor\" , \"score\" : -8.1033945 }, { \"class_id\" : \"jpn\" , \"score\" : -9.810704 }, { \"class_id\" : \"cmn\" , \"score\" : -10.923913 }, { \"class_id\" : \"rus\" , \"score\" : -11.114039 }, { \"class_id\" : \"amh\" , \"score\" : -19.474201 } ] }, \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" } } } ]","title":"Python Workflow Example"},{"location":"workflows.html#creating-a-workflow-definition","text":"Internally, the OLIVE design uses a simple kitchen metaphor, which is expressed in the elements that compose an OLIVE workflow. If one thinks of OLIVE as a kitchen, orders are submitted to OLIVE, which are then extracted into jobs that are cooked/executed (as tasks). In this analogy, when authoring OLIVE workflows you are authoring both a menu and a recipe. Clients use the Workflow to request an order (analysis, enrollment, unenrollment), while the Workflow also includes the recipe to cook/execute those orders. The \"job\" portion of a Workflow is used to build a simple directed acyclic graph (DAG) within OLIVE. This DAG, which OLIVE refers to as a \"Job Graph\", allows data to be shared by multiple tasks, while also allowing connections between these tasks. When connected, the outputs of upstream tasks (tasks that have been executed) and provided to downstream tasks (tasks that have not yet been executed). For example the output of a SAD task can be supplied to one or more downstream tasks such as LID or SID, so those tasks do not have to internally run SAD. To enable this complexity, the workflow recipe defines a set of elements/objects: WorkflowDefinition - The outer container for all orders and their jobs (Job Graphs) WorkflowOrderDefinition - Used to group jobs into processing pipelines for analysis, enrollment, or unenrollment activities JobDefinition - Defines the set of tasks that are executed in a DAG and how task output is shared/returned. To simplify authoring, tasks in a job are executed sequentially. WorkflowTask - The task that is executed, which is usually implemented by a plugin A quick note on the structure of elements: The WorkflowDefinition, WorkflowOrderDefinition, JobDefinition, and WorkflowTask are all based on a Protobuf message of the same name in the Enterprise API. Should you have questions about these elements it maybe be helpful to refer to these protobuf messages; however, the relationship between these messages/elements is not clear when looking at the Enterprise API messages, so the tables below include a \"cardinality\" column to make it clearer about the relationship between these elements.","title":"Creating a Workflow Definition"},{"location":"workflows.html#workflowdefinition","text":"At the top-level, the Workflow Definition element includes these required and optional fields: Attribute Type Cardinality Description order WorkflowOrderDefinition 1 to 3 See WorkflowOrderDefinition below. There must be one or more of these elements actualized boolean 1 Always set to 'false' when creating a new Workflow. This is an internal field set by the OLIVE server when actualized version string 0 or 1 An optional user defined version number for this workflow description string 0 or 1 An optional description of this workflow created DateTime 0 or 1 An optional date this workflow was created updated DateTime 0 or 1 An optional date this workflow was updated An example Workflow Definition element (the WorkflowOrderDefinition element(s) within order defined later to avoid confusion ) { \"order\" : [], \"actualized\" : false , \"version\" : \"1.0\" , \"description\" : \"Example of a Workflow Definition Object\" , \"created\" : { \"year\" : 2021 , \"month\" : 6 , \"day\" : 29 , \"hour\" : 8 , \"min\" : 0 , \"sec\" : 0 } }","title":"WorkflowDefinition"},{"location":"workflows.html#workfloworderdefinition","text":"Within a WorkflowDefinition element there can be 1 to 3 WorkflowOrderDefinition elements defined (one for analysis, one for enrollment, and one for unenrollment). The WorkflowOrderDefinition includes these required and optional fields: Attribute Type Cardinality Description workflow_type WorkflowType 1 One of: 'WORKFLOW_ANALYSIS_TYPE', 'WORKFLOW_ENROLLMENT_TYPE', or 'WORKFLOW_UNENROLLMENT_TYPE' job_definition JobDefinition 1 or more See JobDefinition element below. An order must have at least one job order_name JobDefinition 1 or more See JobDefinition element below. An order must have at least one job Below is an example of a WorkflowOrderDefinition (JobDefinition element(s) defined later). 1 to 3 of these elements can be added to a WorkflowDefintion: { \"workflow_type\" : \"WORKFLOW_ANALYSIS_TYPE\" , \"job_definition\" : [], \"order_name\" : \"Analysis Order\" }","title":"WorkflowOrderDefinition"},{"location":"workflows.html#jobdefinition","text":"One or more JobDefinition elements must be added to WorkflowOrderDefinition element (see above). This element includes these required and optional fields: Attribute Type Cardinality Description job_name string 1 A unique name for this job tasks WorkflowTask 1 or more See the WorkflowTask below. There must be at least one task defined for a Job. Assume tasks are executed in the order they are defined in this list. data_properties DataHandlerProperty 1 See DataHandlerProperty below. This defines the data (normally audio) properties for all tasks in this job description string 0 or 1 An optional description processing_type WorkflowJobType 1 Optional, do not specify unless creating a conditional workflow. The default value is \"MERGE\", with allowable values: \"MERGE\", \"PARALLEL\", or \"SEQUENTIAL\". See section on conditional workflows conditional_job_output boolean 0 or 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows dynamic_job_name string 0 or 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows resolved bool 0 or 1 DO NOT SET. This value is assgiend by the server transfer_result_labels string 1 Optional, do not specify unless creating a conditional workflow. See section on conditional workflows Here is an example of JobDefinition element (the WorkflowTask and DataHandlerProperty elements are defined below). One or more of these elements must be added to a WorkflowOrderDefinition. [ { \"job_name\" : \"SAD, LID, SID analysis with LID and SID enrollment\" , \"tasks\" : [], \"data_properties\" : {} } ]","title":"JobDefinition"},{"location":"workflows.html#workflowtask","text":"One or more WorkflowTask elements must be defined for a JobDefinition element (see above). This element includes these required and optional fields: Attribute Type Cardinality Description message_type MessageType 1 The type of the task. One of: \"REGION_SCORER_REQUEST\", \"GLOBAL_SCORER_REQUEST\", \"FRAME_SCORER_REQUEST\", \"AUDIO_ALIGN_REQUEST\", \"CLASS_MODIFICATION_REQUEST\", \"CLASS_REMOVAL_REQUEST\", or \"PLUGIN_2_PLUGIN_REQUEST\" message_data varies 1 This element contains values based on the message type. For non-enrollment requests this is a dictionary that contains the keys 'plugin', and 'domain' For enrollment tasks, the dictionary also includes \"class id\": \"None\" trait_output TraitType 1 Classifies the type of output produced by this task. Must be one of: \"GLOBAL_SCORER\", 'REGION_SCORER', 'FRAME_SCORER', \"CLASS_MODIFIER\", \"PLUGIN_2_PLUGIN\", \"AUDIO_ALIGNMENT_SCORER\" task string 1 A label used to define the task type. By convention of one: \"SAD\", \"LID\", \"SID\", \"LDD\", \"SDD\", \"QBE\", \"ASR\", etc; however, one can define your own type name. For example if you prefere \"STT\" to \"ASR\" consumer_data_label string 1 This task consumes data data input having this name, which is 'audio' for almost all tasks. If using a non 'audio' lable, then this value must match a 'consumer_data_label' used in the workflow's DataHandlerProperty. consumer_result_label string 1 The unique name assigned to this task. Each task in a job must specify a unique name, which is often the same as task. One can also consider this 'task_id' return_result bool 0 or 1 If true, then output produced by this task is returned to the client. option_mappings OptionMap 0 or more This is used to connect the outputs from one or more upstream tasks to this (plugin) task. If one or more values are defined in this mapping, then any upstream (completed) tasks that produced output (defined by the tasks 'consumer_result_label') matching the value of 'workflow_keyword_name' are added to the option dictionary as 'plugin_keyword_name'. allow_failure bool zero or 1 If true, then this task can fail without stopping the execution of workflow. If false then a failure of this task will prevent downstream tasks/jobs from being ran supported_options OptionDefinition 0 or more DO NOT SET. This is set by the server when the workflow is actualized, letting clients know the options supported by the plugin/task available on the server class_id string 0 or 1 NOT YET SUPPORTED - class IDs can be added to the message_data section (if supported by the message type) description string 0 or 1 An optional description of this task Example of defining a SAD task: { \"message_type\" : \"REGION_SCORER_REQUEST\" , \"message_data\" : { \"plugin\" : \"sad-dnn-v7.0.1\" , \"domain\" : \"multi-v1\" }, \"trait_output\" : \"REGION_SCORER\" , \"task\" : \"SAD\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"SAD\" , \"return_result\" : true } Example of defining an enrollment task for a LID plugin (note the inclusion of class_id in the message_data element): { \"message_type\" : \"CLASS_MODIFICATION_REQUEST\" , \"message_data\" : { \"plugin\" : \"lid-embedplda-v2.0.1\" , \"domain\" : \"multi-v1\" , \"class_id\" : \"none\" }, \"trait_output\" : \"CLASS_MODIFIER\" , \"task\" : \"LID\" , \"consumer_data_label\" : \"audio\" , \"consumer_result_label\" : \"LID_Enroll\" , \"return_result\" : true , \"allow_failure\" : false }","title":"WorkflowTask"},{"location":"workflows.html#datahandlerproperty","text":"The JobDefinition element requires a DataHandlerProperty element that defines the type of data (currently only supporting 'audio') data handling properties used by tasks in the job. This element includes these required and optional fields: Attribute Type Cardinality Description min_number_inputs int 1 The minimum number of data inputs required for a job. This value can be 0, but almost all tasks require 1 audio input. An audio comparison task is one of the few tasks that will require 2 inputs max_number_inputs int 0 or 1 Optional value, for furture use of batch processing of tasks that consume more than one input. This specifies the max number of data inputs consumed by task(s) in the job when doing batch processing, but is not currently used by any tasks type InputDataType 1 For now use \"AUDIO\", but can be one of \"AUDIO\", \"VIDEO\", \"TEXT\", or \"IMAGE\" preprocessing_required boolean 1 Set to 'true'. Not configurable at this time resample_rate int 0 or 1 Do not specify. Currently a value of 8000 is used mode MultiChannelMode 0 or 1 One of \"MONO\", \"SPLIT\", or \"SELECTED\". This determines how multi channel audio is handled in a workflow, with \"MONO\" being the default. In \"MONO\" mode, any multi channel data/audio is converted to mono when processed by task(s) in this job. For \"SPLIT\" each channel is handled by the task(s) in this job, so there is a set of results for each channel. For \"SELECTED\" a channel number must be provided when packaing the audio for the workflow and that channel is used for the job task(s) consumer_data_label string 0 or 1 Data supplied for Analysis, Enrollment, or Unenrollment is labeled by this name when passed to tasks within the job. Bu default use a value of 'audio' { \"data_properties\" : { \"min_number_inputs\" : 1 , \"type\" : \"AUDIO\" , \"preprocessing_required\" : true , \"mode\" : \"MONO\" } }","title":"DataHandlerProperty"},{"location":"workflows.html#api-specification","text":"Auto generated PyDoc","title":"API Specification"},{"location":"workflows.html#class-olivepyapiworkflowoliveworkflowdefinitionfilename","text":"Used to load a Workflow Definition from a file. Parameters - filename: the path/filename of a workflow definition file to load","title":"class olivepy.api.workflow.OliveWorkflowDefinition(filename)"},{"location":"workflows.html#get_jsonindent1","text":"Create a JSON structure of the Workflow Returns A JSON (dictionary) representation of the Workflow Definition","title":"get_json(indent=1)"},{"location":"workflows.html#to_jsonindentnone","text":"Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document Returns the Workflow Definition as as JSON string:","title":"to_json(indent=None)"},{"location":"workflows.html#create_workflowclient","text":"Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, enrollment, or adaptation requests Parameters client ( AsyncOliveClient ) \u2013 an open client connection to an OLIVE server Returns a new OliveWorkflow object, which has been actualized (activated) by the olive server","title":"create_workflow(client)"},{"location":"workflows.html#exception-olivepyapiworkflowworkflowexception","text":"This exception means that an error occurred handling a Workflow","title":"exception olivepy.api.workflow.WorkflowException()"},{"location":"workflows.html#class-olivepyworkflowoliveworkflowolive_async_client-olivepyapiolive_async_clientasyncoliveclient-actualized_workflow-olivepymessagingresponseoliveworkflowactualizedresponse","text":"An OliveWorkflow instance represents a Workflow Definition actualized by an OLIVE server. An OliveWorkflow instance is used to make analysis, or enrollment requests. An OliveWorkflow should be created using an OliveWorkflowDefinition's create_workflow() method. All calls to the server include an optional callback. When the callback is provided, the call does not block and the callback method is invoked when a response is received from the server. A callback method has 3 arguments: the original request, the response, and an error message if the request failed. Parameters filename : the name of the workflow definition file Raises WorkflowException \u2013 If the workflow was not actualized","title":"class olivepy..workflow.OliveWorkflow(olive_async_client: olivepy.api.olive_async_client.AsyncOliveClient, actualized_workflow: olivepy.messaging.response.OliveWorkflowActualizedResponse)"},{"location":"workflows.html#get_analysis_job_names","text":"The names of analysis jobs in this workflow (usually only one analysis job) Return type `List` [ `str` ] Returns A list of analysis job names in this workflow","title":"get_analysis_job_names()"},{"location":"workflows.html#get_enrollment_job_names","text":"The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment Return type `List` [ `str` ] Returns A list of enrollment job names in this workflow","title":"get_enrollment_job_names()"},{"location":"workflows.html#get_unenrollment_job_names","text":"The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment Return type List [ str ] Returns A list of un-enrollment job names in this workflow","title":"get_unenrollment_job_names()"},{"location":"workflows.html#get_analysis_tasksjob_namenone","text":"Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) Parameters job_name - ( Optional [ str ]) \u2013 filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. Return type List [ str ] Returns A list of task names","title":"get_analysis_tasks(job_name=None)"},{"location":"workflows.html#get_enrollment_tasksjob_namenone-type2","text":"Return a list of tasks that support enrollment in this workflow. Parameters job_name ( Optional [ str ]) \u2013 optionally the name of the enrollment job. Optional since most workflows only support one job Return type List [ str ] Returns a list of task names","title":"get_enrollment_tasks(job_name=None, type=2)"},{"location":"workflows.html#get_unenrollment_tasksjob_namenone","text":"Return a list of tasks that support UNenrollment in this workflow. Parameters job_name ( Optional [ str ]) \u2013 optionally the name of the enrollment job. Optional since most workflows only support one job Return type List [ str ] Returns a list of task names","title":"get_unenrollment_tasks(job_name=None)"},{"location":"workflows.html#get_analysis_task_info","text":"A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is not known until runtime) Return type List [ Dict [ str , Dict ]] Returns JSON structured detailed information of analysis tasks used in this workflow","title":"get_analysis_task_info()"},{"location":"workflows.html#to_jsonindentnone_1","text":"Generate the workflow as a JSON string Parameters indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document Returns the Workflow Definition as as JSON string:","title":"to_json(indent=None)"},{"location":"workflows.html#serialize_audiofilename","text":"Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters filename ( str ) \u2013 the local path to the file to serialize Return type AnyStr Returns the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples","title":"serialize_audio(filename)"},{"location":"workflows.html#package_audioaudio_data-mode-annotationsnone-task_annotationsnone-selected_channelnone-num_channelsnone-sample_ratenone-num_samplesnone-validate_local_pathtrue-labelnone","text":"Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. Parameters audio_data ( AnyStr ) \u2013 the input data is a string (file path) if mode is \u2018AUDIO_PATH\u2019, otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples mode \u2013 specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. annotations ( Optional [ List [ Tuple [ float , float ]]]) \u2013 optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) task_annotations ( Optional [ Dict [ str , Dict [ str , List [ Tuple [ float , float ]]]]]) \u2013 optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {\u2018SHL\u2019: {\u2018speaker\u2019\u2019:[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the \u2018SHL\u2019 task, which are labeled as class \u2018speaker\u2019 having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . selected_channel ( Optional [ int ]) \u2013 optional - the channel to process if using multi-channel audio num_channels ( Optional [ int ]) \u2013 The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored sample_rate ( Optional [ int ]) \u2013 The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored num_samples ( Optional [ int ]) \u2013 The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored validate_local_path ( bool ) \u2013 If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired label \u2013 an optional name to use with the audio Return type WorkflowDataRequest Returns A populated WorkflowDataRequest to use in a workflow activity","title":"package_audio(audio_data, mode=, annotations=None, task_annotations=None, selected_channel=None, num_channels=None, sample_rate=None, num_samples=None, validate_local_path=True, label=None)"},{"location":"workflows.html#package_texttext_input","text":"Not yet supported Parameters text_input ( str ) \u2013 a text input Return type WorkflowDataRequest Returns TBD","title":"package_text(text_input)"},{"location":"workflows.html#package_imageimage_input","text":"Not yet supported Parameters image_input \u2013 An image input Return type WorkflowDataRequest Returns TBD","title":"package_image(image_input)"},{"location":"workflows.html#package_videovideo_input","text":"Not yet supported Parameters video_input \u2013 a video input Return type WorkflowDataRequest Returns TBD","title":"package_video(video_input)"},{"location":"workflows.html#get_analysis_class_idscallbacknone","text":"Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. Parameters callback \u2013 an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) Return type OliveClassStatusResponse Returns an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server","title":"get_analysis_class_ids(callback=None)"},{"location":"workflows.html#analyzedata_inputs-callbacknone-optionsnone","text":"Perform a workflow analysis Parameters data_inputs ( List [ WorkflowDataRequest ]) \u2013 a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. callback \u2013 an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. options ( Optional [ str ]) \u2013 a JSON string of name/value options to include with the analysis request such as \u2018{\u201cfilter_length\u201d:99, \u201cinterpolate\u201d:1.0, \u201ctest_name\u201d:\u201dmidge\u201d}\u2019 Return type OliveWorkflowAnalysisResponse Returns an OliveWorkflowAnalysisResponse","title":"analyze(data_inputs, callback=None, options=None)"},{"location":"workflows.html#enrolldata_inputs-class_id-job_names-callbacknone-optionsnone","text":"Submit data for enrollment. Parameters data_inputs ( List [ WorkflowDataRequest ]) \u2013 a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. class_id ( str ) \u2013 the name of the enrollment job_names ( List [ str ]) \u2013 a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. callback \u2013 an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. options \u2013 a dictionary of name/value option pairs to include with the enrollment request Returns","title":"enroll(data_inputs, class_id, job_names, callback=None, options=None)"},{"location":"workflows.html#unenrollclass_id-job_names-callbacknone-optionsnone","text":"Submit a class id (speaker name, language name, etc) for un-enrollment. Parameters class_id ( str ) \u2013 the name of the enrollment class to remove job_names ( List [ str ]) \u2013 a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). callback \u2013 an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. options \u2013 a dictionary of name/value option pairs to include with the enrollment request Returns","title":"unenroll(class_id, job_names, callback=None, options=None)"},{"location":"workflows.html#class-olivepyapiolive_async_clientasyncoliveclientclient_id-addresslocalhost-request_port5588-timeout_second10","text":"Bases: threading.Thread This class is used to make asynchronous requests to the OLIVE server","title":"class olivepy.api.olive_async_client.AsyncOliveClient(client_id, address='localhost', request_port=5588, timeout_second=10)"},{"location":"workflows.html#connectmonitor_statusfalse","text":"Connect this client to the server Parameters monitor_server \u2013 if true, starts a thread to monitor the server status connection for heartbeat messages","title":"connect(monitor_status=False)"},{"location":"workflows.html#add_heartbeat_listenerheartbeat_callback","text":"Register a callback function to be notified when a heartbeat is received from the OLIVE server Parameters heartbeat_callback ( Callable [[ Heartbeat ], None ]) \u2013 The callback method that is notified each time a heartbeat message is received from the OLIVE server","title":"add_heartbeat_listener(heartbeat_callback)"},{"location":"workflows.html#clear_heartbeat_listeners","text":"Remove all heartbeat listeners","title":"clear_heartbeat_listeners()"},{"location":"workflows.html#enqueue_requestmessage-callback-wrappernone","text":"Add a message request to the outbound queue Parameters message \u2013 the request message to send callback \u2013 this is called when response message is received from the server wrapper \u2013 the message wrapper","title":"enqueue_request(message, callback, wrapper=None)"},{"location":"workflows.html#sync_requestmessage-wrappernone","text":"Send a request to the OLIVE server, but wait for a response from the server Parameters message \u2013 the request message to send to the OLIVE server Returns the response from the server","title":"sync_request(message, wrapper=None)"},{"location":"workflows.html#run","text":"Starts the thread to handle async messages","title":"run()"},{"location":"workflows.html#disconnect","text":"Closes the connection to the OLIVE server","title":"disconnect()"},{"location":"workflows.html#is_connected","text":"Status of the connection to the OLIVE server Returns True if connected","title":"is_connected()"},{"location":"workflows.html#classmethod-setup_multithreading","text":"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program.","title":"classmethod setup_multithreading()"},{"location":"workflows.html#request_pluginscallbacknone","text":"Used to make a PluginDirectoryRequest Parameters callback ( Optional [ Callable [[ OliveServerResponse ], None ]]) \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult)","title":"request_plugins(callback=None)"},{"location":"workflows.html#get_update_statusplugin-domain-callbacknone","text":"Used to make a GetUpdateStatusRequest Parameters callback ( Optional [ Callable [[ OliveServerResponse ], None ]]) \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult","title":"get_update_status(plugin, domain, callback=None)"},{"location":"workflows.html#load_plugin_domainplugin-domain-callback","text":"Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest messge) :plugin: the name of the plugin to pre-load :domain: the name of hte domain to pre-load :param callback: optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the update status of the request (LoadPluginDomainResult)","title":"load_plugin_domain(plugin, domain, callback)"},{"location":"workflows.html#unload_plugin_domainplugin-domain-callback","text":"Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded plugin from server memory) Plugin the name of the plugin to unload Domain the name of hte domain to unload Parameters callback \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the status of the request (RemovePluginDomainResult)","title":"unload_plugin_domain(plugin, domain, callback)"},{"location":"workflows.html#update_plugin_domainplugin-domain-metadata-callback","text":"Used to make a ApplyUpdateRequest plugin: the name of the plugin to update :domain: the name of hte domain to update :param callback: optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the status of the request (ApplyUpdateResult)","title":"update_plugin_domain(plugin, domain, metadata, callback)"},{"location":"workflows.html#get_activecallback","text":"Used to make a GetActiveRequest Parameters callback \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the status of the request (GetActiveResult)","title":"get_active(callback)"},{"location":"workflows.html#get_statuscallbacknone","text":"Used to make a GetStatusRequest and receive a GetStatusResult Parameters callback ( Optional [ Callable [[ OliveServerResponse ], None ]]) \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse that contains the most recent server status (GetStatusResult)","title":"get_status(callback=None)"},{"location":"workflows.html#analyze_framesplugin-domain-audio_input-callback-mode-optsnone","text":"Request a analysis of \u2018filename\u2019, returning frame scores. Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain audio_input \u2013 the audio to score callback \u2013 optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode opts \u2013 a dictionary of name/value pair options for this plugin request Returns a OliveServerResponse containing the status of the request (FrameScorerResult)","title":"analyze_frames(plugin, domain, audio_input, callback, mode=, opts=None)"},{"location":"workflows.html#analyze_regionsplugin-domain-filename-callback-mode","text":"Request a analysis of \u2018filename\u2019, returning regions Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain filename \u2013 the name of the audio file to score callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode Returns a OliveServerResponse containing the status of the request (RegionScorerResult)","title":"analyze_regions(plugin, domain, filename, callback, mode=)"},{"location":"workflows.html#analyze_globalplugin-domain-audio_input-callback-mode","text":"Request a global score analysis of \u2018filename\u2019 Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain audio_input \u2013 the name of the audio file to score callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode Returns a OliveServerResponse containing the status of the request (GlobalScorerResult)","title":"analyze_global(plugin, domain, audio_input, callback, mode=)"},{"location":"workflows.html#enrollplugin-domain-class_id-audio_input-callback-mode","text":"Request a enrollment of \u2018audio\u2019 Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain class_id \u2013 the name of the class (i.e. speaker) to enroll audio_input \u2013 the Audio message to add as an enrollment addition callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode Returns a OliveServerResponse containing the status of the request (ClassModificationResult)","title":"enroll(plugin, domain, class_id, audio_input, callback, mode=)"},{"location":"workflows.html#unenrollplugin-domain-class_id-callback","text":"Unenroll class_id Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain class_id \u2013 the name of the class (i.e. speaker) to remove callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse Returns a OliveServerResponse containing the status of the request (ClassRemovalResult)","title":"unenroll(plugin, domain, class_id, callback)"},{"location":"workflows.html#audio_modificationplugin-domain-audio_input-callback-mode","text":"Used to make a AudioModificationRequest (enhancement). Parameters plugin \u2013 the name of the plugin domain \u2013 the name of the plugin domain audio_input \u2013 the audio path or buffer to submit for modification callback \u2013 optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse mode \u2013 the audio transfer mode Returns a OliveServerResponse containing the status of the request (AudioModificationResult)","title":"audio_modification(plugin, domain, audio_input, callback, mode=)"},{"location":"olivepy-docs/api.html","text":"olivepy api module olivepy.api.oliveclient ClientBrokerWorker ( Thread ) Performs async interactions with Olive run ( self ) Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/oliveclient.py def run ( self ): logging . debug ( \"Starting Olive Status Monitor Worker for id: {} \" . format ( self . client_id )) self . working = True self . status_socket . subscribe ( \"\" ) poller = zmq . Poller () poller . register ( self . status_socket , zmq . POLLIN ) while self . working : # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . status_socket in socks : logging . debug ( \"Received status message from OLIVE...\" ) heatbeat_data = self . status_socket . recv () heatbeat = Heartbeat () heatbeat . ParseFromString ( heatbeat_data ) # do something with heartbeat... if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats logging . info ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) logging . info ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) logging . info ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) logging . info ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) logging . info ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) logging . info ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) logging . debug ( \"Number active jobs: \" + str ( stats . pool_busy )) logging . debug ( \"Number pending jobs: \" + str ( stats . pool_pending )) logging . debug ( \"Number finished jobs: \" + str ( stats . pool_finished )) logging . debug ( \"Max number jobs: \" + str ( stats . max_num_jobs )) logging . debug ( \"Server version: \" + str ( stats . server_version )) self . status_socket . close () OliveClient This is a simplified version of network library used to contact the Olive server via python code. All OLIVE calls below are synchronous, and block and until a response is received from the OLIVE server. These example API calls are intended to make working with the OLIVE API clearer since all calls are blocking. To make asynchronous requests to the OLIVE server use olivepy.api.olive_async_client.AsyncOliveClient for your enterprise application. __init__ ( self , client_id , address = 'localhost' , request_port = 5588 , timeout_second = 10 ) special Parameters: Name Type Description Default client_id The unique name of this client. Due to a ZMQ bug on some platforms this ID can not end in '1' required address the address of the olive server, such as localhost 'localhost' request_port default olive port is 5588 5588 timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/oliveclient.py def __init__ ( self , client_id , address = 'localhost' , request_port = 5588 , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug on some platforms this ID can not end in '1' :param address: the address of the olive server, such as localhost :param request_port: default olive port is 5588 :param timeout_second: time in seconds, to wait for a response from the server \"\"\" self . client_id = client_id # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . server_request_port = request_port self . server_status_port = request_port + 1 self . timeout_seconds = timeout_second self . olive_connected = False self . info = self . fullobj = None OliveClient . setup_multithreading () adapt_supervised ( self , plugin , domain , annotations_file_name , new_domain_name ) Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required adapt_workspace a unique label for this client's adaptation required annotations_file_name the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. required Returns: Type Description the full path name of the new domain. Source code in olivepy/api/oliveclient.py def adapt_supervised ( self , plugin , domain , annotations_file_name , new_domain_name ): \"\"\" :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param adapt_workspace: a unique label for this client's adaptation :param annotations_file_name: the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. :return: the full path name of the new domain. \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] file_annotations = self . parse_annotation_file ( annotations_file_name ) for filename , regions in file_annotations . items (): audio_id = self . preprocess_supervised_audio ( plugin , domain , filename , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace ) adapt_supervised_old ( self , plugin , domain , file_annotations , new_domain_name ) Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required adapt_workspace a unique label for this client's adaptation required file_annotations a dictionary of files to preprocess, each file has one or more annotated regions for processing {filename: [(start_ms, end_ms, class)]}, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} required Returns: Type Description the full path name of the new domain. Source code in olivepy/api/oliveclient.py def adapt_supervised_old ( self , plugin , domain , file_annotations , new_domain_name ): \"\"\" :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param adapt_workspace: a unique label for this client's adaptation :param file_annotations: a dictionary of files to preprocess, each file has one or more annotated regions for processing {filename: [(start_ms, end_ms, class)]}, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} :return: the full path name of the new domain. \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] for filename , regions in file_annotations . items (): audio_id = self . preprocess_supervised_audio ( plugin , domain , filename , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace ) analyze_bounding_box ( self , plugin , domain , filename , data_msg = None , mode =< AudioTransferType . AUDIO_PATH : 1 > , opts = None , classes = None ) Request a analysis of 'filename' returning bounding box scores Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server <AudioTransferType.AUDIO_PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. Source code in olivepy/api/oliveclient.py def analyze_bounding_box ( self , plugin , domain , filename , data_msg = None , mode = AudioTransferType . AUDIO_PATH , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' returning bounding box scores :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. \"\"\" self . info = self . fullobj = None region_score_result = self . _request_bounding_box_scores ( plugin , domain , filename , data_msg = data_msg , mode = mode , opts = opts , classes = classes ) self . fullobj = region_score_result return region_score_result analyze_frames ( self , plugin , domain , filename , data_msg = None , opts = None , classes = None , mode =< AudioTransferType . AUDIO_PATH : 1 > ) Request a analysis of 'filename' returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score. if None, then provide (audio) input as a required data_msg Optionally specify the data input as a fully formed Audio or BinaryMedia message instead of creating from filename None opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis as a list of (frame) scores Source code in olivepy/api/oliveclient.py def analyze_frames ( self , plugin , domain , filename , data_msg = None , opts = None , classes = None , mode = AudioTransferType . AUDIO_PATH ): \"\"\" Request a analysis of 'filename' returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score. if None, then provide (audio) input as a :param data_msg: Optionally specify the data input as a fully formed Audio or BinaryMedia message instead of creating from filename :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis as a list of (frame) scores \"\"\" self . info = self . fullobj = None frame_score_result = self . _request_frame_scores ( plugin , domain , filename , data_msg = data_msg , opts = opts , classes = classes , mode = mode ) if frame_score_result is not None : return frame_score_result . score return [] analyze_global ( self , plugin , domain , filename , data_msg = None , mode =< AudioTransferType . AUDIO_PATH : 1 > , opts = None , classes = None ) Request a LID analysis of 'filename' Parameters: Name Type Description Default plugin the name of the LID plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the audio transfer mode <AudioTransferType.AUDIO_PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis result as a list of (global) scores Source code in olivepy/api/oliveclient.py def analyze_global ( self , plugin , domain , filename , data_msg = None , mode = AudioTransferType . AUDIO_PATH , opts = None , classes = None ): \"\"\" Request a LID analysis of 'filename' :param plugin: the name of the LID plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the audio transfer mode :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis result as a list of (global) scores \"\"\" self . info = self . fullobj = None request = GlobalScorerRequest () request . plugin = plugin request . domain = domain if data_msg : request . audio . CopyFrom ( data_msg ) else : audio = request . audio package_audio ( audio , filename , mode = mode ) self . _add_options ( request , opts ) self . _add_classes ( request , classes ) # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) _ , env = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a global score request message\" ) result = self . _sync_request ( env ) return result . score analyze_regions ( self , plugin , domain , filename , data_msg = None , mode =< AudioTransferType . AUDIO_PATH : 1 > , opts = None , classes = None ) Request a analysis of 'filename' returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server <AudioTransferType.AUDIO_PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. Source code in olivepy/api/oliveclient.py def analyze_regions ( self , plugin , domain , filename , data_msg = None , mode = AudioTransferType . AUDIO_PATH , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. \"\"\" self . info = self . fullobj = None region_score_result = self . _request_region_scores ( plugin , domain , filename , data_msg = data_msg , mode = mode , opts = opts , classes = classes ) self . fullobj = region_score_result return region_score_result apply_threshold ( self , scores , threshold , rate ) Very simple method to convert frame scores to regions. If speech regions are desired we can provide a SAD plugin that returns regions instead of frame scores Parameters: Name Type Description Default scores required threshold required rate required Returns: Type Description frame scores a regions Source code in olivepy/api/oliveclient.py def apply_threshold ( self , scores , threshold , rate ): \"\"\" Very simple method to convert frame scores to regions. If speech regions are desired we can provide a SAD plugin that returns regions instead of frame scores :param scores: :param threshold: :param rate: :return: frame scores a regions \"\"\" inSegment = False start = 0 segments = [] for i in range ( len ( scores )): if not inSegment and scores [ i ] >= threshold : inSegment = True start = i elif inSegment and ( scores [ i ] < threshold or i == len ( scores ) - 1 ): inSegment = False startT = (( 1.0 * start / rate )) endT = ( 1.0 * i / rate ) segments . append (( startT , endT )) return segments audio_modification ( self , plugin , domain , filename , data_msg = None , mode =< AudioTransferType . AUDIO_PATH : 1 > ) Do an audio modification (such as an enhansement). This function only accepts one audio and returns on modified audio. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required Returns: Type Description the analysis as a list of (frame) scores Source code in olivepy/api/oliveclient.py def audio_modification ( self , plugin , domain , filename , data_msg = None , mode = AudioTransferType . AUDIO_PATH ): \"\"\" Do an audio modification (such as an enhansement). This function only accepts one audio and returns on modified audio. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :return: the analysis as a list of (frame) scores \"\"\" if mode != AudioTransferType . AUDIO_PATH : raise Exception ( 'oliveclient.audio_modification requires an filename path and will not work with binary audio data.' ) request = AudioModificationRequest () request . plugin = plugin request . domain = domain request . requested_channels = 1 request . requested_rate = 8000 if data_msg : request . modifications . append ( data_msg ) else : audio = Audio () package_audio ( audio , filename , mode = mode ) # audio = Audio() # audio.path = filename request . modifications . append ( audio ) _ , env = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a audio modification/enhancement request message\" ) result = self . _sync_request ( env ) return result . successful , result . modification_result [ 0 ] connect ( self , monitor_server = False ) Connect this client to the server Parameters: Name Type Description Default monitor_server if true, start a thread to monitor the server connection (helpful if debugging connection issues) False Source code in olivepy/api/oliveclient.py def connect ( self , monitor_server = False ): \"\"\" Connect this client to the server :param monitor_server: if true, start a thread to monitor the server connection (helpful if debugging connection issues) \"\"\" # init the request and status socket request_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_request_port ) status_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_status_port ) context = zmq . Context () self . request_socket = context . socket ( zmq . DEALER ) self . status_socket = context . socket ( zmq . SUB ) self . request_socket . connect ( request_addr ) self . status_socket . connect ( status_addr ) # logging.debug(\"Starting Olive status monitor...\") # Run this to get status about the server (helpful to confirm the server is connected and up) if ( monitor_server ): self . worker = ClientBrokerWorker ( self . status_socket , self . client_id ) self . worker . start () else : self . worker = None self . olive_connected = True logging . debug ( \"Olive client ready\" ) convert_preprocessed_annotations ( self , processed_audio_list ) Convert the file annotations (a dictionary grouped by file ID, where annotations are grouped by file ID, which has one or more regions/classes) into class annotations (where annotations are grouped by class ID, with each class having one or more files, then each file having one or more regions). Parameters: Name Type Description Default processed_audio_list the list of files (indexed by an OLIVE generated ID) and the regions/classes annotated in that file required Returns: Type Description a dictionary of ClassAnnotation objects, indexed by class ID Source code in olivepy/api/oliveclient.py def convert_preprocessed_annotations ( self , processed_audio_list ): \"\"\" Convert the file annotations (a dictionary grouped by file ID, where annotations are grouped by file ID, which has one or more regions/classes) into class annotations (where annotations are grouped by class ID, with each class having one or more files, then each file having one or more regions). :param processed_audio_list: the list of files (indexed by an OLIVE generated ID) and the regions/classes annotated in that file :return: a dictionary of ClassAnnotation objects, indexed by class ID \"\"\" # Now convert the annotations that are grouped by file into a list of annotations grouped by class ID # (speech, non-speech). This is done in two passes, the first passes builds then new mapping of # class_id -->* audio_id -->* region, # then we convert this new data structure into ClassAnnotation (Protobuf) message(s) class_annots = {} for audio_id , regions in processed_audio_list : for region in regions : start = region [ 0 ] end = region [ 1 ] class_id = region [ 2 ] if class_id not in class_annots : class_annots [ class_id ] = {} if audio_id not in class_annots [ class_id ]: class_annots [ class_id ][ audio_id ] = [] class_annots [ class_id ][ audio_id ] . append (( start , end )) # now that the annotations have been grouped by class id, create the annotation protobuf(s) protobuf_class_annots = {} for class_id in class_annots . keys (): protobuf_class_annots [ class_id ] = ClassAnnotation () protobuf_class_annots [ class_id ] . class_id = class_id # Add AudioAnnotation(s) for audio_id in class_annots [ class_id ]: aa = AudioAnnotation () # aa = protobuf_class_annots[class_id].annotations.add() in python2.7? aa . audio_id = audio_id for region in class_annots [ class_id ][ audio_id ]: # times are in milliseconds ar = AnnotationRegion () # might need to do ar = aa.regions.add() for Python2.7 ar . start_t = region [ 0 ] ar . end_t = region [ 1 ] aa . regions . append ( ar ) protobuf_class_annots [ class_id ] . annotations . append ( aa ) return protobuf_class_annots enroll ( self , plugin , domain , class_id , filename , data_msg = None ) Request a enrollment of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required filename the filename to add as an audio only enrollment addition required data_msg an BinaryMedia message to add as an enrollment addition None Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def enroll ( self , plugin , domain , class_id , filename , data_msg = None ): \"\"\" Request a enrollment of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :param filename: the filename to add as an audio only enrollment addition :param data_msg: an BinaryMedia message to add as an enrollment addition :return: True if enrollment successful \"\"\" self . info = self . fullobj = None enrollment = ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True if data_msg : if isinstance ( data_msg , Audio ): enrollment . addition . append ( data_msg ) else : enrollment . addition_media . append ( data_msg ) else : audio = Audio () package_audio ( audio , filename ) enrollment . addition . append ( audio ) # Wrap message in an Envelope _ , env = _wrap_message ( self . client_id , enrollment ) # Now send the envelope logging . debug ( \"Sending an enrollment message\" ) result = self . _sync_request ( env ) return result # ClassModificationResult # Wrap message in an Envelope # request = self._wrap_message(enrollment) # # Now send the message # logging.debug(\"Sending a class modification request (enrollment) message\") # self.request_socket.send(request.SerializeToString()) # logging.debug(\"Sending a class modification request (enrollment) message\") # # TODO THIS IS A SYNC REQUST, CAN BE DONE ASYN WITH A CALLBACK... # # Wait for the response from the server # # logging.info(\"checking for response\") # protobuf_data = self.request_socket.recv() # logging.info(\"Received message from server...\") # envelope = Envelope() # envelope.ParseFromString(protobuf_data) # # # for this use case the server will only have one response in the evevelope: # for i in range(len(envelope.message)): # olive_msg = envelope.message[i] # # if olive_msg.HasField(\"info\"): # self.info = olive_msg.info # if olive_msg.HasField(\"error\"): # raise ExceptionFromServer('Got an error from the server: ' + olive_msg.error) # else: # enrollment_msg = ClassModificationResult() # enrollment_msg.ParseFromString(olive_msg.message_data[0]) # # # Assume there is only one result set (for 'speech'): frame_score_msg.result[0] # # TODO - clean up return. Maybe do something with message. # self.fullobj = enrollment_msg # self.info = enrollment_msg.addition_result[0].message # CLG this would only be set if there was an issue with the enrollment # return enrollment_msg.addition_result[0].successful # # return False # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) finalize_supervised_adaptation ( self , plugin , domain , new_domain_name , class_annotations , adapt_workspace ) Complete the adaptation Parameters: Name Type Description Default plugin the name of the plugin to adapt required domain the name of the plugin domain to adapt required new_domain_name the name of the new domain that is created within the plugin required class_annotations the audio annotations, grouped by class ID required Returns: Type Description the name of the new domain Source code in olivepy/api/oliveclient.py def finalize_supervised_adaptation ( self , plugin , domain , new_domain_name , class_annotations , adapt_workspace ): \"\"\" Complete the adaptation :param plugin: the name of the plugin to adapt :param domain: the name of the plugin domain to adapt :param new_domain_name: the name of the new domain that is created within the plugin :param class_annotations: the audio annotations, grouped by class ID :return: the name of the new domain \"\"\" self . info = self . fullobj = None request = SupervisedAdaptationRequest () request . plugin = plugin request . domain = domain request . adapt_space = adapt_workspace request . new_domain = new_domain_name # Add the class annotations for class_id in class_annotations : request . class_annotations . append ( class_annotations [ class_id ]) # request.class_annotations.extend([class_annotations[class_id]]) for Python2.7? # package the request _ , request = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a finalize adatation message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message - boiler plate code, this can be simplified envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): olive_msg = envelope . message [ i ] if olive_msg . HasField ( \"info\" ): self . info = olive_msg . info if olive_msg . HasField ( \"error\" ): raise ExceptionFromServer ( 'Got an error from the server: ' + olive_msg . error ) else : result_msg = SupervisedAdaptationResult () result_msg . ParseFromString ( olive_msg . message_data [ 0 ]) # get the new domain #if hasattr(result_msg, 'new_domain') and result_msg.new_domain is not None: # print(\"Adaptation successfully created new domain: '{}'\".format(result_msg.new_domain)) self . fullobj = result_msg return result_msg . new_domain # adapt failed... TODO: thrown exception instead? return None get_fullobj ( self ) This object should be used for debugging only. Example use::success = client.enroll('sid-embed-v5-py3', 'multilang-v1', 'joshua', 'file') if troubleshooting: fullobj = client.get_fullobj() print('Whole object returned from server: '+str(fullobj)) Returns: Type Description the full object returned from the last call to the server. Source code in olivepy/api/oliveclient.py def get_fullobj ( self ): \"\"\" This object should be used for debugging only. Example use::success = client.enroll('sid-embed-v5-py3', 'multilang-v1', 'joshua', 'file') \\ if troubleshooting: fullobj = client.get_fullobj() print('Whole object returned from server: '+str(fullobj)) :return: the full object returned from the last call to the server. \"\"\" return self . fullobj get_info ( self ) Returns: Type Description the info data from the last call to the server. Will return None if the last call did not return any info. Source code in olivepy/api/oliveclient.py def get_info ( self ): \"\"\" :return: the info data from the last call to the server. Will return None if the last call did not return any info. \"\"\" return self . info parse_annotation_file ( self , filename ) Parse a file for the names of files of audio files and their regions to use for adaptation. Parameters: Name Type Description Default filename the path and name of the file that contains the input. This file must have one or more lines having 4 columns: # filename, class, start_region_ms, end_region_ms required Returns: Type Description the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} Source code in olivepy/api/oliveclient.py def parse_annotation_file ( self , filename ): \"\"\" Parse a file for the names of files of audio files and their regions to use for adaptation. :param filename: the path and name of the file that contains the input. This file must have one or more lines having 4 columns: # filename, class, start_region_ms, end_region_ms :return: the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} \"\"\" data_lines = [] file_annotations = {} if not os . path . exists ( filename ): raise Exception ( \"The annotation file ' {} ' does not exist\" . format ( filename )) with open ( filename ) as f : data_lines . extend ([ line . strip () for line in f . readlines ()]) # process the file for line in data_lines : pieces = line . split () if len ( pieces ) != 4 : raise Exception ( \"The annotation file does not contain data in the correct format, found line ' {} '\" . format ( line )) adapt_audio_path = pieces [ 0 ] # assume a relative file is used, so the full path must be specified since being sent to server # This is being sent to server. If full path is given, do nothing. Otherwise make absolute. # TODO: this will not work from UNIX to Windows or other way around. # TODO: should use Python's abspath here, don't you think? if adapt_audio_path [ 0 ] != '/' and adapt_audio_path [ 1 ] != ':' : adapt_audio_path = os . path . join ( os . getcwd (), adapt_audio_path ) # todo validate file is valid... if adapt_audio_path not in file_annotations : file_annotations [ adapt_audio_path ] = [] class_id = pieces [ 1 ] start = float ( pieces [ 2 ]) end = float ( pieces [ 3 ]) file_annotations [ adapt_audio_path ] . append (( start , end , class_id )) return file_annotations preprocess_supervised_audio ( self , plugin , domain , filename , adapt_workspace ) Submit audio for pre-processing phase of adaptation. Parameters: Name Type Description Default plugin the name of the plugin to adapt required domain the name of the plugin domain to adapt required filename the name of the audio file to submit to the server/plugin/domain for preprocessing required Returns: Type Description the unique id generated by the server for the preprocess audio, which must be used Source code in olivepy/api/oliveclient.py def preprocess_supervised_audio ( self , plugin , domain , filename , adapt_workspace ): \"\"\" Submit audio for pre-processing phase of adaptation. :param plugin: the name of the plugin to adapt :param domain: the name of the plugin domain to adapt :param filename: the name of the audio file to submit to the server/plugin/domain for preprocessing :return: the unique id generated by the server for the preprocess audio, which must be used \"\"\" # [(2.618, 6.2, 'S'), (7.2, 9.5, 'NS')] self . info = self . fullobj = None request = PreprocessAudioAdaptRequest () request . plugin = plugin request . domain = domain request . adapt_space = adapt_workspace request . class_id = \"supervised\" # HACK: for supervised validation in the backend - we will fix this in a future release so not needed # we currently don't need to set annotations (start_t, end_t) when doing pre-processing # finally, set the audio: audio = request . audio # send the name of the file to the server: audio . path = filename # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) # TODO SERIALIZE EXAMPLE... # package the request _ , request = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a preprocess audio (for adaptation) message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): olive_msg = envelope . message [ i ] if olive_msg . HasField ( \"info\" ): self . info = olive_msg . info if olive_msg . HasField ( \"error\" ): raise ExceptionFromServer ( 'Got an error from the server: ' + olive_msg . error ) else : result_msg = PreprocessAudioAdaptResult () result_msg . ParseFromString ( olive_msg . message_data [ 0 ]) # get audio id from results, use for final annotations... # print(\"Preprocess audio ID {} having duration {}\".format(result_msg.audio_id, result_msg.duration)) self . fullobj = result_msg return result_msg . audio_id # preprocessing failed... TODO: thrown exception instead? return None requst_sad_adaptation ( self ) Example of performing SAD adaptation Returns: Type Description Source code in olivepy/api/oliveclient.py def requst_sad_adaptation ( self ): \"\"\" Example of performing SAD adaptation :return: \"\"\" # todo move to client example (i.e. olivelearn) # using Julie's sadRegression dataset... # Assume the working directory is root directory for the SAD regression tests # Setup processing variables (get this config or via command line optons plugin = \"sad-dnn-v6a\" domain = \"multi-v1\" new_domain_name = \"python_adapted_multi-v2\" # Build the list of files plus the regions in the those files to adaptn by parsing the input file: file_annotations = self . parse_annotation_file ( \"lists/adapt_ms.lst\" ) return self . adapt_supervised_old ( plugin , domain , file_annotations , new_domain_name ) setup_multithreading () classmethod This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/oliveclient.py @classmethod def setup_multithreading ( cls ): \"\"\"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. \"\"\" # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL ) unenroll ( self , plugin , domain , class_id ) Unenrollment the class_id Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def unenroll ( self , plugin , domain , class_id ): \"\"\" Unenrollment the class_id :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :return: True if enrollment successful \"\"\" self . info = self . fullobj = None removal = ClassRemovalRequest () removal . plugin = plugin removal . domain = domain removal . class_id = class_id # Wrap message in an Envelope _ , request = _wrap_message ( self . client_id , removal ) logging . debug ( \"Sending a class modification request (removal) message\" ) result = self . _sync_request ( request ) # do something? return True get_bit_depth ( audio ) Not using since not assuming numpy is available... Source code in olivepy/api/oliveclient.py def get_bit_depth ( audio ): \"\"\"Not using since not assuming numpy is available...\"\"\" # Numpy is needed to support this... dt = audio . dtype if dt == np . int8 : return BIT_DEPTH_8 elif dt == np . int16 : return BIT_DEPTH_16 elif dt == np . int32 : return BIT_DEPTH_24 else : return BIT_DEPTH_32 package_buffer_audio ( audio , data , num_samples , sample_rate = 8000 , num_channels = 1 ) Helper function to wrap audio data (decoded samples) into a AudioBuffer message that can submitted to the server instead of a file name. Parameters: Name Type Description Default data the data as a numpy ndarray required num_samples the number of samples required sample_rate the audio sample rate 8000 num_channels the number of channels in the audio 1 Returns: Type Description Source code in olivepy/api/oliveclient.py def package_buffer_audio ( audio , data , num_samples , sample_rate = 8000 , num_channels = 1 ): \"\"\" Helper function to wrap audio data (decoded samples) into a AudioBuffer message that can submitted to the server instead of a file name. :param data: the data as a numpy ndarray :param num_samples: the number of samples :param sample_rate: the audio sample rate :param num_channels: the number of channels in the audio :return: \"\"\" # from scipy.io import wavfile # sample_rate, data = wavfile.read('somefilename.wav') buffer = audio . audioSamples buffer . channels = num_channels buffer . samples = num_samples #data.shape[0] buffer . rate = sample_rate buffer . bit_depth = get_bit_depth ( data ) buffer . data = data . tostring () return audio olivepy.api.olive_async_client AsyncOliveClient ( Thread ) This class is used to make asynchronous requests to the OLIVE server __init__ ( self , client_id , address = 'localhost' , request_port = 5588 , timeout_second = 10 ) special Parameters: Name Type Description Default client_id str The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems required address the address of the olive server, such as localhost 'localhost' request_port default olive port is 5588 5588 timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/olive_async_client.py def __init__ ( self , client_id : str , address = 'localhost' , request_port = 5588 , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems :param address: the address of the olive server, such as localhost :param request_port: default olive port is 5588 :param timeout_second: time in seconds, to wait for a response from the server \"\"\" threading . Thread . __init__ ( self ) self . client_id = client_id # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . server_request_port = request_port self . server_status_port = request_port + 1 self . timeout_seconds = timeout_second self . request_queue = queue . Queue () # special queue used to emulate blocking requests # self.completed_sync_request_queue = queue.Queue() self . sync_message = {} self . response_queue = {} self . working = False self . request_socket = None self . status_socket = None # thread to monitor OLIVE server heartbeats self . worker = None # self.status_socket = context.socket(zmq.SUB) self . olive_connected = False self . monitor_status = False oc . OliveClient . setup_multithreading () add_heartbeat_listener ( self , heartbeat_callback ) Register a callback function to be notified when a heartbeat is received from the OLIVE server Parameters: Name Type Description Default heartbeat_callback Callable[[olive_pb2.Heartbeat], NoneType] The callback method that is notified each time a heartbeat message is received from the OLIVE server required Source code in olivepy/api/olive_async_client.py def add_heartbeat_listener ( self , heartbeat_callback : Callable [[ Heartbeat ], None ]): \"\"\" Register a callback function to be notified when a heartbeat is received from the OLIVE server :param heartbeat_callback: The callback method that is notified each time a heartbeat message is received \\ from the OLIVE server \"\"\" if self . worker : self . worker . add_event_callback ( heartbeat_callback ) else : print ( \"Unable to add a heartbeat listener because this client was not started with the status \" \" heartbeat monitor enabled\" ) analyze_frames ( self , plugin , domain , audio_input , callback , mode =< AudioTransferType . AUDIO_SERIALIZED : 3 > , opts = None ) Request a analysis of 'filename', returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the audio to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> opts a dictionary of name/value pair options for this plugin request None Returns: Type Description a OliveServerResponse containing the status of the request (FrameScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_frames ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED , opts = None ): \"\"\" Request a analysis of 'filename', returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the audio to score :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :param opts: a dictionary of name/value pair options for this plugin request :return: a OliveServerResponse containing the status of the request (FrameScorerResult) \"\"\" request = FrameScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio olivepy . messaging . msgutil . package_audio ( audio , audio_input , mode = mode ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) analyze_global ( self , plugin , domain , audio_input , callback , mode =< AudioTransferType . AUDIO_SERIALIZED : 3 > ) Request a global score analysis of 'filename' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the name of the audio file to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (GlobalScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_global ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED ): \"\"\" Request a global score analysis of 'filename' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the name of the audio file to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (GlobalScorerResult) \"\"\" self . info = self . fullobj = None request = GlobalScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio olivepy . messaging . msgutil . package_audio ( audio , audio_input , mode = mode ) self . enqueue_request ( request , callback ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) analyze_regions ( self , plugin , domain , filename , callback , mode =< AudioTransferType . AUDIO_SERIALIZED : 3 > ) Request a analysis of 'filename', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_regions ( self , plugin , domain , filename , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED ): \"\"\" Request a analysis of 'filename', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (RegionScorerResult) \"\"\" request = RegionScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio olivepy . messaging . msgutil . package_audio ( audio , filename , mode = mode ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) audio_modification ( self , plugin , domain , audio_input , callback , mode =< AudioTransferType . AUDIO_SERIALIZED : 3 > ) Used to make a AudioModificationRequest (enhancement). Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the audio path or buffer to submit for modification required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (AudioModificationResult) Source code in olivepy/api/olive_async_client.py def audio_modification ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED ): \"\"\" Used to make a AudioModificationRequest (enhancement). :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the audio path or buffer to submit for modification :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (AudioModificationResult) \"\"\" if mode != olivepy . messaging . msgutil . AudioTransferType . AUDIO_PATH : raise Exception ( 'oliveclient.audio_modification requires an filename path and will not work with binary audio data.' ) request = AudioModificationRequest () request . plugin = plugin request . domain = domain request . requested_channels = 1 request . requested_rate = 8000 audio = Audio () olivepy . messaging . msgutil . package_audio ( audio , audio_input , mode = mode ) request . modifications . append ( audio ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) clear_heartbeat_listeners ( self ) Remove all heartbeat listeners Source code in olivepy/api/olive_async_client.py def clear_heartbeat_listeners ( self ): \"\"\" Remove all heartbeat listeners \"\"\" if self . worker : self . worker . clear_callback () connect ( self , monitor_status = False ) Connect this client to the server Parameters: Name Type Description Default monitor_server if true, starts a thread to monitor the server status connection for heartbeat messages required Source code in olivepy/api/olive_async_client.py def connect ( self , monitor_status = False ): \"\"\" Connect this client to the server :param monitor_server: if true, starts a thread to monitor the server status connection for heartbeat messages \"\"\" # logging.debug(\"Starting Olive async monitor...\") self . monitor_status = monitor_status self . connection_done = threading . Event () self . start () # block until connected self . olive_connected = True self . connection_done . wait () logging . debug ( \"Olive async client ready\" ) disconnect ( self ) Closes the connection to the OLIVE server Source code in olivepy/api/olive_async_client.py def disconnect ( self ): \"\"\" Closes the connection to the OLIVE server \"\"\" if self . worker : self . worker . stopWorker () self . working = False self . olive_connected = False self . join () self . request_socket . close () enqueue_request ( self , message , callback , wrapper = None ) Add a message request to the outbound queue Parameters: Name Type Description Default message the request message to send required callback this is called when response message is received from the server required wrapper the message wrapper None Source code in olivepy/api/olive_async_client.py def enqueue_request ( self , message , callback , wrapper = None ): \"\"\" Add a message request to the outbound queue :param message: the request message to send :param callback: this is called when response message is received from the server :param wrapper: the message wrapper \"\"\" if wrapper is None : wrapper = response . OliveServerResponse () self . request_queue . put (( message , callback , wrapper )) enroll ( self , plugin , domain , class_id , audio_input , callback , mode =< AudioTransferType . AUDIO_SERIALIZED : 3 > ) Request a enrollment of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to enroll required audio_input the Audio message to add as an enrollment addition required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (ClassModificationResult) Source code in olivepy/api/olive_async_client.py def enroll ( self , plugin , domain , class_id , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED ): \"\"\" Request a enrollment of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to enroll :param audio_input: the Audio message to add as an enrollment addition :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (ClassModificationResult) \"\"\" enrollment = ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True audio = Audio () olivepy . messaging . msgutil . package_audio ( audio , audio_input , mode = mode ) enrollment . addition . append ( audio ) if callback : self . enqueue_request ( enrollment , callback ) else : return self . sync_request ( enrollment ) get_active ( self , callback ) Used to make a GetActiveRequest Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (GetActiveResult) Source code in olivepy/api/olive_async_client.py def get_active ( self , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a GetActiveRequest :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (GetActiveResult) \"\"\" request = GetActiveRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) get_status ( self , callback = None ) Used to make a GetStatusRequest and receive a GetStatusResult Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse that contains the most recent server status (GetStatusResult) Source code in olivepy/api/olive_async_client.py def get_status ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetStatusRequest and receive a GetStatusResult :param callback: optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse that contains the most recent server status (GetStatusResult) \"\"\" request = GetStatusRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) get_update_status ( self , plugin , domain , callback = None ) Used to make a GetUpdateStatusRequest Parameters: Name Type Description Default plugin the name of the plugin to query required domain the name of the domain to query required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult Source code in olivepy/api/olive_async_client.py def get_update_status ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetUpdateStatusRequest :param plugin: the name of the plugin to query :param domain: the name of the domain to query :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult \"\"\" request = GetUpdateStatusRequest () request . plugin = plugin request . domain = domain if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) is_connected ( self ) Status of the connection to the OLIVE server Returns: Type Description True if connected Source code in olivepy/api/olive_async_client.py def is_connected ( self ): \"\"\" Status of the connection to the OLIVE server :return: True if connected \"\"\" return self . olive_connected load_plugin_domain ( self , plugin , domain , callback ) Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest message) Parameters: Name Type Description Default plugin the name of the plugin to pre-load required domain the name of hte domain to pre-load required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the update status of the request (LoadPluginDomainResult) Source code in olivepy/api/olive_async_client.py def load_plugin_domain ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest message) :param plugin: the name of the plugin to pre-load :param domain: the name of hte domain to pre-load :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the update status of the request (LoadPluginDomainResult) \"\"\" request = LoadPluginDomainRequest () request . plugin = plugin request . domain = domain if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) request_plugins ( self , callback = None ) Used to make a PluginDirectoryRequest Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult) Source code in olivepy/api/olive_async_client.py def request_plugins ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a PluginDirectoryRequest :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult) \"\"\" request = PluginDirectoryRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) run ( self ) Starts the thread to handle async messages Source code in olivepy/api/olive_async_client.py def run ( self ): \"\"\" Starts the thread to handle async messages \"\"\" try : logging . debug ( \"Starting OLIVE Async Message Worker for id: {} \" . format ( self . client_id )) context = zmq . Context () self . request_socket = context . socket ( zmq . DEALER ) # init the request and status socket request_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_request_port ) status_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_status_port ) self . request_socket . connect ( request_addr ) if self . monitor_status : logging . debug ( \"connecting to status socket...\" ) self . status_socket = context . socket ( zmq . SUB ) self . status_socket . connect ( status_addr ) self . worker = ClientMonitorThread ( self . status_socket , self . client_id ) self . worker . start () else : self . worker = None self . working = True poller = zmq . Poller () poller . register ( self . request_socket , zmq . POLLIN ) except Exception as e : logging . error ( \"Error connecting to the OLIVE server: {} \" . format ( e )) self . olive_connected = False finally : self . connection_done . set () while self . working : # First, send any client requests while not self . request_queue . empty (): request_msg , cb , wrapper = self . request_queue . get () msg_id , env = msgutil . _wrap_message ( self . client_id , request_msg ) # Add to our callback Q self . response_queue [ msg_id ] = ( request_msg , cb , wrapper ) # Now send the message logging . debug ( \"Sending client request msg type: {} \" . format ( env . message [ 0 ] . message_type )) self . request_socket . send ( env . SerializeToString ()) # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . request_socket in socks : # logging.info(\"Received message from OLIVE...\") protobuf_data = self . request_socket . recv () envelope = Envelope () envelope . ParseFromString ( protobuf_data ) for i in range ( len ( envelope . message )): self . _process_response ( envelope . message [ i ]) poller . unregister ( self . request_socket ) self . request_socket . close () # todo what do we do with the status socket? # self.status_socket.close() # print(\"Async client worker stopped\") setup_multithreading () classmethod This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/olive_async_client.py @classmethod def setup_multithreading ( cls ): '''This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. ''' # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL ) sync_request ( self , message , wrapper = None ) Send a request to the OLIVE server, but wait for a response from the server Parameters: Name Type Description Default message the request message to send to the OLIVE server required Returns: Type Description the response from the server Source code in olivepy/api/olive_async_client.py def sync_request ( self , message , wrapper = None ): \"\"\" Send a request to the OLIVE server, but wait for a response from the server :param message: the request message to send to the OLIVE server :return: the response from the server \"\"\" if wrapper is None : wrapper = response . OliveServerResponse () # create an ID for this sync_request sync_id = msgutil . get_uuid () result_available = threading . Event () # result_event = None cb = lambda response : self . _sync_callback ( response , sync_id , result_available ) self . enqueue_request ( message , cb , wrapper ) result_available . wait () # get the result if sync_id in self . sync_message : return self . sync_message . pop ( sync_id ) else : # unexpected.... callback event completed with no result raise Exception ( \"Error waiting for a response from the server\" ) # self.completed_sync_request_queue.put() unenroll ( self , plugin , domain , class_id , callback ) Unenroll class_id Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to remove required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (ClassRemovalResult) Source code in olivepy/api/olive_async_client.py def unenroll ( self , plugin , domain , class_id , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Unenroll class_id :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to remove :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (ClassRemovalResult) \"\"\" removal = ClassRemovalRequest () removal . plugin = plugin removal . domain = domain removal . class_id = class_id if callback : self . enqueue_request ( removal , callback ) else : return self . sync_request ( removal ) unload_plugin_domain ( self , plugin , domain , callback ) Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded plugin from server memory) Parameters: Name Type Description Default plugin the name of the plugin to unload required domain the name of hte domain to unload required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RemovePluginDomainResult) Source code in olivepy/api/olive_async_client.py def unload_plugin_domain ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded \\ plugin from server memory) :param plugin: the name of the plugin to unload :param domain: the name of hte domain to unload :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RemovePluginDomainResult) \"\"\" request = RemovePluginDomainRequest () request . plugin = plugin . strip () request . domain = domain . strip () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) update_plugin_domain ( self , plugin , domain , metadata , callback ) Used to make a ApplyUpdateRequest Parameters: Name Type Description Default plugin the name of the plugin to update required domain the name of hte domain to update required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (ApplyUpdateResult) Source code in olivepy/api/olive_async_client.py def update_plugin_domain ( self , plugin , domain , metadata , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a ApplyUpdateRequest :param plugin: the name of the plugin to update :param domain: the name of hte domain to update :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (ApplyUpdateResult) \"\"\" request = ApplyUpdateRequest () request . plugin = plugin request . domain = domain mds = request . params for key , item in metadata : md = Metadata () md . name = key if isinstance ( item , str ): md . type = 1 elif isinstance ( item , int ): md . type = 2 elif isinstance ( item , float ): md . type = 3 elif isinstance ( item , bool ): md . type = 4 elif isinstance ( item , list ): md . type = 5 else : raise Exception ( 'Metadata {} had a {} type that was not str, int, float, bool, or list.' . format ( key , str ( type ( item )))) md . value = item mds . append ( md ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request ) ClientMonitorThread ( Thread ) Helper used to monitor the status of the Oliveserver add_event_callback ( self , callback ) Callback function that is notified of a heartbeat Parameters: Name Type Description Default callback Callable[[olive_pb2.Heartbeat], NoneType] the function that is called with a Heartbeat object required Source code in olivepy/api/olive_async_client.py def add_event_callback ( self , callback : Callable [[ Heartbeat ], None ]): \"\"\" Callback function that is notified of a heartbeat :param callback: the function that is called with a Heartbeat object \"\"\" self . event_callback . append ( callback ) run ( self ) Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/olive_async_client.py def run ( self ): # print(\"Starting Olive Status Monitor for id: {}\".format(self.client_id)) self . working = True self . status_socket . subscribe ( \"\" ) poller = zmq . Poller () poller . register ( self . status_socket , zmq . POLLIN ) last_heartbeat = time . time () while self . working : # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . status_socket in socks : last_heartbeat = time . time () # print(\"Received status message from OLIVE...\") heatbeat_data = self . status_socket . recv () heatbeat = Heartbeat () heatbeat . ParseFromString ( heatbeat_data ) for cb in self . event_callback : cb ( heatbeat ) else : # Consider using the same timeout for messages? if time . time () - last_heartbeat > HEARTBEAT_TIMEOUT_SECONDS : print ( \"heartbeat timeout\" ) # it has been too long since a heatbeat message was received from the server... assume there server is down for cb in self . event_callback : cb ( None ) self . status_socket . close () olivepy.api.workflow OliveWorkflow An OliveWorkflow instance represents a Workflow Definition actualized by an OLIVE server. Once actualized, an OliveWorkflow instance is used to make analysis, or enrollment/unenrollment requests. An OliveWorkflow should be created using an OliveWorkflowDefinition's create_workflow() method. All calls to the server include an optional callback. When the callback is provided, the call does not block and the callback method is invoked when a response is received from the server. A callback method has 3 arguments: the original request, the response, and an error message if the request failed. Exceptions: Type Description WorkflowException If the workflow was not actualized __init__ ( self , olive_async_client , actualized_workflow ) special Parameters: Name Type Description Default olive_async_client AsyncOliveClient the client connection to the OLIVE server required actualized_workflow OliveWorkflowActualizedResponse An OliveWorkflowDefinition actualized by the server required Source code in olivepy/api/workflow.py def __init__ ( self , olive_async_client : AsyncOliveClient , actualized_workflow : response . OliveWorkflowActualizedResponse ): \"\"\" :param olive_async_client: the client connection to the OLIVE server :param actualized_workflow: An OliveWorkflowDefinition actualized by the server \"\"\" self . client = olive_async_client self . workflow_response = actualized_workflow actualized_workflow_definition = actualized_workflow . get_workflow () # make sure an OLIvE server has actualized this workflow if not actualized_workflow_definition . actualized : raise WorkflowException ( \"Error: Can not create an OliveWorkflow using a Workflow Definition that has not \" \"been actualized by an OLIVE server\" ) self . workflow_def = actualized_workflow_definition # note: enrollment and adapt should only have one task/job # but there could be multiple plugins/task that could support enrollment or adaptation.. so we focus on # analysis adapt ( self , data_input , callback , options = None , finalize = True ) NOT YET SUPPORTED -- and not sure it will ever be supported via workflow Parameters: Name Type Description Default data_input required callback required options None finalize True Returns: Type Description not supported Source code in olivepy/api/workflow.py def adapt ( self , data_input , callback , options = None , finalize = True ): \"\"\" NOT YET SUPPORTED -- and not sure it will ever be supported via workflow :param data_input: :param callback: :param options: :param finalize: :return: not supported \"\"\" raise Exception ( \"Workflow adaption not supported\" ) analyze ( self , data_inputs , callback = None , options = None ) Perform a workflow analysis Parameters: Name Type Description Default data_inputs List[olive_pb2.WorkflowDataRequest] a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. required callback an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. None options str a JSON string of name/value options to include with the analysis request such as '{\"filter_length\":99, \"interpolate\":1.0, \"test_name\":\"midge\"}' None Returns: Type Description OliveWorkflowAnalysisResponse an OliveWorkflowAnalysisResponse (if no callback provided) Source code in olivepy/api/workflow.py def analyze ( self , data_inputs : List [ WorkflowDataRequest ], callback = None , options : str = None ) -> response . OliveWorkflowAnalysisResponse : \"\"\" Perform a workflow analysis :param data_inputs: a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. :param callback: an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. :param options: a JSON string of name/value options to include with the analysis request such as '{\"filter_length\":99, \"interpolate\":1.0, \"test_name\":\"midge\"}' :return: an OliveWorkflowAnalysisResponse (if no callback provided) \"\"\" # make call blocking if no callback or always assume it is async? analysis_request = WorkflowAnalysisRequest () for di in data_inputs : analysis_request . workflow_data_input . append ( di ) analysis_request . workflow_definition . CopyFrom ( self . workflow_def ) # Parse options (if any) if options : jopts = utils . parse_json_options ( options ) analysis_request . option . extend ( jopts ) if callback : self . client . enqueue_request ( analysis_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( analysis_request , response . OliveWorkflowAnalysisResponse ()) enroll ( self , data_inputs , class_id , job_names , callback = None , options = None ) Submit data for enrollment. Parameters: Name Type Description Default data_inputs List[olive_pb2.WorkflowDataRequest] a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. required class_id str the name of the enrollment required job_names List[str] a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. required callback an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. None options a dictionary of name/value option pairs to include with the enrollment request None Returns: Type Description server enrollment response if no callback provided Source code in olivepy/api/workflow.py def enroll ( self , data_inputs : List [ WorkflowDataRequest ], class_id : str , job_names : List [ str ], callback = None , options = None ): \"\"\" Submit data for enrollment. :param data_inputs: a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. :param class_id: the name of the enrollment :param job_names: a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. :param callback: an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. :param options: a dictionary of name/value option pairs to include with the enrollment request :return: server enrollment response if no callback provided \"\"\" # # first, get the enrollment order # for order in self.workflow_def.order: # if order.workflow_type == WORKFLOW_ENROLLMENT_TYPE: # workflow_enrollment_order_msg = order # break # # if workflow_enrollment_order_msg is None: # raise Exception(\"This workflow does not contain any \") # # # for name in task_names: # make call blocking if no callback or always assume it is async? enroll_request = WorkflowEnrollRequest () for di in data_inputs : enroll_request . workflow_data_input . append ( di ) enroll_request . workflow_definition . CopyFrom ( self . workflow_def ) enroll_request . class_id = class_id for job_task in job_names : enroll_request . job_names . append ( job_task ) if options : jopts = utils . parse_json_options ( options ) enroll_request . option . extend ( jopts ) if callback : # self.client.enqueue_request(enroll_request, callback, response.OliveWorkflowEnrollmentResponse()) self . client . enqueue_request ( enroll_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( enroll_request , response . OliveWorkflowAnalysisResponse ()) get_analysis_class_ids ( self , type = 1 , callback = None ) Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. Parameters: Name Type Description Default callback an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) None Returns: Type Description OliveClassStatusResponse an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server Source code in olivepy/api/workflow.py def get_analysis_class_ids ( self , type = WORKFLOW_ANALYSIS_TYPE , callback = None ) -> response . OliveClassStatusResponse : \"\"\" Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. :param type the WorkflowOrder type (WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, or WORKFLOW_UNENROLLMENT_TYPE) :param callback: an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) :return: an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server \"\"\" class_request = WorkflowClassStatusRequest () class_request . workflow_definition . CopyFrom ( self . workflow_def ) if type : class_request . type = type if callback : self . client . enqueue_request ( class_request , callback , response . OliveClassStatusResponse ()) else : return self . client . sync_request ( class_request , response . OliveClassStatusResponse ()) get_analysis_job_names ( self ) The names of analysis jobs in this workflow (usually only one analysis job) Returns: Type Description List[str] A list of analysis job names in this workflow Source code in olivepy/api/workflow.py def get_analysis_job_names ( self ) -> List [ str ]: \"\"\" The names of analysis jobs in this workflow (usually only one analysis job) :return: A list of analysis job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_ANALYSIS_TYPE ) get_analysis_task_info ( self ) A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is not known until runtime) Returns: Type Description List[Dict[str, Dict]] JSON structured detailed information of analysis tasks used in this workflow Source code in olivepy/api/workflow.py def get_analysis_task_info ( self ) -> List [ Dict [ str , Dict ]]: \"\"\" A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report \\ includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is \\ not known until runtime) :return: JSON structured detailed information of analysis tasks used in this workflow \"\"\" # return [task.consumer_result_label for task in analysis_jobs[job_name]] return self . workflow_response . to_json ( indent = 1 ) get_analysis_tasks ( self , job_name = None ) Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) Parameters: Name Type Description Default job_name str filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_analysis_tasks ( self , job_name : str = None ) -> List [ str ]: \"\"\" Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) :param job_name: filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. :return: a list of task names \"\"\" analysis_jobs = response . get_workflow_jobs ( self . workflow_def , WORKFLOW_ANALYSIS_TYPE ) # better to exception or empty dict???? if len ( analysis_jobs ) == 0 : return None if job_name is not None : if job_name not in analysis_jobs : return None else : # get the default job name job_name = list ( analysis_jobs . keys ())[ 0 ] return [ task . consumer_result_label for task in analysis_jobs [ job_name ]] get_enrollment_job_names ( self ) The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment Returns: Type Description List[str] A list of enrollment job names in this workflow Source code in olivepy/api/workflow.py def get_enrollment_job_names ( self ) -> List [ str ]: \"\"\" The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment :return: A list of enrollment job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_ENROLLMENT_TYPE ) get_enrollment_tasks ( self , job_name = None , type = 2 ) Return a list of tasks that support enrollment in this workflow. Parameters: Name Type Description Default job_name str optionally the name of the enrollment job. Optional since most workflows only support one job None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_enrollment_tasks ( self , job_name : str = None , type = WORKFLOW_ENROLLMENT_TYPE ) -> List [ str ]: \"\"\" Return a list of tasks that support enrollment in this workflow. :param job_name: optionally the name of the enrollment job. Optional since most workflows only support one job :return: a list of task names \"\"\" enrollment_jobs = response . get_workflow_jobs ( self . workflow_def , type ) if len ( enrollment_jobs ) == 0 : return None if job_name is not None : if job_name not in enrollment_jobs : return None # normally (and currently the only supported option) should be just one enrollment_job... return list ( response . get_workflow_job_tasks ( enrollment_jobs , job_name ) . keys ()) get_unenrollment_job_names ( self ) The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment Returns: Type Description List[str] A list of un-enrollment job names in this workflow Source code in olivepy/api/workflow.py def get_unenrollment_job_names ( self ) -> List [ str ]: \"\"\" The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment :return: A list of un-enrollment job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_UNENROLLMENT_TYPE ) get_unenrollment_tasks ( self , job_name = None ) Return a list of tasks that support UNenrollment in this workflow. Parameters: Name Type Description Default job_name str optionally the name of the enrollment job. Optional since most workflows only support one job None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_unenrollment_tasks ( self , job_name : str = None ) -> List [ str ]: \"\"\" Return a list of tasks that support UNenrollment in this workflow. :param job_name: optionally the name of the enrollment job. Optional since most workflows only support one job :return: a list of task names \"\"\" return self . get_enrollment_tasks ( job_name , type = WORKFLOW_UNENROLLMENT_TYPE ) package_audio ( self , audio_data , mode =< InputTransferType . SERIALIZED : 3 > , annotations = None , task_annotations = None , selected_channel = None , num_channels = None , sample_rate = None , num_samples = None , validate_local_path = True , label = None ) Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. Parameters: Name Type Description Default audio_data ~AnyStr the input data is a string (file path) if mode is 'AUDIO_PATH', otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples required mode specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. <InputTransferType.SERIALIZED: 3> annotations List[Tuple[float, float]] optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) None task_annotations Dict[str, Dict[str, List[Tuple[float, float]]]] optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {'SHL': {'speaker'':[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the 'SHL' task, which are labeled as class 'speaker' having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . None selected_channel int optional - the channel to process if using multi-channel audio None num_channels int The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None sample_rate int The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None num_samples int The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None validate_local_path bool If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired True label an optional name to use with the audio None Returns: Type Description WorkflowDataRequest A populated WorkflowDataRequest to use in a workflow activity Source code in olivepy/api/workflow.py def package_audio ( self , audio_data : AnyStr , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , annotations : List [ Tuple [ float , float ]] = None , task_annotations : Dict [ str , Dict [ str , List [ Tuple [ float , float ]]]] = None , selected_channel : int = None , num_channels : int = None , sample_rate : int = None , num_samples : int = None , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. :param audio_data: the input data is a string (file path) if mode is 'AUDIO_PATH', otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples :param mode: specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. :param annotations: optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) :param task_annotations: optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {'SHL': {'speaker'':[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the 'SHL' task, which are labeled as class 'speaker' having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . :param selected_channel: optional - the channel to process if using multi-channel audio :param num_channels: The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param sample_rate: The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param num_samples: The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param validate_local_path: If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired :param label: an optional name to use with the audio :return: A populated WorkflowDataRequest to use in a workflow activity \"\"\" audio = Audio () msgutil . package_audio ( audio , audio_data , annotations , selected_channel , mode , num_channels , sample_rate , num_samples , validate_local_path ) # Add any task specific regions: if task_annotations : for task_label in task_annotations . keys (): ta = audio . task_annotations . add () ta . task_label = task_label # we only expect to have one set of annotations, so just one region_label for region_label in task_annotations [ task_label ]: ta . region_label = region_label for annots in task_annotations [ task_label ][ region_label ]: region = ta . regions . add () region . start_t = np . float ( annots [ 0 ]) region . end_t = np . float ( annots [ 1 ]) wkf_data_request = WorkflowDataRequest () #fixme: this should be set based on the audio.label (filename) or given a unique name here... wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = AUDIO wkf_data_request . workflow_data = audio . SerializeToString () # consumer_data_label doesn't need to be set... use default # set job name? Currently we assume one job per workflow so punting on this for now return wkf_data_request package_binary ( self , binary_input , mode =< InputTransferType . SERIALIZED : 3 > , annotations = None , validate_local_path = True , label = None ) Parameters: Name Type Description Default video_input a video input required Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_binary ( self , binary_input , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , annotations : List [ Tuple [ float , float ]] = None , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" :param video_input: a video input :return: TBD \"\"\" media = BinaryMedia () msgutil . package_binary_media ( media , binary_input , mode = mode , validate_local_path = validate_local_path ) if label : media . label = label wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = VIDEO wkf_data_request . workflow_data = media . SerializeToString () return wkf_data_request package_image ( self , image_input , mode =< InputTransferType . SERIALIZED : 3 > , validate_local_path = True , label = None ) Not yet supported Parameters: Name Type Description Default image_input An image input required Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_image ( self , image_input , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" Not yet supported :param image_input: An image input :return: TBD \"\"\" media = BinaryMedia () msgutil . package_binary_media ( media , image_input , mode = mode , validate_local_path = validate_local_path ) if label : media . label = label media . motion = False # todo if annotations... wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = IMAGE wkf_data_request . workflow_data = media . SerializeToString () return wkf_data_request package_text ( self , text_input , optional_label = None , text_workflow_key = None ) Used to package data for a workflow that accepts string (text) input Parameters: Name Type Description Default text_input str a text input required optional_label str an optional label, namoe or comment associated with this input None text_workflow_key str the keyword used to identify this data in the workflow. By default a value of 'text' is assumed and recommend None Returns: Type Description WorkflowDataRequest a WorkflowDataRequest populated with the text input Source code in olivepy/api/workflow.py def package_text ( self , text_input : str , optional_label : str = None , text_workflow_key : str = None ) -> WorkflowDataRequest : \"\"\" Used to package data for a workflow that accepts string (text) input :param text_input: a text input :param optional_label: an optional label, namoe or comment associated with this input :param text_workflow_key: the keyword used to identify this data in the workflow. By default a value of 'text' is assumed and recommend :return: a WorkflowDataRequest populated with the text input \"\"\" text_msg = Text () # not (yet?) supported multiple text inputs in a request text_msg . text . append ( text_input ) if optional_label : text_msg . label = optional_label wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = text_workflow_key if text_workflow_key else 'text' wkf_data_request . data_type = TEXT wkf_data_request . workflow_data = text_msg . SerializeToString () return wkf_data_request package_workflow_input ( self , input_msg , expected_data_type =< OliveInputDataType . AUDIO_DATA_TYPE : 2 > ) Parameters: Name Type Description Default input_msg the OLIVE data message to package required expected_data_type the data type of the message (Binary <OliveInputDataType.AUDIO_DATA_TYPE: 2> Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_workflow_input ( self , input_msg , expected_data_type = msgutil . OliveInputDataType . AUDIO_DATA_TYPE ) -> WorkflowDataRequest : \"\"\" :param input_msg: the OLIVE data message to package :param expected_data_type: the data type of the message (Binary :return: TBD \"\"\" wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = input_msg . label if input_msg . label else msgutil . get_uuid () wkf_data_request . data_type = msgutil . data_type_class_map [ expected_data_type ] wkf_data_request . workflow_data = input_msg . SerializeToString () return wkf_data_request serialize_audio ( self , filename ) Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters: Name Type Description Default filename str the local path to the file to serialize required Returns: Type Description ~AnyStr the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples Source code in olivepy/api/workflow.py def serialize_audio ( self , filename : str ) -> AnyStr : \"\"\" Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() \\ when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized :param filename: the local path to the file to serialize :return: the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples \"\"\" with open ( os . path . expanduser ( filename ), 'rb' ) as f : serialized_buffer = f . read () # return the buffer return serialized_buffer to_json ( self , indent = None ) Generate the workflow as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/api/workflow.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" return self . workflow_response . to_json ( indent = indent ) unenroll ( self , class_id , job_names , callback = None , options = None ) Submit a class id (speaker name, language name, etc) for un-enrollment. Parameters: Name Type Description Default class_id str the name of the enrollment class to remove required job_names List[str] a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). required callback an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. None options a dictionary of name/value option pairs to include with the enrollment request None Returns: Type Description server unenrollment response if no callback provided Source code in olivepy/api/workflow.py def unenroll ( self , class_id : str , job_names : List [ str ], callback = None , options = None ): \"\"\" Submit a class id (speaker name, language name, etc) for un-enrollment. :param class_id: the name of the enrollment class to remove :param job_names: a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). :param callback: an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. :param options: a dictionary of name/value option pairs to include with the enrollment request :return: server unenrollment response if no callback provided \"\"\" # make call blocking if no callback or always assume it is async? unenroll_request = WorkflowUnenrollRequest () unenroll_request . workflow_definition . CopyFrom ( self . workflow_def ) unenroll_request . class_id = class_id for job_task in job_names : unenroll_request . job_names . append ( job_task ) if options : jopts = utils . parse_json_options ( options ) unenroll_request . option . extend ( jopts ) if callback : # self.client.enqueue_request(enroll_request, callback, response.OliveWorkflowEnrollmentResponse()) self . client . enqueue_request ( unenroll_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( unenroll_request , response . OliveWorkflowAnalysisResponse ()) OliveWorkflowDefinition Used to load a Workflow Definition from a file. __init__ ( self , filename ) special Create an OliveWorkflowDefinition to access a workflow definition file Parameters: Name Type Description Default filename str the path/filename of a workflow definition file to load required Source code in olivepy/api/workflow.py def __init__ ( self , filename : str ): \"\"\" Create an OliveWorkflowDefinition to access a workflow definition file :param filename: the path/filename of a workflow definition file to load \"\"\" # First, make sure the workflow definition (WD) file exists filename = os . path . expanduser ( filename ) if not os . path . exists ( filename ): raise IOError ( \"Workflow definition file ' {} ' does not exists\" . format ( filename )) # Load the WD, then submit to the server # Read the workflow - either a workflow or a text file try : with open ( filename , 'rb' ) as f : self . wd = WorkflowDefinition () self . wd . ParseFromString ( f . read ()) except IOError as e : raise IOError ( \"Workflow definition file ' {} ' does not exist\" . format ( filename )) except DecodeError as de : self . wd = WorkflowDefinition () # Try parsing as text file (will fail for a protobuf file) with open ( filename , 'r' ) as f : # First load as json json_input = json . loads ( f . read ()) # Next, we need to convert message data in task(s) to byte strings for element in json_input : if element == 'order' : for job in json_input [ element ]: # print(\"Job: {}\".format(job)) for job_def in job [ 'job_definition' ]: for task in job_def [ 'tasks' ]: task_type = task [ 'message_type' ] # Covert 'messageData' into a protobuf and save the byte string in the json # so it can be correctly deserialized tmp_json = task [ 'message_data' ] msg = msgutil . type_class_map [ MessageType . Value ( task_type )]() Parse ( json . dumps ( tmp_json ), msg ) # now serialized msg as messageData data = base64 . b64encode ( msg . SerializeToString ()) . decode ( 'utf-8' ) task [ 'message_data' ] = data # Now we should be able to create a WorkflowDefinition from the json data Parse ( json . dumps ( json_input ), self . wd ) # Create JSON formatted output from the the Workflow? create_workflow ( self , client ) Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, or enrollment requests Parameters: Name Type Description Default client AsyncOliveClient an open client connection to an OLIVE server required Returns: Type Description a new OliveWorkflow object, which has been actualized (activated) by the olive server Source code in olivepy/api/workflow.py def create_workflow ( self , client : olivepy . api . olive_async_client . AsyncOliveClient ): \"\"\" Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, or enrollment requests :param client: an open client connection to an OLIVE server :return: a new OliveWorkflow object, which has been actualized (activated) by the olive server \"\"\" if not client . is_connected (): raise IOError ( \"No connection to the Olive server\" ) # Create a workflow request request = WorkflowActualizeRequest () request . workflow_definition . CopyFrom ( self . wd ) workflow_result = client . sync_request ( request , response . OliveWorkflowActualizedResponse ()) if workflow_result . is_error (): raise msgutil . ExceptionFromServer ( workflow_result . get_error ()) # if msg: # raise msgutil.ExceptionFromServer(msg) return OliveWorkflow ( client , workflow_result ) # todo send WD to server, return an OliveWorklow to the user get_json ( self , indent = 1 ) Create a JSON structure of the Workflow Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document 1 Returns: Type Description A JSON (dictionary) representation of the Workflow Definition Source code in olivepy/api/workflow.py def get_json ( self , indent = 1 ): \"\"\" Create a JSON structure of the Workflow :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document :return: A JSON (dictionary) representation of the Workflow Definition \"\"\" analysis_task = [] job_names = set () workflow_analysis_order_msg = None for order in self . wd . order : if order . workflow_type == WORKFLOW_ANALYSIS_TYPE : workflow_analysis_order_msg = order break if workflow_analysis_order_msg is None : # no analysis results return analysis_task # for job in self._response.job_result: for job in workflow_analysis_order_msg . job_definition : # create a dictionary for each job result job_dict = dict () job_name = job . job_name job_names . add ( job_name ) # get data handling info for this job data_prop = job . data_properties job_dict [ 'Data Input' ] = json . loads ( MessageToJson ( data_prop , preserving_proto_field_name = True )) # if data_prop.mode == SPLIT: # # Hack to make split/mulit-channel mode more clear # job_dict['data']['mode'] = 'SPLIT: Process each channel as a job' # and a dictionary of tasks: # add to our results - in most cases we will have just one job analysis_task . append ( job_dict ) for task in job . tasks : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # Deserialize message_data, and replace it in the task_result_dict task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ 'job_name' ] = job_name task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) del task_result_dict [ 'message_data' ] job_dict [ task . consumer_result_label ] = task_result_dict return json . dumps ( analysis_task , indent = indent ) to_json ( self , indent = None ) Generate the workflow as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/api/workflow.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" json_str_output = MessageToJson ( self . wd , preserving_proto_field_name = True ) json_output = json . loads ( json_str_output ) for element in json_output : if element == 'order' : for job in json_output [ element ]: # print(\"Job: {}\".format(job)) for job_def in job [ 'job_definition' ]: for task in job_def [ 'tasks' ]: task_type = task [ 'message_type' ] data = base64 . b64decode ( task [ 'message_data' ]) msg = self . _extract_serialized_message ( MessageType . Value ( task_type ), data ) task [ 'message_data' ] = json . loads ( MessageToJson ( msg , preserving_proto_field_name = True )) # print(\"Task: {}\".format(task)) if indent and indent < 0 : return json_output return json . dumps ( json_output , indent = indent ) WorkflowException ( Exception ) This exception means that an error occurred handling a Workflow","title":"`olivepy` `api` module"},{"location":"olivepy-docs/api.html#olivepy-api-module","text":"","title":"olivepy api module"},{"location":"olivepy-docs/api.html#olivepyapioliveclient","text":"","title":"olivepy.api.oliveclient"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.ClientBrokerWorker","text":"Performs async interactions with Olive","title":"ClientBrokerWorker"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.ClientBrokerWorker.run","text":"Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/oliveclient.py def run ( self ): logging . debug ( \"Starting Olive Status Monitor Worker for id: {} \" . format ( self . client_id )) self . working = True self . status_socket . subscribe ( \"\" ) poller = zmq . Poller () poller . register ( self . status_socket , zmq . POLLIN ) while self . working : # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . status_socket in socks : logging . debug ( \"Received status message from OLIVE...\" ) heatbeat_data = self . status_socket . recv () heatbeat = Heartbeat () heatbeat . ParseFromString ( heatbeat_data ) # do something with heartbeat... if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats logging . info ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) logging . info ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) logging . info ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) logging . info ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) logging . info ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) logging . info ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) logging . debug ( \"Number active jobs: \" + str ( stats . pool_busy )) logging . debug ( \"Number pending jobs: \" + str ( stats . pool_pending )) logging . debug ( \"Number finished jobs: \" + str ( stats . pool_finished )) logging . debug ( \"Max number jobs: \" + str ( stats . max_num_jobs )) logging . debug ( \"Server version: \" + str ( stats . server_version )) self . status_socket . close ()","title":"run()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient","text":"This is a simplified version of network library used to contact the Olive server via python code. All OLIVE calls below are synchronous, and block and until a response is received from the OLIVE server. These example API calls are intended to make working with the OLIVE API clearer since all calls are blocking. To make asynchronous requests to the OLIVE server use olivepy.api.olive_async_client.AsyncOliveClient for your enterprise application.","title":"OliveClient"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.__init__","text":"Parameters: Name Type Description Default client_id The unique name of this client. Due to a ZMQ bug on some platforms this ID can not end in '1' required address the address of the olive server, such as localhost 'localhost' request_port default olive port is 5588 5588 timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/oliveclient.py def __init__ ( self , client_id , address = 'localhost' , request_port = 5588 , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug on some platforms this ID can not end in '1' :param address: the address of the olive server, such as localhost :param request_port: default olive port is 5588 :param timeout_second: time in seconds, to wait for a response from the server \"\"\" self . client_id = client_id # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . server_request_port = request_port self . server_status_port = request_port + 1 self . timeout_seconds = timeout_second self . olive_connected = False self . info = self . fullobj = None OliveClient . setup_multithreading ()","title":"__init__()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.adapt_supervised","text":"Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required adapt_workspace a unique label for this client's adaptation required annotations_file_name the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. required Returns: Type Description the full path name of the new domain. Source code in olivepy/api/oliveclient.py def adapt_supervised ( self , plugin , domain , annotations_file_name , new_domain_name ): \"\"\" :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param adapt_workspace: a unique label for this client's adaptation :param annotations_file_name: the name of a file containing annotations. This file contains lines with four tokens: filename, start, end, and class. start and end are in milliseconds, but that should change to seconds. :return: the full path name of the new domain. \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] file_annotations = self . parse_annotation_file ( annotations_file_name ) for filename , regions in file_annotations . items (): audio_id = self . preprocess_supervised_audio ( plugin , domain , filename , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace )","title":"adapt_supervised()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.adapt_supervised_old","text":"Parameters: Name Type Description Default plugin the plugin for adaptation required domain the domain for adaptation required adapt_workspace a unique label for this client's adaptation required file_annotations a dictionary of files to preprocess, each file has one or more annotated regions for processing {filename: [(start_ms, end_ms, class)]}, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} required Returns: Type Description the full path name of the new domain. Source code in olivepy/api/oliveclient.py def adapt_supervised_old ( self , plugin , domain , file_annotations , new_domain_name ): \"\"\" :param plugin: the plugin for adaptation :param domain: the domain for adaptation :param adapt_workspace: a unique label for this client's adaptation :param file_annotations: a dictionary of files to preprocess, each file has one or more annotated regions for processing {filename: [(start_ms, end_ms, class)]}, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} :return: the full path name of the new domain. \"\"\" adapt_workspace = 'adapt-' + msgutil . get_uuid () processed_audio_list = [] for filename , regions in file_annotations . items (): audio_id = self . preprocess_supervised_audio ( plugin , domain , filename , adapt_workspace ) if audio_id : processed_audio_list . append ([ audio_id , regions ]) if len ( processed_audio_list ) == 0 : raise Exception ( \"All audio requests failed\" ) # Now convert the file based annotations into class based annotations protobuf_class_annots = self . convert_preprocessed_annotations ( processed_audio_list ) #Finally, complete the adaptation request by making a finalize reqeust return self . finalize_supervised_adaptation ( plugin , domain , new_domain_name , protobuf_class_annots , adapt_workspace )","title":"adapt_supervised_old()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.analyze_bounding_box","text":"Request a analysis of 'filename' returning bounding box scores Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server <AudioTransferType.AUDIO_PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. Source code in olivepy/api/oliveclient.py def analyze_bounding_box ( self , plugin , domain , filename , data_msg = None , mode = AudioTransferType . AUDIO_PATH , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' returning bounding box scores :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. \"\"\" self . info = self . fullobj = None region_score_result = self . _request_bounding_box_scores ( plugin , domain , filename , data_msg = data_msg , mode = mode , opts = opts , classes = classes ) self . fullobj = region_score_result return region_score_result","title":"analyze_bounding_box()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.analyze_frames","text":"Request a analysis of 'filename' returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score. if None, then provide (audio) input as a required data_msg Optionally specify the data input as a fully formed Audio or BinaryMedia message instead of creating from filename None opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis as a list of (frame) scores Source code in olivepy/api/oliveclient.py def analyze_frames ( self , plugin , domain , filename , data_msg = None , opts = None , classes = None , mode = AudioTransferType . AUDIO_PATH ): \"\"\" Request a analysis of 'filename' returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score. if None, then provide (audio) input as a :param data_msg: Optionally specify the data input as a fully formed Audio or BinaryMedia message instead of creating from filename :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis as a list of (frame) scores \"\"\" self . info = self . fullobj = None frame_score_result = self . _request_frame_scores ( plugin , domain , filename , data_msg = data_msg , opts = opts , classes = classes , mode = mode ) if frame_score_result is not None : return frame_score_result . score return []","title":"analyze_frames()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.analyze_global","text":"Request a LID analysis of 'filename' Parameters: Name Type Description Default plugin the name of the LID plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the audio transfer mode <AudioTransferType.AUDIO_PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description the analysis result as a list of (global) scores Source code in olivepy/api/oliveclient.py def analyze_global ( self , plugin , domain , filename , data_msg = None , mode = AudioTransferType . AUDIO_PATH , opts = None , classes = None ): \"\"\" Request a LID analysis of 'filename' :param plugin: the name of the LID plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the audio transfer mode :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: the analysis result as a list of (global) scores \"\"\" self . info = self . fullobj = None request = GlobalScorerRequest () request . plugin = plugin request . domain = domain if data_msg : request . audio . CopyFrom ( data_msg ) else : audio = request . audio package_audio ( audio , filename , mode = mode ) self . _add_options ( request , opts ) self . _add_classes ( request , classes ) # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) _ , env = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a global score request message\" ) result = self . _sync_request ( env ) return result . score","title":"analyze_global()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.analyze_regions","text":"Request a analysis of 'filename' returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required mode the way audio is submitted to the server <AudioTransferType.AUDIO_PATH: 1> opts a dictionary of name/value pair options None classes optionally, a list of classes classes to be scored None Returns: Type Description a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. Source code in olivepy/api/oliveclient.py def analyze_regions ( self , plugin , domain , filename , data_msg = None , mode = AudioTransferType . AUDIO_PATH , opts = None , classes = None ): \"\"\" Request a analysis of 'filename' returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param mode: the way audio is submitted to the server :param opts: a dictionary of name/value pair options :param classes: optionally, a list of classes classes to be scored :return: a list of (start, end) regions in seconds, each region indicates a speech region found in the submitted file. \"\"\" self . info = self . fullobj = None region_score_result = self . _request_region_scores ( plugin , domain , filename , data_msg = data_msg , mode = mode , opts = opts , classes = classes ) self . fullobj = region_score_result return region_score_result","title":"analyze_regions()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.apply_threshold","text":"Very simple method to convert frame scores to regions. If speech regions are desired we can provide a SAD plugin that returns regions instead of frame scores Parameters: Name Type Description Default scores required threshold required rate required Returns: Type Description frame scores a regions Source code in olivepy/api/oliveclient.py def apply_threshold ( self , scores , threshold , rate ): \"\"\" Very simple method to convert frame scores to regions. If speech regions are desired we can provide a SAD plugin that returns regions instead of frame scores :param scores: :param threshold: :param rate: :return: frame scores a regions \"\"\" inSegment = False start = 0 segments = [] for i in range ( len ( scores )): if not inSegment and scores [ i ] >= threshold : inSegment = True start = i elif inSegment and ( scores [ i ] < threshold or i == len ( scores ) - 1 ): inSegment = False startT = (( 1.0 * start / rate )) endT = ( 1.0 * i / rate ) segments . append (( startT , endT )) return segments","title":"apply_threshold()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.audio_modification","text":"Do an audio modification (such as an enhansement). This function only accepts one audio and returns on modified audio. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required Returns: Type Description the analysis as a list of (frame) scores Source code in olivepy/api/oliveclient.py def audio_modification ( self , plugin , domain , filename , data_msg = None , mode = AudioTransferType . AUDIO_PATH ): \"\"\" Do an audio modification (such as an enhansement). This function only accepts one audio and returns on modified audio. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :return: the analysis as a list of (frame) scores \"\"\" if mode != AudioTransferType . AUDIO_PATH : raise Exception ( 'oliveclient.audio_modification requires an filename path and will not work with binary audio data.' ) request = AudioModificationRequest () request . plugin = plugin request . domain = domain request . requested_channels = 1 request . requested_rate = 8000 if data_msg : request . modifications . append ( data_msg ) else : audio = Audio () package_audio ( audio , filename , mode = mode ) # audio = Audio() # audio.path = filename request . modifications . append ( audio ) _ , env = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a audio modification/enhancement request message\" ) result = self . _sync_request ( env ) return result . successful , result . modification_result [ 0 ]","title":"audio_modification()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.connect","text":"Connect this client to the server Parameters: Name Type Description Default monitor_server if true, start a thread to monitor the server connection (helpful if debugging connection issues) False Source code in olivepy/api/oliveclient.py def connect ( self , monitor_server = False ): \"\"\" Connect this client to the server :param monitor_server: if true, start a thread to monitor the server connection (helpful if debugging connection issues) \"\"\" # init the request and status socket request_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_request_port ) status_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_status_port ) context = zmq . Context () self . request_socket = context . socket ( zmq . DEALER ) self . status_socket = context . socket ( zmq . SUB ) self . request_socket . connect ( request_addr ) self . status_socket . connect ( status_addr ) # logging.debug(\"Starting Olive status monitor...\") # Run this to get status about the server (helpful to confirm the server is connected and up) if ( monitor_server ): self . worker = ClientBrokerWorker ( self . status_socket , self . client_id ) self . worker . start () else : self . worker = None self . olive_connected = True logging . debug ( \"Olive client ready\" )","title":"connect()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.convert_preprocessed_annotations","text":"Convert the file annotations (a dictionary grouped by file ID, where annotations are grouped by file ID, which has one or more regions/classes) into class annotations (where annotations are grouped by class ID, with each class having one or more files, then each file having one or more regions). Parameters: Name Type Description Default processed_audio_list the list of files (indexed by an OLIVE generated ID) and the regions/classes annotated in that file required Returns: Type Description a dictionary of ClassAnnotation objects, indexed by class ID Source code in olivepy/api/oliveclient.py def convert_preprocessed_annotations ( self , processed_audio_list ): \"\"\" Convert the file annotations (a dictionary grouped by file ID, where annotations are grouped by file ID, which has one or more regions/classes) into class annotations (where annotations are grouped by class ID, with each class having one or more files, then each file having one or more regions). :param processed_audio_list: the list of files (indexed by an OLIVE generated ID) and the regions/classes annotated in that file :return: a dictionary of ClassAnnotation objects, indexed by class ID \"\"\" # Now convert the annotations that are grouped by file into a list of annotations grouped by class ID # (speech, non-speech). This is done in two passes, the first passes builds then new mapping of # class_id -->* audio_id -->* region, # then we convert this new data structure into ClassAnnotation (Protobuf) message(s) class_annots = {} for audio_id , regions in processed_audio_list : for region in regions : start = region [ 0 ] end = region [ 1 ] class_id = region [ 2 ] if class_id not in class_annots : class_annots [ class_id ] = {} if audio_id not in class_annots [ class_id ]: class_annots [ class_id ][ audio_id ] = [] class_annots [ class_id ][ audio_id ] . append (( start , end )) # now that the annotations have been grouped by class id, create the annotation protobuf(s) protobuf_class_annots = {} for class_id in class_annots . keys (): protobuf_class_annots [ class_id ] = ClassAnnotation () protobuf_class_annots [ class_id ] . class_id = class_id # Add AudioAnnotation(s) for audio_id in class_annots [ class_id ]: aa = AudioAnnotation () # aa = protobuf_class_annots[class_id].annotations.add() in python2.7? aa . audio_id = audio_id for region in class_annots [ class_id ][ audio_id ]: # times are in milliseconds ar = AnnotationRegion () # might need to do ar = aa.regions.add() for Python2.7 ar . start_t = region [ 0 ] ar . end_t = region [ 1 ] aa . regions . append ( ar ) protobuf_class_annots [ class_id ] . annotations . append ( aa ) return protobuf_class_annots","title":"convert_preprocessed_annotations()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.enroll","text":"Request a enrollment of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required filename the filename to add as an audio only enrollment addition required data_msg an BinaryMedia message to add as an enrollment addition None Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def enroll ( self , plugin , domain , class_id , filename , data_msg = None ): \"\"\" Request a enrollment of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :param filename: the filename to add as an audio only enrollment addition :param data_msg: an BinaryMedia message to add as an enrollment addition :return: True if enrollment successful \"\"\" self . info = self . fullobj = None enrollment = ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True if data_msg : if isinstance ( data_msg , Audio ): enrollment . addition . append ( data_msg ) else : enrollment . addition_media . append ( data_msg ) else : audio = Audio () package_audio ( audio , filename ) enrollment . addition . append ( audio ) # Wrap message in an Envelope _ , env = _wrap_message ( self . client_id , enrollment ) # Now send the envelope logging . debug ( \"Sending an enrollment message\" ) result = self . _sync_request ( env ) return result # ClassModificationResult # Wrap message in an Envelope # request = self._wrap_message(enrollment) # # Now send the message # logging.debug(\"Sending a class modification request (enrollment) message\") # self.request_socket.send(request.SerializeToString()) # logging.debug(\"Sending a class modification request (enrollment) message\") # # TODO THIS IS A SYNC REQUST, CAN BE DONE ASYN WITH A CALLBACK... # # Wait for the response from the server # # logging.info(\"checking for response\") # protobuf_data = self.request_socket.recv() # logging.info(\"Received message from server...\") # envelope = Envelope() # envelope.ParseFromString(protobuf_data) # # # for this use case the server will only have one response in the evevelope: # for i in range(len(envelope.message)): # olive_msg = envelope.message[i] # # if olive_msg.HasField(\"info\"): # self.info = olive_msg.info # if olive_msg.HasField(\"error\"): # raise ExceptionFromServer('Got an error from the server: ' + olive_msg.error) # else: # enrollment_msg = ClassModificationResult() # enrollment_msg.ParseFromString(olive_msg.message_data[0]) # # # Assume there is only one result set (for 'speech'): frame_score_msg.result[0] # # TODO - clean up return. Maybe do something with message. # self.fullobj = enrollment_msg # self.info = enrollment_msg.addition_result[0].message # CLG this would only be set if there was an issue with the enrollment # return enrollment_msg.addition_result[0].successful # # return False # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate)","title":"enroll()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.finalize_supervised_adaptation","text":"Complete the adaptation Parameters: Name Type Description Default plugin the name of the plugin to adapt required domain the name of the plugin domain to adapt required new_domain_name the name of the new domain that is created within the plugin required class_annotations the audio annotations, grouped by class ID required Returns: Type Description the name of the new domain Source code in olivepy/api/oliveclient.py def finalize_supervised_adaptation ( self , plugin , domain , new_domain_name , class_annotations , adapt_workspace ): \"\"\" Complete the adaptation :param plugin: the name of the plugin to adapt :param domain: the name of the plugin domain to adapt :param new_domain_name: the name of the new domain that is created within the plugin :param class_annotations: the audio annotations, grouped by class ID :return: the name of the new domain \"\"\" self . info = self . fullobj = None request = SupervisedAdaptationRequest () request . plugin = plugin request . domain = domain request . adapt_space = adapt_workspace request . new_domain = new_domain_name # Add the class annotations for class_id in class_annotations : request . class_annotations . append ( class_annotations [ class_id ]) # request.class_annotations.extend([class_annotations[class_id]]) for Python2.7? # package the request _ , request = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a finalize adatation message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message - boiler plate code, this can be simplified envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): olive_msg = envelope . message [ i ] if olive_msg . HasField ( \"info\" ): self . info = olive_msg . info if olive_msg . HasField ( \"error\" ): raise ExceptionFromServer ( 'Got an error from the server: ' + olive_msg . error ) else : result_msg = SupervisedAdaptationResult () result_msg . ParseFromString ( olive_msg . message_data [ 0 ]) # get the new domain #if hasattr(result_msg, 'new_domain') and result_msg.new_domain is not None: # print(\"Adaptation successfully created new domain: '{}'\".format(result_msg.new_domain)) self . fullobj = result_msg return result_msg . new_domain # adapt failed... TODO: thrown exception instead? return None","title":"finalize_supervised_adaptation()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.get_fullobj","text":"This object should be used for debugging only. Example use::success = client.enroll('sid-embed-v5-py3', 'multilang-v1', 'joshua', 'file') if troubleshooting: fullobj = client.get_fullobj() print('Whole object returned from server: '+str(fullobj)) Returns: Type Description the full object returned from the last call to the server. Source code in olivepy/api/oliveclient.py def get_fullobj ( self ): \"\"\" This object should be used for debugging only. Example use::success = client.enroll('sid-embed-v5-py3', 'multilang-v1', 'joshua', 'file') \\ if troubleshooting: fullobj = client.get_fullobj() print('Whole object returned from server: '+str(fullobj)) :return: the full object returned from the last call to the server. \"\"\" return self . fullobj","title":"get_fullobj()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.get_info","text":"Returns: Type Description the info data from the last call to the server. Will return None if the last call did not return any info. Source code in olivepy/api/oliveclient.py def get_info ( self ): \"\"\" :return: the info data from the last call to the server. Will return None if the last call did not return any info. \"\"\" return self . info","title":"get_info()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.parse_annotation_file","text":"Parse a file for the names of files of audio files and their regions to use for adaptation. Parameters: Name Type Description Default filename the path and name of the file that contains the input. This file must have one or more lines having 4 columns: # filename, class, start_region_ms, end_region_ms required Returns: Type Description the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} Source code in olivepy/api/oliveclient.py def parse_annotation_file ( self , filename ): \"\"\" Parse a file for the names of files of audio files and their regions to use for adaptation. :param filename: the path and name of the file that contains the input. This file must have one or more lines having 4 columns: # filename, class, start_region_ms, end_region_ms :return: the parsed output, in a dictionary indexed by the filename, each element having one or more regions, for example {test.wav: [(2618, 6200, 'S'), (7200, 9500, 'NS')]} \"\"\" data_lines = [] file_annotations = {} if not os . path . exists ( filename ): raise Exception ( \"The annotation file ' {} ' does not exist\" . format ( filename )) with open ( filename ) as f : data_lines . extend ([ line . strip () for line in f . readlines ()]) # process the file for line in data_lines : pieces = line . split () if len ( pieces ) != 4 : raise Exception ( \"The annotation file does not contain data in the correct format, found line ' {} '\" . format ( line )) adapt_audio_path = pieces [ 0 ] # assume a relative file is used, so the full path must be specified since being sent to server # This is being sent to server. If full path is given, do nothing. Otherwise make absolute. # TODO: this will not work from UNIX to Windows or other way around. # TODO: should use Python's abspath here, don't you think? if adapt_audio_path [ 0 ] != '/' and adapt_audio_path [ 1 ] != ':' : adapt_audio_path = os . path . join ( os . getcwd (), adapt_audio_path ) # todo validate file is valid... if adapt_audio_path not in file_annotations : file_annotations [ adapt_audio_path ] = [] class_id = pieces [ 1 ] start = float ( pieces [ 2 ]) end = float ( pieces [ 3 ]) file_annotations [ adapt_audio_path ] . append (( start , end , class_id )) return file_annotations","title":"parse_annotation_file()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.preprocess_supervised_audio","text":"Submit audio for pre-processing phase of adaptation. Parameters: Name Type Description Default plugin the name of the plugin to adapt required domain the name of the plugin domain to adapt required filename the name of the audio file to submit to the server/plugin/domain for preprocessing required Returns: Type Description the unique id generated by the server for the preprocess audio, which must be used Source code in olivepy/api/oliveclient.py def preprocess_supervised_audio ( self , plugin , domain , filename , adapt_workspace ): \"\"\" Submit audio for pre-processing phase of adaptation. :param plugin: the name of the plugin to adapt :param domain: the name of the plugin domain to adapt :param filename: the name of the audio file to submit to the server/plugin/domain for preprocessing :return: the unique id generated by the server for the preprocess audio, which must be used \"\"\" # [(2.618, 6.2, 'S'), (7.2, 9.5, 'NS')] self . info = self . fullobj = None request = PreprocessAudioAdaptRequest () request . plugin = plugin request . domain = domain request . adapt_space = adapt_workspace request . class_id = \"supervised\" # HACK: for supervised validation in the backend - we will fix this in a future release so not needed # we currently don't need to set annotations (start_t, end_t) when doing pre-processing # finally, set the audio: audio = request . audio # send the name of the file to the server: audio . path = filename # alternatively, you could send an audio buffer: # from scipy.io import wavfile # sample_rate, data = wavfile.read(filename) # package_buffer_audio(audio, data, data.shape[0], sample_rate) # TODO SERIALIZE EXAMPLE... # package the request _ , request = _wrap_message ( self . client_id , request ) # Now send the message logging . debug ( \"Sending a preprocess audio (for adaptation) message\" ) self . request_socket . send ( request . SerializeToString ()) # Wait for the response from the server # logging.info(\"checking for response\") protobuf_data = self . request_socket . recv () logging . info ( \"Received message from server...\" ) #Unpack message envelope = Envelope () envelope . ParseFromString ( protobuf_data ) # for this use case the server will only have one response in the envelope: for i in range ( len ( envelope . message )): olive_msg = envelope . message [ i ] if olive_msg . HasField ( \"info\" ): self . info = olive_msg . info if olive_msg . HasField ( \"error\" ): raise ExceptionFromServer ( 'Got an error from the server: ' + olive_msg . error ) else : result_msg = PreprocessAudioAdaptResult () result_msg . ParseFromString ( olive_msg . message_data [ 0 ]) # get audio id from results, use for final annotations... # print(\"Preprocess audio ID {} having duration {}\".format(result_msg.audio_id, result_msg.duration)) self . fullobj = result_msg return result_msg . audio_id # preprocessing failed... TODO: thrown exception instead? return None","title":"preprocess_supervised_audio()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.requst_sad_adaptation","text":"Example of performing SAD adaptation Returns: Type Description Source code in olivepy/api/oliveclient.py def requst_sad_adaptation ( self ): \"\"\" Example of performing SAD adaptation :return: \"\"\" # todo move to client example (i.e. olivelearn) # using Julie's sadRegression dataset... # Assume the working directory is root directory for the SAD regression tests # Setup processing variables (get this config or via command line optons plugin = \"sad-dnn-v6a\" domain = \"multi-v1\" new_domain_name = \"python_adapted_multi-v2\" # Build the list of files plus the regions in the those files to adaptn by parsing the input file: file_annotations = self . parse_annotation_file ( \"lists/adapt_ms.lst\" ) return self . adapt_supervised_old ( plugin , domain , file_annotations , new_domain_name )","title":"requst_sad_adaptation()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.setup_multithreading","text":"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/oliveclient.py @classmethod def setup_multithreading ( cls ): \"\"\"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. \"\"\" # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL )","title":"setup_multithreading()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.OliveClient.unenroll","text":"Unenrollment the class_id Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (speaker) to enroll required Returns: Type Description True if enrollment successful Source code in olivepy/api/oliveclient.py def unenroll ( self , plugin , domain , class_id ): \"\"\" Unenrollment the class_id :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (speaker) to enroll :return: True if enrollment successful \"\"\" self . info = self . fullobj = None removal = ClassRemovalRequest () removal . plugin = plugin removal . domain = domain removal . class_id = class_id # Wrap message in an Envelope _ , request = _wrap_message ( self . client_id , removal ) logging . debug ( \"Sending a class modification request (removal) message\" ) result = self . _sync_request ( request ) # do something? return True","title":"unenroll()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.get_bit_depth","text":"Not using since not assuming numpy is available... Source code in olivepy/api/oliveclient.py def get_bit_depth ( audio ): \"\"\"Not using since not assuming numpy is available...\"\"\" # Numpy is needed to support this... dt = audio . dtype if dt == np . int8 : return BIT_DEPTH_8 elif dt == np . int16 : return BIT_DEPTH_16 elif dt == np . int32 : return BIT_DEPTH_24 else : return BIT_DEPTH_32","title":"get_bit_depth()"},{"location":"olivepy-docs/api.html#olivepy.api.oliveclient.package_buffer_audio","text":"Helper function to wrap audio data (decoded samples) into a AudioBuffer message that can submitted to the server instead of a file name. Parameters: Name Type Description Default data the data as a numpy ndarray required num_samples the number of samples required sample_rate the audio sample rate 8000 num_channels the number of channels in the audio 1 Returns: Type Description Source code in olivepy/api/oliveclient.py def package_buffer_audio ( audio , data , num_samples , sample_rate = 8000 , num_channels = 1 ): \"\"\" Helper function to wrap audio data (decoded samples) into a AudioBuffer message that can submitted to the server instead of a file name. :param data: the data as a numpy ndarray :param num_samples: the number of samples :param sample_rate: the audio sample rate :param num_channels: the number of channels in the audio :return: \"\"\" # from scipy.io import wavfile # sample_rate, data = wavfile.read('somefilename.wav') buffer = audio . audioSamples buffer . channels = num_channels buffer . samples = num_samples #data.shape[0] buffer . rate = sample_rate buffer . bit_depth = get_bit_depth ( data ) buffer . data = data . tostring () return audio","title":"package_buffer_audio()"},{"location":"olivepy-docs/api.html#olivepyapiolive_async_client","text":"","title":"olivepy.api.olive_async_client"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient","text":"This class is used to make asynchronous requests to the OLIVE server","title":"AsyncOliveClient"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.__init__","text":"Parameters: Name Type Description Default client_id str The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems required address the address of the olive server, such as localhost 'localhost' request_port default olive port is 5588 5588 timeout_second time in seconds, to wait for a response from the server 10 Source code in olivepy/api/olive_async_client.py def __init__ ( self , client_id : str , address = 'localhost' , request_port = 5588 , timeout_second = 10 ): \"\"\" :param client_id: The unique name of this client. Due to a ZMQ bug this ID can not end in '1' on some systems :param address: the address of the olive server, such as localhost :param request_port: default olive port is 5588 :param timeout_second: time in seconds, to wait for a response from the server \"\"\" threading . Thread . __init__ ( self ) self . client_id = client_id # due to a ZMQ bug the last character of the client ID can not be 1, so remove it if client_id [ - 1 ] == \"1\" : self . client_id = client_id [: - 1 ] logging . warning ( \"Last character of the client ID can not be '1', removing to avoid a ZMQ bug\" ) self . server_address = address self . server_request_port = request_port self . server_status_port = request_port + 1 self . timeout_seconds = timeout_second self . request_queue = queue . Queue () # special queue used to emulate blocking requests # self.completed_sync_request_queue = queue.Queue() self . sync_message = {} self . response_queue = {} self . working = False self . request_socket = None self . status_socket = None # thread to monitor OLIVE server heartbeats self . worker = None # self.status_socket = context.socket(zmq.SUB) self . olive_connected = False self . monitor_status = False oc . OliveClient . setup_multithreading ()","title":"__init__()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.add_heartbeat_listener","text":"Register a callback function to be notified when a heartbeat is received from the OLIVE server Parameters: Name Type Description Default heartbeat_callback Callable[[olive_pb2.Heartbeat], NoneType] The callback method that is notified each time a heartbeat message is received from the OLIVE server required Source code in olivepy/api/olive_async_client.py def add_heartbeat_listener ( self , heartbeat_callback : Callable [[ Heartbeat ], None ]): \"\"\" Register a callback function to be notified when a heartbeat is received from the OLIVE server :param heartbeat_callback: The callback method that is notified each time a heartbeat message is received \\ from the OLIVE server \"\"\" if self . worker : self . worker . add_event_callback ( heartbeat_callback ) else : print ( \"Unable to add a heartbeat listener because this client was not started with the status \" \" heartbeat monitor enabled\" )","title":"add_heartbeat_listener()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_frames","text":"Request a analysis of 'filename', returning frame scores. Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the audio to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> opts a dictionary of name/value pair options for this plugin request None Returns: Type Description a OliveServerResponse containing the status of the request (FrameScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_frames ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED , opts = None ): \"\"\" Request a analysis of 'filename', returning frame scores. :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the audio to score :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :param opts: a dictionary of name/value pair options for this plugin request :return: a OliveServerResponse containing the status of the request (FrameScorerResult) \"\"\" request = FrameScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio olivepy . messaging . msgutil . package_audio ( audio , audio_input , mode = mode ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"analyze_frames()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_global","text":"Request a global score analysis of 'filename' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the name of the audio file to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (GlobalScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_global ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED ): \"\"\" Request a global score analysis of 'filename' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the name of the audio file to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (GlobalScorerResult) \"\"\" self . info = self . fullobj = None request = GlobalScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio olivepy . messaging . msgutil . package_audio ( audio , audio_input , mode = mode ) self . enqueue_request ( request , callback ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"analyze_global()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.analyze_regions","text":"Request a analysis of 'filename', returning regions Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required filename the name of the audio file to score required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (RegionScorerResult) Source code in olivepy/api/olive_async_client.py def analyze_regions ( self , plugin , domain , filename , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED ): \"\"\" Request a analysis of 'filename', returning regions :param plugin: the name of the plugin :param domain: the name of the plugin domain :param filename: the name of the audio file to score :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (RegionScorerResult) \"\"\" request = RegionScorerRequest () request . plugin = plugin request . domain = domain audio = request . audio olivepy . messaging . msgutil . package_audio ( audio , filename , mode = mode ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"analyze_regions()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.audio_modification","text":"Used to make a AudioModificationRequest (enhancement). Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required audio_input the audio path or buffer to submit for modification required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (AudioModificationResult) Source code in olivepy/api/olive_async_client.py def audio_modification ( self , plugin , domain , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED ): \"\"\" Used to make a AudioModificationRequest (enhancement). :param plugin: the name of the plugin :param domain: the name of the plugin domain :param audio_input: the audio path or buffer to submit for modification :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (AudioModificationResult) \"\"\" if mode != olivepy . messaging . msgutil . AudioTransferType . AUDIO_PATH : raise Exception ( 'oliveclient.audio_modification requires an filename path and will not work with binary audio data.' ) request = AudioModificationRequest () request . plugin = plugin request . domain = domain request . requested_channels = 1 request . requested_rate = 8000 audio = Audio () olivepy . messaging . msgutil . package_audio ( audio , audio_input , mode = mode ) request . modifications . append ( audio ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"audio_modification()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.clear_heartbeat_listeners","text":"Remove all heartbeat listeners Source code in olivepy/api/olive_async_client.py def clear_heartbeat_listeners ( self ): \"\"\" Remove all heartbeat listeners \"\"\" if self . worker : self . worker . clear_callback ()","title":"clear_heartbeat_listeners()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.connect","text":"Connect this client to the server Parameters: Name Type Description Default monitor_server if true, starts a thread to monitor the server status connection for heartbeat messages required Source code in olivepy/api/olive_async_client.py def connect ( self , monitor_status = False ): \"\"\" Connect this client to the server :param monitor_server: if true, starts a thread to monitor the server status connection for heartbeat messages \"\"\" # logging.debug(\"Starting Olive async monitor...\") self . monitor_status = monitor_status self . connection_done = threading . Event () self . start () # block until connected self . olive_connected = True self . connection_done . wait () logging . debug ( \"Olive async client ready\" )","title":"connect()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.disconnect","text":"Closes the connection to the OLIVE server Source code in olivepy/api/olive_async_client.py def disconnect ( self ): \"\"\" Closes the connection to the OLIVE server \"\"\" if self . worker : self . worker . stopWorker () self . working = False self . olive_connected = False self . join () self . request_socket . close ()","title":"disconnect()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.enqueue_request","text":"Add a message request to the outbound queue Parameters: Name Type Description Default message the request message to send required callback this is called when response message is received from the server required wrapper the message wrapper None Source code in olivepy/api/olive_async_client.py def enqueue_request ( self , message , callback , wrapper = None ): \"\"\" Add a message request to the outbound queue :param message: the request message to send :param callback: this is called when response message is received from the server :param wrapper: the message wrapper \"\"\" if wrapper is None : wrapper = response . OliveServerResponse () self . request_queue . put (( message , callback , wrapper ))","title":"enqueue_request()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.enroll","text":"Request a enrollment of 'audio' Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to enroll required audio_input the Audio message to add as an enrollment addition required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required mode the audio transfer mode <AudioTransferType.AUDIO_SERIALIZED: 3> Returns: Type Description a OliveServerResponse containing the status of the request (ClassModificationResult) Source code in olivepy/api/olive_async_client.py def enroll ( self , plugin , domain , class_id , audio_input , callback : Callable [[ response . OliveServerResponse ], None ], mode = olivepy . messaging . msgutil . AudioTransferType . AUDIO_SERIALIZED ): \"\"\" Request a enrollment of 'audio' :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to enroll :param audio_input: the Audio message to add as an enrollment addition :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :param mode: the audio transfer mode :return: a OliveServerResponse containing the status of the request (ClassModificationResult) \"\"\" enrollment = ClassModificationRequest () enrollment . plugin = plugin enrollment . domain = domain enrollment . class_id = class_id enrollment . finalize = True audio = Audio () olivepy . messaging . msgutil . package_audio ( audio , audio_input , mode = mode ) enrollment . addition . append ( audio ) if callback : self . enqueue_request ( enrollment , callback ) else : return self . sync_request ( enrollment )","title":"enroll()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.get_active","text":"Used to make a GetActiveRequest Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (GetActiveResult) Source code in olivepy/api/olive_async_client.py def get_active ( self , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a GetActiveRequest :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (GetActiveResult) \"\"\" request = GetActiveRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"get_active()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.get_status","text":"Used to make a GetStatusRequest and receive a GetStatusResult Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse that contains the most recent server status (GetStatusResult) Source code in olivepy/api/olive_async_client.py def get_status ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetStatusRequest and receive a GetStatusResult :param callback: optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse that contains the most recent server status (GetStatusResult) \"\"\" request = GetStatusRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"get_status()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.get_update_status","text":"Used to make a GetUpdateStatusRequest Parameters: Name Type Description Default plugin the name of the plugin to query required domain the name of the domain to query required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult Source code in olivepy/api/olive_async_client.py def get_update_status ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a GetUpdateStatusRequest :param plugin: the name of the plugin to query :param domain: the name of the domain to query :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the update status of the requested plugin/domain (GetUpdateStatusResult \"\"\" request = GetUpdateStatusRequest () request . plugin = plugin request . domain = domain if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"get_update_status()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.is_connected","text":"Status of the connection to the OLIVE server Returns: Type Description True if connected Source code in olivepy/api/olive_async_client.py def is_connected ( self ): \"\"\" Status of the connection to the OLIVE server :return: True if connected \"\"\" return self . olive_connected","title":"is_connected()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.load_plugin_domain","text":"Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest message) Parameters: Name Type Description Default plugin the name of the plugin to pre-load required domain the name of hte domain to pre-load required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the update status of the request (LoadPluginDomainResult) Source code in olivepy/api/olive_async_client.py def load_plugin_domain ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a request to pre-load a plugin/domain (via a LoadPluginDomainRequest message) :param plugin: the name of the plugin to pre-load :param domain: the name of hte domain to pre-load :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the update status of the request (LoadPluginDomainResult) \"\"\" request = LoadPluginDomainRequest () request . plugin = plugin request . domain = domain if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"load_plugin_domain()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.request_plugins","text":"Used to make a PluginDirectoryRequest Parameters: Name Type Description Default callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse None Returns: Type Description a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult) Source code in olivepy/api/olive_async_client.py def request_plugins ( self , callback : Callable [[ response . OliveServerResponse ], None ] = None ): \"\"\" Used to make a PluginDirectoryRequest :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing information about available plugin/domains (PluginDirectoryResult) \"\"\" request = PluginDirectoryRequest () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"request_plugins()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.run","text":"Starts the thread to handle async messages Source code in olivepy/api/olive_async_client.py def run ( self ): \"\"\" Starts the thread to handle async messages \"\"\" try : logging . debug ( \"Starting OLIVE Async Message Worker for id: {} \" . format ( self . client_id )) context = zmq . Context () self . request_socket = context . socket ( zmq . DEALER ) # init the request and status socket request_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_request_port ) status_addr = \"tcp://\" + self . server_address + \":\" + str ( self . server_status_port ) self . request_socket . connect ( request_addr ) if self . monitor_status : logging . debug ( \"connecting to status socket...\" ) self . status_socket = context . socket ( zmq . SUB ) self . status_socket . connect ( status_addr ) self . worker = ClientMonitorThread ( self . status_socket , self . client_id ) self . worker . start () else : self . worker = None self . working = True poller = zmq . Poller () poller . register ( self . request_socket , zmq . POLLIN ) except Exception as e : logging . error ( \"Error connecting to the OLIVE server: {} \" . format ( e )) self . olive_connected = False finally : self . connection_done . set () while self . working : # First, send any client requests while not self . request_queue . empty (): request_msg , cb , wrapper = self . request_queue . get () msg_id , env = msgutil . _wrap_message ( self . client_id , request_msg ) # Add to our callback Q self . response_queue [ msg_id ] = ( request_msg , cb , wrapper ) # Now send the message logging . debug ( \"Sending client request msg type: {} \" . format ( env . message [ 0 ] . message_type )) self . request_socket . send ( env . SerializeToString ()) # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . request_socket in socks : # logging.info(\"Received message from OLIVE...\") protobuf_data = self . request_socket . recv () envelope = Envelope () envelope . ParseFromString ( protobuf_data ) for i in range ( len ( envelope . message )): self . _process_response ( envelope . message [ i ]) poller . unregister ( self . request_socket ) self . request_socket . close () # todo what do we do with the status socket? # self.status_socket.close() # print(\"Async client worker stopped\")","title":"run()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.setup_multithreading","text":"This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. Source code in olivepy/api/olive_async_client.py @classmethod def setup_multithreading ( cls ): '''This function is only needed for multithreaded programs. For those programs, you must call this function from the main thread, so it can properly set up your signals so that control-C will work properly to exit your program. ''' # https://stackoverflow.com/questions/17174001/stop-pyzmq-receiver-by-keyboardinterrupt # https://stackoverflow.com/questions/23206787/check-if-current-thread-is-main-thread-in-python if threading . current_thread () is threading . main_thread (): signal . signal ( signal . SIGINT , signal . SIG_DFL )","title":"setup_multithreading()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.sync_request","text":"Send a request to the OLIVE server, but wait for a response from the server Parameters: Name Type Description Default message the request message to send to the OLIVE server required Returns: Type Description the response from the server Source code in olivepy/api/olive_async_client.py def sync_request ( self , message , wrapper = None ): \"\"\" Send a request to the OLIVE server, but wait for a response from the server :param message: the request message to send to the OLIVE server :return: the response from the server \"\"\" if wrapper is None : wrapper = response . OliveServerResponse () # create an ID for this sync_request sync_id = msgutil . get_uuid () result_available = threading . Event () # result_event = None cb = lambda response : self . _sync_callback ( response , sync_id , result_available ) self . enqueue_request ( message , cb , wrapper ) result_available . wait () # get the result if sync_id in self . sync_message : return self . sync_message . pop ( sync_id ) else : # unexpected.... callback event completed with no result raise Exception ( \"Error waiting for a response from the server\" ) # self.completed_sync_request_queue.put()","title":"sync_request()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.unenroll","text":"Unenroll class_id Parameters: Name Type Description Default plugin the name of the plugin required domain the name of the plugin domain required class_id the name of the class (i.e. speaker) to remove required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to the request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (ClassRemovalResult) Source code in olivepy/api/olive_async_client.py def unenroll ( self , plugin , domain , class_id , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Unenroll class_id :param plugin: the name of the plugin :param domain: the name of the plugin domain :param class_id: the name of the class (i.e. speaker) to remove :param callback: optional method called when the OLIVE server returns a response to the request. If a \\ callback is not provided, this call blocks until a response is received from the OLIVE server. The callback \\ method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (ClassRemovalResult) \"\"\" removal = ClassRemovalRequest () removal . plugin = plugin removal . domain = domain removal . class_id = class_id if callback : self . enqueue_request ( removal , callback ) else : return self . sync_request ( removal )","title":"unenroll()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.unload_plugin_domain","text":"Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded plugin from server memory) Parameters: Name Type Description Default plugin the name of the plugin to unload required domain the name of hte domain to unload required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (RemovePluginDomainResult) Source code in olivepy/api/olive_async_client.py def unload_plugin_domain ( self , plugin , domain , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a unload plugin/domain request (RemovePluginDomainRequest). This request will un-load a loaded \\ plugin from server memory) :param plugin: the name of the plugin to unload :param domain: the name of hte domain to unload :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (RemovePluginDomainResult) \"\"\" request = RemovePluginDomainRequest () request . plugin = plugin . strip () request . domain = domain . strip () if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"unload_plugin_domain()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.AsyncOliveClient.update_plugin_domain","text":"Used to make a ApplyUpdateRequest Parameters: Name Type Description Default plugin the name of the plugin to update required domain the name of hte domain to update required callback Callable[[olivepy.messaging.response.OliveServerResponse], NoneType] optional method called when the OLIVE server returns a response to this request. If a callback is not provided, this call blocks until a response is received from the OLIVE server. The callback method accepts one argument: OliveServerResponse required Returns: Type Description a OliveServerResponse containing the status of the request (ApplyUpdateResult) Source code in olivepy/api/olive_async_client.py def update_plugin_domain ( self , plugin , domain , metadata , callback : Callable [[ response . OliveServerResponse ], None ]): \"\"\" Used to make a ApplyUpdateRequest :param plugin: the name of the plugin to update :param domain: the name of hte domain to update :param callback: optional method called when the OLIVE server returns a response to this request. \\ If a callback is not provided, this call blocks until a response is received from the OLIVE server. \\ The callback method accepts one argument: OliveServerResponse :return: a OliveServerResponse containing the status of the request (ApplyUpdateResult) \"\"\" request = ApplyUpdateRequest () request . plugin = plugin request . domain = domain mds = request . params for key , item in metadata : md = Metadata () md . name = key if isinstance ( item , str ): md . type = 1 elif isinstance ( item , int ): md . type = 2 elif isinstance ( item , float ): md . type = 3 elif isinstance ( item , bool ): md . type = 4 elif isinstance ( item , list ): md . type = 5 else : raise Exception ( 'Metadata {} had a {} type that was not str, int, float, bool, or list.' . format ( key , str ( type ( item )))) md . value = item mds . append ( md ) if callback : self . enqueue_request ( request , callback ) else : return self . sync_request ( request )","title":"update_plugin_domain()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.ClientMonitorThread","text":"Helper used to monitor the status of the Oliveserver","title":"ClientMonitorThread"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.ClientMonitorThread.add_event_callback","text":"Callback function that is notified of a heartbeat Parameters: Name Type Description Default callback Callable[[olive_pb2.Heartbeat], NoneType] the function that is called with a Heartbeat object required Source code in olivepy/api/olive_async_client.py def add_event_callback ( self , callback : Callable [[ Heartbeat ], None ]): \"\"\" Callback function that is notified of a heartbeat :param callback: the function that is called with a Heartbeat object \"\"\" self . event_callback . append ( callback )","title":"add_event_callback()"},{"location":"olivepy-docs/api.html#olivepy.api.olive_async_client.ClientMonitorThread.run","text":"Method representing the thread's activity. You may override this method in a subclass. The standard run() method invokes the callable object passed to the object's constructor as the target argument, if any, with sequential and keyword arguments taken from the args and kwargs arguments, respectively. Source code in olivepy/api/olive_async_client.py def run ( self ): # print(\"Starting Olive Status Monitor for id: {}\".format(self.client_id)) self . working = True self . status_socket . subscribe ( \"\" ) poller = zmq . Poller () poller . register ( self . status_socket , zmq . POLLIN ) last_heartbeat = time . time () while self . working : # Now check for any results from the server # logging.info(\"checking for response\") socks = dict ( poller . poll ( BLOCK_TIMEOUT_MS )) if self . status_socket in socks : last_heartbeat = time . time () # print(\"Received status message from OLIVE...\") heatbeat_data = self . status_socket . recv () heatbeat = Heartbeat () heatbeat . ParseFromString ( heatbeat_data ) for cb in self . event_callback : cb ( heatbeat ) else : # Consider using the same timeout for messages? if time . time () - last_heartbeat > HEARTBEAT_TIMEOUT_SECONDS : print ( \"heartbeat timeout\" ) # it has been too long since a heatbeat message was received from the server... assume there server is down for cb in self . event_callback : cb ( None ) self . status_socket . close ()","title":"run()"},{"location":"olivepy-docs/api.html#olivepyapiworkflow","text":"","title":"olivepy.api.workflow"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow","text":"An OliveWorkflow instance represents a Workflow Definition actualized by an OLIVE server. Once actualized, an OliveWorkflow instance is used to make analysis, or enrollment/unenrollment requests. An OliveWorkflow should be created using an OliveWorkflowDefinition's create_workflow() method. All calls to the server include an optional callback. When the callback is provided, the call does not block and the callback method is invoked when a response is received from the server. A callback method has 3 arguments: the original request, the response, and an error message if the request failed. Exceptions: Type Description WorkflowException If the workflow was not actualized","title":"OliveWorkflow"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.__init__","text":"Parameters: Name Type Description Default olive_async_client AsyncOliveClient the client connection to the OLIVE server required actualized_workflow OliveWorkflowActualizedResponse An OliveWorkflowDefinition actualized by the server required Source code in olivepy/api/workflow.py def __init__ ( self , olive_async_client : AsyncOliveClient , actualized_workflow : response . OliveWorkflowActualizedResponse ): \"\"\" :param olive_async_client: the client connection to the OLIVE server :param actualized_workflow: An OliveWorkflowDefinition actualized by the server \"\"\" self . client = olive_async_client self . workflow_response = actualized_workflow actualized_workflow_definition = actualized_workflow . get_workflow () # make sure an OLIvE server has actualized this workflow if not actualized_workflow_definition . actualized : raise WorkflowException ( \"Error: Can not create an OliveWorkflow using a Workflow Definition that has not \" \"been actualized by an OLIVE server\" ) self . workflow_def = actualized_workflow_definition # note: enrollment and adapt should only have one task/job # but there could be multiple plugins/task that could support enrollment or adaptation.. so we focus on # analysis","title":"__init__()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.adapt","text":"NOT YET SUPPORTED -- and not sure it will ever be supported via workflow Parameters: Name Type Description Default data_input required callback required options None finalize True Returns: Type Description not supported Source code in olivepy/api/workflow.py def adapt ( self , data_input , callback , options = None , finalize = True ): \"\"\" NOT YET SUPPORTED -- and not sure it will ever be supported via workflow :param data_input: :param callback: :param options: :param finalize: :return: not supported \"\"\" raise Exception ( \"Workflow adaption not supported\" )","title":"adapt()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.analyze","text":"Perform a workflow analysis Parameters: Name Type Description Default data_inputs List[olive_pb2.WorkflowDataRequest] a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. required callback an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. None options str a JSON string of name/value options to include with the analysis request such as '{\"filter_length\":99, \"interpolate\":1.0, \"test_name\":\"midge\"}' None Returns: Type Description OliveWorkflowAnalysisResponse an OliveWorkflowAnalysisResponse (if no callback provided) Source code in olivepy/api/workflow.py def analyze ( self , data_inputs : List [ WorkflowDataRequest ], callback = None , options : str = None ) -> response . OliveWorkflowAnalysisResponse : \"\"\" Perform a workflow analysis :param data_inputs: a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. :param callback: an optional callback that is invoked with the workflow completes. If not specified this method blocks, returning OliveWorkflowAnalysisResponse when done. Otherwise this method immediately returns and the callback method is invoked when the response is received. The callback method signature requires 3 arguments: requst, result, error_mssage. :param options: a JSON string of name/value options to include with the analysis request such as '{\"filter_length\":99, \"interpolate\":1.0, \"test_name\":\"midge\"}' :return: an OliveWorkflowAnalysisResponse (if no callback provided) \"\"\" # make call blocking if no callback or always assume it is async? analysis_request = WorkflowAnalysisRequest () for di in data_inputs : analysis_request . workflow_data_input . append ( di ) analysis_request . workflow_definition . CopyFrom ( self . workflow_def ) # Parse options (if any) if options : jopts = utils . parse_json_options ( options ) analysis_request . option . extend ( jopts ) if callback : self . client . enqueue_request ( analysis_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( analysis_request , response . OliveWorkflowAnalysisResponse ())","title":"analyze()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.enroll","text":"Submit data for enrollment. Parameters: Name Type Description Default data_inputs List[olive_pb2.WorkflowDataRequest] a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. required class_id str the name of the enrollment required job_names List[str] a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. required callback an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. None options a dictionary of name/value option pairs to include with the enrollment request None Returns: Type Description server enrollment response if no callback provided Source code in olivepy/api/workflow.py def enroll ( self , data_inputs : List [ WorkflowDataRequest ], class_id : str , job_names : List [ str ], callback = None , options = None ): \"\"\" Submit data for enrollment. :param data_inputs: a list of data inputs created using the package_audio(), package_text(), package_image(), or package_video() method. :param class_id: the name of the enrollment :param job_names: a list of job names, where the audio is enrolled with these jobs support enrollment. This value can be None, in which case the data input(s) is enrolled for each job. :param callback: an optional callback that is invoked when the workflow completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. :param options: a dictionary of name/value option pairs to include with the enrollment request :return: server enrollment response if no callback provided \"\"\" # # first, get the enrollment order # for order in self.workflow_def.order: # if order.workflow_type == WORKFLOW_ENROLLMENT_TYPE: # workflow_enrollment_order_msg = order # break # # if workflow_enrollment_order_msg is None: # raise Exception(\"This workflow does not contain any \") # # # for name in task_names: # make call blocking if no callback or always assume it is async? enroll_request = WorkflowEnrollRequest () for di in data_inputs : enroll_request . workflow_data_input . append ( di ) enroll_request . workflow_definition . CopyFrom ( self . workflow_def ) enroll_request . class_id = class_id for job_task in job_names : enroll_request . job_names . append ( job_task ) if options : jopts = utils . parse_json_options ( options ) enroll_request . option . extend ( jopts ) if callback : # self.client.enqueue_request(enroll_request, callback, response.OliveWorkflowEnrollmentResponse()) self . client . enqueue_request ( enroll_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( enroll_request , response . OliveWorkflowAnalysisResponse ())","title":"enroll()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_analysis_class_ids","text":"Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. Parameters: Name Type Description Default callback an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) None Returns: Type Description OliveClassStatusResponse an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server Source code in olivepy/api/workflow.py def get_analysis_class_ids ( self , type = WORKFLOW_ANALYSIS_TYPE , callback = None ) -> response . OliveClassStatusResponse : \"\"\" Query OLIVE for the current class IDs (i.e. speaker names for SID, keywords for QbE, etc). For tasks that support enrollment, their class IDs can change over time. :param type the WorkflowOrder type (WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, or WORKFLOW_UNENROLLMENT_TYPE) :param callback: an optional callback method that accepts a OliveClassStatusResponse object. Such as: my_callback(result : response.OliveClassStatusResponse) :return: an OliveClassStatusResponse object if no callback specified, otherwise the callback receives the OliveClassStatusResponse object when a response is received from the OLIVE server \"\"\" class_request = WorkflowClassStatusRequest () class_request . workflow_definition . CopyFrom ( self . workflow_def ) if type : class_request . type = type if callback : self . client . enqueue_request ( class_request , callback , response . OliveClassStatusResponse ()) else : return self . client . sync_request ( class_request , response . OliveClassStatusResponse ())","title":"get_analysis_class_ids()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_analysis_job_names","text":"The names of analysis jobs in this workflow (usually only one analysis job) Returns: Type Description List[str] A list of analysis job names in this workflow Source code in olivepy/api/workflow.py def get_analysis_job_names ( self ) -> List [ str ]: \"\"\" The names of analysis jobs in this workflow (usually only one analysis job) :return: A list of analysis job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_ANALYSIS_TYPE )","title":"get_analysis_job_names()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_analysis_task_info","text":"A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is not known until runtime) Returns: Type Description List[Dict[str, Dict]] JSON structured detailed information of analysis tasks used in this workflow Source code in olivepy/api/workflow.py def get_analysis_task_info ( self ) -> List [ Dict [ str , Dict ]]: \"\"\" A JSON like report of the tasks used for analysis from the actualized workflow. When possible, this report \\ includes the plugins used in the workflow (although there can be cases when the final plugin/domain used is \\ not known until runtime) :return: JSON structured detailed information of analysis tasks used in this workflow \"\"\" # return [task.consumer_result_label for task in analysis_jobs[job_name]] return self . workflow_response . to_json ( indent = 1 )","title":"get_analysis_task_info()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_analysis_tasks","text":"Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) Parameters: Name Type Description Default job_name str filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_analysis_tasks ( self , job_name : str = None ) -> List [ str ]: \"\"\" Return a list of tasks supported by this workflow. These names are unique and can generally be assumed they are named after the task type (SAD, LID, SID, etc) they support but they could use alternate names if there are multiple tasks with the same task type in a workflow (for example a workflow could have a SAD task that does frame scoring and a SAD task that does regions scoring) :param job_name: filter the returned task names to those belonging to this job name. Optional since most workflows only support one analysis job. :return: a list of task names \"\"\" analysis_jobs = response . get_workflow_jobs ( self . workflow_def , WORKFLOW_ANALYSIS_TYPE ) # better to exception or empty dict???? if len ( analysis_jobs ) == 0 : return None if job_name is not None : if job_name not in analysis_jobs : return None else : # get the default job name job_name = list ( analysis_jobs . keys ())[ 0 ] return [ task . consumer_result_label for task in analysis_jobs [ job_name ]]","title":"get_analysis_tasks()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_enrollment_job_names","text":"The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment Returns: Type Description List[str] A list of enrollment job names in this workflow Source code in olivepy/api/workflow.py def get_enrollment_job_names ( self ) -> List [ str ]: \"\"\" The names of enrollment jobs in this workflow. There should be one enrollment job for each analysis tasks that supports class enrollment :return: A list of enrollment job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_ENROLLMENT_TYPE )","title":"get_enrollment_job_names()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_enrollment_tasks","text":"Return a list of tasks that support enrollment in this workflow. Parameters: Name Type Description Default job_name str optionally the name of the enrollment job. Optional since most workflows only support one job None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_enrollment_tasks ( self , job_name : str = None , type = WORKFLOW_ENROLLMENT_TYPE ) -> List [ str ]: \"\"\" Return a list of tasks that support enrollment in this workflow. :param job_name: optionally the name of the enrollment job. Optional since most workflows only support one job :return: a list of task names \"\"\" enrollment_jobs = response . get_workflow_jobs ( self . workflow_def , type ) if len ( enrollment_jobs ) == 0 : return None if job_name is not None : if job_name not in enrollment_jobs : return None # normally (and currently the only supported option) should be just one enrollment_job... return list ( response . get_workflow_job_tasks ( enrollment_jobs , job_name ) . keys ())","title":"get_enrollment_tasks()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_unenrollment_job_names","text":"The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment Returns: Type Description List[str] A list of un-enrollment job names in this workflow Source code in olivepy/api/workflow.py def get_unenrollment_job_names ( self ) -> List [ str ]: \"\"\" The names of un-enrollment jobs in this workflow. There should be one un-enrollment job for each analysis task that supports class un-enrollment :return: A list of un-enrollment job names in this workflow \"\"\" return response . get_workflow_job_names ( self . workflow_def , WORKFLOW_UNENROLLMENT_TYPE )","title":"get_unenrollment_job_names()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.get_unenrollment_tasks","text":"Return a list of tasks that support UNenrollment in this workflow. Parameters: Name Type Description Default job_name str optionally the name of the enrollment job. Optional since most workflows only support one job None Returns: Type Description List[str] a list of task names Source code in olivepy/api/workflow.py def get_unenrollment_tasks ( self , job_name : str = None ) -> List [ str ]: \"\"\" Return a list of tasks that support UNenrollment in this workflow. :param job_name: optionally the name of the enrollment job. Optional since most workflows only support one job :return: a list of task names \"\"\" return self . get_enrollment_tasks ( job_name , type = WORKFLOW_UNENROLLMENT_TYPE )","title":"get_unenrollment_tasks()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_audio","text":"Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. Parameters: Name Type Description Default audio_data ~AnyStr the input data is a string (file path) if mode is 'AUDIO_PATH', otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples required mode specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. <InputTransferType.SERIALIZED: 3> annotations List[Tuple[float, float]] optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) None task_annotations Dict[str, Dict[str, List[Tuple[float, float]]]] optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {'SHL': {'speaker'':[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the 'SHL' task, which are labeled as class 'speaker' having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . None selected_channel int optional - the channel to process if using multi-channel audio None num_channels int The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None sample_rate int The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None num_samples int The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored None validate_local_path bool If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired True label an optional name to use with the audio None Returns: Type Description WorkflowDataRequest A populated WorkflowDataRequest to use in a workflow activity Source code in olivepy/api/workflow.py def package_audio ( self , audio_data : AnyStr , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , annotations : List [ Tuple [ float , float ]] = None , task_annotations : Dict [ str , Dict [ str , List [ Tuple [ float , float ]]]] = None , selected_channel : int = None , num_channels : int = None , sample_rate : int = None , num_samples : int = None , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" Creates an Audio object that can be submitted with a Workflow analysis, enrollment, or adapt request. :param audio_data: the input data is a string (file path) if mode is 'AUDIO_PATH', otherwise the input data is a binary buffer. Use serialize_audio() to serialize a file into a buffer, or pass in a list of PCM_16 encoded samples :param mode: specifies how the audio is sent to the server: either as (string) file path or as a binary buffer. NOTE: if sending a path, the path must be valid for the server. :param annotations: optional regions (start/end regions in seconds) as a list of tuples (start_seconds, end_seconds) :param task_annotations: optional and more regions (start/end regions in seconds) targeted for a task and classifed by a lable (such as speech, non-speech, speaker). For example: {'SHL': {'speaker'':[(0.5, 4.5), (6.8, 9.2)]}, are annotations for the 'SHL' task, which are labeled as class 'speaker' having regions 0.5 to 4.5, and 6.8 to 9.2. Use get_analysis_tasks() to get the name of workflow tasks . :param selected_channel: optional - the channel to process if using multi-channel audio :param num_channels: The number of channels if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param sample_rate: The sample rate if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param num_samples: The number of samples if audio input is a list of decoded (PCM-16) samples, if not using a buffer of PCM-16 samples this is value is ignored :param validate_local_path: If sending audio as as a string path name, then check that the path exists on the local filesystem. In some cases you may want to pass a path which is valid on the server but not this client so validation is not desired :param label: an optional name to use with the audio :return: A populated WorkflowDataRequest to use in a workflow activity \"\"\" audio = Audio () msgutil . package_audio ( audio , audio_data , annotations , selected_channel , mode , num_channels , sample_rate , num_samples , validate_local_path ) # Add any task specific regions: if task_annotations : for task_label in task_annotations . keys (): ta = audio . task_annotations . add () ta . task_label = task_label # we only expect to have one set of annotations, so just one region_label for region_label in task_annotations [ task_label ]: ta . region_label = region_label for annots in task_annotations [ task_label ][ region_label ]: region = ta . regions . add () region . start_t = np . float ( annots [ 0 ]) region . end_t = np . float ( annots [ 1 ]) wkf_data_request = WorkflowDataRequest () #fixme: this should be set based on the audio.label (filename) or given a unique name here... wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = AUDIO wkf_data_request . workflow_data = audio . SerializeToString () # consumer_data_label doesn't need to be set... use default # set job name? Currently we assume one job per workflow so punting on this for now return wkf_data_request","title":"package_audio()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_binary","text":"Parameters: Name Type Description Default video_input a video input required Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_binary ( self , binary_input , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , annotations : List [ Tuple [ float , float ]] = None , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" :param video_input: a video input :return: TBD \"\"\" media = BinaryMedia () msgutil . package_binary_media ( media , binary_input , mode = mode , validate_local_path = validate_local_path ) if label : media . label = label wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = VIDEO wkf_data_request . workflow_data = media . SerializeToString () return wkf_data_request","title":"package_binary()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_image","text":"Not yet supported Parameters: Name Type Description Default image_input An image input required Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_image ( self , image_input , mode = olivepy . messaging . msgutil . InputTransferType . SERIALIZED , validate_local_path : bool = True , label = None ) -> WorkflowDataRequest : \"\"\" Not yet supported :param image_input: An image input :return: TBD \"\"\" media = BinaryMedia () msgutil . package_binary_media ( media , image_input , mode = mode , validate_local_path = validate_local_path ) if label : media . label = label media . motion = False # todo if annotations... wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = label if label else msgutil . get_uuid () wkf_data_request . data_type = IMAGE wkf_data_request . workflow_data = media . SerializeToString () return wkf_data_request","title":"package_image()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_text","text":"Used to package data for a workflow that accepts string (text) input Parameters: Name Type Description Default text_input str a text input required optional_label str an optional label, namoe or comment associated with this input None text_workflow_key str the keyword used to identify this data in the workflow. By default a value of 'text' is assumed and recommend None Returns: Type Description WorkflowDataRequest a WorkflowDataRequest populated with the text input Source code in olivepy/api/workflow.py def package_text ( self , text_input : str , optional_label : str = None , text_workflow_key : str = None ) -> WorkflowDataRequest : \"\"\" Used to package data for a workflow that accepts string (text) input :param text_input: a text input :param optional_label: an optional label, namoe or comment associated with this input :param text_workflow_key: the keyword used to identify this data in the workflow. By default a value of 'text' is assumed and recommend :return: a WorkflowDataRequest populated with the text input \"\"\" text_msg = Text () # not (yet?) supported multiple text inputs in a request text_msg . text . append ( text_input ) if optional_label : text_msg . label = optional_label wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = text_workflow_key if text_workflow_key else 'text' wkf_data_request . data_type = TEXT wkf_data_request . workflow_data = text_msg . SerializeToString () return wkf_data_request","title":"package_text()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.package_workflow_input","text":"Parameters: Name Type Description Default input_msg the OLIVE data message to package required expected_data_type the data type of the message (Binary <OliveInputDataType.AUDIO_DATA_TYPE: 2> Returns: Type Description WorkflowDataRequest TBD Source code in olivepy/api/workflow.py def package_workflow_input ( self , input_msg , expected_data_type = msgutil . OliveInputDataType . AUDIO_DATA_TYPE ) -> WorkflowDataRequest : \"\"\" :param input_msg: the OLIVE data message to package :param expected_data_type: the data type of the message (Binary :return: TBD \"\"\" wkf_data_request = WorkflowDataRequest () wkf_data_request . data_id = input_msg . label if input_msg . label else msgutil . get_uuid () wkf_data_request . data_type = msgutil . data_type_class_map [ expected_data_type ] wkf_data_request . workflow_data = input_msg . SerializeToString () return wkf_data_request","title":"package_workflow_input()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.serialize_audio","text":"Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters: Name Type Description Default filename str the local path to the file to serialize required Returns: Type Description ~AnyStr the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples Source code in olivepy/api/workflow.py def serialize_audio ( self , filename : str ) -> AnyStr : \"\"\" Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() \\ when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized :param filename: the local path to the file to serialize :return: the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples \"\"\" with open ( os . path . expanduser ( filename ), 'rb' ) as f : serialized_buffer = f . read () # return the buffer return serialized_buffer","title":"serialize_audio()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.to_json","text":"Generate the workflow as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/api/workflow.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" return self . workflow_response . to_json ( indent = indent )","title":"to_json()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflow.unenroll","text":"Submit a class id (speaker name, language name, etc) for un-enrollment. Parameters: Name Type Description Default class_id str the name of the enrollment class to remove required job_names List[str] a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). required callback an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. None options a dictionary of name/value option pairs to include with the enrollment request None Returns: Type Description server unenrollment response if no callback provided Source code in olivepy/api/workflow.py def unenroll ( self , class_id : str , job_names : List [ str ], callback = None , options = None ): \"\"\" Submit a class id (speaker name, language name, etc) for un-enrollment. :param class_id: the name of the enrollment class to remove :param job_names: a list of job names, where the class is to be unenrolled. Jobs must support class modification . This value can be None, in which case the data input(s) is unenrolled for each job (which is likely dangerous). :param callback: an optional callback that is invoked when this workflow action completes. If not specified this method blocks, returning an OliveWorkflowAnalysisResponse when the enrollment completes on the server. Otherwise this method immediately returns and the callback method is invoked when the response is received. :param options: a dictionary of name/value option pairs to include with the enrollment request :return: server unenrollment response if no callback provided \"\"\" # make call blocking if no callback or always assume it is async? unenroll_request = WorkflowUnenrollRequest () unenroll_request . workflow_definition . CopyFrom ( self . workflow_def ) unenroll_request . class_id = class_id for job_task in job_names : unenroll_request . job_names . append ( job_task ) if options : jopts = utils . parse_json_options ( options ) unenroll_request . option . extend ( jopts ) if callback : # self.client.enqueue_request(enroll_request, callback, response.OliveWorkflowEnrollmentResponse()) self . client . enqueue_request ( unenroll_request , callback , response . OliveWorkflowAnalysisResponse ()) else : return self . client . sync_request ( unenroll_request , response . OliveWorkflowAnalysisResponse ())","title":"unenroll()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition","text":"Used to load a Workflow Definition from a file.","title":"OliveWorkflowDefinition"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition.__init__","text":"Create an OliveWorkflowDefinition to access a workflow definition file Parameters: Name Type Description Default filename str the path/filename of a workflow definition file to load required Source code in olivepy/api/workflow.py def __init__ ( self , filename : str ): \"\"\" Create an OliveWorkflowDefinition to access a workflow definition file :param filename: the path/filename of a workflow definition file to load \"\"\" # First, make sure the workflow definition (WD) file exists filename = os . path . expanduser ( filename ) if not os . path . exists ( filename ): raise IOError ( \"Workflow definition file ' {} ' does not exists\" . format ( filename )) # Load the WD, then submit to the server # Read the workflow - either a workflow or a text file try : with open ( filename , 'rb' ) as f : self . wd = WorkflowDefinition () self . wd . ParseFromString ( f . read ()) except IOError as e : raise IOError ( \"Workflow definition file ' {} ' does not exist\" . format ( filename )) except DecodeError as de : self . wd = WorkflowDefinition () # Try parsing as text file (will fail for a protobuf file) with open ( filename , 'r' ) as f : # First load as json json_input = json . loads ( f . read ()) # Next, we need to convert message data in task(s) to byte strings for element in json_input : if element == 'order' : for job in json_input [ element ]: # print(\"Job: {}\".format(job)) for job_def in job [ 'job_definition' ]: for task in job_def [ 'tasks' ]: task_type = task [ 'message_type' ] # Covert 'messageData' into a protobuf and save the byte string in the json # so it can be correctly deserialized tmp_json = task [ 'message_data' ] msg = msgutil . type_class_map [ MessageType . Value ( task_type )]() Parse ( json . dumps ( tmp_json ), msg ) # now serialized msg as messageData data = base64 . b64encode ( msg . SerializeToString ()) . decode ( 'utf-8' ) task [ 'message_data' ] = data # Now we should be able to create a WorkflowDefinition from the json data Parse ( json . dumps ( json_input ), self . wd ) # Create JSON formatted output from the the Workflow?","title":"__init__()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition.create_workflow","text":"Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, or enrollment requests Parameters: Name Type Description Default client AsyncOliveClient an open client connection to an OLIVE server required Returns: Type Description a new OliveWorkflow object, which has been actualized (activated) by the olive server Source code in olivepy/api/workflow.py def create_workflow ( self , client : olivepy . api . olive_async_client . AsyncOliveClient ): \"\"\" Create a new, executable (actualized), Workflow, which can be used to make OLIVE analysis, or enrollment requests :param client: an open client connection to an OLIVE server :return: a new OliveWorkflow object, which has been actualized (activated) by the olive server \"\"\" if not client . is_connected (): raise IOError ( \"No connection to the Olive server\" ) # Create a workflow request request = WorkflowActualizeRequest () request . workflow_definition . CopyFrom ( self . wd ) workflow_result = client . sync_request ( request , response . OliveWorkflowActualizedResponse ()) if workflow_result . is_error (): raise msgutil . ExceptionFromServer ( workflow_result . get_error ()) # if msg: # raise msgutil.ExceptionFromServer(msg) return OliveWorkflow ( client , workflow_result ) # todo send WD to server, return an OliveWorklow to the user","title":"create_workflow()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition.get_json","text":"Create a JSON structure of the Workflow Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document 1 Returns: Type Description A JSON (dictionary) representation of the Workflow Definition Source code in olivepy/api/workflow.py def get_json ( self , indent = 1 ): \"\"\" Create a JSON structure of the Workflow :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document :return: A JSON (dictionary) representation of the Workflow Definition \"\"\" analysis_task = [] job_names = set () workflow_analysis_order_msg = None for order in self . wd . order : if order . workflow_type == WORKFLOW_ANALYSIS_TYPE : workflow_analysis_order_msg = order break if workflow_analysis_order_msg is None : # no analysis results return analysis_task # for job in self._response.job_result: for job in workflow_analysis_order_msg . job_definition : # create a dictionary for each job result job_dict = dict () job_name = job . job_name job_names . add ( job_name ) # get data handling info for this job data_prop = job . data_properties job_dict [ 'Data Input' ] = json . loads ( MessageToJson ( data_prop , preserving_proto_field_name = True )) # if data_prop.mode == SPLIT: # # Hack to make split/mulit-channel mode more clear # job_dict['data']['mode'] = 'SPLIT: Process each channel as a job' # and a dictionary of tasks: # add to our results - in most cases we will have just one job analysis_task . append ( job_dict ) for task in job . tasks : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # Deserialize message_data, and replace it in the task_result_dict task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ 'job_name' ] = job_name task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) del task_result_dict [ 'message_data' ] job_dict [ task . consumer_result_label ] = task_result_dict return json . dumps ( analysis_task , indent = indent )","title":"get_json()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.OliveWorkflowDefinition.to_json","text":"Generate the workflow as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/api/workflow.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" json_str_output = MessageToJson ( self . wd , preserving_proto_field_name = True ) json_output = json . loads ( json_str_output ) for element in json_output : if element == 'order' : for job in json_output [ element ]: # print(\"Job: {}\".format(job)) for job_def in job [ 'job_definition' ]: for task in job_def [ 'tasks' ]: task_type = task [ 'message_type' ] data = base64 . b64decode ( task [ 'message_data' ]) msg = self . _extract_serialized_message ( MessageType . Value ( task_type ), data ) task [ 'message_data' ] = json . loads ( MessageToJson ( msg , preserving_proto_field_name = True )) # print(\"Task: {}\".format(task)) if indent and indent < 0 : return json_output return json . dumps ( json_output , indent = indent )","title":"to_json()"},{"location":"olivepy-docs/api.html#olivepy.api.workflow.WorkflowException","text":"This exception means that an error occurred handling a Workflow","title":"WorkflowException"},{"location":"olivepy-docs/client.html","text":"olivepy client module olivepy.client.analyze_client olivepy.client.client_common extract_input_data ( args , expected_data_type =< OliveInputDataType . AUDIO_DATA_TYPE : 2 > , fail_if_no_data = True , has_class_ids = False ) Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server Parameters: Name Type Description Default args required fail_if_no_data True has_class_ids False Returns: Type Description Source code in olivepy/client/client_common.py def extract_input_data ( args , expected_data_type = OliveInputDataType . AUDIO_DATA_TYPE , fail_if_no_data = True , has_class_ids = False ): \"\"\" Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server :param args: :param fail_if_no_data: :param has_class_ids: :return: \"\"\" send_pathname = True if args . path else False send_serialized = not send_pathname data_input = [] if send_pathname : # fixme more generic for image and video? transfer_mode = InputTransferType . PATH else : transfer_mode = InputTransferType . SERIALIZED audio = text = image = video = False if args . input_list : # parse input list to make sure we actually have one or more files... data_input = parse_data_list2 ( args . input_list , transfer_mode , expected_data_type , has_class_ids ) # data_intpu --> [{filename: DATA_MSG, {channel: [start, end, class]}] , filename: Audio, {-1: (None, None, None)} if len ( data_input ) == 0 : args_bad = True print ( \"Data input list ' {} ' contains no valid files\" . format ( args . list )) elif args . input : # Do we want to support sending a path that is not local? In case they want to specify a path that is a # available for the server if has_class_ids : # data_input.append( (convert_filename_to_data(args.input, transfer_mode, expected_data_type), args.enroll)) data_input = { args . enroll : [ convert_filename_to_data ( args . input , transfer_mode , expected_data_type )]} else : data_input . append ( convert_filename_to_data ( args . input , transfer_mode , expected_data_type ) ) # do some basic validation # If mo data input supplied, make sure that is okay with the other options provided if not data_input and fail_if_no_data : print ( 'The command requires data input(s).' ) exit ( 1 ) return data_input , transfer_mode , send_pathname olivepy.client.enroll_client olivepy.client.learn_client olivepy.client.status_client heartbeat_notification ( heatbeat ) Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server Source code in olivepy/client/status_client.py def heartbeat_notification ( heatbeat ): \"\"\"Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server\"\"\" if heatbeat : if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats print ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) print ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) print ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) print ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) print ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) print ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) print ( \"Number active jobs: \" + str ( stats . pool_busy )) print ( \"Number pending jobs: \" + str ( stats . pool_pending )) print ( \"Number finished jobs: \" + str ( stats . pool_finished )) print ( \"Max number jobs: \" + str ( stats . max_num_jobs )) print ( \"Server version: \" + str ( stats . server_version )) print ( \" \\n \" ) else : print ( \"No OLIVE heatbeat received. Olive server or connection down\" ) olivepy.client.utils_client main () Returns: Type Description Source code in olivepy/client/utils_client.py def main (): \"\"\" :return: \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyutils' ) parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--save_as_text' , action = 'store' , help = 'Save the workflow to a JSON formatted text file having this name.' ) parser . add_argument ( '--save_as_binary' , action = 'store' , help = 'Save the workflow to a binary formatted workflow file.' ) parser . add_argument ( '--print_workflow' , action = 'store_true' , help = 'Print the workflow definition file info (before it is actualized/sent to server)' ) args_bad = False args = parser . parse_args () if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True if not ( args . save_as_text or args . print_workflow or args . save_as_binary ): args_bad = True print ( 'The command requires one or more tasks.' ) if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) try : # first, create the workflow definition from the workflow file: owd = ow . OliveWorkflowDefinition ( args . workflow ) if args . save_as_text : print ( \"Saving Workflow Definition ' {} ' as ' {} '\" . format ( args . workflow , args . save_as_text )) owd . _save_as_json ( args . save_as_text ) if args . save_as_binary : print ( \"Saving Workflow Definition ' {} ' as ' {} '\" . format ( args . workflow , args . save_as_binary )) owd . _save_as_binary ( args . save_as_binary ) if args . print_workflow : wdef_json = owd . to_json ( indent = 1 ) print ( \"Workflow Definition Task Info: {} \" . format ( wdef_json )) print ( \"\" ) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) olivepy.client.workflow_client heartbeat_notification ( heatbeat ) Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server Source code in olivepy/client/workflow_client.py def heartbeat_notification ( heatbeat ): \"\"\" Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server \"\"\" if heatbeat : if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats print ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) print ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) print ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) print ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) print ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) print ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) print ( \"Number active jobs: \" + str ( stats . pool_busy )) print ( \"Number pending jobs: \" + str ( stats . pool_pending )) print ( \"Number finished jobs: \" + str ( stats . pool_finished )) print ( \"Max number jobs: \" + str ( stats . max_num_jobs )) print ( \"Server version: \" + str ( stats . server_version )) print ( \" \\n \" ) else : print ( \"Too long since a heatbeat message was received. Olive server or connection down\" ) main () Client for interacting with the workflow API Source code in olivepy/client/workflow_client.py def main (): \"\"\" Client for interacting with the workflow API \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyworkflow' , description = \"Perform OLIVE analysis using a Workflow \" \"Definition file\" ) # Required positional option parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--tasks' , action = 'store_true' , help = 'Print the workflow analysis tasks.' ) parser . add_argument ( '--class_ids' , action = 'store_true' , help = 'Print the class IDs available for analysis in the specified workflow.' ) parser . add_argument ( '--print_actualized' , action = 'store_true' , help = 'Print the actualized workflow info.' ) parser . add_argument ( '--print_workflow' , action = 'store_true' , help = 'Print the workflow definition file info (before it is actualized, if requested)' ) # # parser.add_argument('--print_options', action='store_true', # help='Print the options recognized for each task') parser . add_argument ( '-s' , '--server' , action = 'store' , default = 'localhost' , help = 'The machine the server is running on. Defaults to %(default)s .' ) parser . add_argument ( '-P' , '--port' , type = int , action = 'store' , default = 5588 , help = 'The port to use.' ) parser . add_argument ( '-t' , '--timeout' , type = int , action = 'store' , default = 10 , help = 'The timeout (in seconds) to wait for a response from the server ' ) parser . add_argument ( '-i' , '--input' , action = 'store' , help = 'The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag' ) parser . add_argument ( '--input_list' , action = 'store' , help = 'A list of files to analyze. One file per line.' ) parser . add_argument ( '--text' , action = 'store_true' , help = 'Indicates that input (or input list) is a literal text string to send in the analysis request.' ) parser . add_argument ( '--options' , action = 'store' , help = 'A JSON formatted string of workflow options such as ' '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}] or ' '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}, where the former ' 'options are only applied to the SAD task, and the later are applied to all tasks ' ) parser . add_argument ( '--path' , action = 'store_true' , help = 'Send the path of the audio instead of a buffer. ' 'Server and client must share a filesystem to use this option' ) # # parser.add_argument('--heartbeat', action='store_true', # help='Listen for server heartbeats ') # parser.add_argument('--status', action='store_true', # help='get server status') parser . add_argument ( '--debug' , action = 'store_true' , help = 'Debug mode ' ) # Not supported since it needs an additional 3rd party lib: # parser.add_argument('--decoded', action='store_true', # help='Send audio file as decoded PCM16 samples instead of sending as serialized buffer. ' # 'Input file must be a wav file') args_bad = False args = parser . parse_args () # Simple logging config if args . debug : log_level = logging . DEBUG else : log_level = logging . INFO # log_level = logging.WARN logging . basicConfig ( level = log_level ) if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True if ( args . tasks or args . class_ids or args . print_actualized or args . print_workflow ): data_required = False else : data_required = True # Our workflow should consume one of the 4 data types (but not a combination of types) .... # data_input, audio_mode, send_pathname, audio, text, image, video = client_com.extract_input_data_type(args, fail_if_no_data=data_required) if args . text : # special case of handling text data expected_data_type = OliveInputDataType . TEXT_DATA_TYPE else : expected_data_type = OliveInputDataType . BINARY_DATA_TYPE # expected_data_type = OliveInputDataType.AUDIO_DATA_TYPE # if you only want to send audio data_input , transfer_mode , send_pathname = client_com . extract_input_data ( args , expected_data_type = expected_data_type , fail_if_no_data = data_required ) json_opts = None if args . options : json_opts = args . options print ( \"Options: {} \" . format ( json_opts )) if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) enable_status_socket = False # Create the connection to the OLIVE server client = oc . AsyncOliveClient ( \"olivepy_workflow\" , args . server , args . port , args . timeout ) client . connect ( monitor_status = enable_status_socket ) try : # if args.heartbeat: # # Register to be notified of heartbeats from the OLIVE server # client.add_heartbeat_listener(heartbeat_notification) # if args.status: # # Request the current server status # server_status_response = client.get_status() # if server_status_response.is_successful(): # print(\"OLIVE JSON Server status: {}\".format(server_status_response.to_json(indent=10))) # # # # Or you can access the GetStatusResult protobuf: # print(\"OLIVE Server status: pending: {}, busy: {}, finished: {}, version: {}\" # .format(server_status_response.get_response().num_pending, # server_status_response.get_response().num_busy, # server_status_response.get_response().num_finished, # server_status_response.get_response().version)) # first, create the workflow definition from the workflow file: workflow_def = ow . OliveWorkflowDefinition ( args . workflow ) if args . print_workflow : wdef_json = workflow_def . to_json ( indent = 1 ) print ( \"Workflow Definition: \\n {} \" . format ( wdef_json )) print ( \"\" ) # Submit that workflow definition to the client for actualization (instantiation): workflow = workflow_def . create_workflow ( client ) if args . print_actualized : # tasks_json = workflow.get_analysis_task_info() print ( \"Actualized Workflow: {} \" . format ( workflow . to_json ( indent = 1 ))) print ( \"\" ) if args . tasks : # Print the analysis tasks: print ( \"Analysis Tasks: {} \" . format ( workflow . get_analysis_tasks ())) for enroll_job_name in workflow . get_enrollment_job_names (): print ( \"Enrollment job ' {} ' has Tasks: {} \" . format ( enroll_job_name , workflow . get_enrollment_tasks ( enroll_job_name ))) for unenroll_job_name in workflow . get_unenrollment_job_names (): print ( \"Unenrollment job ' {} ' has Tasks: {} \" . format ( unenroll_job_name , workflow . get_unenrollment_tasks ( unenroll_job_name ))) if args . class_ids : # Print the class IDs available for the workflow tasks: # support other types: type=olive_pb2.WORKFLOW_ENROLLMENT_TYPE? class_status_response = workflow . get_analysis_class_ids () print ( \"Class Info: {} \" . format ( class_status_response . to_json ( indent = 1 ))) buffers = [] for input in data_input : buffers . append ( workflow . package_workflow_input ( input , expected_data_type )) if ( data_required ): print ( \"Sending analysis request...\" ) response = workflow . analyze ( buffers , options = json_opts ) print ( \"Workflow analysis results:\" ) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) finally : client . disconnect () olivepy.client.workflow_enroll_client main () Returns: Type Description Source code in olivepy/client/workflow_enroll_client.py def main (): \"\"\" :return: \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyworkflowenroll' , description = \"Perform OLIVE enrollment using a Workflow \" \"Definition file\" ) # parser.add_argument('-C', '--client-id', action='store', default='olivepy_', # help='Experimental: the client_id to use') # Required positional option parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--print_jobs' , action = 'store_true' , help = 'Print the supported workflow enrollment jobs.' ) parser . add_argument ( '--job' , action = 'store' , help = 'Enroll/Unenroll an Class ID for a job(s) in the specified workflow. If not specified enroll or unenroll for ALL enrollment/unenrollment jobst' ) parser . add_argument ( '--enroll' , action = 'store' , help = 'Enroll using this (class) name. Should be used with the job argument to specify a target job to enroll with (if there are more than one enrollment jobs) ' ) parser . add_argument ( '--unenroll' , action = 'store' , help = 'Enroll using this (class) name. Should be used with the job argument to specify a job to unenroll (if there are more than one unenrollment jobs)' ) parser . add_argument ( '-i' , '--input' , action = 'store' , help = 'The data input to enroll. Either a pathname to an audio/image/video file or a string for text input' ) parser . add_argument ( '--input_list' , action = 'store' , help = 'A list of files to enroll. One file per line plus the class id to enroll.' ) parser . add_argument ( '--path' , action = 'store_true' , help = 'Send the path of the audio instead of a buffer. ' 'Server and client must share a filesystem to use this option' ) # Connection arguments parser . add_argument ( '-s' , '--server' , action = 'store' , default = 'localhost' , help = 'The machine the server is running on. Defaults to %(default)s .' ) parser . add_argument ( '-P' , '--port' , type = int , action = 'store' , default = 5588 , help = 'The port to use.' ) parser . add_argument ( '-t' , '--timeout' , type = int , action = 'store' , default = 10 , help = 'The timeout (in seconds) to wait for a response from the server ' ) # not supporting batch enrollments: # parser.add_argument('--audio_list', action='store', # help='A list of audio files to analyze. One file per line') args_bad = False args = parser . parse_args () if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True require_data = True if args . unenroll or args . print_jobs : require_data = False expected_data_type = OliveInputDataType . BINARY_DATA_TYPE data_input , transfer_mode , send_pathname = client_com . extract_input_data ( args , expected_data_type = expected_data_type , fail_if_no_data = require_data , has_class_ids = True ) # if args.enroll: # # there must be only one input # if len(data_input) > 1: # args_bad = True # print(\"The enroll and audio_list argument are mutually exclusive. Pick one and run again\") # else: # data_input = [(data_input[0], args.enroll)] print ( \"enrolling {} files\" . format ( len ( data_input ))) # if len(data_input) > 1 and not audio: # args_bad = True # print(\"Non-audio files can not be enrolled from an input list\") # support other data types.... # audios = [] using_pem = False # TODO GET CLASS IDS FROM ENROLLMENT FILE enroll = False unenroll = False if args . enroll : action_str = \"Enrollment\" enroll = True if args . unenroll : print ( \"Enrollment and un-enrollment are mutually exclusive. Pick one and run again\" ) args_bad = True elif args . unenroll : action_str = \"Unenrollment\" unenroll = True elif len ( data_input ) > 1 : enroll = True elif not args . print_jobs : args_bad = True print ( \"Must use one of the options: --enroll, --unenroll, or --print_jobs \" ) action_str = \"\" # support enrollments from a file (list and/or PEM format)? # if not (audio or image or video): # # no input provided, make sure this is a status request and not an analysis task # if (enroll): # args_bad = True # print('The command requires data (audio, image, or video) input.') if args . job : jobs = [] jobs . extend ( str . split ( args . job , ',' )) if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) # Create the connection to the OLIVE server client = oc . AsyncOliveClient ( \"olivepy_workflow\" , args . server , args . port , args . timeout ) client . connect () try : # right now, we only support analysis, so that is what we do... # first, create the workflow definition from the workflow file: owd = ow . OliveWorkflowDefinition ( args . workflow ) # Submit that workflow definition to the client for actualization (instantiation): workflow = owd . create_workflow ( client ) if args . print_jobs : # Print available jobs: print ( \"Enrollment jobs ' {} '\" . format ( workflow . get_enrollment_job_names ())) print ( \"Un-Enrollment jobs ' {} '\" . format ( workflow . get_unenrollment_job_names ())) # for enroll_job_name in workflow.get_enrollment_job_names(): # print(\"Enrollment job '{}' has Tasks: {}\".format(enroll_job_name, workflow.get_enrollment_tasks(enroll_job_name))) # for unenroll_job_name in workflow.get_unenrollment_job_names(): # print(\"Unenrollment job '{}' has Tasks: {}\".format(unenroll_job_name, workflow.get_unenrollment_tasks(unenroll_job_name))) if not args . job : if enroll : print ( \"Enrolling for all jobs: {} \" . format ( workflow . get_enrollment_job_names ())) if unenroll : print ( \"Unenrolling for all job: {} \" . format ( workflow . get_unenrollment_job_names ())) jobs = [] if len ( data_input ) > 0 : enroll_jobs = workflow . get_enrollment_job_names () if enroll_jobs is None : print ( \"ERROR: This workflow has no jobs that support enrollment\" ) quit ( 1 ) for t in jobs : if t not in enroll_jobs : print ( \"Error: Job ' {} ' can not be enrolled via this workflow. Only jobs(s) ' {} ' support enrollment.\" . format ( t , enroll_jobs )) quit ( 1 ) enroll_buffers = {} for classid in data_input . keys (): for input_msg in data_input [ classid ]: if classid not in enroll_buffers : enroll_buffers [ classid ] = [] # buffers.append(workflow.package_workflow_input(input, expected_data_type)) enroll_buffers [ classid ] . append ( workflow . package_workflow_input ( input_msg , expected_data_type )) # if audio: # # NOT SUPPORTING PEM # # # if using_pem: # # for filename, channel_dict in list(data_input.items()): # # for channel, regions in list(channel_dict.items()): # # try: # # if channel is None: # # ch_label = 0 # # else: # # ch_label = int(channel) # # # # buffers.append(workflow.package_audio(filename, mode=audio_mode, label=os.path.basename(filename), # # annotations=regions, selected_channel=ch_label)) # # # # except Exception as e: # # print(\"Failed to parse regions from (PEM) input file: {}\".format(e)) # # quit(1) # elif text: # print(\"Text enrollment not supported\") # elif video: # print(\"clg adding video file: {}\".format(filename)) # enroll_buffers[classid].append( # workflow.package_binary(filename, mode=audio_mode, label=os.path.basename(filename))) # elif image: # enroll_buffers[classid].append( # workflow.package_image(filename, mode=audio_mode, label=os.path.basename(filename))) print ( \"Workflow {} results:\" . format ( action_str . lower ())) for classid in enroll_buffers . keys (): buffers = enroll_buffers [ classid ] print ( \"enrolling {} files for class: {} \" . format ( len ( buffers ), classid )) response = workflow . enroll ( buffers , classid , jobs ) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) elif unenroll : # TODO use options unenroll_jobs = workflow . get_unenrollment_job_names () if unenroll_jobs is None : print ( \"ERROR: This workflow has no job that support unenrollment\" ) quit ( 1 ) for t in jobs : if t not in unenroll_jobs : print ( \"Error: Job ' {} ' can not be un-enrolled via this workflow. Only job(s) ' {} ' support \" \"un-enrollment.\" . format ( t , unenroll_jobs )) quit ( 1 ) response = workflow . unenroll ( args . unenroll , jobs ) print ( \"Workflow {} results:\" . format ( action_str . lower ())) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) finally : client . disconnect () parse_pem_file ( data_lines ) Parse a PEM file, grouping the results by audio file and channel Parameters: Name Type Description Default data_lines required Returns: Type Description a dictionary of audio files to score and the channel region: {'filename': {channel: [(start_region, end_region)]} } Source code in olivepy/client/workflow_enroll_client.py def parse_pem_file ( data_lines ): ''' Parse a PEM file, grouping the results by audio file and channel :param data_lines: :return: a dictionary of audio files to score and the channel region: {'filename': {channel: [(start_region, end_region)]} } ''' # We process by file and channel - the class/label is ignored regions = {} input_pem = Pem () input_pem . add_records_from_data_lines ( data_lines ) for id in input_pem . get_ids (): audio_id = os . path . expandvars ( id ) # Create a dictionary of the regions specified for the the current file regions [ audio_id ] = {} for rec in input_pem . get_records ( id ): # channel could be a list... channels = [] if type ( rec . channel ) is str : # convert to a list channels = map ( int , str . split ( rec . channel , ',' )) elif type ( rec . channel ) is int : channels . append ( rec . channel ) else : print ( \"Unsupported channel value: {} \" . format ( rec . channel )) for ch in channels : if ch not in regions [ audio_id ]: regions [ audio_id ][ ch ] = [] regions [ audio_id ][ ch ] . append (( rec . start_t , rec . end_t )) return regions","title":"`olivepy` `client` module"},{"location":"olivepy-docs/client.html#olivepy-client-module","text":"","title":"olivepy client module"},{"location":"olivepy-docs/client.html#olivepyclientanalyze_client","text":"","title":"olivepy.client.analyze_client"},{"location":"olivepy-docs/client.html#olivepyclientclient_common","text":"","title":"olivepy.client.client_common"},{"location":"olivepy-docs/client.html#olivepy.client.client_common.extract_input_data","text":"Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server Parameters: Name Type Description Default args required fail_if_no_data True has_class_ids False Returns: Type Description Source code in olivepy/client/client_common.py def extract_input_data ( args , expected_data_type = OliveInputDataType . AUDIO_DATA_TYPE , fail_if_no_data = True , has_class_ids = False ): \"\"\" Helper util to use the standard CLI arguments to package input (audio, video, image, text) for processing by the OLIVE server :param args: :param fail_if_no_data: :param has_class_ids: :return: \"\"\" send_pathname = True if args . path else False send_serialized = not send_pathname data_input = [] if send_pathname : # fixme more generic for image and video? transfer_mode = InputTransferType . PATH else : transfer_mode = InputTransferType . SERIALIZED audio = text = image = video = False if args . input_list : # parse input list to make sure we actually have one or more files... data_input = parse_data_list2 ( args . input_list , transfer_mode , expected_data_type , has_class_ids ) # data_intpu --> [{filename: DATA_MSG, {channel: [start, end, class]}] , filename: Audio, {-1: (None, None, None)} if len ( data_input ) == 0 : args_bad = True print ( \"Data input list ' {} ' contains no valid files\" . format ( args . list )) elif args . input : # Do we want to support sending a path that is not local? In case they want to specify a path that is a # available for the server if has_class_ids : # data_input.append( (convert_filename_to_data(args.input, transfer_mode, expected_data_type), args.enroll)) data_input = { args . enroll : [ convert_filename_to_data ( args . input , transfer_mode , expected_data_type )]} else : data_input . append ( convert_filename_to_data ( args . input , transfer_mode , expected_data_type ) ) # do some basic validation # If mo data input supplied, make sure that is okay with the other options provided if not data_input and fail_if_no_data : print ( 'The command requires data input(s).' ) exit ( 1 ) return data_input , transfer_mode , send_pathname","title":"extract_input_data()"},{"location":"olivepy-docs/client.html#olivepyclientenroll_client","text":"","title":"olivepy.client.enroll_client"},{"location":"olivepy-docs/client.html#olivepyclientlearn_client","text":"","title":"olivepy.client.learn_client"},{"location":"olivepy-docs/client.html#olivepyclientstatus_client","text":"","title":"olivepy.client.status_client"},{"location":"olivepy-docs/client.html#olivepy.client.status_client.heartbeat_notification","text":"Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server Source code in olivepy/client/status_client.py def heartbeat_notification ( heatbeat ): \"\"\"Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server\"\"\" if heatbeat : if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats print ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) print ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) print ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) print ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) print ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) print ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) print ( \"Number active jobs: \" + str ( stats . pool_busy )) print ( \"Number pending jobs: \" + str ( stats . pool_pending )) print ( \"Number finished jobs: \" + str ( stats . pool_finished )) print ( \"Max number jobs: \" + str ( stats . max_num_jobs )) print ( \"Server version: \" + str ( stats . server_version )) print ( \" \\n \" ) else : print ( \"No OLIVE heatbeat received. Olive server or connection down\" )","title":"heartbeat_notification()"},{"location":"olivepy-docs/client.html#olivepyclientutils_client","text":"","title":"olivepy.client.utils_client"},{"location":"olivepy-docs/client.html#olivepy.client.utils_client.main","text":"Returns: Type Description Source code in olivepy/client/utils_client.py def main (): \"\"\" :return: \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyutils' ) parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--save_as_text' , action = 'store' , help = 'Save the workflow to a JSON formatted text file having this name.' ) parser . add_argument ( '--save_as_binary' , action = 'store' , help = 'Save the workflow to a binary formatted workflow file.' ) parser . add_argument ( '--print_workflow' , action = 'store_true' , help = 'Print the workflow definition file info (before it is actualized/sent to server)' ) args_bad = False args = parser . parse_args () if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True if not ( args . save_as_text or args . print_workflow or args . save_as_binary ): args_bad = True print ( 'The command requires one or more tasks.' ) if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) try : # first, create the workflow definition from the workflow file: owd = ow . OliveWorkflowDefinition ( args . workflow ) if args . save_as_text : print ( \"Saving Workflow Definition ' {} ' as ' {} '\" . format ( args . workflow , args . save_as_text )) owd . _save_as_json ( args . save_as_text ) if args . save_as_binary : print ( \"Saving Workflow Definition ' {} ' as ' {} '\" . format ( args . workflow , args . save_as_binary )) owd . _save_as_binary ( args . save_as_binary ) if args . print_workflow : wdef_json = owd . to_json ( indent = 1 ) print ( \"Workflow Definition Task Info: {} \" . format ( wdef_json )) print ( \"\" ) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e ))","title":"main()"},{"location":"olivepy-docs/client.html#olivepyclientworkflow_client","text":"","title":"olivepy.client.workflow_client"},{"location":"olivepy-docs/client.html#olivepy.client.workflow_client.heartbeat_notification","text":"Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server Source code in olivepy/client/workflow_client.py def heartbeat_notification ( heatbeat ): \"\"\" Callback method, notified by the async client that a heartbeat message has been received from the OLIVE server \"\"\" if heatbeat : if heatbeat . HasField ( \"stats\" ): stats = heatbeat . stats print ( \"System CPU Used: %02.01f%% \" % stats . cpu_percent ) print ( \"System CPU Average: %02.01f%% \" % stats . cpu_average ) print ( \"System MEM Used: %02.01f%% \" % stats . mem_percent ) print ( \"System MEM Max: %02.01f%% \" % stats . max_mem_percent ) print ( \"System SWAP Used: %02.01f%% \" % stats . swap_percent ) print ( \"System SWAP Max: %02.01f%% \" % stats . max_swap_percent ) print ( \"Number active jobs: \" + str ( stats . pool_busy )) print ( \"Number pending jobs: \" + str ( stats . pool_pending )) print ( \"Number finished jobs: \" + str ( stats . pool_finished )) print ( \"Max number jobs: \" + str ( stats . max_num_jobs )) print ( \"Server version: \" + str ( stats . server_version )) print ( \" \\n \" ) else : print ( \"Too long since a heatbeat message was received. Olive server or connection down\" )","title":"heartbeat_notification()"},{"location":"olivepy-docs/client.html#olivepy.client.workflow_client.main","text":"Client for interacting with the workflow API Source code in olivepy/client/workflow_client.py def main (): \"\"\" Client for interacting with the workflow API \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyworkflow' , description = \"Perform OLIVE analysis using a Workflow \" \"Definition file\" ) # Required positional option parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--tasks' , action = 'store_true' , help = 'Print the workflow analysis tasks.' ) parser . add_argument ( '--class_ids' , action = 'store_true' , help = 'Print the class IDs available for analysis in the specified workflow.' ) parser . add_argument ( '--print_actualized' , action = 'store_true' , help = 'Print the actualized workflow info.' ) parser . add_argument ( '--print_workflow' , action = 'store_true' , help = 'Print the workflow definition file info (before it is actualized, if requested)' ) # # parser.add_argument('--print_options', action='store_true', # help='Print the options recognized for each task') parser . add_argument ( '-s' , '--server' , action = 'store' , default = 'localhost' , help = 'The machine the server is running on. Defaults to %(default)s .' ) parser . add_argument ( '-P' , '--port' , type = int , action = 'store' , default = 5588 , help = 'The port to use.' ) parser . add_argument ( '-t' , '--timeout' , type = int , action = 'store' , default = 10 , help = 'The timeout (in seconds) to wait for a response from the server ' ) parser . add_argument ( '-i' , '--input' , action = 'store' , help = 'The data input to analyze. Either a pathname to an audio/image/video file or a string for text input. For text input, also specify the --text flag' ) parser . add_argument ( '--input_list' , action = 'store' , help = 'A list of files to analyze. One file per line.' ) parser . add_argument ( '--text' , action = 'store_true' , help = 'Indicates that input (or input list) is a literal text string to send in the analysis request.' ) parser . add_argument ( '--options' , action = 'store' , help = 'A JSON formatted string of workflow options such as ' '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}] or ' '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}, where the former ' 'options are only applied to the SAD task, and the later are applied to all tasks ' ) parser . add_argument ( '--path' , action = 'store_true' , help = 'Send the path of the audio instead of a buffer. ' 'Server and client must share a filesystem to use this option' ) # # parser.add_argument('--heartbeat', action='store_true', # help='Listen for server heartbeats ') # parser.add_argument('--status', action='store_true', # help='get server status') parser . add_argument ( '--debug' , action = 'store_true' , help = 'Debug mode ' ) # Not supported since it needs an additional 3rd party lib: # parser.add_argument('--decoded', action='store_true', # help='Send audio file as decoded PCM16 samples instead of sending as serialized buffer. ' # 'Input file must be a wav file') args_bad = False args = parser . parse_args () # Simple logging config if args . debug : log_level = logging . DEBUG else : log_level = logging . INFO # log_level = logging.WARN logging . basicConfig ( level = log_level ) if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True if ( args . tasks or args . class_ids or args . print_actualized or args . print_workflow ): data_required = False else : data_required = True # Our workflow should consume one of the 4 data types (but not a combination of types) .... # data_input, audio_mode, send_pathname, audio, text, image, video = client_com.extract_input_data_type(args, fail_if_no_data=data_required) if args . text : # special case of handling text data expected_data_type = OliveInputDataType . TEXT_DATA_TYPE else : expected_data_type = OliveInputDataType . BINARY_DATA_TYPE # expected_data_type = OliveInputDataType.AUDIO_DATA_TYPE # if you only want to send audio data_input , transfer_mode , send_pathname = client_com . extract_input_data ( args , expected_data_type = expected_data_type , fail_if_no_data = data_required ) json_opts = None if args . options : json_opts = args . options print ( \"Options: {} \" . format ( json_opts )) if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) enable_status_socket = False # Create the connection to the OLIVE server client = oc . AsyncOliveClient ( \"olivepy_workflow\" , args . server , args . port , args . timeout ) client . connect ( monitor_status = enable_status_socket ) try : # if args.heartbeat: # # Register to be notified of heartbeats from the OLIVE server # client.add_heartbeat_listener(heartbeat_notification) # if args.status: # # Request the current server status # server_status_response = client.get_status() # if server_status_response.is_successful(): # print(\"OLIVE JSON Server status: {}\".format(server_status_response.to_json(indent=10))) # # # # Or you can access the GetStatusResult protobuf: # print(\"OLIVE Server status: pending: {}, busy: {}, finished: {}, version: {}\" # .format(server_status_response.get_response().num_pending, # server_status_response.get_response().num_busy, # server_status_response.get_response().num_finished, # server_status_response.get_response().version)) # first, create the workflow definition from the workflow file: workflow_def = ow . OliveWorkflowDefinition ( args . workflow ) if args . print_workflow : wdef_json = workflow_def . to_json ( indent = 1 ) print ( \"Workflow Definition: \\n {} \" . format ( wdef_json )) print ( \"\" ) # Submit that workflow definition to the client for actualization (instantiation): workflow = workflow_def . create_workflow ( client ) if args . print_actualized : # tasks_json = workflow.get_analysis_task_info() print ( \"Actualized Workflow: {} \" . format ( workflow . to_json ( indent = 1 ))) print ( \"\" ) if args . tasks : # Print the analysis tasks: print ( \"Analysis Tasks: {} \" . format ( workflow . get_analysis_tasks ())) for enroll_job_name in workflow . get_enrollment_job_names (): print ( \"Enrollment job ' {} ' has Tasks: {} \" . format ( enroll_job_name , workflow . get_enrollment_tasks ( enroll_job_name ))) for unenroll_job_name in workflow . get_unenrollment_job_names (): print ( \"Unenrollment job ' {} ' has Tasks: {} \" . format ( unenroll_job_name , workflow . get_unenrollment_tasks ( unenroll_job_name ))) if args . class_ids : # Print the class IDs available for the workflow tasks: # support other types: type=olive_pb2.WORKFLOW_ENROLLMENT_TYPE? class_status_response = workflow . get_analysis_class_ids () print ( \"Class Info: {} \" . format ( class_status_response . to_json ( indent = 1 ))) buffers = [] for input in data_input : buffers . append ( workflow . package_workflow_input ( input , expected_data_type )) if ( data_required ): print ( \"Sending analysis request...\" ) response = workflow . analyze ( buffers , options = json_opts ) print ( \"Workflow analysis results:\" ) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) finally : client . disconnect ()","title":"main()"},{"location":"olivepy-docs/client.html#olivepyclientworkflow_enroll_client","text":"","title":"olivepy.client.workflow_enroll_client"},{"location":"olivepy-docs/client.html#olivepy.client.workflow_enroll_client.main","text":"Returns: Type Description Source code in olivepy/client/workflow_enroll_client.py def main (): \"\"\" :return: \"\"\" parser = argparse . ArgumentParser ( prog = 'olivepyworkflowenroll' , description = \"Perform OLIVE enrollment using a Workflow \" \"Definition file\" ) # parser.add_argument('-C', '--client-id', action='store', default='olivepy_', # help='Experimental: the client_id to use') # Required positional option parser . add_argument ( 'workflow' , action = 'store' , help = 'The workflow definition to use.' ) parser . add_argument ( '--print_jobs' , action = 'store_true' , help = 'Print the supported workflow enrollment jobs.' ) parser . add_argument ( '--job' , action = 'store' , help = 'Enroll/Unenroll an Class ID for a job(s) in the specified workflow. If not specified enroll or unenroll for ALL enrollment/unenrollment jobst' ) parser . add_argument ( '--enroll' , action = 'store' , help = 'Enroll using this (class) name. Should be used with the job argument to specify a target job to enroll with (if there are more than one enrollment jobs) ' ) parser . add_argument ( '--unenroll' , action = 'store' , help = 'Enroll using this (class) name. Should be used with the job argument to specify a job to unenroll (if there are more than one unenrollment jobs)' ) parser . add_argument ( '-i' , '--input' , action = 'store' , help = 'The data input to enroll. Either a pathname to an audio/image/video file or a string for text input' ) parser . add_argument ( '--input_list' , action = 'store' , help = 'A list of files to enroll. One file per line plus the class id to enroll.' ) parser . add_argument ( '--path' , action = 'store_true' , help = 'Send the path of the audio instead of a buffer. ' 'Server and client must share a filesystem to use this option' ) # Connection arguments parser . add_argument ( '-s' , '--server' , action = 'store' , default = 'localhost' , help = 'The machine the server is running on. Defaults to %(default)s .' ) parser . add_argument ( '-P' , '--port' , type = int , action = 'store' , default = 5588 , help = 'The port to use.' ) parser . add_argument ( '-t' , '--timeout' , type = int , action = 'store' , default = 10 , help = 'The timeout (in seconds) to wait for a response from the server ' ) # not supporting batch enrollments: # parser.add_argument('--audio_list', action='store', # help='A list of audio files to analyze. One file per line') args_bad = False args = parser . parse_args () if args . workflow is None : print ( 'No workflow definition is specified.' ) args_bad = True require_data = True if args . unenroll or args . print_jobs : require_data = False expected_data_type = OliveInputDataType . BINARY_DATA_TYPE data_input , transfer_mode , send_pathname = client_com . extract_input_data ( args , expected_data_type = expected_data_type , fail_if_no_data = require_data , has_class_ids = True ) # if args.enroll: # # there must be only one input # if len(data_input) > 1: # args_bad = True # print(\"The enroll and audio_list argument are mutually exclusive. Pick one and run again\") # else: # data_input = [(data_input[0], args.enroll)] print ( \"enrolling {} files\" . format ( len ( data_input ))) # if len(data_input) > 1 and not audio: # args_bad = True # print(\"Non-audio files can not be enrolled from an input list\") # support other data types.... # audios = [] using_pem = False # TODO GET CLASS IDS FROM ENROLLMENT FILE enroll = False unenroll = False if args . enroll : action_str = \"Enrollment\" enroll = True if args . unenroll : print ( \"Enrollment and un-enrollment are mutually exclusive. Pick one and run again\" ) args_bad = True elif args . unenroll : action_str = \"Unenrollment\" unenroll = True elif len ( data_input ) > 1 : enroll = True elif not args . print_jobs : args_bad = True print ( \"Must use one of the options: --enroll, --unenroll, or --print_jobs \" ) action_str = \"\" # support enrollments from a file (list and/or PEM format)? # if not (audio or image or video): # # no input provided, make sure this is a status request and not an analysis task # if (enroll): # args_bad = True # print('The command requires data (audio, image, or video) input.') if args . job : jobs = [] jobs . extend ( str . split ( args . job , ',' )) if args_bad : print ( 'Run the command with --help or -h to see all the command line options.' ) quit ( 1 ) # Create the connection to the OLIVE server client = oc . AsyncOliveClient ( \"olivepy_workflow\" , args . server , args . port , args . timeout ) client . connect () try : # right now, we only support analysis, so that is what we do... # first, create the workflow definition from the workflow file: owd = ow . OliveWorkflowDefinition ( args . workflow ) # Submit that workflow definition to the client for actualization (instantiation): workflow = owd . create_workflow ( client ) if args . print_jobs : # Print available jobs: print ( \"Enrollment jobs ' {} '\" . format ( workflow . get_enrollment_job_names ())) print ( \"Un-Enrollment jobs ' {} '\" . format ( workflow . get_unenrollment_job_names ())) # for enroll_job_name in workflow.get_enrollment_job_names(): # print(\"Enrollment job '{}' has Tasks: {}\".format(enroll_job_name, workflow.get_enrollment_tasks(enroll_job_name))) # for unenroll_job_name in workflow.get_unenrollment_job_names(): # print(\"Unenrollment job '{}' has Tasks: {}\".format(unenroll_job_name, workflow.get_unenrollment_tasks(unenroll_job_name))) if not args . job : if enroll : print ( \"Enrolling for all jobs: {} \" . format ( workflow . get_enrollment_job_names ())) if unenroll : print ( \"Unenrolling for all job: {} \" . format ( workflow . get_unenrollment_job_names ())) jobs = [] if len ( data_input ) > 0 : enroll_jobs = workflow . get_enrollment_job_names () if enroll_jobs is None : print ( \"ERROR: This workflow has no jobs that support enrollment\" ) quit ( 1 ) for t in jobs : if t not in enroll_jobs : print ( \"Error: Job ' {} ' can not be enrolled via this workflow. Only jobs(s) ' {} ' support enrollment.\" . format ( t , enroll_jobs )) quit ( 1 ) enroll_buffers = {} for classid in data_input . keys (): for input_msg in data_input [ classid ]: if classid not in enroll_buffers : enroll_buffers [ classid ] = [] # buffers.append(workflow.package_workflow_input(input, expected_data_type)) enroll_buffers [ classid ] . append ( workflow . package_workflow_input ( input_msg , expected_data_type )) # if audio: # # NOT SUPPORTING PEM # # # if using_pem: # # for filename, channel_dict in list(data_input.items()): # # for channel, regions in list(channel_dict.items()): # # try: # # if channel is None: # # ch_label = 0 # # else: # # ch_label = int(channel) # # # # buffers.append(workflow.package_audio(filename, mode=audio_mode, label=os.path.basename(filename), # # annotations=regions, selected_channel=ch_label)) # # # # except Exception as e: # # print(\"Failed to parse regions from (PEM) input file: {}\".format(e)) # # quit(1) # elif text: # print(\"Text enrollment not supported\") # elif video: # print(\"clg adding video file: {}\".format(filename)) # enroll_buffers[classid].append( # workflow.package_binary(filename, mode=audio_mode, label=os.path.basename(filename))) # elif image: # enroll_buffers[classid].append( # workflow.package_image(filename, mode=audio_mode, label=os.path.basename(filename))) print ( \"Workflow {} results:\" . format ( action_str . lower ())) for classid in enroll_buffers . keys (): buffers = enroll_buffers [ classid ] print ( \"enrolling {} files for class: {} \" . format ( len ( buffers ), classid )) response = workflow . enroll ( buffers , classid , jobs ) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) elif unenroll : # TODO use options unenroll_jobs = workflow . get_unenrollment_job_names () if unenroll_jobs is None : print ( \"ERROR: This workflow has no job that support unenrollment\" ) quit ( 1 ) for t in jobs : if t not in unenroll_jobs : print ( \"Error: Job ' {} ' can not be un-enrolled via this workflow. Only job(s) ' {} ' support \" \"un-enrollment.\" . format ( t , unenroll_jobs )) quit ( 1 ) response = workflow . unenroll ( args . unenroll , jobs ) print ( \"Workflow {} results:\" . format ( action_str . lower ())) print ( \" {} \" . format ( response . to_json ( indent = 1 ))) except Exception as e : print ( \"Workflow failed with error: {} \" . format ( e )) finally : client . disconnect ()","title":"main()"},{"location":"olivepy-docs/client.html#olivepy.client.workflow_enroll_client.parse_pem_file","text":"Parse a PEM file, grouping the results by audio file and channel Parameters: Name Type Description Default data_lines required Returns: Type Description a dictionary of audio files to score and the channel region: {'filename': {channel: [(start_region, end_region)]} } Source code in olivepy/client/workflow_enroll_client.py def parse_pem_file ( data_lines ): ''' Parse a PEM file, grouping the results by audio file and channel :param data_lines: :return: a dictionary of audio files to score and the channel region: {'filename': {channel: [(start_region, end_region)]} } ''' # We process by file and channel - the class/label is ignored regions = {} input_pem = Pem () input_pem . add_records_from_data_lines ( data_lines ) for id in input_pem . get_ids (): audio_id = os . path . expandvars ( id ) # Create a dictionary of the regions specified for the the current file regions [ audio_id ] = {} for rec in input_pem . get_records ( id ): # channel could be a list... channels = [] if type ( rec . channel ) is str : # convert to a list channels = map ( int , str . split ( rec . channel , ',' )) elif type ( rec . channel ) is int : channels . append ( rec . channel ) else : print ( \"Unsupported channel value: {} \" . format ( rec . channel )) for ch in channels : if ch not in regions [ audio_id ]: regions [ audio_id ][ ch ] = [] regions [ audio_id ][ ch ] . append (( rec . start_t , rec . end_t )) return regions","title":"parse_pem_file()"},{"location":"olivepy-docs/messaging.html","text":"olivepy messaging module olivepy.messaging.msgutil Contains data structures that map message_type enum values to protobuf types and vice versa. Should be updated whenever new message types are added. AllowableErrorFromServer ( Exception ) This exception means that the server could not complete a request; however, the reason it could not do isn't considered an error. This special case most often occurs when requesting analysis of a submission that contains no speech, in which case the analysis could not complete since there was not speech and not due to an error running the plugin. Otherwise, this is identical to Python's plain old Exception AudioTransferType ( Enum ) The method used to send audio to the OLIVE server. There are three options for sending audio to the server: AUDIO_PATH: Send the path of the audio file to the server. NOTE: If using this option, the path must be accessible to the server AUDIO_DECODED: Send the audio as a buffer of decoded samples (PCM-16). This option is not well supported by this client since it does not AUDIO_SERIALIZED: Send the file as a binary buffer ExceptionFromServer ( Exception ) This exception means that an error occured on the server side, and this error is being sent \"up the chain\" on the client side. Otherwise, it is identical to Python's plain old Exception InputTransferType ( Enum ) The method used to send audio/data to the OLIVE server. There are three options for sending data to the server: PATH: Send the path of the audio file to the server. NOTE: If using this option, the path must be accessible to the server DECODED: Send the audio as a buffer of decoded samples (PCM-16). This option is not well supported by this client since it does not SERIALIZED: Send the file as a binary buffer OliveInputDataType ( Enum ) The type of input data send to the OLIVE server. package_audio ( audio_msg , audio_data , annotations = None , selected_channel = None , mode =< InputTransferType . PATH : 1 > , num_channels = None , sample_rate = None , num_samples = None , validate_local_path = True , label = None ) Parameters: Name Type Description Default audio_msg Audio the Olive Audio message to populate required audio_data ~AnyStr either a filename or binary buffer required annotations a list of tuple start/end regions (in seconds) None selected_channel if audio_data is multi-channel then select this channel for processing None mode the submission mode: pathname, serialized, samples <InputTransferType.PATH: 1> num_channels the number of channels in the audio None sample_rate the sample rate of the audio None num_samples the number of samples in the audio. None validate_local_path if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. True Returns: Type Description a valid Audio message Source code in olivepy/messaging/msgutil.py def package_audio ( audio_msg : Audio , audio_data : AnyStr , annotations = None , selected_channel = None , mode = InputTransferType . PATH , num_channels = None , sample_rate = None , num_samples = None , validate_local_path = True , label = None ): \"\"\" :param audio_msg: the Olive Audio message to populate :param audio_data: either a filename or binary buffer :param annotations: a list of tuple start/end regions (in seconds) :param selected_channel: if audio_data is multi-channel then select this channel for processing :param mode: the submission mode: pathname, serialized, samples :param num_channels: the number of channels in the audio :param sample_rate: the sample rate of the audio :param num_samples: the number of samples in the audio. :param validate_local_path: if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. :return: a valid Audio message :raises Exception if unable to package the audio for the specified mode. \"\"\" if mode != InputTransferType . PATH and mode != InputTransferType . DECODED and mode != InputTransferType . SERIALIZED : raise Exception ( 'Called package_audio with an unknown mode. Must be PATH, DECODED, or SERIALIZED.' ) # only supporting pathname now if mode == InputTransferType . PATH : if validate_local_path : if not os . path . exists ( audio_data ): raise Exception ( \"Error creating an OLIVE Audio message, the Audio file ' {} ' does not exist.\" . format ( audio_data )) audio_msg . path = audio_data else : audio_buffer = audio_msg . audioSamples if isinstance ( audio_data , bytes ): # audio has already been converted to a buffer... no need to change audio_buffer . data = audio_data else : # Assume we have a filename with we will serialize (decoded samples not supported) if mode != InputTransferType . SERIALIZED : raise Exception ( \"Converting ' {} ' into a decoded buffer is not supported. Client must \" \"manually decode the file and pass bytes to package_audio()\" . format ( audio_data )) buffer = serialize_audio ( audio_data ) audio_buffer . data = buffer if mode == InputTransferType . SERIALIZED : # olive.proto says these are all ignored for serialized buffers: # channels, rate, bitdepth, channels audio_buffer . serialized_file = True if mode == InputTransferType . DECODED : # This mode assumes the client has passed in a numpy array of samples, but we don't assume numpy is # installed for all clients so we don't do checks in ths this code # Get the data as shorts: # not if audio_data.dtype.kind == np.dtype(np.integer).kind: # audio_data = audio_data.astype( np.int16 ).flatten().tolist() # raise Exception(\"Error: Transferring decoded samples not supported\") problem = '' if num_channels is None or num_channels == 0 : problem += 'channel ' if sample_rate is None or sample_rate == 0 : problem += 'sample_rate ' if num_samples is None or num_samples == 0 : problem += 'num_samples' if problem != '' : raise Exception ( 'Error: can not create an OLIVE audio message from decoded samples because missing required argument(s): {} ' . format ( problem )) audio_buffer . serialized_file = False audio_buffer . channels = num_channels audio_buffer . rate = sample_rate audio_buffer . samples = num_samples audio_buffer . bit_depth = BIT_DEPTH_16 audio_buffer . encoding = PCM16 if annotations : for a in annotations : # np.float32(a[0] would be better but can we assume numpy is installed? region = audio_msg . regions . add () region . start_t = a [ 0 ] region . end_t = a [ 1 ] if selected_channel : # we can't do much validation, but if they selected a channel and specified the number of channels if num_channels : if selected_channel > num_channels : raise Exception ( \"Error: can not select channel ' {} ' if audio only contains ' {} ' channel(s)\" . format ( selected_channel , num_channels )) if selected_channel < 1 : raise Exception ( \"Error: invalid value for selected channel ' {} '. Channel must be 1 or higher \" . format ( selected_channel )) audio_msg . selected_channel = selected_channel if label : audio_msg . label = label return audio_msg package_binary_media ( binary_media_msg , media_data , annotations = None , mode =< InputTransferType . PATH : 1 > , validate_local_path = True , label = None , selected_channel = None ) Parameters: Name Type Description Default binary_media_msg BinaryMedia the Olive BinaryMedia message to populate required media_data ~AnyStr either a filename or binary buffer required annotations a list of tuple start/end regions (in seconds) None mode the submission mode: pathname, serialized, samples <InputTransferType.PATH: 1> validate_local_path if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. True Returns: Type Description a valid Audio message Source code in olivepy/messaging/msgutil.py def package_binary_media ( binary_media_msg : BinaryMedia , media_data : AnyStr , annotations = None , mode = InputTransferType . PATH , validate_local_path = True , label = None , selected_channel = None ): \"\"\" :param binary_media_msg: the Olive BinaryMedia message to populate :param media_data: either a filename or binary buffer :param annotations: a list of tuple start/end regions (in seconds) :param mode: the submission mode: pathname, serialized, samples :param validate_local_path: if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. :return: a valid Audio message :raises Exception if unable to package the audio for the specified mode. \"\"\" print ( \"adding binary media\" ) # todo support selected channel (if audio is to be handled form this data), label, and annotations if mode != InputTransferType . PATH and mode != InputTransferType . SERIALIZED : raise Exception ( 'Called package_visual_media with an unknown mode. Must be AUDIO_PATH, or AUDIO_SERIALIZED.' ) # only supporting pathname now if mode == InputTransferType . PATH : if validate_local_path : if not os . path . exists ( media_data ): raise Exception ( \"Error creating an OLIVE media message, the Audio file ' {} ' does not exist.\" . format ( media_data )) binary_media_msg . path = media_data else : media_buffer = binary_media_msg . buffer if isinstance ( media_data , bytes ): # audio has already been converted to a buffer... no need to change media_buffer . data = media_data else : # Assume we have a filename with we will serialize (decoded samples not supported) if mode != InputTransferType . SERIALIZED : raise Exception ( \"Converting ' {} ' into a decoded buffer is not supported.\" . format ( media_data )) buffer = serialize_audio ( media_data ) media_buffer . data = buffer if label : binary_media_msg . label = label if selected_channel : binary_media_msg . selected_channel = selected_channel if annotations : classic_region = binary_media_msg . regions . add () for a in annotations : # np.float32(a[0] would be better but can we assume numpy is installed? region = classic_region . regions . add () # print(\"Adding region: {} to {}\".format(a[0], a[1])) region . start_t = a [ 0 ] region . end_t = a [ 1 ] return binary_media_msg serialize_audio ( filename ) Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters: Name Type Description Default filename str the local path to the file to serialize required Returns: Type Description ~AnyStr the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples Source code in olivepy/messaging/msgutil.py def serialize_audio ( filename : str ) -> AnyStr : \"\"\" Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() \\ when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized :param filename: the local path to the file to serialize :return: the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. \\ This buffer contains the raw content of the file, it does NOT contain encoded samples \"\"\" if not os . path . exists ( os . path . expanduser ( filename )): raise Exception ( \"Error serializing an audio file, the file ' {} ' does not exist.\" . format ( filename )) with open ( os . path . expanduser ( filename ), 'rb' ) as f : serialized_buffer = f . read () # return the buffer return serialized_buffer olivepy.messaging.olive_pb2 olivepy.messaging.response OliveClassStatusResponse ( OliveServerResponse ) The container/wrapper for WorkflowClassStatusResult from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server. get_workflow_type ( self ) Return the type of workflow done in this response (analysis, enrollment, adaptation) Returns: Type Description A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped Source code in olivepy/messaging/response.py def get_workflow_type ( self ): return WORKFLOW_ANALYSIS_TYPE parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) if self . is_error (): return status_result = [] for jc in self . _response . job_class : job_name = jc . job_name job_dict = dict () self . _job_names . add ( job_name ) job_dict [ KEY_JOB_NAME ] = job_name # we have a list of data items: job_dict [ KEY_TASkS ] = {} status_result . append ( job_dict ) for task_class in jc . task : task_class_dict = json . loads ( MessageToJson ( task_class , preserving_proto_field_name = True )) del task_class_dict [ 'task_name' ] if task_class . task_name not in job_dict [ KEY_TASkS ]: job_dict [ KEY_TASkS ][ task_class . task_name ] = [] job_dict [ KEY_TASkS ][ task_class . task_name ] . append ( task_class_dict ) #job_dict[KEY_TASkS].append({task_class.task_name: task_class_dict}) self . _json_result = status_result to_json ( self , indent = None ) Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" # consider setting preserving_proto_field_name to true # return json.dumps(json.loads(MessageToJson(self._response, preserving_proto_field_name=True)), indent=indent) if self . is_error (): return self . get_error () if indent and indent < 0 : return json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )) return json . dumps ( json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )), indent = indent , ensure_ascii = False ) OliveServerResponse The default container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server. get_response ( self ) The Protobuf message returned from the OLIVE server Returns: Type Description Source code in olivepy/messaging/response.py def get_response ( self ): \"\"\" The Protobuf message returned from the OLIVE server :return: \"\"\" # todo exception if none? return self . _response get_workflow_type ( self ) Return the type of workflow done in this response (analysis, enrollment, adaptation) Returns: Type Description A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped Source code in olivepy/messaging/response.py def get_workflow_type ( self ): \"\"\" Return the type of workflow done in this response (analysis, enrollment, adaptation) :return: A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped \"\"\" if not self . _response : raise Exception ( \"No valid response\" ) if isinstance ( self . _request , WorkflowAnalysisRequest ): return WORKFLOW_ANALYSIS_TYPE elif isinstance ( self . _request , WorkflowEnrollRequest ): return WORKFLOW_ENROLLMENT_TYPE elif isinstance ( self . _request , WorkflowAdaptRequest ): return WORKFLOW_ADAPT_TYPE elif isinstance ( self . _request , WorkflowUnenrollRequest ): return WORKFLOW_UNENROLLMENT_TYPE raise Exception ( \"Unknown Workflow Message: {} \" . format ( type ( self . _request ))) parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): \"\"\" Create this response from the :param request: :param response: :param message: :return: \"\"\" self . _request = request if message : # No results due to error self . _iserror = True self . _message = message # self._request = request if response is not None : try : if response . HasField ( \"error\" ): self . _iserror = True self . _message = response . error else : # we assume no errors self . _issuccessful = True except : # Some messages have no error field self . _issuccessful = True self . _response = response to_json ( self , indent = None ) Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the response as as JSON string: \"\"\" #consider setting preserving_proto_field_name to true if indent and indent < 0 : return json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )) return json . dumps ( json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )), indent = indent , ensure_ascii = False ) OliveWorkflowActualizedResponse ( OliveServerResponse ) Extracts info from an actualized workflow definition get_analysis_jobs ( self ) Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. Returns: Type Description a list of job names in the analysis Source code in olivepy/messaging/response.py def get_analysis_jobs ( self ): \"\"\" Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. :return: a list of job names in the analysis \"\"\" return [ job_dict [ KEY_JOB_NAME ] for job_dict in self . _json_result ] #todo get analysis job name(s) get_request_jobs ( self , workflow_type ) return the jobs in the original request for the specified analysis type Parameters: Name Type Description Default workflow_type the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) required Returns: Type Description the list of jobs for this type Source code in olivepy/messaging/response.py def get_request_jobs ( self , workflow_type ): \"\"\" return the jobs in the original request for the specified analysis type :param workflow_type: the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) :return: the list of jobs for this type \"\"\" if self . _request is not None : return get_workflow_jobs ( self . _request . workflow_definition , workflow_type ) raise Exception ( \"No jobs for the requested workflow type: {} \" . format ( workflow_type )) is_allowable_error ( self ) Returns: Type Description true if this response failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this response failed with an allowable error \"\"\" return self . _isallowable_error parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will walk the tree and deserialize any encoded messages if self . is_error (): # todo provide error info in JSON? return # make a new message for this type... if not isinstance ( self . _request , WorkflowActualizeRequest ): # we received an some other workflow message so se don't need to convert it to json... return # we only parse the analyze part now analysis_task = [] workflow_analysis_order_msg = None for order in self . _response . workflow . order : if order . workflow_type == WORKFLOW_ANALYSIS_TYPE : workflow_analysis_order_msg = order break if workflow_analysis_order_msg is None : # no analysis results return # for job in self._response.job_result: for job in workflow_analysis_order_msg . job_definition : # create a dictionary for each job result job_dict = dict () job_name = job . job_name self . _job_names . add ( job_name ) # and a dictionary of tasks: # job_dict[KEY_TASkS] = {} # add to our results - in most cases we will have just one job analysis_task . append ( job_dict ) # get data handling info for this job data_prop = job . data_properties job_dict [ 'Data Input' ] = json . loads ( MessageToJson ( data_prop , preserving_proto_field_name = True )) # if data_prop.mode == SPLIT: # # Hack to make split/mulit-channel mode more clear # job_dict['data']['mode'] = 'SPLIT: Process each channel as a job' for task in job . tasks : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # Deserialize message_data, and replace it in the task_result_dict task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ KEY_JOB_NAME ] = job_name task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) del task_result_dict [ 'message_data' ] job_dict [ task . consumer_result_label ] = task_result_dict self . _json_result = analysis_task to_json ( self , indent = None ) Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False ) # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=False)), indent=indent) OliveWorkflowAnalysisResponse ( OliveServerResponse ) The default container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server. get_analysis_jobs ( self ) Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. Returns: Type Description a list of job names in the analysis Source code in olivepy/messaging/response.py def get_analysis_jobs ( self ): \"\"\" Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. :return: a list of job names in the analysis \"\"\" return [ job_dict [ KEY_JOB_NAME ] for job_dict in self . _json_result ] #todo get analysis job name(s) get_analysis_task_result ( self , job_name , task_name ) Get the result(s) for the specified job_name and task_name, also include the data used for this task. If the workflow analyzes each channel in multi-channel data then there can be multiple jobs with the same name. Parameters: Name Type Description Default job_name for convenience can be None, since there is normally only one job. But if the workflow has multiple jobs then a valid name must be specified. required task_name the name to the task required Returns: Type Description a list of dictionaries, where each dictionary in the list includes the results for the specified task and a list of the data analyzed by this task, such as [ {task_name:{}, data:[] }] Source code in olivepy/messaging/response.py def get_analysis_task_result ( self , job_name , task_name ): \"\"\" Get the result(s) for the specified job_name and task_name, also include the data used for this task. If the workflow analyzes each channel in multi-channel data then there can be multiple jobs with the same name. :param job_name: for convenience can be None, since there is normally only one job. But if the workflow has multiple jobs then a valid name must be specified. :param task_name: the name to the task :return: a list of dictionaries, where each dictionary in the list includes the results for the specified task and a list of the data analyzed by this task, such as [ {task_name:{}, data:[] }] \"\"\" if job_name is None : job_name = self . _get_default_job_name () results = [] for job_dict in self . _json_result : if job_name == job_dict [ KEY_JOB_NAME ]: task_dict = dict () # there may be one or more result for task_name task_dict [ task_name ] = job_dict [ KEY_TASkS ][ task_name ] task_dict [ KEY_DATA ] = job_dict [ KEY_DATA ] results . append ( task_dict ) return results get_request_jobs ( self , workflow_type ) return the jobs in the original request for the specified analysis type Parameters: Name Type Description Default workflow_type the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) required Returns: Type Description the list of jobs for this type Source code in olivepy/messaging/response.py def get_request_jobs ( self , workflow_type ): \"\"\" return the jobs in the original request for the specified analysis type :param workflow_type: the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) :return: the list of jobs for this type \"\"\" if self . _request is not None : return get_workflow_jobs ( self . _request . workflow_definition , workflow_type ) raise Exception ( \"No jobs for the requested workflow type: {} \" . format ( workflow_type )) is_allowable_error ( self ) Returns: Type Description true if this response failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this response failed with an allowable error \"\"\" return self . _isallowable_error parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will walk the tree and deserialize any encoded messages if self . is_error (): self . _json_result = {} self . _json_result [ 'error' ] = self . get_error () return # make a new message for this type... if isinstance ( self . _request , WorkflowActualizeRequest ): # we received an actualized workflow, so se don't need to convert it to json... return # this should only contain an analysis request... but check just in case: wk_type = self . get_workflow_type () if wk_type == WORKFLOW_ANALYSIS_TYPE or wk_type == WORKFLOW_ENROLLMENT_TYPE or wk_type == WORKFLOW_UNENROLLMENT_TYPE : # analysis is a list of dictionary elements, which looks like: [ {job_name: X, data: [], tasks: {}} ] # there is a dictionary for each job, but due to the way jobs work in OLIVE for mulit-channel data we # consider a jobs to be unique by a combination of job_name plus data, so multiples dictionary elements may # have the same job_name, but will have different data properties (channel numbers) # get the analysis job order from the original request: analysis_result = [] # or enrollment/unenrollment result # analysis_result['jobs'] = [] # analysis_result['data inputs'] = [] job_requests = get_workflow_jobs ( self . _request . workflow_definition , wk_type ) for job in self . _response . job_result : # create a dictionary for each job result job_dict = dict () job_name = job . job_name self . _job_names . add ( job_name ) # job_dict[job_name] = dict() job_dict [ KEY_JOB_NAME ] = job_name if job . error : job_dict [ 'error' ] = job . error # we have a list of data items: job_dict [ KEY_DATA ] = [] # and a dictionary of tasks: (although note it is possible to have multiple tasks with the same name, so a task has a list of results) job_dict [ KEY_TASkS ] = {} # add to our results - in most cases we will have just one job analysis_result . append ( job_dict ) # get the tasks for the current (and likely, only) job: task_requests = get_workflow_job_tasks ( job_requests , job_name ) # task_result_dict = dict() # I don't think this can happen yet: # if job.HasField('error'): # Allowable job error # for task in job . task_results : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # check if this task failed with an error if task . HasField ( 'error' ): # Allowable error if KEY_ERROR in job_dict : job_dict [ KEY_ERROR ] = job_dict [ KEY_ERROR ] + \",\" + task . error else : job_dict [ KEY_ERROR ] = task . error self . _isallowable_error = True # should have an empty message data; del task_result_dict [ 'message_data' ] if job_name not in self . _allowable_failed_job_tasks : self . _allowable_failed_job_tasks [ job_name ] = [] self . _allowable_failed_job_tasks [ job_name ] . append ( task . task_name ) else : # Deserialize message_data, and replace it in the task_result_dict if task . message_type in msgutil . debug_message_map : # Get the pimiento message (debug only - these messages are not guaranteed to be supported print ( \"CLG special msg type: {} \" . format ( msgutil . MessageType . Name ( task . message_type ))) pimiento_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) if task . message_type == DATA_OUTPUT_TRANSFORMER_RESULT : if pimiento_msg . data_type == TEXT : # only supported type for now... pie_data_msg = WorkflowTextResult () pie_data_msg . ParseFromString ( pimiento_msg . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( pie_data_msg , preserving_proto_field_name = True )) else : print ( \"Unsupported debug message type: {} \" . format ( msgutil . InputDataType . Name ( pimiento_msg . data_type ))) elif task . message_type == SCORE_OUTPUT_TRANSFORMER_RESULT : # these should be standard trait message pie_score_msg = self . _extract_serialized_message ( pimiento_msg . message_type , pimiento_msg . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( pie_score_msg , preserving_proto_field_name = True )) else : task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) # Should we create special handlers for analysis results, like we sort global score results, # but what should be do with an AUDIO_MODIFICATION_RESULT? if task . message_type == GLOBAL_SCORER_RESULT : # Sort region scores task_result_dict [ 'analysis' ][ 'score' ] = sorted ( task_result_dict [ 'analysis' ][ 'score' ], key = sort_global_scores , reverse = True ) # messageData has been replaced with the actual task del task_result_dict [ 'message_data' ] # taskName is the key, so remove it: del task_result_dict [ 'task_name' ] # check if we need to add the plugin/domain name orig_task = task_requests [ task . task_name ] if orig_task . message_type in msgutil . plugin_message_map : # Get the original task task_req_msg = self . _extract_serialized_message ( orig_task . message_type , orig_task . message_data ) task_result_dict [ 'plugin' ] = task_req_msg . plugin task_result_dict [ 'domain' ] = task_req_msg . domain if orig_task . message_type == CLASS_MODIFICATION_REQUEST or orig_task . message_type == CLASS_REMOVAL_REQUEST : # add class ID task_result_dict [ 'class_id' ] = self . _request . class_id # print(\"adding {} as {}\".format(task_result_dict, task.task_name)) # fixme: there can be multiple taks with the same name if conditions in a workflow caused the task to be ran twice if task . task_name not in job_dict [ KEY_TASkS ]: job_dict [ KEY_TASkS ][ task . task_name ] = [] job_dict [ KEY_TASkS ][ task . task_name ] . append ( task_result_dict ) #job_dict[KEY_TASkS].append({task.task_name: task_result_dict}) # A job usually has one data input/output, but we handle as if there are multiple for data_result in job . data_results : data_result_dict = json . loads ( MessageToJson ( data_result , preserving_proto_field_name = True )) # Deserialize the data portion data_type_msg = self . _extract_serialized_message ( data_result . msg_type , data_result . result_data ) del data_result_dict [ 'result_data' ] # del data_result_dict['dataId'] # redundant into, used as the key # data_result_dict['data'] = json.loads(MessageToJson(data_type_msg)) data_result_dict . update ( json . loads ( MessageToJson ( data_type_msg , preserving_proto_field_name = True ))) job_dict [ KEY_DATA ] . append ( data_result_dict ) # analysis_result['data inputs'].append(data_result_dict) self . _json_result = analysis_result to_json ( self , indent = None ) Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" # return json.dumps(self._json_result, indent=indent, ensure_ascii=False) if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=True)), indent=indent, ensure_ascii=False) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False ) OliveWorkflowEnrollmentResponse ( OliveServerResponse ) The container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient) for enrollment. This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server. is_allowable_error ( self ) Returns: Type Description true if this message failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this message failed with an allowable error \"\"\" return self . _isallowable_error parse_from_response ( self , request , response , message ) Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will the tree and deserialize any encoded messages if self . is_error (): # todo provide error info in JSON return # make a new message for this type... if isinstance ( self . _request , WorkflowActualizeRequest ): # we received an actualized workflow, so se don't need to convert it to json... return # this should only contain an analysis request... but check just in case: type = self . get_workflow_type () if type == WORKFLOW_ENROLLMENT_TYPE : # not much info in an enrollment response... enroll_result = dict () if self . _response . HasField ( 'error' ): enroll_result [ 'error' ] = self . _response . error else : enroll_result [ 'successful' ] = True self . _json_result = enroll_result to_json ( self , indent = None ) Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" #consider setting preserving_proto_field_name to true # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=True)), indent=indent) if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False ) get_workflow_job_names ( workflow_definition , workflow_type ) parse a workflow definition, returning a list of job definition name (job_name) return [job_name] for the requested workflow_type Source code in olivepy/messaging/response.py def get_workflow_job_names ( workflow_definition , workflow_type ): \"\"\" parse a workflow definition, returning a list of job definition name (job_name) return [job_name] for the requested workflow_type \"\"\" rtn_job_names = list () if workflow_definition is not None : for order in workflow_definition . order : if order . workflow_type == workflow_type : for job in order . job_definition : rtn_job_names . append ( job . job_name ) return rtn_job_names get_workflow_job_tasks ( jobs , job_name = None ) Fetch the tasks from a job Parameters: Name Type Description Default jobs a dictionary of WorkflowTasks, indexed by a job names required job_name find tasks that belong to a job having this name. This can be None if there is only one job None Returns: Type Description a dictionary of WorkflowTask indexed by the task's consumer_result_label for the specified job. An exception is thrown if there are multiple jobs but no job_name was specified Source code in olivepy/messaging/response.py def get_workflow_job_tasks ( jobs , job_name = None ): \"\"\" Fetch the tasks from a job :param jobs: a dictionary of WorkflowTasks, indexed by a job names :param job_name: find tasks that belong to a job having this name. This can be None if there is only one job :return: a dictionary of WorkflowTask indexed by the task's consumer_result_label for the specified job. An exception is thrown if there are multiple jobs but no job_name was specified \"\"\" if job_name is None : if len ( jobs ) == 1 : # hack to make it easier to fetch data since most workflows only have one job job_name = list ( jobs . keys ())[ 0 ] else : raise Exception ( \"Must specify a job name when there are multiple JobDefinitions in a Workflow\" ) rtn_tasks = dict () # and job_name is None: if job_name in jobs : for workflow_task in jobs [ job_name ]: rtn_tasks [ workflow_task . consumer_result_label ] = workflow_task else : print ( \"Job ' {} ' not one of the expected job names: {} \" . format ( job_name , list ( jobs . keys ()))) return rtn_tasks get_workflow_jobs ( workflow_definition , workflow_type ) parse a workflow definition, returning a dictionary indexed job (definition) name (job_name) and a list of WorkflowTask elements. Parameters: Name Type Description Default workflow_definition find jobs in this workflow definition required workflow_type the type of workflow order (analysis, enrollment, unenrollment) return {job_name: [WorkflowTask]} for the requested workflow_type required Source code in olivepy/messaging/response.py def get_workflow_jobs ( workflow_definition , workflow_type ): \"\"\" parse a workflow definition, returning a dictionary indexed job (definition) name (job_name) and a list of \\ WorkflowTask elements. :param workflow_definition: find jobs in this workflow definition :param workflow_type: the type of workflow order (analysis, enrollment, unenrollment) return {job_name: [WorkflowTask]} for the requested workflow_type \"\"\" rtn_jobs = dict () if workflow_definition is not None : for order in workflow_definition . order : if order . workflow_type == workflow_type : for job in order . job_definition : rtn_jobs [ job . job_name ] = job . tasks return rtn_jobs","title":"`olivepy` `messaging` module"},{"location":"olivepy-docs/messaging.html#olivepy-messaging-module","text":"","title":"olivepy messaging module"},{"location":"olivepy-docs/messaging.html#olivepymessagingmsgutil","text":"Contains data structures that map message_type enum values to protobuf types and vice versa. Should be updated whenever new message types are added.","title":"olivepy.messaging.msgutil"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.AllowableErrorFromServer","text":"This exception means that the server could not complete a request; however, the reason it could not do isn't considered an error. This special case most often occurs when requesting analysis of a submission that contains no speech, in which case the analysis could not complete since there was not speech and not due to an error running the plugin. Otherwise, this is identical to Python's plain old Exception","title":"AllowableErrorFromServer"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.AudioTransferType","text":"The method used to send audio to the OLIVE server. There are three options for sending audio to the server: AUDIO_PATH: Send the path of the audio file to the server. NOTE: If using this option, the path must be accessible to the server AUDIO_DECODED: Send the audio as a buffer of decoded samples (PCM-16). This option is not well supported by this client since it does not AUDIO_SERIALIZED: Send the file as a binary buffer","title":"AudioTransferType"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.ExceptionFromServer","text":"This exception means that an error occured on the server side, and this error is being sent \"up the chain\" on the client side. Otherwise, it is identical to Python's plain old Exception","title":"ExceptionFromServer"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.InputTransferType","text":"The method used to send audio/data to the OLIVE server. There are three options for sending data to the server: PATH: Send the path of the audio file to the server. NOTE: If using this option, the path must be accessible to the server DECODED: Send the audio as a buffer of decoded samples (PCM-16). This option is not well supported by this client since it does not SERIALIZED: Send the file as a binary buffer","title":"InputTransferType"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.OliveInputDataType","text":"The type of input data send to the OLIVE server.","title":"OliveInputDataType"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.package_audio","text":"Parameters: Name Type Description Default audio_msg Audio the Olive Audio message to populate required audio_data ~AnyStr either a filename or binary buffer required annotations a list of tuple start/end regions (in seconds) None selected_channel if audio_data is multi-channel then select this channel for processing None mode the submission mode: pathname, serialized, samples <InputTransferType.PATH: 1> num_channels the number of channels in the audio None sample_rate the sample rate of the audio None num_samples the number of samples in the audio. None validate_local_path if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. True Returns: Type Description a valid Audio message Source code in olivepy/messaging/msgutil.py def package_audio ( audio_msg : Audio , audio_data : AnyStr , annotations = None , selected_channel = None , mode = InputTransferType . PATH , num_channels = None , sample_rate = None , num_samples = None , validate_local_path = True , label = None ): \"\"\" :param audio_msg: the Olive Audio message to populate :param audio_data: either a filename or binary buffer :param annotations: a list of tuple start/end regions (in seconds) :param selected_channel: if audio_data is multi-channel then select this channel for processing :param mode: the submission mode: pathname, serialized, samples :param num_channels: the number of channels in the audio :param sample_rate: the sample rate of the audio :param num_samples: the number of samples in the audio. :param validate_local_path: if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. :return: a valid Audio message :raises Exception if unable to package the audio for the specified mode. \"\"\" if mode != InputTransferType . PATH and mode != InputTransferType . DECODED and mode != InputTransferType . SERIALIZED : raise Exception ( 'Called package_audio with an unknown mode. Must be PATH, DECODED, or SERIALIZED.' ) # only supporting pathname now if mode == InputTransferType . PATH : if validate_local_path : if not os . path . exists ( audio_data ): raise Exception ( \"Error creating an OLIVE Audio message, the Audio file ' {} ' does not exist.\" . format ( audio_data )) audio_msg . path = audio_data else : audio_buffer = audio_msg . audioSamples if isinstance ( audio_data , bytes ): # audio has already been converted to a buffer... no need to change audio_buffer . data = audio_data else : # Assume we have a filename with we will serialize (decoded samples not supported) if mode != InputTransferType . SERIALIZED : raise Exception ( \"Converting ' {} ' into a decoded buffer is not supported. Client must \" \"manually decode the file and pass bytes to package_audio()\" . format ( audio_data )) buffer = serialize_audio ( audio_data ) audio_buffer . data = buffer if mode == InputTransferType . SERIALIZED : # olive.proto says these are all ignored for serialized buffers: # channels, rate, bitdepth, channels audio_buffer . serialized_file = True if mode == InputTransferType . DECODED : # This mode assumes the client has passed in a numpy array of samples, but we don't assume numpy is # installed for all clients so we don't do checks in ths this code # Get the data as shorts: # not if audio_data.dtype.kind == np.dtype(np.integer).kind: # audio_data = audio_data.astype( np.int16 ).flatten().tolist() # raise Exception(\"Error: Transferring decoded samples not supported\") problem = '' if num_channels is None or num_channels == 0 : problem += 'channel ' if sample_rate is None or sample_rate == 0 : problem += 'sample_rate ' if num_samples is None or num_samples == 0 : problem += 'num_samples' if problem != '' : raise Exception ( 'Error: can not create an OLIVE audio message from decoded samples because missing required argument(s): {} ' . format ( problem )) audio_buffer . serialized_file = False audio_buffer . channels = num_channels audio_buffer . rate = sample_rate audio_buffer . samples = num_samples audio_buffer . bit_depth = BIT_DEPTH_16 audio_buffer . encoding = PCM16 if annotations : for a in annotations : # np.float32(a[0] would be better but can we assume numpy is installed? region = audio_msg . regions . add () region . start_t = a [ 0 ] region . end_t = a [ 1 ] if selected_channel : # we can't do much validation, but if they selected a channel and specified the number of channels if num_channels : if selected_channel > num_channels : raise Exception ( \"Error: can not select channel ' {} ' if audio only contains ' {} ' channel(s)\" . format ( selected_channel , num_channels )) if selected_channel < 1 : raise Exception ( \"Error: invalid value for selected channel ' {} '. Channel must be 1 or higher \" . format ( selected_channel )) audio_msg . selected_channel = selected_channel if label : audio_msg . label = label return audio_msg","title":"package_audio()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.package_binary_media","text":"Parameters: Name Type Description Default binary_media_msg BinaryMedia the Olive BinaryMedia message to populate required media_data ~AnyStr either a filename or binary buffer required annotations a list of tuple start/end regions (in seconds) None mode the submission mode: pathname, serialized, samples <InputTransferType.PATH: 1> validate_local_path if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. True Returns: Type Description a valid Audio message Source code in olivepy/messaging/msgutil.py def package_binary_media ( binary_media_msg : BinaryMedia , media_data : AnyStr , annotations = None , mode = InputTransferType . PATH , validate_local_path = True , label = None , selected_channel = None ): \"\"\" :param binary_media_msg: the Olive BinaryMedia message to populate :param media_data: either a filename or binary buffer :param annotations: a list of tuple start/end regions (in seconds) :param mode: the submission mode: pathname, serialized, samples :param validate_local_path: if sending audio as a path, throw an exception if the file does not exist. We let this be an option for the possible case where the client may want to provide a path on the server's filesystem, but not the local filesystem. :return: a valid Audio message :raises Exception if unable to package the audio for the specified mode. \"\"\" print ( \"adding binary media\" ) # todo support selected channel (if audio is to be handled form this data), label, and annotations if mode != InputTransferType . PATH and mode != InputTransferType . SERIALIZED : raise Exception ( 'Called package_visual_media with an unknown mode. Must be AUDIO_PATH, or AUDIO_SERIALIZED.' ) # only supporting pathname now if mode == InputTransferType . PATH : if validate_local_path : if not os . path . exists ( media_data ): raise Exception ( \"Error creating an OLIVE media message, the Audio file ' {} ' does not exist.\" . format ( media_data )) binary_media_msg . path = media_data else : media_buffer = binary_media_msg . buffer if isinstance ( media_data , bytes ): # audio has already been converted to a buffer... no need to change media_buffer . data = media_data else : # Assume we have a filename with we will serialize (decoded samples not supported) if mode != InputTransferType . SERIALIZED : raise Exception ( \"Converting ' {} ' into a decoded buffer is not supported.\" . format ( media_data )) buffer = serialize_audio ( media_data ) media_buffer . data = buffer if label : binary_media_msg . label = label if selected_channel : binary_media_msg . selected_channel = selected_channel if annotations : classic_region = binary_media_msg . regions . add () for a in annotations : # np.float32(a[0] would be better but can we assume numpy is installed? region = classic_region . regions . add () # print(\"Adding region: {} to {}\".format(a[0], a[1])) region . start_t = a [ 0 ] region . end_t = a [ 1 ] return binary_media_msg","title":"package_binary_media()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.msgutil.serialize_audio","text":"Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized Parameters: Name Type Description Default filename str the local path to the file to serialize required Returns: Type Description ~AnyStr the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. This buffer contains the raw content of the file, it does NOT contain encoded samples Source code in olivepy/messaging/msgutil.py def serialize_audio ( filename : str ) -> AnyStr : \"\"\" Helper function used to read in an audio file and output a serialized buffer. Can be used with package_audio() \\ when using the AUDIO_SERIALIZED mode and the audio input has not already been serialized :param filename: the local path to the file to serialize :return: the contents of the file as a byte buffer, otherwise an exception if the file can not be opened. \\ This buffer contains the raw content of the file, it does NOT contain encoded samples \"\"\" if not os . path . exists ( os . path . expanduser ( filename )): raise Exception ( \"Error serializing an audio file, the file ' {} ' does not exist.\" . format ( filename )) with open ( os . path . expanduser ( filename ), 'rb' ) as f : serialized_buffer = f . read () # return the buffer return serialized_buffer","title":"serialize_audio()"},{"location":"olivepy-docs/messaging.html#olivepymessagingolive_pb2","text":"","title":"olivepy.messaging.olive_pb2"},{"location":"olivepy-docs/messaging.html#olivepymessagingresponse","text":"","title":"olivepy.messaging.response"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveClassStatusResponse","text":"The container/wrapper for WorkflowClassStatusResult from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server.","title":"OliveClassStatusResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveClassStatusResponse.get_workflow_type","text":"Return the type of workflow done in this response (analysis, enrollment, adaptation) Returns: Type Description A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped Source code in olivepy/messaging/response.py def get_workflow_type ( self ): return WORKFLOW_ANALYSIS_TYPE","title":"get_workflow_type()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveClassStatusResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) if self . is_error (): return status_result = [] for jc in self . _response . job_class : job_name = jc . job_name job_dict = dict () self . _job_names . add ( job_name ) job_dict [ KEY_JOB_NAME ] = job_name # we have a list of data items: job_dict [ KEY_TASkS ] = {} status_result . append ( job_dict ) for task_class in jc . task : task_class_dict = json . loads ( MessageToJson ( task_class , preserving_proto_field_name = True )) del task_class_dict [ 'task_name' ] if task_class . task_name not in job_dict [ KEY_TASkS ]: job_dict [ KEY_TASkS ][ task_class . task_name ] = [] job_dict [ KEY_TASkS ][ task_class . task_name ] . append ( task_class_dict ) #job_dict[KEY_TASkS].append({task_class.task_name: task_class_dict}) self . _json_result = status_result","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveClassStatusResponse.to_json","text":"Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" # consider setting preserving_proto_field_name to true # return json.dumps(json.loads(MessageToJson(self._response, preserving_proto_field_name=True)), indent=indent) if self . is_error (): return self . get_error () if indent and indent < 0 : return json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )) return json . dumps ( json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )), indent = indent , ensure_ascii = False )","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse","text":"The default container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server.","title":"OliveServerResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse.get_response","text":"The Protobuf message returned from the OLIVE server Returns: Type Description Source code in olivepy/messaging/response.py def get_response ( self ): \"\"\" The Protobuf message returned from the OLIVE server :return: \"\"\" # todo exception if none? return self . _response","title":"get_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse.get_workflow_type","text":"Return the type of workflow done in this response (analysis, enrollment, adaptation) Returns: Type Description A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped Source code in olivepy/messaging/response.py def get_workflow_type ( self ): \"\"\" Return the type of workflow done in this response (analysis, enrollment, adaptation) :return: A WorkflowType: WORKFLOW_ANALYSIS_TYPE, WORKFLOW_ENROLLMENT_TYPE, WORKFLOW_ADAPT_TYPE or an Exception if an non-workflow response message was wrapped \"\"\" if not self . _response : raise Exception ( \"No valid response\" ) if isinstance ( self . _request , WorkflowAnalysisRequest ): return WORKFLOW_ANALYSIS_TYPE elif isinstance ( self . _request , WorkflowEnrollRequest ): return WORKFLOW_ENROLLMENT_TYPE elif isinstance ( self . _request , WorkflowAdaptRequest ): return WORKFLOW_ADAPT_TYPE elif isinstance ( self . _request , WorkflowUnenrollRequest ): return WORKFLOW_UNENROLLMENT_TYPE raise Exception ( \"Unknown Workflow Message: {} \" . format ( type ( self . _request )))","title":"get_workflow_type()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): \"\"\" Create this response from the :param request: :param response: :param message: :return: \"\"\" self . _request = request if message : # No results due to error self . _iserror = True self . _message = message # self._request = request if response is not None : try : if response . HasField ( \"error\" ): self . _iserror = True self . _message = response . error else : # we assume no errors self . _issuccessful = True except : # Some messages have no error field self . _issuccessful = True self . _response = response","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveServerResponse.to_json","text":"Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the response as as JSON string: \"\"\" #consider setting preserving_proto_field_name to true if indent and indent < 0 : return json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )) return json . dumps ( json . loads ( MessageToJson ( self . _response , preserving_proto_field_name = True )), indent = indent , ensure_ascii = False )","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse","text":"Extracts info from an actualized workflow definition","title":"OliveWorkflowActualizedResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.get_analysis_jobs","text":"Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. Returns: Type Description a list of job names in the analysis Source code in olivepy/messaging/response.py def get_analysis_jobs ( self ): \"\"\" Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. :return: a list of job names in the analysis \"\"\" return [ job_dict [ KEY_JOB_NAME ] for job_dict in self . _json_result ] #todo get analysis job name(s)","title":"get_analysis_jobs()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.get_request_jobs","text":"return the jobs in the original request for the specified analysis type Parameters: Name Type Description Default workflow_type the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) required Returns: Type Description the list of jobs for this type Source code in olivepy/messaging/response.py def get_request_jobs ( self , workflow_type ): \"\"\" return the jobs in the original request for the specified analysis type :param workflow_type: the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) :return: the list of jobs for this type \"\"\" if self . _request is not None : return get_workflow_jobs ( self . _request . workflow_definition , workflow_type ) raise Exception ( \"No jobs for the requested workflow type: {} \" . format ( workflow_type ))","title":"get_request_jobs()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.is_allowable_error","text":"Returns: Type Description true if this response failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this response failed with an allowable error \"\"\" return self . _isallowable_error","title":"is_allowable_error()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will walk the tree and deserialize any encoded messages if self . is_error (): # todo provide error info in JSON? return # make a new message for this type... if not isinstance ( self . _request , WorkflowActualizeRequest ): # we received an some other workflow message so se don't need to convert it to json... return # we only parse the analyze part now analysis_task = [] workflow_analysis_order_msg = None for order in self . _response . workflow . order : if order . workflow_type == WORKFLOW_ANALYSIS_TYPE : workflow_analysis_order_msg = order break if workflow_analysis_order_msg is None : # no analysis results return # for job in self._response.job_result: for job in workflow_analysis_order_msg . job_definition : # create a dictionary for each job result job_dict = dict () job_name = job . job_name self . _job_names . add ( job_name ) # and a dictionary of tasks: # job_dict[KEY_TASkS] = {} # add to our results - in most cases we will have just one job analysis_task . append ( job_dict ) # get data handling info for this job data_prop = job . data_properties job_dict [ 'Data Input' ] = json . loads ( MessageToJson ( data_prop , preserving_proto_field_name = True )) # if data_prop.mode == SPLIT: # # Hack to make split/mulit-channel mode more clear # job_dict['data']['mode'] = 'SPLIT: Process each channel as a job' for task in job . tasks : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # Deserialize message_data, and replace it in the task_result_dict task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ KEY_JOB_NAME ] = job_name task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) del task_result_dict [ 'message_data' ] job_dict [ task . consumer_result_label ] = task_result_dict self . _json_result = analysis_task","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowActualizedResponse.to_json","text":"Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False ) # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=False)), indent=indent)","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse","text":"The default container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient). This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server.","title":"OliveWorkflowAnalysisResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.get_analysis_jobs","text":"Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. Returns: Type Description a list of job names in the analysis Source code in olivepy/messaging/response.py def get_analysis_jobs ( self ): \"\"\" Return the names of analysis jobs. Typically a workflow has just one job with multiple tasks, the most likely reason to have multiple jobs is for workflows using multi-channel audio so there may be a set of job tasks for each channel of audio submitted. :return: a list of job names in the analysis \"\"\" return [ job_dict [ KEY_JOB_NAME ] for job_dict in self . _json_result ] #todo get analysis job name(s)","title":"get_analysis_jobs()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.get_analysis_task_result","text":"Get the result(s) for the specified job_name and task_name, also include the data used for this task. If the workflow analyzes each channel in multi-channel data then there can be multiple jobs with the same name. Parameters: Name Type Description Default job_name for convenience can be None, since there is normally only one job. But if the workflow has multiple jobs then a valid name must be specified. required task_name the name to the task required Returns: Type Description a list of dictionaries, where each dictionary in the list includes the results for the specified task and a list of the data analyzed by this task, such as [ {task_name:{}, data:[] }] Source code in olivepy/messaging/response.py def get_analysis_task_result ( self , job_name , task_name ): \"\"\" Get the result(s) for the specified job_name and task_name, also include the data used for this task. If the workflow analyzes each channel in multi-channel data then there can be multiple jobs with the same name. :param job_name: for convenience can be None, since there is normally only one job. But if the workflow has multiple jobs then a valid name must be specified. :param task_name: the name to the task :return: a list of dictionaries, where each dictionary in the list includes the results for the specified task and a list of the data analyzed by this task, such as [ {task_name:{}, data:[] }] \"\"\" if job_name is None : job_name = self . _get_default_job_name () results = [] for job_dict in self . _json_result : if job_name == job_dict [ KEY_JOB_NAME ]: task_dict = dict () # there may be one or more result for task_name task_dict [ task_name ] = job_dict [ KEY_TASkS ][ task_name ] task_dict [ KEY_DATA ] = job_dict [ KEY_DATA ] results . append ( task_dict ) return results","title":"get_analysis_task_result()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.get_request_jobs","text":"return the jobs in the original request for the specified analysis type Parameters: Name Type Description Default workflow_type the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) required Returns: Type Description the list of jobs for this type Source code in olivepy/messaging/response.py def get_request_jobs ( self , workflow_type ): \"\"\" return the jobs in the original request for the specified analysis type :param workflow_type: the type of workflow (i.e. WORKFLOW_ANALYSIS_TYPE) :return: the list of jobs for this type \"\"\" if self . _request is not None : return get_workflow_jobs ( self . _request . workflow_definition , workflow_type ) raise Exception ( \"No jobs for the requested workflow type: {} \" . format ( workflow_type ))","title":"get_request_jobs()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.is_allowable_error","text":"Returns: Type Description true if this response failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this response failed with an allowable error \"\"\" return self . _isallowable_error","title":"is_allowable_error()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will walk the tree and deserialize any encoded messages if self . is_error (): self . _json_result = {} self . _json_result [ 'error' ] = self . get_error () return # make a new message for this type... if isinstance ( self . _request , WorkflowActualizeRequest ): # we received an actualized workflow, so se don't need to convert it to json... return # this should only contain an analysis request... but check just in case: wk_type = self . get_workflow_type () if wk_type == WORKFLOW_ANALYSIS_TYPE or wk_type == WORKFLOW_ENROLLMENT_TYPE or wk_type == WORKFLOW_UNENROLLMENT_TYPE : # analysis is a list of dictionary elements, which looks like: [ {job_name: X, data: [], tasks: {}} ] # there is a dictionary for each job, but due to the way jobs work in OLIVE for mulit-channel data we # consider a jobs to be unique by a combination of job_name plus data, so multiples dictionary elements may # have the same job_name, but will have different data properties (channel numbers) # get the analysis job order from the original request: analysis_result = [] # or enrollment/unenrollment result # analysis_result['jobs'] = [] # analysis_result['data inputs'] = [] job_requests = get_workflow_jobs ( self . _request . workflow_definition , wk_type ) for job in self . _response . job_result : # create a dictionary for each job result job_dict = dict () job_name = job . job_name self . _job_names . add ( job_name ) # job_dict[job_name] = dict() job_dict [ KEY_JOB_NAME ] = job_name if job . error : job_dict [ 'error' ] = job . error # we have a list of data items: job_dict [ KEY_DATA ] = [] # and a dictionary of tasks: (although note it is possible to have multiple tasks with the same name, so a task has a list of results) job_dict [ KEY_TASkS ] = {} # add to our results - in most cases we will have just one job analysis_result . append ( job_dict ) # get the tasks for the current (and likely, only) job: task_requests = get_workflow_job_tasks ( job_requests , job_name ) # task_result_dict = dict() # I don't think this can happen yet: # if job.HasField('error'): # Allowable job error # for task in job . task_results : task_result_dict = json . loads ( MessageToJson ( task , preserving_proto_field_name = True )) # check if this task failed with an error if task . HasField ( 'error' ): # Allowable error if KEY_ERROR in job_dict : job_dict [ KEY_ERROR ] = job_dict [ KEY_ERROR ] + \",\" + task . error else : job_dict [ KEY_ERROR ] = task . error self . _isallowable_error = True # should have an empty message data; del task_result_dict [ 'message_data' ] if job_name not in self . _allowable_failed_job_tasks : self . _allowable_failed_job_tasks [ job_name ] = [] self . _allowable_failed_job_tasks [ job_name ] . append ( task . task_name ) else : # Deserialize message_data, and replace it in the task_result_dict if task . message_type in msgutil . debug_message_map : # Get the pimiento message (debug only - these messages are not guaranteed to be supported print ( \"CLG special msg type: {} \" . format ( msgutil . MessageType . Name ( task . message_type ))) pimiento_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) if task . message_type == DATA_OUTPUT_TRANSFORMER_RESULT : if pimiento_msg . data_type == TEXT : # only supported type for now... pie_data_msg = WorkflowTextResult () pie_data_msg . ParseFromString ( pimiento_msg . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( pie_data_msg , preserving_proto_field_name = True )) else : print ( \"Unsupported debug message type: {} \" . format ( msgutil . InputDataType . Name ( pimiento_msg . data_type ))) elif task . message_type == SCORE_OUTPUT_TRANSFORMER_RESULT : # these should be standard trait message pie_score_msg = self . _extract_serialized_message ( pimiento_msg . message_type , pimiento_msg . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( pie_score_msg , preserving_proto_field_name = True )) else : task_type_msg = self . _extract_serialized_message ( task . message_type , task . message_data ) task_result_dict [ 'analysis' ] = json . loads ( MessageToJson ( task_type_msg , preserving_proto_field_name = True )) # Should we create special handlers for analysis results, like we sort global score results, # but what should be do with an AUDIO_MODIFICATION_RESULT? if task . message_type == GLOBAL_SCORER_RESULT : # Sort region scores task_result_dict [ 'analysis' ][ 'score' ] = sorted ( task_result_dict [ 'analysis' ][ 'score' ], key = sort_global_scores , reverse = True ) # messageData has been replaced with the actual task del task_result_dict [ 'message_data' ] # taskName is the key, so remove it: del task_result_dict [ 'task_name' ] # check if we need to add the plugin/domain name orig_task = task_requests [ task . task_name ] if orig_task . message_type in msgutil . plugin_message_map : # Get the original task task_req_msg = self . _extract_serialized_message ( orig_task . message_type , orig_task . message_data ) task_result_dict [ 'plugin' ] = task_req_msg . plugin task_result_dict [ 'domain' ] = task_req_msg . domain if orig_task . message_type == CLASS_MODIFICATION_REQUEST or orig_task . message_type == CLASS_REMOVAL_REQUEST : # add class ID task_result_dict [ 'class_id' ] = self . _request . class_id # print(\"adding {} as {}\".format(task_result_dict, task.task_name)) # fixme: there can be multiple taks with the same name if conditions in a workflow caused the task to be ran twice if task . task_name not in job_dict [ KEY_TASkS ]: job_dict [ KEY_TASkS ][ task . task_name ] = [] job_dict [ KEY_TASkS ][ task . task_name ] . append ( task_result_dict ) #job_dict[KEY_TASkS].append({task.task_name: task_result_dict}) # A job usually has one data input/output, but we handle as if there are multiple for data_result in job . data_results : data_result_dict = json . loads ( MessageToJson ( data_result , preserving_proto_field_name = True )) # Deserialize the data portion data_type_msg = self . _extract_serialized_message ( data_result . msg_type , data_result . result_data ) del data_result_dict [ 'result_data' ] # del data_result_dict['dataId'] # redundant into, used as the key # data_result_dict['data'] = json.loads(MessageToJson(data_type_msg)) data_result_dict . update ( json . loads ( MessageToJson ( data_type_msg , preserving_proto_field_name = True ))) job_dict [ KEY_DATA ] . append ( data_result_dict ) # analysis_result['data inputs'].append(data_result_dict) self . _json_result = analysis_result","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowAnalysisResponse.to_json","text":"Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document Returns: Type Description the Workflow Definition as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the workflow as a JSON string :indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow Definition as as JSON string: \"\"\" # return json.dumps(self._json_result, indent=indent, ensure_ascii=False) if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=True)), indent=indent, ensure_ascii=False) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False )","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowEnrollmentResponse","text":"The container/wrapper for responses from an OLIVE server (when using the AsyncOliveClient) for enrollment. This is intended to make it easier for clients to handle the traditional (original) protobuf message results (such as RegionScorerResult) returned from the server.","title":"OliveWorkflowEnrollmentResponse"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowEnrollmentResponse.is_allowable_error","text":"Returns: Type Description true if this message failed with an allowable error Source code in olivepy/messaging/response.py def is_allowable_error ( self ): \"\"\" :return: true if this message failed with an allowable error \"\"\" return self . _isallowable_error","title":"is_allowable_error()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowEnrollmentResponse.parse_from_response","text":"Create this response from the Parameters: Name Type Description Default request required response required message required Returns: Type Description Source code in olivepy/messaging/response.py def parse_from_response ( self , request , response , message ): OliveServerResponse . parse_from_response ( self , request , response , message ) # Now create a JSON representation from the response # we will the tree and deserialize any encoded messages if self . is_error (): # todo provide error info in JSON return # make a new message for this type... if isinstance ( self . _request , WorkflowActualizeRequest ): # we received an actualized workflow, so se don't need to convert it to json... return # this should only contain an analysis request... but check just in case: type = self . get_workflow_type () if type == WORKFLOW_ENROLLMENT_TYPE : # not much info in an enrollment response... enroll_result = dict () if self . _response . HasField ( 'error' ): enroll_result [ 'error' ] = self . _response . error else : enroll_result [ 'successful' ] = True self . _json_result = enroll_result","title":"parse_from_response()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.OliveWorkflowEnrollmentResponse.to_json","text":"Generate the response as a JSON string Parameters: Name Type Description Default indent if a non-negative integer, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0 will only insert newlines. None is the most compact representation. A negative value will return the JSON document None Returns: Type Description the Workflow response as as JSON string: Source code in olivepy/messaging/response.py def to_json ( self , indent = None ): \"\"\" Generate the response as a JSON string :param indent: if a non-negative integer, then JSON array elements and object members will be pretty-printed with \\ that indent level. An indent level of 0 will only insert newlines. ``None`` is the most compact \\ representation. A negative value will return the JSON document :return: the Workflow response as as JSON string: \"\"\" #consider setting preserving_proto_field_name to true # return json.dumps(json.loads(MessageToJson(self._json_result, preserving_proto_field_name=True)), indent=indent) if indent and indent < 0 : return json . loads ( MessageToJson ( self . _json_result , preserving_proto_field_name = True )) return json . dumps ( self . _json_result , indent = indent , ensure_ascii = False )","title":"to_json()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.get_workflow_job_names","text":"parse a workflow definition, returning a list of job definition name (job_name) return [job_name] for the requested workflow_type Source code in olivepy/messaging/response.py def get_workflow_job_names ( workflow_definition , workflow_type ): \"\"\" parse a workflow definition, returning a list of job definition name (job_name) return [job_name] for the requested workflow_type \"\"\" rtn_job_names = list () if workflow_definition is not None : for order in workflow_definition . order : if order . workflow_type == workflow_type : for job in order . job_definition : rtn_job_names . append ( job . job_name ) return rtn_job_names","title":"get_workflow_job_names()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.get_workflow_job_tasks","text":"Fetch the tasks from a job Parameters: Name Type Description Default jobs a dictionary of WorkflowTasks, indexed by a job names required job_name find tasks that belong to a job having this name. This can be None if there is only one job None Returns: Type Description a dictionary of WorkflowTask indexed by the task's consumer_result_label for the specified job. An exception is thrown if there are multiple jobs but no job_name was specified Source code in olivepy/messaging/response.py def get_workflow_job_tasks ( jobs , job_name = None ): \"\"\" Fetch the tasks from a job :param jobs: a dictionary of WorkflowTasks, indexed by a job names :param job_name: find tasks that belong to a job having this name. This can be None if there is only one job :return: a dictionary of WorkflowTask indexed by the task's consumer_result_label for the specified job. An exception is thrown if there are multiple jobs but no job_name was specified \"\"\" if job_name is None : if len ( jobs ) == 1 : # hack to make it easier to fetch data since most workflows only have one job job_name = list ( jobs . keys ())[ 0 ] else : raise Exception ( \"Must specify a job name when there are multiple JobDefinitions in a Workflow\" ) rtn_tasks = dict () # and job_name is None: if job_name in jobs : for workflow_task in jobs [ job_name ]: rtn_tasks [ workflow_task . consumer_result_label ] = workflow_task else : print ( \"Job ' {} ' not one of the expected job names: {} \" . format ( job_name , list ( jobs . keys ()))) return rtn_tasks","title":"get_workflow_job_tasks()"},{"location":"olivepy-docs/messaging.html#olivepy.messaging.response.get_workflow_jobs","text":"parse a workflow definition, returning a dictionary indexed job (definition) name (job_name) and a list of WorkflowTask elements. Parameters: Name Type Description Default workflow_definition find jobs in this workflow definition required workflow_type the type of workflow order (analysis, enrollment, unenrollment) return {job_name: [WorkflowTask]} for the requested workflow_type required Source code in olivepy/messaging/response.py def get_workflow_jobs ( workflow_definition , workflow_type ): \"\"\" parse a workflow definition, returning a dictionary indexed job (definition) name (job_name) and a list of \\ WorkflowTask elements. :param workflow_definition: find jobs in this workflow definition :param workflow_type: the type of workflow order (analysis, enrollment, unenrollment) return {job_name: [WorkflowTask]} for the requested workflow_type \"\"\" rtn_jobs = dict () if workflow_definition is not None : for order in workflow_definition . order : if order . workflow_type == workflow_type : for job in order . job_definition : rtn_jobs [ job . job_name ] = job . tasks return rtn_jobs","title":"get_workflow_jobs()"},{"location":"olivepy-docs/utils.html","text":"olivepy utils module olivepy.utils.pem Encapsulate pem file IO Abstracts reading from and writing to PEM files and should provide common operations on PEM records such as duration(). The str() method of this class will result in a string that is legal PEM format. All operations are performed in memory so don't use any extraordinarily huge PEM files. Each line in a PEM file has: filename, channel, class_label, start_t, end_t Pem get_maximum_duration ( self ) Get duration of maxium duration record in PEM. Intended only for cases where PEM contains only one ID. Source code in olivepy/utils/pem.py def get_maximum_duration ( self ): \"\"\" Get duration of maxium duration record in PEM. Intended only for cases where PEM contains only one ID. \"\"\" duration = 0 for id in list ( self . __record_map . keys ()): for rec in self . __record_map [ id ]: this_duration = float ( rec . duration ()) if ( this_duration > duration ): duration = this_duration return duration get_minimum_duration ( self ) Get duration of minimum duration record in PEM. Intended only for cases where PEM contains only one ID. Source code in olivepy/utils/pem.py def get_minimum_duration ( self ): \"\"\" Get duration of minimum duration record in PEM. Intended only for cases where PEM contains only one ID. \"\"\" duration = self . get_total_duration () for id in list ( self . __record_map . keys ()): for rec in self . __record_map [ id ]: this_duration = float ( rec . duration ()) if ( this_duration < duration ): duration = this_duration return duration PemRecord The underlying PEM container __init__ ( self , id , channel , label , start_t , end_t , decimal = False ) special Parameters: Name Type Description Default id generally the filename required channel the channel (if stereo). May be a string list (i.e. \"1,2\"). No validation is done for the channel value required label a \"class\" label for this segment. Examples include speaker, language, speech, etc required start_t the start time in seconds required end_t the end time in seconds required decimal if true, value is stored as a float False Source code in olivepy/utils/pem.py def __init__ ( self , id , channel , label , start_t , end_t , decimal = False ): ''' :param id: generally the filename :param channel: the channel (if stereo). May be a string list (i.e. \"1,2\"). No validation is done for the channel value :param label: a \"class\" label for this segment. Examples include speaker, language, speech, etc :param start_t: the start time in seconds :param end_t: the end time in seconds :param decimal: if true, value is stored as a float ''' self . id = id self . channel = channel self . label = label # Using Decimal instead of floats made the code 100x slower # Use floats for now in the intrest of speed if decimal : self . start_t = start_t self . end_t = end_t else : self . start_t = float ( start_t ) self . end_t = float ( end_t ) if ( self . start_t > self . end_t ): raise Exception ( \"Start is after end in PemRecord: {self} \" . format ( ** vars ())) split_channels ( self ) Split the channel into an array, so that if a channel value of '1,2' is supplied it is returned as an array [1,2] Returns: Type Description an array of channel numbers Source code in olivepy/utils/pem.py def split_channels ( self ): ''' Split the channel into an array, so that if a channel value of '1,2' is supplied it is returned as an array [1,2] :return: an array of channel numbers ''' channels = [] if type ( self . channel ) is str : # convert to a list channels = list ( map ( int , str . split ( self . channel , ',' ))) elif type ( self . channel ) is int : channels . append ( self . channel ) else : print ( \"Unsupported channel value: {} \" . format ( self . channel )) return channels olivepy.utils.utils parse_json_options ( option_str ) Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name Parameters: Name Type Description Default option_str the options to parse required Source code in olivepy/utils/utils.py def parse_json_options ( option_str ): \"\"\" Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name :param option_str: the options to parse :return a list of OptionValue objects created from the JSON option input \"\"\" # Examples of inputs to handle: # [{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' # '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}' # '[{\"task\":\"LID\", \"options\": {\"filter_length\":99, \"interpolate\":11.0, \"test_name\":\"midge\"}}]' # '[{\"job\":\"SAD LID Job\", \"task\":\"LID\", \"options\": {\"filter_length\":99, \"interpolate\":11.0, \"test_name\":\"midge\"}}]' # Parse options json_opts = json . loads ( option_str ) out_opts = [] # Options can be a list of task specific options # currently we don't support task specific options so just create one dictionary of name/value options if isinstance ( json_opts , list ): for item in json_opts : in_opts = item [ 'options' ] for opt in in_opts : # print(\"\\t{} = {}, value type: {}\".format(opt, in_opts[opt], type(in_opts[opt]))) opt_msg = olivepb . OptionValue () opt_msg . name = opt opt_msg . value = str ( in_opts [ opt ]) # optionally check if this option is restricted to a job/task: if 'task' in item : opt_msg . task_filter_name = item [ 'task' ] if 'job' in item : opt_msg . job_filter_name = item [ 'job' ] out_opts . append ( opt_msg ) else : # or options that are applied to each task, which is just a simple dictionary # like: {\"filter_length\":99, \"interpolate\":1.0} # OLIVE wil internally ignore these options if the keyname does not match one of the option name # a plugin supports for the requested trait (i.e. plugin.get_region_scoring_opts() for opt in json_opts : opt_msg = olivepb . OptionValue () opt_msg . name = opt opt_msg . value = str ( json_opts [ opt ]) out_opts . append ( opt_msg ) # print(\"\\t{} = {}, value type: {}\".format(opt, json_opts[opt], type(json_opts[opt]))) print ( \"Final json options: {} \" . format ( out_opts )) return out_opts parse_json_options_as_dict ( option_str ) Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name Parameters: Name Type Description Default option_str the options to parse required Returns: Type Description a dictionary of options name/value pairs Source code in olivepy/utils/utils.py def parse_json_options_as_dict ( option_str ): \"\"\" Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name :param option_str: the options to parse :return: a dictionary of options name/value pairs \"\"\" # Parse options json_opts = json . loads ( option_str ) out_opts = dict () # Options can be a list of task specific options # currently we don't support task specific options so just create one dictionary of name/value options if isinstance ( json_opts , list ): for item in json_opts : in_opts = item [ 'options' ] print ( \"Found {} options for task: {} \" . format ( len ( in_opts ), item [ 'task' ])) out_opts . update ( in_opts ) for opt in in_opts : print ( \" \\t {} = {} , value type: {} \" . format ( opt , in_opts [ opt ], type ( in_opts [ opt ]))) else : # or options that are applied to each task, which is just a simple dictionary # like: {\"filter_length\":99, \"interpolate\":1.0} # OLIVE wil internally ignore these options if the keyname does not match one of the option name # a plugin supports for the requested trait (i.e. plugin.get_region_scoring_opts() out_opts = json_opts for opt in json_opts : print ( \" \\t {} = {} , value type: {} \" . format ( opt , json_opts [ opt ], type ( json_opts [ opt ]))) print ( \"Final json options: {} \" . format ( out_opts )) return out_opts parse_pem_file ( data_lines ) Parse a PEM file, grouping the results by audio file and channel. Parameters: Name Type Description Default data_lines the data line to parse required Returns: Type Description a dictionary of audio files to score and the channel region: # {'filename': {channel: {class_id : [(start_region, end_region, class_id)]} } } Source code in olivepy/utils/utils.py def parse_pem_file ( data_lines ): \"\"\" Parse a PEM file, grouping the results by audio file and channel. :param data_lines: the data line to parse :return: a dictionary of audio files to score and the channel region: # {'filename': {channel: {class_id : [(start_region, end_region, class_id)]} } } \"\"\" # We process by file and channel - the class/label is ignored regions = {} input_pem = Pem () input_pem . add_records_from_data_lines ( data_lines ) for id in input_pem . get_ids (): audio_id = os . path . expandvars ( id ) # Create a dictionary of the regions specified for the the current file regions [ audio_id ] = {} for rec in input_pem . get_records ( id ): # channel could be a list... channels = [] if type ( rec . channel ) is str : # convert to a list channels = map ( int , str . split ( rec . channel , ',' )) elif type ( rec . channel ) is int : channels . append ( rec . channel ) else : print ( \"Unsupported channel value: {} \" . format ( rec . channel )) for ch in channels : if ch not in regions [ audio_id ]: regions [ audio_id ][ ch ] = {} class_id = rec . label if class_id not in regions [ audio_id ][ ch ]: regions [ audio_id ][ ch ][ class_id ] = [] regions [ audio_id ][ ch ][ class_id ] . append (( rec . start_t , rec . end_t )) return regions","title":"`olivepy` `utils` module"},{"location":"olivepy-docs/utils.html#olivepy-utils-module","text":"","title":"olivepy utils module"},{"location":"olivepy-docs/utils.html#olivepyutilspem","text":"Encapsulate pem file IO Abstracts reading from and writing to PEM files and should provide common operations on PEM records such as duration(). The str() method of this class will result in a string that is legal PEM format. All operations are performed in memory so don't use any extraordinarily huge PEM files. Each line in a PEM file has: filename, channel, class_label, start_t, end_t","title":"olivepy.utils.pem"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.Pem","text":"","title":"Pem"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.Pem.get_maximum_duration","text":"Get duration of maxium duration record in PEM. Intended only for cases where PEM contains only one ID. Source code in olivepy/utils/pem.py def get_maximum_duration ( self ): \"\"\" Get duration of maxium duration record in PEM. Intended only for cases where PEM contains only one ID. \"\"\" duration = 0 for id in list ( self . __record_map . keys ()): for rec in self . __record_map [ id ]: this_duration = float ( rec . duration ()) if ( this_duration > duration ): duration = this_duration return duration","title":"get_maximum_duration()"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.Pem.get_minimum_duration","text":"Get duration of minimum duration record in PEM. Intended only for cases where PEM contains only one ID. Source code in olivepy/utils/pem.py def get_minimum_duration ( self ): \"\"\" Get duration of minimum duration record in PEM. Intended only for cases where PEM contains only one ID. \"\"\" duration = self . get_total_duration () for id in list ( self . __record_map . keys ()): for rec in self . __record_map [ id ]: this_duration = float ( rec . duration ()) if ( this_duration < duration ): duration = this_duration return duration","title":"get_minimum_duration()"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.PemRecord","text":"The underlying PEM container","title":"PemRecord"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.PemRecord.__init__","text":"Parameters: Name Type Description Default id generally the filename required channel the channel (if stereo). May be a string list (i.e. \"1,2\"). No validation is done for the channel value required label a \"class\" label for this segment. Examples include speaker, language, speech, etc required start_t the start time in seconds required end_t the end time in seconds required decimal if true, value is stored as a float False Source code in olivepy/utils/pem.py def __init__ ( self , id , channel , label , start_t , end_t , decimal = False ): ''' :param id: generally the filename :param channel: the channel (if stereo). May be a string list (i.e. \"1,2\"). No validation is done for the channel value :param label: a \"class\" label for this segment. Examples include speaker, language, speech, etc :param start_t: the start time in seconds :param end_t: the end time in seconds :param decimal: if true, value is stored as a float ''' self . id = id self . channel = channel self . label = label # Using Decimal instead of floats made the code 100x slower # Use floats for now in the intrest of speed if decimal : self . start_t = start_t self . end_t = end_t else : self . start_t = float ( start_t ) self . end_t = float ( end_t ) if ( self . start_t > self . end_t ): raise Exception ( \"Start is after end in PemRecord: {self} \" . format ( ** vars ()))","title":"__init__()"},{"location":"olivepy-docs/utils.html#olivepy.utils.pem.PemRecord.split_channels","text":"Split the channel into an array, so that if a channel value of '1,2' is supplied it is returned as an array [1,2] Returns: Type Description an array of channel numbers Source code in olivepy/utils/pem.py def split_channels ( self ): ''' Split the channel into an array, so that if a channel value of '1,2' is supplied it is returned as an array [1,2] :return: an array of channel numbers ''' channels = [] if type ( self . channel ) is str : # convert to a list channels = list ( map ( int , str . split ( self . channel , ',' ))) elif type ( self . channel ) is int : channels . append ( self . channel ) else : print ( \"Unsupported channel value: {} \" . format ( self . channel )) return channels","title":"split_channels()"},{"location":"olivepy-docs/utils.html#olivepyutilsutils","text":"","title":"olivepy.utils.utils"},{"location":"olivepy-docs/utils.html#olivepy.utils.utils.parse_json_options","text":"Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name Parameters: Name Type Description Default option_str the options to parse required Source code in olivepy/utils/utils.py def parse_json_options ( option_str ): \"\"\" Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name :param option_str: the options to parse :return a list of OptionValue objects created from the JSON option input \"\"\" # Examples of inputs to handle: # [{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' # '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}' # '[{\"task\":\"LID\", \"options\": {\"filter_length\":99, \"interpolate\":11.0, \"test_name\":\"midge\"}}]' # '[{\"job\":\"SAD LID Job\", \"task\":\"LID\", \"options\": {\"filter_length\":99, \"interpolate\":11.0, \"test_name\":\"midge\"}}]' # Parse options json_opts = json . loads ( option_str ) out_opts = [] # Options can be a list of task specific options # currently we don't support task specific options so just create one dictionary of name/value options if isinstance ( json_opts , list ): for item in json_opts : in_opts = item [ 'options' ] for opt in in_opts : # print(\"\\t{} = {}, value type: {}\".format(opt, in_opts[opt], type(in_opts[opt]))) opt_msg = olivepb . OptionValue () opt_msg . name = opt opt_msg . value = str ( in_opts [ opt ]) # optionally check if this option is restricted to a job/task: if 'task' in item : opt_msg . task_filter_name = item [ 'task' ] if 'job' in item : opt_msg . job_filter_name = item [ 'job' ] out_opts . append ( opt_msg ) else : # or options that are applied to each task, which is just a simple dictionary # like: {\"filter_length\":99, \"interpolate\":1.0} # OLIVE wil internally ignore these options if the keyname does not match one of the option name # a plugin supports for the requested trait (i.e. plugin.get_region_scoring_opts() for opt in json_opts : opt_msg = olivepb . OptionValue () opt_msg . name = opt opt_msg . value = str ( json_opts [ opt ]) out_opts . append ( opt_msg ) # print(\"\\t{} = {}, value type: {}\".format(opt, json_opts[opt], type(json_opts[opt]))) print ( \"Final json options: {} \" . format ( out_opts )) return out_opts","title":"parse_json_options()"},{"location":"olivepy-docs/utils.html#olivepy.utils.utils.parse_json_options_as_dict","text":"Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name Parameters: Name Type Description Default option_str the options to parse required Returns: Type Description a dictionary of options name/value pairs Source code in olivepy/utils/utils.py def parse_json_options_as_dict ( option_str ): \"\"\" Parse options from a json string. Intended to be used for workflow options that may be grouped by one or more tasks. Options can be passed in a couple of different structures. In the more complicated case they can be a list of dictionaries, that specify the task/job name these options are used for, for example: '[{\"task\":\"SAD\", \"options\":{\"filter_length\":99, \"interpolate\":1.0}}]' They can also be passed in a simple dictionary, like: '{\"filter_length\":99, \"interpolate\":1.0, \"name\":\"midge\"}'. In the former example, options are only passed to the job/task specified. In the latter case, these options are passed to all tasks. In both cases, OLIVE will only pass options to a task if the task supports that option name :param option_str: the options to parse :return: a dictionary of options name/value pairs \"\"\" # Parse options json_opts = json . loads ( option_str ) out_opts = dict () # Options can be a list of task specific options # currently we don't support task specific options so just create one dictionary of name/value options if isinstance ( json_opts , list ): for item in json_opts : in_opts = item [ 'options' ] print ( \"Found {} options for task: {} \" . format ( len ( in_opts ), item [ 'task' ])) out_opts . update ( in_opts ) for opt in in_opts : print ( \" \\t {} = {} , value type: {} \" . format ( opt , in_opts [ opt ], type ( in_opts [ opt ]))) else : # or options that are applied to each task, which is just a simple dictionary # like: {\"filter_length\":99, \"interpolate\":1.0} # OLIVE wil internally ignore these options if the keyname does not match one of the option name # a plugin supports for the requested trait (i.e. plugin.get_region_scoring_opts() out_opts = json_opts for opt in json_opts : print ( \" \\t {} = {} , value type: {} \" . format ( opt , json_opts [ opt ], type ( json_opts [ opt ]))) print ( \"Final json options: {} \" . format ( out_opts )) return out_opts","title":"parse_json_options_as_dict()"},{"location":"olivepy-docs/utils.html#olivepy.utils.utils.parse_pem_file","text":"Parse a PEM file, grouping the results by audio file and channel. Parameters: Name Type Description Default data_lines the data line to parse required Returns: Type Description a dictionary of audio files to score and the channel region: # {'filename': {channel: {class_id : [(start_region, end_region, class_id)]} } } Source code in olivepy/utils/utils.py def parse_pem_file ( data_lines ): \"\"\" Parse a PEM file, grouping the results by audio file and channel. :param data_lines: the data line to parse :return: a dictionary of audio files to score and the channel region: # {'filename': {channel: {class_id : [(start_region, end_region, class_id)]} } } \"\"\" # We process by file and channel - the class/label is ignored regions = {} input_pem = Pem () input_pem . add_records_from_data_lines ( data_lines ) for id in input_pem . get_ids (): audio_id = os . path . expandvars ( id ) # Create a dictionary of the regions specified for the the current file regions [ audio_id ] = {} for rec in input_pem . get_records ( id ): # channel could be a list... channels = [] if type ( rec . channel ) is str : # convert to a list channels = map ( int , str . split ( rec . channel , ',' )) elif type ( rec . channel ) is int : channels . append ( rec . channel ) else : print ( \"Unsupported channel value: {} \" . format ( rec . channel )) for ch in channels : if ch not in regions [ audio_id ]: regions [ audio_id ][ ch ] = {} class_id = rec . label if class_id not in regions [ audio_id ][ ch ]: regions [ audio_id ][ ch ][ class_id ] = [] regions [ audio_id ][ ch ][ class_id ] . append (( rec . start_t , rec . end_t )) return regions","title":"parse_pem_file()"},{"location":"olivepy_api/README-Dev.html","text":"Qucik Build (without running unittests) mvn install -DskipTests=True","title":"Qucik Build (without running unittests)"},{"location":"olivepy_api/README-Dev.html#qucik-build-without-running-unittests","text":"mvn install -DskipTests=True","title":"Qucik Build (without running unittests)"},{"location":"plugins/aed-enrollable-v1.html","text":"aed-enrollable-v1.0.0 (Acoustic Event Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 v1.0.1 Various bug fixes and stability improvements, including fix to event/overlap deconfliction Description The goal of acoustic event detection is to identify and label enrolled sound classes within an audio file. This capability is designed to detect sound class regions within an audio file. Regions are discrete segments of the waveform where the event is detected. This plugin comes with a core set of enrolled classes: bird song dog bark door slamming restaurant noise traffic noise music wind gunshot explosion This plugin also allows the users to enroll new classes with example data similarly to speaker recognition. This capability is based on sound embeddings with Probabilistic Linear Discriminant Analysis (PLDA) backend and multi-class linear calibration and capable of enrolling new sound classes. This is the first release of this acoustic event detection (AED) plugin. This plugin detects and labels both discrete (things like gunshots and door slams) and continuous (things like music, winds, traffic, etc.) acoustic events. This plugin allows the detection of multiple sound classes within the same audio file or buffer, depending on the classes in the plugin. This plugin is capable of enrolling new sound classes from samples of audio. A minimum of five examples is needed to begin a reasonably effective model. Domains default-v1 Default domain meant for general-purpose audio conditions. Inputs For enrollment, an audio file or buffer with a corresponding sound labels. This assumes that the entire audio file is the event. To identify sub-regions of the file time annotations should also be provided using the standard methods. An example: Input-audio_1.wav MUSIC Input-audio_2.wav MUSIC Input-audio_3.wav DOG Input-audio_4.wav DOG Input-audio_5.wav TRAFFIC For scoring, an audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specified the entire audio file or buffer will be scored. Outputs The AED plugin returns a list of regions with a score for each detected classes. Regions are represented in seconds. Scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The list of results has a file name (or in the case of a buffer, an identifier), class label, a score for each detected sound class and the begin and end seconds of where the class was detected. The command-line interface output is formatted in this form: < file_name > < start _seconds > < end _seconds > < class > < score > An example output excerpt: audio.wav 0.00 1.49 GUNSHOT 4.38631201 audio.wav 2.45 7.49 MUSIC 3.77045155 audio.wav 8.70 10.74 DOOR 3.85196638 audio.wav 10.74 11.74 TRAFFIC 0.08894229 audio.wav 12.20 13.74 WIND 0.65049744 Enrollments This plugin allows class modifications. A class modification is essentially the capability to enroll a new class of acoustic event with sample(s) of a new sound class. A new sound enrollment is created with the class modification functionality, which consists of sending the system one or more audio samples from a sound class along with a label for that class. These samples must be passed in with annotations indicating where in the file the event is located, or the sample event must be manually segmented out by the users so the whole file submitted consists only of the event being enrolled. These enrollments can be augmented with subsequent class modification requests by adding more segmented audio with the same sound label. The same process can be used to augment an existing class (Example: BIRD, DOG, etc.) with new samples. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected event of interest and corresponding score for this event RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new event models or augment existing event models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Minimum Audio Length The system will only attempt to perform sound detection on audio segments longer than 1 second. Enrollment Limitations User can use a single file to enroll a new model, but multiple sound files of at least five or more waveforms are expected (more is better) to produce a better sound model. Enrolled Classes The initial enrolled classes are bird song, dog bark, door slamming, restaurant noise, traffic noise, music, wind, gunshot, and explosion. Global Options Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false detection segments. Reduce the threshold value if there are too many missed segments. 0.0 -5.0 to 5.0 min_output_seg Minimum output segment duration: Rejects all detections less than minimum output segment duration. 0.5 0.0 to 1.0","title":"aed-enrollable-v1.0.0 (Acoustic Event Detection)"},{"location":"plugins/aed-enrollable-v1.html#aed-enrollable-v100-acoustic-event-detection","text":"","title":"aed-enrollable-v1.0.0 (Acoustic Event Detection)"},{"location":"plugins/aed-enrollable-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 v1.0.1 Various bug fixes and stability improvements, including fix to event/overlap deconfliction","title":"Version Changelog"},{"location":"plugins/aed-enrollable-v1.html#description","text":"The goal of acoustic event detection is to identify and label enrolled sound classes within an audio file. This capability is designed to detect sound class regions within an audio file. Regions are discrete segments of the waveform where the event is detected. This plugin comes with a core set of enrolled classes: bird song dog bark door slamming restaurant noise traffic noise music wind gunshot explosion This plugin also allows the users to enroll new classes with example data similarly to speaker recognition. This capability is based on sound embeddings with Probabilistic Linear Discriminant Analysis (PLDA) backend and multi-class linear calibration and capable of enrolling new sound classes. This is the first release of this acoustic event detection (AED) plugin. This plugin detects and labels both discrete (things like gunshots and door slams) and continuous (things like music, winds, traffic, etc.) acoustic events. This plugin allows the detection of multiple sound classes within the same audio file or buffer, depending on the classes in the plugin. This plugin is capable of enrolling new sound classes from samples of audio. A minimum of five examples is needed to begin a reasonably effective model.","title":"Description"},{"location":"plugins/aed-enrollable-v1.html#domains","text":"default-v1 Default domain meant for general-purpose audio conditions.","title":"Domains"},{"location":"plugins/aed-enrollable-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding sound labels. This assumes that the entire audio file is the event. To identify sub-regions of the file time annotations should also be provided using the standard methods. An example: Input-audio_1.wav MUSIC Input-audio_2.wav MUSIC Input-audio_3.wav DOG Input-audio_4.wav DOG Input-audio_5.wav TRAFFIC For scoring, an audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specified the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/aed-enrollable-v1.html#outputs","text":"The AED plugin returns a list of regions with a score for each detected classes. Regions are represented in seconds. Scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The list of results has a file name (or in the case of a buffer, an identifier), class label, a score for each detected sound class and the begin and end seconds of where the class was detected. The command-line interface output is formatted in this form: < file_name > < start _seconds > < end _seconds > < class > < score > An example output excerpt: audio.wav 0.00 1.49 GUNSHOT 4.38631201 audio.wav 2.45 7.49 MUSIC 3.77045155 audio.wav 8.70 10.74 DOOR 3.85196638 audio.wav 10.74 11.74 TRAFFIC 0.08894229 audio.wav 12.20 13.74 WIND 0.65049744","title":"Outputs"},{"location":"plugins/aed-enrollable-v1.html#enrollments","text":"This plugin allows class modifications. A class modification is essentially the capability to enroll a new class of acoustic event with sample(s) of a new sound class. A new sound enrollment is created with the class modification functionality, which consists of sending the system one or more audio samples from a sound class along with a label for that class. These samples must be passed in with annotations indicating where in the file the event is located, or the sample event must be manually segmented out by the users so the whole file submitted consists only of the event being enrolled. These enrollments can be augmented with subsequent class modification requests by adding more segmented audio with the same sound label. The same process can be used to augment an existing class (Example: BIRD, DOG, etc.) with new samples.","title":"Enrollments"},{"location":"plugins/aed-enrollable-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected event of interest and corresponding score for this event RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new event models or augment existing event models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/aed-enrollable-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/aed-enrollable-v1.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/aed-enrollable-v1.html#minimum-audio-length","text":"The system will only attempt to perform sound detection on audio segments longer than 1 second.","title":"Minimum Audio Length"},{"location":"plugins/aed-enrollable-v1.html#enrollment-limitations","text":"User can use a single file to enroll a new model, but multiple sound files of at least five or more waveforms are expected (more is better) to produce a better sound model.","title":"Enrollment Limitations"},{"location":"plugins/aed-enrollable-v1.html#enrolled-classes","text":"The initial enrolled classes are bird song, dog bark, door slamming, restaurant noise, traffic noise, music, wind, gunshot, and explosion.","title":"Enrolled Classes"},{"location":"plugins/aed-enrollable-v1.html#global-options","text":"Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false detection segments. Reduce the threshold value if there are too many missed segments. 0.0 -5.0 to 5.0 min_output_seg Minimum output segment duration: Rejects all detections less than minimum output segment duration. 0.5 0.0 to 1.0","title":"Global Options"},{"location":"plugins/aln-waveformAlignment-v1.html","text":"aln-waveformAlignment-v1 (Waveform Alignment) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Core behavior This plugin temporally aligns a group of waveforms of different durations, to a common reference waveform. The approach was developed based on the idea that there is a reference event in a scene (yelling, gunshot, accident, etc.) in a reference file, and the user would like to align other files to this event. Hence, we assume that the default case is where the reference is shorter than the other files being searched. In the core cases the system thus uses the first input waveform as the reference and generates a temporal offset for each of the other (\"target\") waveforms, if they can be aligned. In the exceptional case, the reference waveform is shorter than a target, then this comparison is aligned with respect to the target start time. Thus, if the beginning of the reference aligns with 50 seconds from the start of the target, the alignment is returned as a \"50\". This is depicted in the figure below, where each \"-\" is 10 seconds: XXXXXXXXXXXXXX Reference -----XXXXXXXXXXXXXX----- Target If the reference is longer than the target then the alignment is reversed, and the results are returned back as negative numbers. The figure below would return \"-50\". Returning this as a negative number is just a convention to indicate how the file should be aligned, results are always offset from the start of the longer file. -----XXXXXXXXXXXXXX----- Reference XXXXXXXXXXXXXX Target For cases where two waveforms are found to align with the same start time, a \"0\" is returned. The alignment of the files is calculated using the highest correlation of the candidate alignments beyond a threshold. If the two files never align with a correlation greater than the threshold no results are returned for that comparison. Thus one may pass in five targets and a reference and receive back less than five results. In the default case (when the reference is smaller) the reference waveform (first in the list) is swept across each target waveform in two-second increments. At each position, the correlation value is recorded. The same processing is applied to each target window. In cases where the target is shorter, the same process is performed using the target. Then, the index of the maximum correlation value that crosses the detection threshold is used to indicate where the reference file aligns to the different targets. Each detection uses the correlation value itself as a proxy for confidence. Annotations Input files may also be accompanied by \"annotations\" that indicate those regions the plugin should process, ignoring other parts of the waveform. This is useful to allow the user to specify a short region in the reference file containing an event of interest that should be searched for in the other files. In the case where annotations are used they must be provided or all files. In general we would expect that the reference waveform would have an annotated region that is relatively short and that the annotations for the target files would simply indicate their start to end times, though this does not have to be the case. wave1.wav 130.5 135.8 (reference) wave2.wav 0 150.9 (target) wave3.wav 0 180.0 (target) wave4.wav 0 254.2 (target) Annotations are optional, but, as noted, when annotations are used they must be provided for all files in the call. Domains default-v1 Default processing domain - performs alignment using correlation values. Inputs The input is a group of audio files, which is different from most OLIVE plugins. The first file is treated as the event, or reference, that must be found in the other longer files. Outputs The plugin returns an estimated lag for each unique pair of the reference waveform with every other waveform. That is, for every (reference, target) pair it returns the relative lag in seconds where reference occurs in target along with a confidence score. referenceFile.wav file1.wav -2.47 0.776 referenceFile.wav file2.wav 7.98 0.84 referenceFile.wav file3.wav 15.91 0.49 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AudioAlignmentPlugin \u2013 First implementation of a plugin that can take multiple input waveforms at the same time. Compatibility OLIVE 5.1+ Limitations If the reference is not alignable with a target, nothing is returned to the user for this comparison. If none of the waveforms can be aligned then nothing is returned. As noted above, if the reference waveform is longer than a target, then the estimated lag will be negative and based off the start time of the longer waveform. Hence \"50\" means that the reference file is aligned with the point 50 seconds from the start of the target waveform. \"-50\" means that the target file is aligned with the point 50 seconds from the start of the reference waveform. In cases where the two waveforms align in such a way that that the very start of the reference waveform aligns with the very end of the target, the attempt to align may fail. This is a know bug, and is unlikely to arise when the reference event is short in duration compared with the target waveform being searched. The figure below is likely to fail to produce an alignment, due to algorithmic limitation that can be overcome with future work to pad the detection process. XXX-------------- --------------XXX The case in the figure below will succeed to align, since no padding is required. This behavior is an artifact of the understanding that users would generally be searching longer target files with limited duration reference events, which may not always be the case. XXX --------------XXX Comments Currently, the plugin uses the first input waveform in the list as the reference event to localize. If the reference is not found in a target, then the user is notified of this via a message. The following message will appear: `'Warning: Reference \"{REF_NAME}\" was not found in Target \"{TARGET_NAME}\" at detection threshold={THRESHOLD}.'` Where the values in {} will be replaced with the appropriate waveform names and internal plugin threshold. Global Options Option Name Description Default Expected Range threshold Threshold for signal energy detection. Has only been tested and verified with default value. 0.1 0 to 1","title":"aln-waveformAlignment-v1 (Waveform Alignment)"},{"location":"plugins/aln-waveformAlignment-v1.html#aln-waveformalignment-v1-waveform-alignment","text":"","title":"aln-waveformAlignment-v1 (Waveform Alignment)"},{"location":"plugins/aln-waveformAlignment-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/aln-waveformAlignment-v1.html#description","text":"","title":"Description"},{"location":"plugins/aln-waveformAlignment-v1.html#core-behavior","text":"This plugin temporally aligns a group of waveforms of different durations, to a common reference waveform. The approach was developed based on the idea that there is a reference event in a scene (yelling, gunshot, accident, etc.) in a reference file, and the user would like to align other files to this event. Hence, we assume that the default case is where the reference is shorter than the other files being searched. In the core cases the system thus uses the first input waveform as the reference and generates a temporal offset for each of the other (\"target\") waveforms, if they can be aligned. In the exceptional case, the reference waveform is shorter than a target, then this comparison is aligned with respect to the target start time. Thus, if the beginning of the reference aligns with 50 seconds from the start of the target, the alignment is returned as a \"50\". This is depicted in the figure below, where each \"-\" is 10 seconds: XXXXXXXXXXXXXX Reference -----XXXXXXXXXXXXXX----- Target If the reference is longer than the target then the alignment is reversed, and the results are returned back as negative numbers. The figure below would return \"-50\". Returning this as a negative number is just a convention to indicate how the file should be aligned, results are always offset from the start of the longer file. -----XXXXXXXXXXXXXX----- Reference XXXXXXXXXXXXXX Target For cases where two waveforms are found to align with the same start time, a \"0\" is returned. The alignment of the files is calculated using the highest correlation of the candidate alignments beyond a threshold. If the two files never align with a correlation greater than the threshold no results are returned for that comparison. Thus one may pass in five targets and a reference and receive back less than five results. In the default case (when the reference is smaller) the reference waveform (first in the list) is swept across each target waveform in two-second increments. At each position, the correlation value is recorded. The same processing is applied to each target window. In cases where the target is shorter, the same process is performed using the target. Then, the index of the maximum correlation value that crosses the detection threshold is used to indicate where the reference file aligns to the different targets. Each detection uses the correlation value itself as a proxy for confidence.","title":"Core behavior"},{"location":"plugins/aln-waveformAlignment-v1.html#annotations","text":"Input files may also be accompanied by \"annotations\" that indicate those regions the plugin should process, ignoring other parts of the waveform. This is useful to allow the user to specify a short region in the reference file containing an event of interest that should be searched for in the other files. In the case where annotations are used they must be provided or all files. In general we would expect that the reference waveform would have an annotated region that is relatively short and that the annotations for the target files would simply indicate their start to end times, though this does not have to be the case. wave1.wav 130.5 135.8 (reference) wave2.wav 0 150.9 (target) wave3.wav 0 180.0 (target) wave4.wav 0 254.2 (target) Annotations are optional, but, as noted, when annotations are used they must be provided for all files in the call.","title":"Annotations"},{"location":"plugins/aln-waveformAlignment-v1.html#domains","text":"default-v1 Default processing domain - performs alignment using correlation values.","title":"Domains"},{"location":"plugins/aln-waveformAlignment-v1.html#inputs","text":"The input is a group of audio files, which is different from most OLIVE plugins. The first file is treated as the event, or reference, that must be found in the other longer files.","title":"Inputs"},{"location":"plugins/aln-waveformAlignment-v1.html#outputs","text":"The plugin returns an estimated lag for each unique pair of the reference waveform with every other waveform. That is, for every (reference, target) pair it returns the relative lag in seconds where reference occurs in target along with a confidence score. referenceFile.wav file1.wav -2.47 0.776 referenceFile.wav file2.wav 7.98 0.84 referenceFile.wav file3.wav 15.91 0.49","title":"Outputs"},{"location":"plugins/aln-waveformAlignment-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AudioAlignmentPlugin \u2013 First implementation of a plugin that can take multiple input waveforms at the same time.","title":"Functionality (Traits)"},{"location":"plugins/aln-waveformAlignment-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/aln-waveformAlignment-v1.html#limitations","text":"If the reference is not alignable with a target, nothing is returned to the user for this comparison. If none of the waveforms can be aligned then nothing is returned. As noted above, if the reference waveform is longer than a target, then the estimated lag will be negative and based off the start time of the longer waveform. Hence \"50\" means that the reference file is aligned with the point 50 seconds from the start of the target waveform. \"-50\" means that the target file is aligned with the point 50 seconds from the start of the reference waveform. In cases where the two waveforms align in such a way that that the very start of the reference waveform aligns with the very end of the target, the attempt to align may fail. This is a know bug, and is unlikely to arise when the reference event is short in duration compared with the target waveform being searched. The figure below is likely to fail to produce an alignment, due to algorithmic limitation that can be overcome with future work to pad the detection process. XXX-------------- --------------XXX The case in the figure below will succeed to align, since no padding is required. This behavior is an artifact of the understanding that users would generally be searching longer target files with limited duration reference events, which may not always be the case. XXX --------------XXX","title":"Limitations"},{"location":"plugins/aln-waveformAlignment-v1.html#comments","text":"Currently, the plugin uses the first input waveform in the list as the reference event to localize. If the reference is not found in a target, then the user is notified of this via a message. The following message will appear: `'Warning: Reference \"{REF_NAME}\" was not found in Target \"{TARGET_NAME}\" at detection threshold={THRESHOLD}.'` Where the values in {} will be replaced with the appropriate waveform names and internal plugin threshold.","title":"Comments"},{"location":"plugins/aln-waveformAlignment-v1.html#global-options","text":"Option Name Description Default Expected Range threshold Threshold for signal energy detection. Has only been tested and verified with default value. 0.1 0 to 1","title":"Global Options"},{"location":"plugins/asr-dynapy-v2.html","text":"asr-dynapy-v2 (Automatic Speech Recognition) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is functionally identical to its predecessor,asr-dynapy-v1, but has been converted to be compatible with OLIVE 5.x. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks(TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models. Domains cmn-tdnnChain-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. eng-tdnnChain-tel-v1 English domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. fas-tdnn-tel-v1 Farsi domain focused on conversational telephony speech. This domain features non-chain TDNN networks, and reports word posterior scores. rus-tdnnChain-tel-v1 Russian domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. It also features a very large and complex language model that makes it quite memory-intensive to use. See the \"Resources\" Limitation listed below for more information. spa-tdnnChain-tel-v1 Spanish domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Out of Vocabulary (OOV) Words, Names The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications. Confidences vs Word Posteriors Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release - so there may be transcription errors resulting from these audio chunk break points. Resources The Russian domain, rus-tdnnChain-tel-v1, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain. Arabic Script Languages Note that for the Farsi domain, fas-tdnn-tel-v1, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Non-Verbal Recognizer Output Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0","title":"asr-dynapy-v2 (Automatic Speech Recognition)"},{"location":"plugins/asr-dynapy-v2.html#asr-dynapy-v2-automatic-speech-recognition","text":"","title":"asr-dynapy-v2 (Automatic Speech Recognition)"},{"location":"plugins/asr-dynapy-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/asr-dynapy-v2.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is functionally identical to its predecessor,asr-dynapy-v1, but has been converted to be compatible with OLIVE 5.x. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks(TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models.","title":"Description"},{"location":"plugins/asr-dynapy-v2.html#domains","text":"cmn-tdnnChain-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. eng-tdnnChain-tel-v1 English domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. fas-tdnn-tel-v1 Farsi domain focused on conversational telephony speech. This domain features non-chain TDNN networks, and reports word posterior scores. rus-tdnnChain-tel-v1 Russian domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. It also features a very large and complex language model that makes it quite memory-intensive to use. See the \"Resources\" Limitation listed below for more information. spa-tdnnChain-tel-v1 Spanish domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores.","title":"Domains"},{"location":"plugins/asr-dynapy-v2.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-dynapy-v2.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Outputs"},{"location":"plugins/asr-dynapy-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-dynapy-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/asr-dynapy-v2.html#limitations","text":"As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-dynapy-v2.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-dynapy-v2.html#out-of-vocabulary-oov-words-names","text":"The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications.","title":"Out of Vocabulary (OOV) Words, Names"},{"location":"plugins/asr-dynapy-v2.html#confidences-vs-word-posteriors","text":"Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores.","title":"Confidences vs Word Posteriors"},{"location":"plugins/asr-dynapy-v2.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release - so there may be transcription errors resulting from these audio chunk break points.","title":"Overlap"},{"location":"plugins/asr-dynapy-v2.html#resources","text":"The Russian domain, rus-tdnnChain-tel-v1, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain.","title":"Resources"},{"location":"plugins/asr-dynapy-v2.html#arabic-script-languages","text":"Note that for the Farsi domain, fas-tdnn-tel-v1, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Arabic Script Languages"},{"location":"plugins/asr-dynapy-v2.html#non-verbal-recognizer-output","text":"Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations.","title":"Non-Verbal Recognizer Output"},{"location":"plugins/asr-dynapy-v2.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-dynapy-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-dynapy-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/asr-dynapy-v3.html","text":"asr-dynapy-v3 (Automatic Speech Recognition) Version Changelog Plugin Version Change v3.0.0 Initial plugin release, published with OLIVE 5.2.0 Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin builds from its predecessor, asr-dynapy-v2, and features bug fixes and compatibility updates, as well as 3 additional domains bringing new language capabilities. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks (TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models. Domains english-tdnnChain-tel-v1 English domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. farsi-tdnnChain-tel-v1 Farsi domain focused on conversational telephony speech. This domain now features chain networks and reports word posterior scores. french-tdnnChain-tel-v2 French domain augmented with African-accented French data. This domain features chain networks and reports word posterior scores. iraqiArabic-tdnnChain-tel-v1 Iraqi Arabic dialect domain. This domain features chain networks and reports word posterior scores. levantineArabic-tdnnChain-tel-v1 Levantine Arabic dialect domain. This domain features chain networks and reports word posterior scores. mandarin-tdnnChain-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. pashto-tdnnChain-tel-v1 Pashto domain. This domain features chain networks and reports word posterior scores. russian-tdnnChain-tel-v2 Russian domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. It also features a very large and complex language model that makes it quite memory-intensive to use. See the \"Resources\" Limitation listed below for more information. This domain has been significantly reduced both memory- and disk-footprint-wise from asr-dynapy-v2, but remains the largest domain within the plugin. spanish-tdnnChain-tel-v1 Spanish domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.2+ Limitations As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Out of Vocabulary (OOV) Words, Names The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications. Confidences vs Word Posteriors Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points. Resources The Russian domain, rus-tdnnChain-tel-v2, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain. Arabic Script Languages Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Non-Verbal Recognizer Output Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"asr-dynapy-v3 (Automatic Speech Recognition)"},{"location":"plugins/asr-dynapy-v3.html#asr-dynapy-v3-automatic-speech-recognition","text":"","title":"asr-dynapy-v3 (Automatic Speech Recognition)"},{"location":"plugins/asr-dynapy-v3.html#version-changelog","text":"Plugin Version Change v3.0.0 Initial plugin release, published with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/asr-dynapy-v3.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin builds from its predecessor, asr-dynapy-v2, and features bug fixes and compatibility updates, as well as 3 additional domains bringing new language capabilities. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks (TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models.","title":"Description"},{"location":"plugins/asr-dynapy-v3.html#domains","text":"english-tdnnChain-tel-v1 English domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. farsi-tdnnChain-tel-v1 Farsi domain focused on conversational telephony speech. This domain now features chain networks and reports word posterior scores. french-tdnnChain-tel-v2 French domain augmented with African-accented French data. This domain features chain networks and reports word posterior scores. iraqiArabic-tdnnChain-tel-v1 Iraqi Arabic dialect domain. This domain features chain networks and reports word posterior scores. levantineArabic-tdnnChain-tel-v1 Levantine Arabic dialect domain. This domain features chain networks and reports word posterior scores. mandarin-tdnnChain-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. pashto-tdnnChain-tel-v1 Pashto domain. This domain features chain networks and reports word posterior scores. russian-tdnnChain-tel-v2 Russian domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores. It also features a very large and complex language model that makes it quite memory-intensive to use. See the \"Resources\" Limitation listed below for more information. This domain has been significantly reduced both memory- and disk-footprint-wise from asr-dynapy-v2, but remains the largest domain within the plugin. spanish-tdnnChain-tel-v1 Spanish domain focused on conversational telephony speech. This domain features chain networks and reports word posterior scores.","title":"Domains"},{"location":"plugins/asr-dynapy-v3.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-dynapy-v3.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Outputs"},{"location":"plugins/asr-dynapy-v3.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-dynapy-v3.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/asr-dynapy-v3.html#limitations","text":"As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-dynapy-v3.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-dynapy-v3.html#out-of-vocabulary-oov-words-names","text":"The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications.","title":"Out of Vocabulary (OOV) Words, Names"},{"location":"plugins/asr-dynapy-v3.html#confidences-vs-word-posteriors","text":"Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores.","title":"Confidences vs Word Posteriors"},{"location":"plugins/asr-dynapy-v3.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points.","title":"Overlap"},{"location":"plugins/asr-dynapy-v3.html#resources","text":"The Russian domain, rus-tdnnChain-tel-v2, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain.","title":"Resources"},{"location":"plugins/asr-dynapy-v3.html#arabic-script-languages","text":"Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Arabic Script Languages"},{"location":"plugins/asr-dynapy-v3.html#non-verbal-recognizer-output","text":"Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations.","title":"Non-Verbal Recognizer Output"},{"location":"plugins/asr-dynapy-v3.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-dynapy-v3.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-dynapy-v3.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Global Options"},{"location":"plugins/asr-dynapy-v4.html","text":"asr-dynapy-v4 (Automatic Speech Recognition) Version Changelog Plugin Version Change v4.0.0 Initial plugin release, based on v3.0.0, but with bug fixes and merged in some specialty features like Streaming, and the addition of memory-saving look-ahead model architectures for some domains. Published with OLIVE 5.5.0 Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin builds from its predecessor, asr-dynapy-v2, and features bug fixes and compatibility updates, as well as 3 additional domains bringing new language capabilities. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks (TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models. Domains (Supported Languages) Distant Speech targeted domains These domains have been augmented with additional audio data sources to better handle distortions and other effects caused by non-close-talk recording conditions. mandarin-tdnnChainStaticSrilm-multi-v1 Mandarin domain augmented for distant speech. russian-tdnnChainLookaheadRnnlm-multi-v2 Russian domain augmented for distant speech. spanish-tdnnChainStaticRnnlm-multi-v2 Spanish domain augmented for distant speech. ukrainian-tdnnChainStaticSrilm-multi-v1 Ukrainian domain augmented for distant speech. Telephony Speech trained domains with Lookahead models english-tdnnLookaheadRnnlm-tel-v2 English domain trained with conversational telephony speech. This domain features lookahead archicture for memory footpring benefits and reports word posterior scores. farsi-tdnnLookaheadRnnlm-tel-v1 Farsi domain trained with conversational telephony speech. This domain now features lookahead models and reports word posterior scores. french-tdnnLookaheadRnnlm-tel-v1 French domain augmented with African-accented French data. This domain features lookahead models and reports word posterior scores. iraqiArabic-tdnnLookaheadRnnlm-tel-v1 Iraqi Arabic dialect domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. korean-tdnnLookahead-tel-v1 Korean domain trained with conversational telephony speech. Features lookahead models and reports word posterior scores. levantineArabic-tdnnLookaheadRnnlm-tel-v1 Levantine Arabic dialect domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. mandarin-tdnnLookaheadRnnlm-tel-v1 Mandarin Chinese domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. pashto-tdnnLookaheadRnnlm-tel-v1 Pashto domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. russian-tdnnLookaheadRnnlm-tel-v1 Russian domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. spanish-tdnnLookaheadRnnlm-tel-v1 Spanish domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.2+ Limitations As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Out of Vocabulary (OOV) Words, Names The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications. Confidences vs Word Posteriors Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points. Resources The Russian domain, rus-tdnnChain-tel-v2, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain. Arabic Script Languages Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Non-Verbal Recognizer Output Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Automatic Speech Recognition (ASR)"},{"location":"plugins/asr-dynapy-v4.html#asr-dynapy-v4-automatic-speech-recognition","text":"","title":"asr-dynapy-v4 (Automatic Speech Recognition)"},{"location":"plugins/asr-dynapy-v4.html#version-changelog","text":"Plugin Version Change v4.0.0 Initial plugin release, based on v3.0.0, but with bug fixes and merged in some specialty features like Streaming, and the addition of memory-saving look-ahead model architectures for some domains. Published with OLIVE 5.5.0","title":"Version Changelog"},{"location":"plugins/asr-dynapy-v4.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin builds from its predecessor, asr-dynapy-v2, and features bug fixes and compatibility updates, as well as 3 additional domains bringing new language capabilities. It is based on SRI's DynaSpeak recognition platform, features word-based region-scoring outputs and provides the capability to output a score for each timestamped word. This score represents a NN Confidence value if available, and will back off to Word Posterior score if the Confidence is not available. If neither of these measures are available, the 'score' field will contain a -1.0. All domains described below currently report word posterior scores, but the plugin is capable of handling NN confidence values in updated domains that will be delivered in the future. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All current domains are trained on conversational telephone speech, and will perform best on matched audio conditions, but are still capable of recognition in mismatched audio domains. Any input audio at a sample rate higher than 8000 Hz will be resampled and processed as 8 kHz audio. Any higher-frequency (> 4 kHz) information will be discarded. All of the current domains are based on the time-delay neural networks (TDNN) architecture. Some of these domains are also chain models, which allow us to compute frames at a lower frequency without sacrificing accuracy, allowing faster processing speed thanks to less computation. The chain model domains use much deeper networks than previous technologies providing much better accuracy. Refer to the domains list below to view which domains are and aren't currently chain models.","title":"Description"},{"location":"plugins/asr-dynapy-v4.html#domains-supported-languages","text":"","title":"Domains (Supported Languages)"},{"location":"plugins/asr-dynapy-v4.html#distant-speech-targeted-domains","text":"These domains have been augmented with additional audio data sources to better handle distortions and other effects caused by non-close-talk recording conditions. mandarin-tdnnChainStaticSrilm-multi-v1 Mandarin domain augmented for distant speech. russian-tdnnChainLookaheadRnnlm-multi-v2 Russian domain augmented for distant speech. spanish-tdnnChainStaticRnnlm-multi-v2 Spanish domain augmented for distant speech. ukrainian-tdnnChainStaticSrilm-multi-v1 Ukrainian domain augmented for distant speech.","title":"Distant Speech targeted domains"},{"location":"plugins/asr-dynapy-v4.html#telephony-speech-trained-domains-with-lookahead-models","text":"english-tdnnLookaheadRnnlm-tel-v2 English domain trained with conversational telephony speech. This domain features lookahead archicture for memory footpring benefits and reports word posterior scores. farsi-tdnnLookaheadRnnlm-tel-v1 Farsi domain trained with conversational telephony speech. This domain now features lookahead models and reports word posterior scores. french-tdnnLookaheadRnnlm-tel-v1 French domain augmented with African-accented French data. This domain features lookahead models and reports word posterior scores. iraqiArabic-tdnnLookaheadRnnlm-tel-v1 Iraqi Arabic dialect domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. korean-tdnnLookahead-tel-v1 Korean domain trained with conversational telephony speech. Features lookahead models and reports word posterior scores. levantineArabic-tdnnLookaheadRnnlm-tel-v1 Levantine Arabic dialect domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. mandarin-tdnnLookaheadRnnlm-tel-v1 Mandarin Chinese domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. pashto-tdnnLookaheadRnnlm-tel-v1 Pashto domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. russian-tdnnLookaheadRnnlm-tel-v1 Russian domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores. spanish-tdnnLookaheadRnnlm-tel-v1 Spanish domain trained with conversational telephony speech. This domain features lookahead models and reports word posterior scores.","title":"Telephony Speech trained domains with Lookahead models"},{"location":"plugins/asr-dynapy-v4.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-dynapy-v4.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 and 43.00000000 input-audio.wav 0.210 0.340 we're 44.00000000 input-audio.wav 0.330 0.460 going 97.00000000 input-audio.wav 0.450 0.520 to 97.00000000 input-audio.wav 0.510 0.940 fly 66.00000000 input-audio.wav 1.080 1.300 was 31.00000000 input-audio.wav 1.290 1.390 that 24.00000000 input-audio.wav 1.290 1.390 it 22.00000000 input-audio.wav 1.380 1.510 we're 27.00000000 input-audio.wav 1.500 1.660 going 97.00000000 input-audio.wav 1.650 1.720 to 98.00000000 input-audio.wav 1.710 1.930 fly 94.00000000 input-audio.wav 1.920 2.110 over 79.00000000 input-audio.wav 2.100 2.380 saint 93.00000000 input-audio.wav 2.370 2.950 louis 96.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 99.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 98.00000000 input-audio.wav 0.870 0.970 \u7684 99.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 86.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 93.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 99.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 100.00000000 input-audio.wav 3.130 3.340 \u7684 100.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 55.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 53.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Outputs"},{"location":"plugins/asr-dynapy-v4.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-dynapy-v4.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/asr-dynapy-v4.html#limitations","text":"As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-dynapy-v4.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-dynapy-v4.html#out-of-vocabulary-oov-words-names","text":"The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications.","title":"Out of Vocabulary (OOV) Words, Names"},{"location":"plugins/asr-dynapy-v4.html#confidences-vs-word-posteriors","text":"Some of the domains that will be delivered for this plugin will report DNN Confidence scores back as part of the output, as a sort of likelihood measure that the predicted word is correct. For other domains, these DNN confidence measures are not available, and Word Posterior scores are provided instead. Generally, the NN confidences are a more reliable score measure, but they require additional networks to be trained to compute, and the confidence networks are specific to the rest of the models within the domain and must be retrained each time. The five domains initially delivered with this plugin only report word posterior scores.","title":"Confidences vs Word Posteriors"},{"location":"plugins/asr-dynapy-v4.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points.","title":"Overlap"},{"location":"plugins/asr-dynapy-v4.html#resources","text":"The Russian domain, rus-tdnnChain-tel-v2, thanks to its large TDNN architecture, complex language model, and large vocabulary thanks to Russian's agglutinative, is currently tuned more for maximum accuracy performance than for speed or resource management. As a result, it currently has a rather high minimum memory requirement for execution, relative to other plugins. Roughly 9GB of free system memory is required as a baseline for performing recognition with this domain.","title":"Resources"},{"location":"plugins/asr-dynapy-v4.html#arabic-script-languages","text":"Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 58.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 51.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 100.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 99.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 99.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 73.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 50.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Arabic Script Languages"},{"location":"plugins/asr-dynapy-v4.html#non-verbal-recognizer-output","text":"Each recognizer's vocabulary may contain many non-verbal annotations that are valid potential word candidates that can show up in the transcription output. These include things like @reject@ or for words the recognizer cannot form a hypothesis for, and also includes notations for phenomena like hesitations or filled pauses. These may or may not be useful for a given user's task or use case, so it is currently left to the end user to decide how to process these non-verbal outputs/notations.","title":"Non-Verbal Recognizer Output"},{"location":"plugins/asr-dynapy-v4.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-dynapy-v4.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-dynapy-v4.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_asr_dynapy_v1_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Global Options"},{"location":"plugins/asr-end2end-v1.html","text":"asr-end2end-v1 (Automatic Speech Recognition) Version Changelog Plugin Version Change v1.0.0 Initial plugin release of the end-to-end models, tested and published with OLIVE 5.5.0 v1.0.1 Updated plugin, improved overall stability and performance, updated sample rate configuration to allow boosted performance when using 16 kHz audio, bug fixes and workflow-compatibility fixes. Description Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is the first release of SRI's end-to-end ASR plugin which uses a wav2vec v2.0 model for mapping the microphone samples to the letters/characters in the target language. Currently, this plugin supports 10 languages which overlaps with the language capabilities of the alternative asr-dynapy-v4 plugin. The output format is identical to the asr-dynapy output which is detailed below. This first version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance. Domains english-v1 (both 8k and 16k data) farsi-v1 french-v1 iraqiArabic-v1 levantineArabic-v1 mandarin-v1 pashto-v1 russian-v1 spanish-v1 ukrainian-v1 Inputs For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Outputs ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 AND 1.00000000 input-audio.wav 0.210 0.340 WE'RE 1.00000000 input-audio.wav 0.330 0.460 GOING 1.00000000 input-audio.wav 0.450 0.520 TO 1.00000000 input-audio.wav 0.510 0.940 FLY 1.00000000 input-audio.wav 1.080 1.300 WAS 1.00000000 input-audio.wav 1.290 1.390 THAT 1.00000000 input-audio.wav 1.290 1.390 IT 1.00000000 input-audio.wav 1.380 1.510 WE'RE 1.00000000 input-audio.wav 1.500 1.660 GOING 1.00000000 input-audio.wav 1.650 1.720 TO 1.00000000 input-audio.wav 1.710 1.930 FLY 1.00000000 input-audio.wav 1.920 2.110 OVER 1.00000000 input-audio.wav 2.100 2.380 SAINT 1.00000000 input-audio.wav 2.370 2.950 LOUIS 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 1.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 1.00000000 input-audio.wav 0.870 0.970 \u7684 1.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 1.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 1.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 1.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 1.00000000 input-audio.wav 3.130 3.340 \u7684 1.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 1.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 1.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest Compatibility OLIVE 5.5+ Limitations As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs. Language Dependence Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin. Overlap To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points. Arabic Script Languages Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order. Minimum Speech Duration The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file. Comments GPU Support This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Automatic Speech Recognition (ASR) end2end/GPU"},{"location":"plugins/asr-end2end-v1.html#asr-end2end-v1-automatic-speech-recognition","text":"","title":"asr-end2end-v1 (Automatic Speech Recognition)"},{"location":"plugins/asr-end2end-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release of the end-to-end models, tested and published with OLIVE 5.5.0 v1.0.1 Updated plugin, improved overall stability and performance, updated sample rate configuration to allow boosted performance when using 16 kHz audio, bug fixes and workflow-compatibility fixes.","title":"Version Changelog"},{"location":"plugins/asr-end2end-v1.html#description","text":"Automatic Speech Recognition plugins perform speech-to-text conversion of speech contained within a submitted audio segment to create a transcript of what is being said. Currently the outputs are based on word-level transcriptions. ASR plugins are not-performing any translation, but simply speech-to-text in the native language. All ASR domains are language-dependent, and each one is meant to work only with a single, specific language. This plugin is the first release of SRI's end-to-end ASR plugin which uses a wav2vec v2.0 model for mapping the microphone samples to the letters/characters in the target language. Currently, this plugin supports 10 languages which overlaps with the language capabilities of the alternative asr-dynapy-v4 plugin. The output format is identical to the asr-dynapy output which is detailed below. This first version does not provide a word-level confidence, which will be a future work, instead inserting a placeholder score of \"1.0\". The input speech is resampled to 16k and processed at this sampling frequency throughout the system. Each domain in the plugin is specific to a single language, and is only capable of transcribing speech in that one language. See below for the domains (and languages) available for this plugin, along with additional details regarding each. All domains are trained on a diverse set of training data covering conversational telephone, broadcast, read, distant speech (training data may vary per domain depending on availability), and are expected to perform well on a wide variety of acoustic conditions. The plugin will have a wider domain coverage on high-resourced domains, e.g. English, Mandarin, Spanish, on account of more diverse training data. Any input audio at a sample rate different than 16000 Hz will be resampled and processed as 16 kHz audio. Any higher-frequency (> 8 kHz) information will be discarded. All of the current domains are based on the wav2vec v2.0 model architecture. An n-gram language model is used during decoding to improve the ASR performance.","title":"Description"},{"location":"plugins/asr-end2end-v1.html#domains","text":"english-v1 (both 8k and 16k data) farsi-v1 french-v1 iraqiArabic-v1 levantineArabic-v1 mandarin-v1 pashto-v1 russian-v1 spanish-v1 ukrainian-v1","title":"Domains"},{"location":"plugins/asr-end2end-v1.html#inputs","text":"For scoring, an audio buffer or file. There is no verification performed by OLIVE or by ASR plugins that the audio passed as input is actually being spoken in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize.","title":"Inputs"},{"location":"plugins/asr-end2end-v1.html#outputs","text":"ASR plugins are region scorers, and as such will return a list of words, in the order they are spoken. Each detected word consists of timestamp regions in seconds (start and end time pairs), each with an accompanying placeholder score, along with the 'value' of that word. Each word output must be part of the vocabulary that specific language's domain was trained with. At this point, out-of-vocabulary words are not supported, so uncommon words, slang words, names, and some other vocabulary may not be able to be recognized by these plugins. If interested in this feature in the future, please contact us to start a conversation about adding such functionality. Note that all current ASR plugin domains will output words in their 'native' script. This means that for languages like English and Spanish, each word will be in ASCII text, with the Latin alphabet. Mandarin Chinese, Russian, and Farsi, however, words will be comprised of unicode characters in the native script. An example output excerpt for an English domain: input-audio.wav 0.000 0.190 AND 1.00000000 input-audio.wav 0.210 0.340 WE'RE 1.00000000 input-audio.wav 0.330 0.460 GOING 1.00000000 input-audio.wav 0.450 0.520 TO 1.00000000 input-audio.wav 0.510 0.940 FLY 1.00000000 input-audio.wav 1.080 1.300 WAS 1.00000000 input-audio.wav 1.290 1.390 THAT 1.00000000 input-audio.wav 1.290 1.390 IT 1.00000000 input-audio.wav 1.380 1.510 WE'RE 1.00000000 input-audio.wav 1.500 1.660 GOING 1.00000000 input-audio.wav 1.650 1.720 TO 1.00000000 input-audio.wav 1.710 1.930 FLY 1.00000000 input-audio.wav 1.920 2.110 OVER 1.00000000 input-audio.wav 2.100 2.380 SAINT 1.00000000 input-audio.wav 2.370 2.950 LOUIS 1.00000000 An example output excerpt for a Mandarin Chinese domain: input-audio.wav 0.280 0.610 \u6218\u6597 1.00000000 input-audio.wav 0.600 0.880 \u7206\u53d1 1.00000000 input-audio.wav 0.870 0.970 \u7684 1.00000000 input-audio.wav 0.960 1.420 \u5c45\u6c11\u533a 1.00000000 input-audio.wav 1.410 2.120 \u6709\u5f88\u591a 1.00000000 input-audio.wav 2.110 2.590 \u5fe0\u4e8e 1.00000000 input-audio.wav 2.580 3.140 \u8428\u5fb7\u5c14 1.00000000 input-audio.wav 3.130 3.340 \u7684 1.00000000 input-audio.wav 3.330 3.720 \u6b66\u88c5 1.00000000 input-audio.wav 3.710 4.190 \u4efd\u5b50 1.00000000 Note that for languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.00 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Outputs"},{"location":"plugins/asr-end2end-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/asr-end2end-v1.html#compatibility","text":"OLIVE 5.5+","title":"Compatibility"},{"location":"plugins/asr-end2end-v1.html#limitations","text":"As the debut ASR plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. ASR is language dependent and also largely audio domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system's acoustic model. The individual words that the plugin is capable of recognizing is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be recognized by a plugin out-of-the-box. Several factors contirbute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, ASR plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. Typically larger, more resource-hungry models are capable of achieving greater accuracy overall, but at the expense of longer runtimes/slower performance, and higher memory requirements. SRI is able to tune our domains to balance these constraints and performance based on customer needs.","title":"Limitations"},{"location":"plugins/asr-end2end-v1.html#language-dependence","text":"Each domain of this ASR plugin is language specific, and is only capable of transcribing speech in a single language. There is no filter or any sort of verification performed by OLIVE to ensure that the speech passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input audio if the source language is unknown. An OLIVE Language ID plugin could be used as a front-end to triage out-of-domain languages before passing audio to the ASR plugin.","title":"Language Dependence"},{"location":"plugins/asr-end2end-v1.html#overlap","text":"To keep memory usage of the plugin under control, incoming audio is segmented into smaller chunks for processing, preventing large audio files from overwhelming system RAM during processing. This allows the plugin performance to be less dependent on the length of input audio, but transcription errors near these break points become much more likely, especially if a word happens to be split in the process. Often when chunking audio like this, the audio is split into chunks that overlap slightly, to minimize the chance of a split word causing errors, and give the recognizer another chance to correctly identify the word or words being spoken near the break points. Typically this requires some sort of duplicated recognition resolution or merging logic, for reconciling differences in recognition output within the overlapped sections. This feature has not yet been added to the plugin, which will be addressed in a future release. Due to the lack of overlap/conflict resolution, this plugin is currently configured to have 0 overlap between consecutive segments, and there may be transcription errors resulting from these audio chunk break points.","title":"Overlap"},{"location":"plugins/asr-end2end-v1.html#arabic-script-languages","text":"Note that for the Farsi and Arabic domains, and for other languages that read from right to left, the direction that text is rendered may appear to 'flip' when viewing the bare text output in a terminal or text editor that doesn't properly deal with the orientation switch mid-line in some operating systems. This can cause the order of the 'word' and 'score' fields to reverse relative to the output of left-to-right read languages, and appear like this Farsi output example: input-audio.wav 0.000 0.480 1.00000000 \u062e\u0648\u0628 input-audio.wav 0.470 0.740 1.00000000 \u0627\u06cc input-audio2.wav 0.000 0.320 1.00000000 \u0622\u0631\u0647 input-audio2.wav 0.310 0.460 1.00000000 \u0645\u06cc input-audio2.wav 0.450 0.680 1.00000000 \u06af\u0645 input-audio2.wav 0.670 0.880 1.00000000 \u0686\u0646\u062f input-audio2.wav 0.870 1.330 1.00000000 \u062f\u0627\u0631\u0647 This is a rendering problem only, however, and rest assured that if interacting with OLIVE through the API, all ordering is properly preserved. Most methods of automatically parsing the raw text output should also properly deal with the ordering, as column-based operators like awk are not affected by the visual order.","title":"Arabic Script Languages"},{"location":"plugins/asr-end2end-v1.html#minimum-speech-duration","text":"The system will not attempt to perform speech recognition unless 0.31 seconds of speech of more is found in the file.","title":"Minimum Speech Duration"},{"location":"plugins/asr-end2end-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/asr-end2end-v1.html#gpu-support","text":"This plugin was designed and developed to run optimally on GPU hardware. It is capable of running on CPU in the absence of an available GPU or the proper configuration, but it will do so at a significantly reduced speed. Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/asr-end2end-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range sad_threshold Speech detection threshold: Higher value results in less speech being found and processed. 0.0 -4.0 to 4.0 unicode_normalization Enable or disable unicode normalization on the plugin output for Arabic languages. None None (no normalization), \"NFC\", \"NFD\", \"NFKC\", or \"NFKD\".","title":"Global Options"},{"location":"plugins/asr.html","text":"redirect: plugins/asr-dynapy-v3.md","title":"Asr"},{"location":"plugins/asr.html#redirect-pluginsasr-dynapy-v3md","text":"","title":"redirect: plugins/asr-dynapy-v3.md"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html","text":"dfa-speakerSpecific-phonetic-v1 (Deep Fake Audio Detection - Speaker Specific) Version Changelog Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.5.0 Description Deep fake speech generators often leave many acoustic and phonetic artifacts in the audio. Traditional synthetic speech detectors typically rely on deep learning models trained with acoustic features alone to classify whether a given speech sample comes from a human or from a synthetic generator. While \u201cdeep fakes\u201d often target known individuals, the common detection techniques do not leverage any phonetic or speaker-specific information in determining whether a sample was generated or real. This plugin integrates phonetic and speaker information into the model to counter spoofing attacks aimed at the most vulnerable individuals. Therefore, it will detect if an audio is a bonafide file or if the audio is a deepfake representation of a specific enrolled speaker (the Speaker Of Interest). Like Speaker Identification (SID), DFA-SpeakerSpecific-Phonetic needs information from the Speaker Of Interest, in the form of the creation of a speaker specific enrollment. In this version, the number of speakers that we are analyzing for each scoring query is limited to one. Finally, the plugin allows the user to adapt the model towards the Speaker Of Interest if there is enough enrollment data for that specific speaker. Currently, the minimum number of utterances enrolled into the system to perform the optional Speaker of Interest adaptation is three. Domains multicondition-16k-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech and can handle audio with 16kHz sampling rate or higher. Downsamples all input to 16kHz. multicondition-8k-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech and can handle audio with 8kHz sampling rate or higher. Downsamples all input to 8kHz. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. Multiple speakers and multiple utterances of the same speaker can be enrolled at the same time. For scoring, an audio buffer or file to evaluate, and a label indicating the speaker-of-interest (enrolled speaker) to compare with. One and only one speaker-of-interest class must be provided - if there are more than one speaker in the list at score time, it will stop processing and provide an error message. Outputs The dfa-speakerSpecific-phonetic plugin returns a list of files with a score for the enrolled speaker. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a bonafide file and a score below \"0\" is considered a deep-fake audio. The plugin uses a global score calibration by default. If desired, the user can choose to instead enable the enable_soi_adaptation option (described below) to adapt the score calibration from speaker-of-interest specific information, as long as there are three or more enrollments from the target speaker. Example output: My_journey_from_Marine_to_actor___Adam_Driver-nCwwVjPNloY_spk0_30sec_006.wav Adam 0.73312998 My_journey_from_Marine_to_actor___Adam_Driver-nCwwVjPNloY_spk0_30sec_012_8k.wav Adam 0.92221069 LAI_VoiceJ_Kylo_Explains_Star_Wars-_ZZlYYC24LY_spk0.wav Adam -0.44476557 LAI_VoiceJ_Kylo_Ren_in_Star_Trek-T-yyabjI_RE_spk0.wav Adam -1.08671188 Enrollments Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 30 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Note that at least 3 audio samples are needed to perform the model adaptation for the speaker-of-interest. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.5+ Limitations Known or potential limitations of the plugin are outlined below. Speaker ID functionality Unlike SID, this plugin has not been designed to identify speakers and, therefore, scoring candidate utterances against models from obviously different speakers can provide unexpected or undesired results. Impersonators This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from profesional impersonators are not within the scope of this plugin. Processing Speed and Memory Adaptation is computationally expensive and it requires more resources than global calibration. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 1 second by default). Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 threshold An offset applied to scores to allow 0.0 to be a tuned decision point. Higher threshold values will lower output scores, and result in less audio labeled \"fake\" 1.5 0.0 - 10.0 enable_soi_adaptation Speaker Of Interest calibration adaptation. If True, the plugin adapts the model using enrolled data of the speaker-of-interest False True or False","title":"Deep Fake Audio Detection Speaker Specific Phonetic (DFA)"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#dfa-speakerspecific-phonetic-v1-deep-fake-audio-detection-speaker-specific","text":"","title":"dfa-speakerSpecific-phonetic-v1 (Deep Fake Audio Detection - Speaker Specific)"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.5.0","title":"Version Changelog"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#description","text":"Deep fake speech generators often leave many acoustic and phonetic artifacts in the audio. Traditional synthetic speech detectors typically rely on deep learning models trained with acoustic features alone to classify whether a given speech sample comes from a human or from a synthetic generator. While \u201cdeep fakes\u201d often target known individuals, the common detection techniques do not leverage any phonetic or speaker-specific information in determining whether a sample was generated or real. This plugin integrates phonetic and speaker information into the model to counter spoofing attacks aimed at the most vulnerable individuals. Therefore, it will detect if an audio is a bonafide file or if the audio is a deepfake representation of a specific enrolled speaker (the Speaker Of Interest). Like Speaker Identification (SID), DFA-SpeakerSpecific-Phonetic needs information from the Speaker Of Interest, in the form of the creation of a speaker specific enrollment. In this version, the number of speakers that we are analyzing for each scoring query is limited to one. Finally, the plugin allows the user to adapt the model towards the Speaker Of Interest if there is enough enrollment data for that specific speaker. Currently, the minimum number of utterances enrolled into the system to perform the optional Speaker of Interest adaptation is three.","title":"Description"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#domains","text":"multicondition-16k-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech and can handle audio with 16kHz sampling rate or higher. Downsamples all input to 16kHz. multicondition-8k-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech and can handle audio with 8kHz sampling rate or higher. Downsamples all input to 8kHz.","title":"Domains"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. Multiple speakers and multiple utterances of the same speaker can be enrolled at the same time. For scoring, an audio buffer or file to evaluate, and a label indicating the speaker-of-interest (enrolled speaker) to compare with. One and only one speaker-of-interest class must be provided - if there are more than one speaker in the list at score time, it will stop processing and provide an error message.","title":"Inputs"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#outputs","text":"The dfa-speakerSpecific-phonetic plugin returns a list of files with a score for the enrolled speaker. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a bonafide file and a score below \"0\" is considered a deep-fake audio. The plugin uses a global score calibration by default. If desired, the user can choose to instead enable the enable_soi_adaptation option (described below) to adapt the score calibration from speaker-of-interest specific information, as long as there are three or more enrollments from the target speaker. Example output: My_journey_from_Marine_to_actor___Adam_Driver-nCwwVjPNloY_spk0_30sec_006.wav Adam 0.73312998 My_journey_from_Marine_to_actor___Adam_Driver-nCwwVjPNloY_spk0_30sec_012_8k.wav Adam 0.92221069 LAI_VoiceJ_Kylo_Explains_Star_Wars-_ZZlYYC24LY_spk0.wav Adam -0.44476557 LAI_VoiceJ_Kylo_Ren_in_Star_Trek-T-yyabjI_RE_spk0.wav Adam -1.08671188","title":"Outputs"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#enrollments","text":"Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 30 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Note that at least 3 audio samples are needed to perform the model adaptation for the speaker-of-interest.","title":"Enrollments"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#compatibility","text":"OLIVE 5.5+","title":"Compatibility"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#speaker-id-functionality","text":"Unlike SID, this plugin has not been designed to identify speakers and, therefore, scoring candidate utterances against models from obviously different speakers can provide unexpected or undesired results.","title":"Speaker ID functionality"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#impersonators","text":"This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from profesional impersonators are not within the scope of this plugin.","title":"Impersonators"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#processing-speed-and-memory","text":"Adaptation is computationally expensive and it requires more resources than global calibration.","title":"Processing Speed and Memory"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 1 second by default).","title":"Minimum Speech Duration"},{"location":"plugins/dfa-speakerSpecific-phonetic-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 threshold An offset applied to scores to allow 0.0 to be a tuned decision point. Higher threshold values will lower output scores, and result in less audio labeled \"fake\" 1.5 0.0 - 10.0 enable_soi_adaptation Speaker Of Interest calibration adaptation. If True, the plugin adapts the model using enrolled data of the speaker-of-interest False True or False","title":"Global Options"},{"location":"plugins/dfa-speakerSpecific-v1.html","text":"dfa-speakerSpecific-v1 (Deep Fake Audio Detection - Speaker Specific) Version Changelog Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.3.0 Description Common \u201cdeep fake\u201d detection techniques apply general approaches based on the detection of synthetic speech or audio artifacts. While \u201cdeep fakes\u201d very often target known individuals, the common detection techniques do not leverage the actual known-samples of the target\u2019s speech, (also called the speaker of interest) in determining whether a sample was generated or real. This is a very valuable piece of information that can be used to assist in detection deep-fake speech from an individual, as well as in building an effective system for the general population. This plugin for DeepFake Audio Detection incorporates information from the speaker-of-interest into the model to avoid specific attacks for certain vulnerable people. Therefore, it will detect if an audio is a bonafide file or if the audio is a deepfake representation of a specific enrolled speaker (the Speaker Of Interest). Like Speaker Identification (SID), DFA-SpeakerSpecific needs information from the Speaker Of Interest, in the form of the creation of a speaker specific enrollment. In this version, the number of speakers that we are analyzing for each scoring query is limited to one. Finally, the plugin allows the user to adapt the model towards the Speaker Of Interest if there is enough enrollment data for that specific speaker. Currently, the minimum number of utterances enrolled into the system to perform the adaptation is four. Domains multicondition-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. Multiple speakers and multiple utterances of the same speaker can be enrolled at the same time. For scoring, an audio buffer or file to evaluate, and a label indicating the speaker-of-interest (enrolled speaker) to compare with. One and only one speaker-of-interest class must be provided - if there are more than one speaker in the list at score time, it will stop processing and provide an error message. Outputs The DFA-speakerSpecific plugin returns a list of files with a score for the enrolled speaker. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a bonafide file and a score below \"0\" is considered a deep-fake audio. The DFA-speakerSpecific plugin adapts the score calibration by default is there are enough utterances of the enrolled speaker. Otherwise, the user can choose to use a global score calibration by disabling the enable_soi_adaptation option (described below) or add more data of the speaker-of-interest. Example output: Airplane-imTrEFnrVCs_spk0.wav JohnTravolta -240.64154053 Car-ajtyqj81b6E_spk0.wav JohnTravolta -135.29077148 Texas-7n1qnUOz4Uk_spk0_30sec_026.wav JohnTravolta 78.67844391 Texas-7n1qnUOz4Uk_spk0_30sec_011.wav JohnTravolta 97.31453705 Enrollments Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 30 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Note that at least 4 audio samples are needed to perform the model adaptation for the speaker-of-interest. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.3+ Limitations Known or potential limitations of the plugin are outlined below. Speaker ID functionality Unlike SID, this plugin has not been designed to identify speakers and, therefore, scoring candidate utterances against models from obviously different speakers can provide unexpected or undesired results. Impersonators This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from profesional impersonators are not within the scope of this plugin. Processing Speed and Memory Adaptation is computational expensive and it requires more resources than global calibration. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 threshold Threshold to determine if audio will be labeled as 'real' or 'fake'. Higher value results in more audio labeled as fake. 3.0 0.0 - 10.0 enable_soi_adaptation Speaker Of Interest calibration adaptation. If True, the plugin adapts the model using enrolled data of the speaker-of-interest True True False","title":"dfa-speakerSpecific-v1 (Deep Fake Audio Detection - Speaker Specific)"},{"location":"plugins/dfa-speakerSpecific-v1.html#dfa-speakerspecific-v1-deep-fake-audio-detection-speaker-specific","text":"","title":"dfa-speakerSpecific-v1 (Deep Fake Audio Detection - Speaker Specific)"},{"location":"plugins/dfa-speakerSpecific-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release. Compatible with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/dfa-speakerSpecific-v1.html#description","text":"Common \u201cdeep fake\u201d detection techniques apply general approaches based on the detection of synthetic speech or audio artifacts. While \u201cdeep fakes\u201d very often target known individuals, the common detection techniques do not leverage the actual known-samples of the target\u2019s speech, (also called the speaker of interest) in determining whether a sample was generated or real. This is a very valuable piece of information that can be used to assist in detection deep-fake speech from an individual, as well as in building an effective system for the general population. This plugin for DeepFake Audio Detection incorporates information from the speaker-of-interest into the model to avoid specific attacks for certain vulnerable people. Therefore, it will detect if an audio is a bonafide file or if the audio is a deepfake representation of a specific enrolled speaker (the Speaker Of Interest). Like Speaker Identification (SID), DFA-SpeakerSpecific needs information from the Speaker Of Interest, in the form of the creation of a speaker specific enrollment. In this version, the number of speakers that we are analyzing for each scoring query is limited to one. Finally, the plugin allows the user to adapt the model towards the Speaker Of Interest if there is enough enrollment data for that specific speaker. Currently, the minimum number of utterances enrolled into the system to perform the adaptation is four.","title":"Description"},{"location":"plugins/dfa-speakerSpecific-v1.html#domains","text":"multicondition-v1 Domain that has been trained with multiple deep fake approaches like voice conversion, or synthetic speech.","title":"Domains"},{"location":"plugins/dfa-speakerSpecific-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. Multiple speakers and multiple utterances of the same speaker can be enrolled at the same time. For scoring, an audio buffer or file to evaluate, and a label indicating the speaker-of-interest (enrolled speaker) to compare with. One and only one speaker-of-interest class must be provided - if there are more than one speaker in the list at score time, it will stop processing and provide an error message.","title":"Inputs"},{"location":"plugins/dfa-speakerSpecific-v1.html#outputs","text":"The DFA-speakerSpecific plugin returns a list of files with a score for the enrolled speaker. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a bonafide file and a score below \"0\" is considered a deep-fake audio. The DFA-speakerSpecific plugin adapts the score calibration by default is there are enough utterances of the enrolled speaker. Otherwise, the user can choose to use a global score calibration by disabling the enable_soi_adaptation option (described below) or add more data of the speaker-of-interest. Example output: Airplane-imTrEFnrVCs_spk0.wav JohnTravolta -240.64154053 Car-ajtyqj81b6E_spk0.wav JohnTravolta -135.29077148 Texas-7n1qnUOz4Uk_spk0_30sec_026.wav JohnTravolta 78.67844391 Texas-7n1qnUOz4Uk_spk0_30sec_011.wav JohnTravolta 97.31453705","title":"Outputs"},{"location":"plugins/dfa-speakerSpecific-v1.html#enrollments","text":"Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 30 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Note that at least 4 audio samples are needed to perform the model adaptation for the speaker-of-interest.","title":"Enrollments"},{"location":"plugins/dfa-speakerSpecific-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/dfa-speakerSpecific-v1.html#compatibility","text":"OLIVE 5.3+","title":"Compatibility"},{"location":"plugins/dfa-speakerSpecific-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/dfa-speakerSpecific-v1.html#speaker-id-functionality","text":"Unlike SID, this plugin has not been designed to identify speakers and, therefore, scoring candidate utterances against models from obviously different speakers can provide unexpected or undesired results.","title":"Speaker ID functionality"},{"location":"plugins/dfa-speakerSpecific-v1.html#impersonators","text":"This plugin has been trained with deep-fake data and the model detects vocoders and synthetic speech. Impressions from profesional impersonators are not within the scope of this plugin.","title":"Impersonators"},{"location":"plugins/dfa-speakerSpecific-v1.html#processing-speed-and-memory","text":"Adaptation is computational expensive and it requires more resources than global calibration.","title":"Processing Speed and Memory"},{"location":"plugins/dfa-speakerSpecific-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/dfa-speakerSpecific-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 1.0 0.5 - 4.0 sad_threshold Threshold for the Speech Activity Detector. The higher the threshold is, less speech will be selected 1.0 0.0 - 3.0 threshold Threshold to determine if audio will be labeled as 'real' or 'fake'. Higher value results in more audio labeled as fake. 3.0 0.0 - 10.0 enable_soi_adaptation Speaker Of Interest calibration adaptation. If True, the plugin adapts the model using enrolled data of the speaker-of-interest True True False","title":"Global Options"},{"location":"plugins/dfa-spoofnet-v1.html","text":"dfa-spoofnet-v1 (Deep Fake Audio Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.3.0 Description Deepfake audio plugins classify whether a given speech sample comes from a human or from a synthetic generator. A speech sample from a human is labeled as \u201cnatural\u201d while generated speech is labeled as \u201csynthetic\u201d. There are two broad classes of speech generators: Text-to-Speech (TTS) systems and Voice Conversion (VC) systems. In TTS systems, a generator takes a text string as input and returns the target raw audio. In VC systems, a non-target speech recording is manipulated until it matches the speech of a given target. The DeepFake audio plugins differentiate between genuine human speech and samples created by TTS and VC systems. This plugin detects whether a given audio sample came from a person or from a synthetic system. The plugin uses a Convolutional Neural Network (CNN) trained on Linear Frequency Filterbank (LFB) features. The LFB features expose the network to the complete frequency range to reveal potential artifacts left by a synthetic generator. This is in contrast to the MFCC features used in most OLIVE tasks that focus more on the frequency range of human speech. The network produces an embedding which is fed into a calibrated PLDA backend to score the audio sample as either \u201cnatural\u201d (from a person) or \u201csynthetic\u201d (from a synthetic system). Domains multi-v1 A generic domain where the system was developed based on its performance on an unseen set of speech generators. Both the type of speech generators and their audio conditions were unseen, representing the worst-case scenario of encountering a completely novel synthetic generator in the field. Inputs Audio file or buffer and an optional identifier. Outputs The scores represent whether a given audio is \"natural\" (from a real person) or \"synthetic\". The score itself comes from the learned embedding in a CNN network. The embedding is passed through a calibrated PLDA backend which outputs the log-likelihood ratios (as commonly seen in other OLIVE plugins) for the two classes: natural and synthetic. The threshold in the configuration file determines the final classification. For example, if the threshold is equal to 0, then positive scores are classified as real while negative scores are classified as fake. An example output excerpt: input-audio.wav synthetic -2.81980443 input-audio.wav natural 0.94153970 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest Compatibility OLIVE 5.3+ Limitations Known or potential limitations of the plugin are outlined below. Minimum Audio Duration Audio samples are assumed to be at least 3 seconds long. Any segment shorter than 3 second is extended with \u201crepeat\u201d padding, where the waveform is repeated as many times as is necessary to reach at least 3 seconds. If the audio is longer than 3 seconds, the plugin steps through the waveform with 3-second windows using a step size determined in the config file. In this case the final output is the averaged score across all of the windows. Types of Speech Generators The plugin performs best on TTS generators that are based on Neural Networks. The plugin has more difficulty with Voice Conversion generators that use lower level, waveform-specific manipulations. Comments Detecting fake speech generators is a cat-and-mouse game. The field is (as of writing) moving incredibly fast with new advances and models constantly released. We strove to develop a system that is robust to unseen generators but cannot guarantee that we captured the universe of in-the-wild deepfake audio generators. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.3 0.3 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"Deep Fake Audio Detection (DFA)"},{"location":"plugins/dfa-spoofnet-v1.html#dfa-spoofnet-v1-deep-fake-audio-detection","text":"","title":"dfa-spoofnet-v1 (Deep Fake Audio Detection)"},{"location":"plugins/dfa-spoofnet-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/dfa-spoofnet-v1.html#description","text":"Deepfake audio plugins classify whether a given speech sample comes from a human or from a synthetic generator. A speech sample from a human is labeled as \u201cnatural\u201d while generated speech is labeled as \u201csynthetic\u201d. There are two broad classes of speech generators: Text-to-Speech (TTS) systems and Voice Conversion (VC) systems. In TTS systems, a generator takes a text string as input and returns the target raw audio. In VC systems, a non-target speech recording is manipulated until it matches the speech of a given target. The DeepFake audio plugins differentiate between genuine human speech and samples created by TTS and VC systems. This plugin detects whether a given audio sample came from a person or from a synthetic system. The plugin uses a Convolutional Neural Network (CNN) trained on Linear Frequency Filterbank (LFB) features. The LFB features expose the network to the complete frequency range to reveal potential artifacts left by a synthetic generator. This is in contrast to the MFCC features used in most OLIVE tasks that focus more on the frequency range of human speech. The network produces an embedding which is fed into a calibrated PLDA backend to score the audio sample as either \u201cnatural\u201d (from a person) or \u201csynthetic\u201d (from a synthetic system).","title":"Description"},{"location":"plugins/dfa-spoofnet-v1.html#domains","text":"multi-v1 A generic domain where the system was developed based on its performance on an unseen set of speech generators. Both the type of speech generators and their audio conditions were unseen, representing the worst-case scenario of encountering a completely novel synthetic generator in the field.","title":"Domains"},{"location":"plugins/dfa-spoofnet-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/dfa-spoofnet-v1.html#outputs","text":"The scores represent whether a given audio is \"natural\" (from a real person) or \"synthetic\". The score itself comes from the learned embedding in a CNN network. The embedding is passed through a calibrated PLDA backend which outputs the log-likelihood ratios (as commonly seen in other OLIVE plugins) for the two classes: natural and synthetic. The threshold in the configuration file determines the final classification. For example, if the threshold is equal to 0, then positive scores are classified as real while negative scores are classified as fake. An example output excerpt: input-audio.wav synthetic -2.81980443 input-audio.wav natural 0.94153970","title":"Outputs"},{"location":"plugins/dfa-spoofnet-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/dfa-spoofnet-v1.html#compatibility","text":"OLIVE 5.3+","title":"Compatibility"},{"location":"plugins/dfa-spoofnet-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/dfa-spoofnet-v1.html#minimum-audio-duration","text":"Audio samples are assumed to be at least 3 seconds long. Any segment shorter than 3 second is extended with \u201crepeat\u201d padding, where the waveform is repeated as many times as is necessary to reach at least 3 seconds. If the audio is longer than 3 seconds, the plugin steps through the waveform with 3-second windows using a step size determined in the config file. In this case the final output is the averaged score across all of the windows.","title":"Minimum Audio Duration"},{"location":"plugins/dfa-spoofnet-v1.html#types-of-speech-generators","text":"The plugin performs best on TTS generators that are based on Neural Networks. The plugin has more difficulty with Voice Conversion generators that use lower level, waveform-specific manipulations.","title":"Types of Speech Generators"},{"location":"plugins/dfa-spoofnet-v1.html#comments","text":"Detecting fake speech generators is a cat-and-mouse game. The field is (as of writing) moving incredibly fast with new advances and models constantly released. We strove to develop a system that is robust to unseen generators but cannot guarantee that we captured the universe of in-the-wild deepfake audio generators.","title":"Comments"},{"location":"plugins/dfa-spoofnet-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.3 0.3 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/dfa.html","text":"redirect: plugins/dfa-spoofnet-v1.md","title":"Dfa"},{"location":"plugins/dfa.html#redirect-pluginsdfa-spoofnet-v1md","text":"","title":"redirect: plugins/dfa-spoofnet-v1.md"},{"location":"plugins/dia-hybrid-v2.html","text":"dia-hybrid-v2 (Speaker Diarization) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1, but updated to be compatible with 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Speaker Diarization plugins segment the submitted audio to determine 'who spoke when.' In constrast to Speaker Detection (SDD) plugins, there is no enrollment functionality or any concept of 'speakers of interest' or 'target speakers.' Instead, speaker clusters are defined automatically with clas names such as 'spk1', 'spk2', etc. This plugin is based on Variational Bayes Diarization in an i-vector space defined by SRI\u2019s hybrid alignment framework, which is powered by deep neural network bottleneck features. Domains multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Inputs An audio file or buffer to be scored. Outputs In the basic case, Diarization returns a list of regions labelled with \u201cspkN\u201d. \u2018N\u2019 is an integer denoting an unknown speaker, and the maximum N is the total number of unknown speakers in the file. All regions of speaker N are deemed to be spoken by the same speaker. Regions are represented in seconds. The 'score' field is reported as -100.0; this is not a confidence value, but a placeholder to maintain output format compatibility with other region-scoring OLIVE plugins. input-audio.wav 0.470 8.210 spk3 1.0 input-audio.wav 8.320 13.110 spk4 1.0 input-audio.wav 13.280 29.960 spk3 1.0 input-audio.wav 30.350 32.030 spk3 1.0 input-audio.wav 32.310 46.980 spk1 1.0 input-audio.wav 47.790 51.120 spk2 1.0 input-audio.wav 51.360 54.290 spk3 1.0 input-audio.wav 54.340 55.400 spk2 1.0 input-audio.wav 55.550 58.790 spk2 1.0 input-audio.wav 58.820 76.340 spk1 1.0 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations Diarization plugins perform their clustering blind of any specific speaker knowledge. These plugins have no concept of individual speakers of interest or enrolled speaker models. Therefore, the only labels that will come back from the plugin will be 'spk1', 'spk2', and so on, corresponding to proposed individual speakers within the audio. If your target use case involves searching for a known individual speaker or speaker(s), consider using a Speaker Detection (SDD) or Speaker Identification (SID) plugin instead. Speed and Memory Usage The current approach to diarization is exceptionally slow (close to real-time). Work is on-going to replace the Variational Bayes Diarization approach with a segmentation by classification approach that will combine segmentation and speaker detection into a single stage and improve system speed. Speaker Persistence and Labeling Across Files The definition of spkN for one processing instance is not retained for use in other processing instances. For instance, spk1 in fileA is not necessarily the same as spk1 in fileB. If speaker persistence and inter-file label consistency is required, please consider Speaker Detection (SDD) technology instead. Minimum Speech Duration The system will only attempt to perform speaker diarization on submitted audio if there is more than 5 seconds of total speech detected in the file or buffer. In addition, only segments that are 2 seconds or longer will be considered for clustering and given a diarization score and proposed speaker label. Maximum number of speakers Currently, the plugin is configured to differentiate a maximum of 6 distinct/unique speakers within any scored audio. If you need to distinguish between a larger set of unknown speakers within files, or if 6 speakers is more than you ever expect to see for your given audio conditions, contact SRI for ways that we can maximize performance for your use case. Comments Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Speaker Diarization (DIA)"},{"location":"plugins/dia-hybrid-v2.html#dia-hybrid-v2-speaker-diarization","text":"","title":"dia-hybrid-v2 (Speaker Diarization)"},{"location":"plugins/dia-hybrid-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1, but updated to be compatible with 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/dia-hybrid-v2.html#description","text":"Speaker Diarization plugins segment the submitted audio to determine 'who spoke when.' In constrast to Speaker Detection (SDD) plugins, there is no enrollment functionality or any concept of 'speakers of interest' or 'target speakers.' Instead, speaker clusters are defined automatically with clas names such as 'spk1', 'spk2', etc. This plugin is based on Variational Bayes Diarization in an i-vector space defined by SRI\u2019s hybrid alignment framework, which is powered by deep neural network bottleneck features.","title":"Description"},{"location":"plugins/dia-hybrid-v2.html#domains","text":"multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/dia-hybrid-v2.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/dia-hybrid-v2.html#outputs","text":"In the basic case, Diarization returns a list of regions labelled with \u201cspkN\u201d. \u2018N\u2019 is an integer denoting an unknown speaker, and the maximum N is the total number of unknown speakers in the file. All regions of speaker N are deemed to be spoken by the same speaker. Regions are represented in seconds. The 'score' field is reported as -100.0; this is not a confidence value, but a placeholder to maintain output format compatibility with other region-scoring OLIVE plugins. input-audio.wav 0.470 8.210 spk3 1.0 input-audio.wav 8.320 13.110 spk4 1.0 input-audio.wav 13.280 29.960 spk3 1.0 input-audio.wav 30.350 32.030 spk3 1.0 input-audio.wav 32.310 46.980 spk1 1.0 input-audio.wav 47.790 51.120 spk2 1.0 input-audio.wav 51.360 54.290 spk3 1.0 input-audio.wav 54.340 55.400 spk2 1.0 input-audio.wav 55.550 58.790 spk2 1.0 input-audio.wav 58.820 76.340 spk1 1.0","title":"Outputs"},{"location":"plugins/dia-hybrid-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/dia-hybrid-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/dia-hybrid-v2.html#limitations","text":"Diarization plugins perform their clustering blind of any specific speaker knowledge. These plugins have no concept of individual speakers of interest or enrolled speaker models. Therefore, the only labels that will come back from the plugin will be 'spk1', 'spk2', and so on, corresponding to proposed individual speakers within the audio. If your target use case involves searching for a known individual speaker or speaker(s), consider using a Speaker Detection (SDD) or Speaker Identification (SID) plugin instead.","title":"Limitations"},{"location":"plugins/dia-hybrid-v2.html#speed-and-memory-usage","text":"The current approach to diarization is exceptionally slow (close to real-time). Work is on-going to replace the Variational Bayes Diarization approach with a segmentation by classification approach that will combine segmentation and speaker detection into a single stage and improve system speed.","title":"Speed and Memory Usage"},{"location":"plugins/dia-hybrid-v2.html#speaker-persistence-and-labeling-across-files","text":"The definition of spkN for one processing instance is not retained for use in other processing instances. For instance, spk1 in fileA is not necessarily the same as spk1 in fileB. If speaker persistence and inter-file label consistency is required, please consider Speaker Detection (SDD) technology instead.","title":"Speaker Persistence and Labeling Across Files"},{"location":"plugins/dia-hybrid-v2.html#minimum-speech-duration","text":"The system will only attempt to perform speaker diarization on submitted audio if there is more than 5 seconds of total speech detected in the file or buffer. In addition, only segments that are 2 seconds or longer will be considered for clustering and given a diarization score and proposed speaker label.","title":"Minimum Speech Duration"},{"location":"plugins/dia-hybrid-v2.html#maximum-number-of-speakers","text":"Currently, the plugin is configured to differentiate a maximum of 6 distinct/unique speakers within any scored audio. If you need to distinguish between a larger set of unknown speakers within files, or if 6 speakers is more than you ever expect to see for your given audio conditions, contact SRI for ways that we can maximize performance for your use case.","title":"Maximum number of speakers"},{"location":"plugins/dia-hybrid-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/dia-hybrid-v2.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/dia.html","text":"redirect: plugins/dia-hybrid-v2.md","title":"Dia"},{"location":"plugins/dia.html#redirect-pluginsdia-hybrid-v2md","text":"","title":"redirect: plugins/dia-hybrid-v2.md"},{"location":"plugins/enh.html","text":"redirect: plugins/enh-mmse-v1.md","title":"Enh"},{"location":"plugins/enh.html#redirect-pluginsenh-mmse-v1md","text":"","title":"redirect: plugins/enh-mmse-v1.md"},{"location":"plugins/env-indoorOutdoor-v1.html","text":"env-indoorOutdoor-v1.0.0 (Environmental Analysis - Indoor vs. Outdoor) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description The goal of indoor-outdoor acoustic environment detection capability is to determine if a given audio segment was collected indoors or outdoors. This plug-in provides a global score for the entire segment. The plugin is based on sound embeddings with a CPLDA backend and binary calibration. This is the first release of indoor-outdoor acoustic environment detection plugin. This plugin provides a global score indicating whether a given audio segment was collected indoors or outdoors. This capability is based on sound embeddings powered by CPLDA backend and binary calibration Domains default-v1 Default domain meant for general-purpose audio conditions. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs Returns score for the detected sound class based on the entire wave form passed in. Scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. If none of the scores crosses the threshold, it will be deemed as no-decision and there will be no output. The list of results has a file name (or in the case of a buffer, an identifier), class label, a score for each detected sound class. An example output excerpt: Input-audio-1.wav INDOOR 1.2345 Input-audio-2.wav OUTDOOR 2.003 Input-audio-3.wav OUTDOOR 0.5643 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single label for the entire audio. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations Minimum Audio Length A minimum duration of 1 second audio file is required in order to produce any meaningful detection. This plugin does not allow any class modification, so users may not alter the behavior by adding data through adaptation of class augmentation. Global Options Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false detection segments. Reduce the threshold value if there are too many missed segments. 0.0 -5.0 to 5.0","title":"env-indoorOutdoor-v1.0.0 (Environmental Analysis - Indoor vs. Outdoor)"},{"location":"plugins/env-indoorOutdoor-v1.html#env-indooroutdoor-v100-environmental-analysis-indoor-vs-outdoor","text":"","title":"env-indoorOutdoor-v1.0.0 (Environmental Analysis - Indoor vs. Outdoor)"},{"location":"plugins/env-indoorOutdoor-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/env-indoorOutdoor-v1.html#description","text":"The goal of indoor-outdoor acoustic environment detection capability is to determine if a given audio segment was collected indoors or outdoors. This plug-in provides a global score for the entire segment. The plugin is based on sound embeddings with a CPLDA backend and binary calibration. This is the first release of indoor-outdoor acoustic environment detection plugin. This plugin provides a global score indicating whether a given audio segment was collected indoors or outdoors. This capability is based on sound embeddings powered by CPLDA backend and binary calibration","title":"Description"},{"location":"plugins/env-indoorOutdoor-v1.html#domains","text":"default-v1 Default domain meant for general-purpose audio conditions.","title":"Domains"},{"location":"plugins/env-indoorOutdoor-v1.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/env-indoorOutdoor-v1.html#outputs","text":"Returns score for the detected sound class based on the entire wave form passed in. Scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. If none of the scores crosses the threshold, it will be deemed as no-decision and there will be no output. The list of results has a file name (or in the case of a buffer, an identifier), class label, a score for each detected sound class. An example output excerpt: Input-audio-1.wav INDOOR 1.2345 Input-audio-2.wav OUTDOOR 2.003 Input-audio-3.wav OUTDOOR 0.5643","title":"Outputs"},{"location":"plugins/env-indoorOutdoor-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single label for the entire audio. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/env-indoorOutdoor-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/env-indoorOutdoor-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/env-indoorOutdoor-v1.html#minimum-audio-length","text":"A minimum duration of 1 second audio file is required in order to produce any meaningful detection. This plugin does not allow any class modification, so users may not alter the behavior by adding data through adaptation of class augmentation.","title":"Minimum Audio Length"},{"location":"plugins/env-indoorOutdoor-v1.html#global-options","text":"Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false detection segments. Reduce the threshold value if there are too many missed segments. 0.0 -5.0 to 5.0","title":"Global Options"},{"location":"plugins/env-multiClass-v2.html","text":"env-multiClass-v2 (Environmental Analysis) Version Changelog Plugin Version Change v2.0.0 Initial plugin release with OLIVE 5.1.0, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.1.0 Description ENV plugins are designed to analyze submitted audio to glean information from the audio. Often not necessarily meant for detecting specific events, languages, or speakers, these plugins are instead pulling information about the environment or conditions of the audio recording, like the noise type(s) and level, codec, background conditions, reverberation, or other such characteristics. The specific classes that are reported will vary from plugin-to-plugin, so for more information regarding these, refer below. This plugin extracts information regarding the acoustic conditions and content of the audio file being analyzed, such as language, codec, SNR levels, etc. This plugin features: Model Ensemble The plugin leverages 7 systems for extracting 40 subclasses of information related to Language, Codec, SNR, Emotion, Reverb, Channel and Gender. Scaled Scores To faciltate interpretation, all scores for a given class (out of 7) are scaled to sum to 1.0. An exception is for Language due to the scores being LLRs and accounting for unknown languages in the model. Supported Classes The table below shows each class and the associated subclasses that this plugin reports scores for. Class Subclasses Channel ac (Acoustic projection via a mobile phone recording of a speakerphone voice output mic (Microphone) tel (Telephone) Codec aac amrnb clean (Uncompressed) g723_1 g726 gsm ilbc mp3 opus real144 speex vorbis Emotion A (Aroused) H (Happy) N (Neutral) Gender f (Female) m (Male) Language amh (Amharic) ara (Arabic) cmn (Chinese Mandarin) eng (English) fas (Farsi) fre (French) hau (Hausa) jpn (Japanese) kor (Korean) pus (Pushto) rus (Russian) spa (Spanish) tur (Turkish) urd (Urdu) vie (Vietnamese) Reverb nml (Limited to no reverberation) rev (Moderate to high levels of reverberation) SNR clean (SNR better than 30dB) snr20db (SNR of approximately 20dB) snr08db (SNR of 8dB or lower) Domains multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. Inputs Audio file or buffer and an optional identifier. Outputs Generally, a list of scores for all classes in the domain, for the entire segment. Unlike with SAD and SID, where scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection, most of the classes within an ENV plugin are set up differently. Some ENV plugins, like this one will have a 'parent' superclass that has several subclasses within it. For example, the Gender superclass has subclasses of m (male) and f (female); the Channel superclass may have subclasses of mic (microphone), tel (telephone), and ac (acoustic coupling). When the plugin reports its results, these classes are reported as: Gender-f <score> Gender-m <score> and Channel-ac <score> Channel-mic <score> Channel-tel <score> respectively. Note that within many of these superclasses, the scores are scaled so that the sum of all subclasses within the superclass will sum to one, to make the scores more legible and easier to interpret. In these cases, the highest score represents the most likely subclass for that category. An example output excerpt: input-audio.wav Channel-ac 0.0 input-audio.wav Channel-mic 1.0 input-audio.wav Channel-tel 0.0 input-audio.wav Codec-aac 0.0 input-audio.wav Codec-amrnb 0.0 input-audio.wav Codec-clean 1.0 input-audio.wav Codec-g723_1 0.0 input-audio.wav Codec-g726 0.0 input-audio.wav Codec-gsm 0.0 input-audio.wav Codec-ilbc 0.0 input-audio.wav Codec-mp3 0.0 input-audio.wav Codec-opus 0.0 input-audio.wav Codec-real144 0.0 input-audio.wav Codec-speex 0.0 input-audio.wav Codec-vorbis 0.0 input-audio.wav Emotion-A 0.0 input-audio.wav Emotion-H 0.0 input-audio.wav Emotion-N 0.9999999 input-audio.wav Gender-f 0.0 input-audio.wav Gender-m 1.0 input-audio.wav Language-amh 0.0 input-audio.wav Language-ara 0.0 input-audio.wav Language-cmn 0.0 input-audio.wav Language-eng 0.99999642 input-audio.wav Language-fas 0.0 input-audio.wav Language-fre 0.0040021786 input-audio.wav Language-hau 0.0 input-audio.wav Language-jpn 0.0 input-audio.wav Language-kor 0.0 input-audio.wav Language-pus 0.0 input-audio.wav Language-rus 0.0 input-audio.wav Language-spa 0.00023994729 input-audio.wav Language-tur 0.0 input-audio.wav Language-urd 0.0 input-audio.wav Language-vie 0.0 input-audio.wav Reverb-nml 1.0 input-audio.wav Reverb-rev 0.0 input-audio.wav SNR-clean 0.90663459 input-audio.wav SNR-snr08db 0.0 input-audio.wav SNR-snr20db 0.093361041 Functionality ( Traits The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Changing Audio Conditions This plugin is a global scoring plugin. The scores reported will represent a summarization of the entire submitted audio buffer or file with respect to each of the classes in the plugin. If conditions change, or there are multiple speakers, or multiple languages spoken, if the environment changes in any way, it may be very difficult or impossible for these changes to be captured and represented by the results from this plugin. Comments Calibration This plugin applies calibration only to the scores from the Language class. All other classes have the sum of scores for that class scaled to sum to 1.0. Subsetting Classes If only a limited number of the metadata classes are of interest, it is possible to subset the report back from the plugin by passing these classes as --ids with the scoring request. Refer to the Command Line Interface or API Documentation for instructions on how to do this, depending on how you are interacting with the plugin. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 0.0 to 1.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"env-multiClass-v2 (Environmental Analysis)"},{"location":"plugins/env-multiClass-v2.html#env-multiclass-v2-environmental-analysis","text":"","title":"env-multiClass-v2 (Environmental Analysis)"},{"location":"plugins/env-multiClass-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release with OLIVE 5.1.0, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/env-multiClass-v2.html#description","text":"ENV plugins are designed to analyze submitted audio to glean information from the audio. Often not necessarily meant for detecting specific events, languages, or speakers, these plugins are instead pulling information about the environment or conditions of the audio recording, like the noise type(s) and level, codec, background conditions, reverberation, or other such characteristics. The specific classes that are reported will vary from plugin-to-plugin, so for more information regarding these, refer below. This plugin extracts information regarding the acoustic conditions and content of the audio file being analyzed, such as language, codec, SNR levels, etc. This plugin features: Model Ensemble The plugin leverages 7 systems for extracting 40 subclasses of information related to Language, Codec, SNR, Emotion, Reverb, Channel and Gender. Scaled Scores To faciltate interpretation, all scores for a given class (out of 7) are scaled to sum to 1.0. An exception is for Language due to the scores being LLRs and accounting for unknown languages in the model.","title":"Description"},{"location":"plugins/env-multiClass-v2.html#supported-classes","text":"The table below shows each class and the associated subclasses that this plugin reports scores for. Class Subclasses Channel ac (Acoustic projection via a mobile phone recording of a speakerphone voice output mic (Microphone) tel (Telephone) Codec aac amrnb clean (Uncompressed) g723_1 g726 gsm ilbc mp3 opus real144 speex vorbis Emotion A (Aroused) H (Happy) N (Neutral) Gender f (Female) m (Male) Language amh (Amharic) ara (Arabic) cmn (Chinese Mandarin) eng (English) fas (Farsi) fre (French) hau (Hausa) jpn (Japanese) kor (Korean) pus (Pushto) rus (Russian) spa (Spanish) tur (Turkish) urd (Urdu) vie (Vietnamese) Reverb nml (Limited to no reverberation) rev (Moderate to high levels of reverberation) SNR clean (SNR better than 30dB) snr20db (SNR of approximately 20dB) snr08db (SNR of 8dB or lower)","title":"Supported Classes"},{"location":"plugins/env-multiClass-v2.html#domains","text":"multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs.","title":"Domains"},{"location":"plugins/env-multiClass-v2.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/env-multiClass-v2.html#outputs","text":"Generally, a list of scores for all classes in the domain, for the entire segment. Unlike with SAD and SID, where scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection, most of the classes within an ENV plugin are set up differently. Some ENV plugins, like this one will have a 'parent' superclass that has several subclasses within it. For example, the Gender superclass has subclasses of m (male) and f (female); the Channel superclass may have subclasses of mic (microphone), tel (telephone), and ac (acoustic coupling). When the plugin reports its results, these classes are reported as: Gender-f <score> Gender-m <score> and Channel-ac <score> Channel-mic <score> Channel-tel <score> respectively. Note that within many of these superclasses, the scores are scaled so that the sum of all subclasses within the superclass will sum to one, to make the scores more legible and easier to interpret. In these cases, the highest score represents the most likely subclass for that category. An example output excerpt: input-audio.wav Channel-ac 0.0 input-audio.wav Channel-mic 1.0 input-audio.wav Channel-tel 0.0 input-audio.wav Codec-aac 0.0 input-audio.wav Codec-amrnb 0.0 input-audio.wav Codec-clean 1.0 input-audio.wav Codec-g723_1 0.0 input-audio.wav Codec-g726 0.0 input-audio.wav Codec-gsm 0.0 input-audio.wav Codec-ilbc 0.0 input-audio.wav Codec-mp3 0.0 input-audio.wav Codec-opus 0.0 input-audio.wav Codec-real144 0.0 input-audio.wav Codec-speex 0.0 input-audio.wav Codec-vorbis 0.0 input-audio.wav Emotion-A 0.0 input-audio.wav Emotion-H 0.0 input-audio.wav Emotion-N 0.9999999 input-audio.wav Gender-f 0.0 input-audio.wav Gender-m 1.0 input-audio.wav Language-amh 0.0 input-audio.wav Language-ara 0.0 input-audio.wav Language-cmn 0.0 input-audio.wav Language-eng 0.99999642 input-audio.wav Language-fas 0.0 input-audio.wav Language-fre 0.0040021786 input-audio.wav Language-hau 0.0 input-audio.wav Language-jpn 0.0 input-audio.wav Language-kor 0.0 input-audio.wav Language-pus 0.0 input-audio.wav Language-rus 0.0 input-audio.wav Language-spa 0.00023994729 input-audio.wav Language-tur 0.0 input-audio.wav Language-urd 0.0 input-audio.wav Language-vie 0.0 input-audio.wav Reverb-nml 1.0 input-audio.wav Reverb-rev 0.0 input-audio.wav SNR-clean 0.90663459 input-audio.wav SNR-snr08db 0.0 input-audio.wav SNR-snr20db 0.093361041","title":"Outputs"},{"location":"plugins/env-multiClass-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest","title":"Functionality (Traits"},{"location":"plugins/env-multiClass-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/env-multiClass-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/env-multiClass-v2.html#changing-audio-conditions","text":"This plugin is a global scoring plugin. The scores reported will represent a summarization of the entire submitted audio buffer or file with respect to each of the classes in the plugin. If conditions change, or there are multiple speakers, or multiple languages spoken, if the environment changes in any way, it may be very difficult or impossible for these changes to be captured and represented by the results from this plugin.","title":"Changing Audio Conditions"},{"location":"plugins/env-multiClass-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/env-multiClass-v2.html#calibration","text":"This plugin applies calibration only to the scores from the Language class. All other classes have the sum of scores for that class scaled to sum to 1.0.","title":"Calibration"},{"location":"plugins/env-multiClass-v2.html#subsetting-classes","text":"If only a limited number of the metadata classes are of interest, it is possible to subset the report back from the plugin by passing these classes as --ids with the scoring request. Refer to the Command Line Interface or API Documentation for instructions on how to do this, depending on how you are interacting with the plugin.","title":"Subsetting Classes"},{"location":"plugins/env-multiClass-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 0.0 to 1.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/env-powerSupplyHum-v1.html","text":"env-powerSupplyHum-v1.0.0 (Environmental Analysis - Power Supply Hum) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Environmental Analysis plugins attempt to characterize the acoustic environment of an audio file in some way. This particular plugin is attempting to detect and label power supply hum contamination and is searching audio waveforms for either 50 or 60 Hz frequencies that come from a mains hum, also known as a line frequency or power line hum. These hums come from the alternating currents running through electrical lines and sockets. Different countries and regions will have either 50 or 60 Hz power line hums determined by their regulations and infrastructure. For example, audio recorded in the United States could have a 60 Hz hum while data recorded in Europe could have a 50 Hz hum. If a mains hum is detected, it gives a potential hint for the geographic provenance of the waveform. This plugin was trained on waveforms from several datasets (ESC-50, DARES-G1, DEMAND, QUT Noise, TUT 2018, AASP-INFO) with 50 and 60 Hz tones artificially mixed in. The data also included waveforms without inserted tones and pure tones. Seeing data without tones helps the model determine when there is no line frequency present. Presenting pure tones is motivated by speech, where systems learn better if they are exposed to the clean signal as well. The model is a Residual Network (ResNet) that was originally trained on ImageNet to differentiate 1000 different classes of objects. We fine-tuned this model on high-resolution spectrograms that were zoomed in to the lower frequency range where power line hums are present. Domains default-v1 Model trained on several varied datasets that showed the best performance on unseen data with real power line frequencies. Inputs The audio waveforms to be searched for power line frequencies. Outputs Detections for 50 and 60 Hz line frequencies in the input waveforms if any were found. The plugin also reports the log-likelihood ratio for the potential detected classes. If no mains hums were found, the plugin reports that the input waveforms have no line frequencies. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single label for the entire audio. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. 50 Cycle Bias The model performs best on detecting 50 Hz hums and the lack of a hum. It struggles most with 60 Hz hums when there is other nearby low frequency activity. Helping the model better discriminate between 60 Hz and nearby activity would be one of the first steps to further improvement. The improvements in 60 Hz would also likely benefit the 50 Hz and lack of hum classes. Training Data Applicability It is difficult to find real-world data with line frequencies, since it is one of the first issues corrected for or removed in official and academic data collects. To overcome this challenge, we artificially inserted 50 and 60 Hz tones into noisy waveforms to simulate power line frequencies while training the models. There is a risk that the model trained with artificial data will struggle to generalize on real, operational data.","title":"env-powerSupplyHum-v1.0.0 (Environmental Analysis - Power Supply Hum)"},{"location":"plugins/env-powerSupplyHum-v1.html#env-powersupplyhum-v100-environmental-analysis-power-supply-hum","text":"","title":"env-powerSupplyHum-v1.0.0 (Environmental Analysis - Power Supply Hum)"},{"location":"plugins/env-powerSupplyHum-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/env-powerSupplyHum-v1.html#description","text":"Environmental Analysis plugins attempt to characterize the acoustic environment of an audio file in some way. This particular plugin is attempting to detect and label power supply hum contamination and is searching audio waveforms for either 50 or 60 Hz frequencies that come from a mains hum, also known as a line frequency or power line hum. These hums come from the alternating currents running through electrical lines and sockets. Different countries and regions will have either 50 or 60 Hz power line hums determined by their regulations and infrastructure. For example, audio recorded in the United States could have a 60 Hz hum while data recorded in Europe could have a 50 Hz hum. If a mains hum is detected, it gives a potential hint for the geographic provenance of the waveform. This plugin was trained on waveforms from several datasets (ESC-50, DARES-G1, DEMAND, QUT Noise, TUT 2018, AASP-INFO) with 50 and 60 Hz tones artificially mixed in. The data also included waveforms without inserted tones and pure tones. Seeing data without tones helps the model determine when there is no line frequency present. Presenting pure tones is motivated by speech, where systems learn better if they are exposed to the clean signal as well. The model is a Residual Network (ResNet) that was originally trained on ImageNet to differentiate 1000 different classes of objects. We fine-tuned this model on high-resolution spectrograms that were zoomed in to the lower frequency range where power line hums are present.","title":"Description"},{"location":"plugins/env-powerSupplyHum-v1.html#domains","text":"default-v1 Model trained on several varied datasets that showed the best performance on unseen data with real power line frequencies.","title":"Domains"},{"location":"plugins/env-powerSupplyHum-v1.html#inputs","text":"The audio waveforms to be searched for power line frequencies.","title":"Inputs"},{"location":"plugins/env-powerSupplyHum-v1.html#outputs","text":"Detections for 50 and 60 Hz line frequencies in the input waveforms if any were found. The plugin also reports the log-likelihood ratio for the potential detected classes. If no mains hums were found, the plugin reports that the input waveforms have no line frequencies.","title":"Outputs"},{"location":"plugins/env-powerSupplyHum-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single label for the entire audio. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/env-powerSupplyHum-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/env-powerSupplyHum-v1.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/env-powerSupplyHum-v1.html#50-cycle-bias","text":"The model performs best on detecting 50 Hz hums and the lack of a hum. It struggles most with 60 Hz hums when there is other nearby low frequency activity. Helping the model better discriminate between 60 Hz and nearby activity would be one of the first steps to further improvement. The improvements in 60 Hz would also likely benefit the 50 Hz and lack of hum classes.","title":"50 Cycle Bias"},{"location":"plugins/env-powerSupplyHum-v1.html#training-data-applicability","text":"It is difficult to find real-world data with line frequencies, since it is one of the first issues corrected for or removed in official and academic data collects. To overcome this challenge, we artificially inserted 50 and 60 Hz tones into noisy waveforms to simulate power line frequencies while training the models. There is a risk that the model trained with artificial data will struggle to generalize on real, operational data.","title":"Training Data Applicability"},{"location":"plugins/env-speakerCount-v1.html","text":"env-speakerCount-v1 (Environmental Analysis - Speaker Count) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Speaker Count (mex-speakerCount) plugins identifies the total number of speakers in a given audio segment. Domains multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Inputs An audio file or buffer to be scored. Outputs In the basic case, Speaker Count plugins returns a list of audio files/segments labelled with the number of speakers. input-audio1.wav 1 1.0 input-audio2.wav 2 1.0 input-audio3.wav 2+ 1.0 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning labeled number of speakers within the submitted audio. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations All current env-speakerCount plugins assume that an audio segment contains at least 1 non-overlapped speaker. The current plugin limits the speaker counting to 1, 2 or more than 2 speakers. In many cases, a minimum duration of speech of 5 seconds is required in order to output scores. This value can optionally be overwritten by the user, but the number of speaker provided for such short segments will be less reliable.","title":"env-speakerCount-v1 (Environmental Analysis - Speaker Count)"},{"location":"plugins/env-speakerCount-v1.html#env-speakercount-v1-environmental-analysis-speaker-count","text":"","title":"env-speakerCount-v1 (Environmental Analysis - Speaker Count)"},{"location":"plugins/env-speakerCount-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/env-speakerCount-v1.html#description","text":"Speaker Count (mex-speakerCount) plugins identifies the total number of speakers in a given audio segment.","title":"Description"},{"location":"plugins/env-speakerCount-v1.html#domains","text":"multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/env-speakerCount-v1.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/env-speakerCount-v1.html#outputs","text":"In the basic case, Speaker Count plugins returns a list of audio files/segments labelled with the number of speakers. input-audio1.wav 1 1.0 input-audio2.wav 2 1.0 input-audio3.wav 2+ 1.0","title":"Outputs"},{"location":"plugins/env-speakerCount-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning labeled number of speakers within the submitted audio. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/env-speakerCount-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/env-speakerCount-v1.html#limitations","text":"All current env-speakerCount plugins assume that an audio segment contains at least 1 non-overlapped speaker. The current plugin limits the speaker counting to 1, 2 or more than 2 speakers. In many cases, a minimum duration of speech of 5 seconds is required in order to output scores. This value can optionally be overwritten by the user, but the number of speaker provided for such short segments will be less reliable.","title":"Limitations"},{"location":"plugins/fdi-pyEmbed-v1.html","text":"fdi-pyEmbed-v1 (Face Detection Image) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 Description Face Detection Image plugins process an input image and attempt to localize one or more faces within the frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be a face. Unlike Face Recognition plugins, a Face Detection plugin is only looking for faces generically - it doesn't care about the identity of the face or faces, and will report any faces detected. Domains multi-v1 A general purpose image processing domain. Inputs An image file to process. Outputs Face Detection Image plugins are 'bounding box' scorers - the output of a bounding box scorer is a class, a corresponding score, and 4 points associated with this class and score that attempt to localize the detected class (in this case a face) within the image frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) Where the bounding box itself is defined by the four coordinates in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) An example output could look like this: input_image.png face 0.9974257349967957 (154, 78, 657, 745) Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon) Compatibility OLIVE 5.3+ Limitations Comments Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Face Detection Image (FDI)"},{"location":"plugins/fdi-pyEmbed-v1.html#fdi-pyembed-v1-face-detection-image","text":"","title":"fdi-pyEmbed-v1 (Face Detection Image)"},{"location":"plugins/fdi-pyEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/fdi-pyEmbed-v1.html#description","text":"Face Detection Image plugins process an input image and attempt to localize one or more faces within the frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be a face. Unlike Face Recognition plugins, a Face Detection plugin is only looking for faces generically - it doesn't care about the identity of the face or faces, and will report any faces detected.","title":"Description"},{"location":"plugins/fdi-pyEmbed-v1.html#domains","text":"multi-v1 A general purpose image processing domain.","title":"Domains"},{"location":"plugins/fdi-pyEmbed-v1.html#inputs","text":"An image file to process.","title":"Inputs"},{"location":"plugins/fdi-pyEmbed-v1.html#outputs","text":"Face Detection Image plugins are 'bounding box' scorers - the output of a bounding box scorer is a class, a corresponding score, and 4 points associated with this class and score that attempt to localize the detected class (in this case a face) within the image frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) Where the bounding box itself is defined by the four coordinates in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) An example output could look like this: input_image.png face 0.9974257349967957 (154, 78, 657, 745)","title":"Outputs"},{"location":"plugins/fdi-pyEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon)","title":"Functionality (Traits)"},{"location":"plugins/fdi-pyEmbed-v1.html#compatibility","text":"OLIVE 5.3+","title":"Compatibility"},{"location":"plugins/fdi-pyEmbed-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/fdi-pyEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/fdi-pyEmbed-v1.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/fdi.html","text":"redirect: plugins/fdi-pyEmbed-v1.md","title":"Fdi"},{"location":"plugins/fdi.html#redirect-pluginsfdi-pyembed-v1md","text":"","title":"redirect: plugins/fdi-pyEmbed-v1.md"},{"location":"plugins/fdv-pyEmbed-v1.html","text":"fdv-pyEmbed-v1 (Face Detection Video) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 Description Face Detection Video plugins process an input video and attempt to localize in both space and time one or more enrolled faces within the video frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to belong to one of the enrolled targets, and an associated start and end time region. Unlike Face Recognition plugins, a Face Detection plugin is only looking for faces generically - it doesn't care about the identity of the face or faces, and will report any faces detected. Domains multi-v1 A general purpose video processing domain. Inputs An video file to process. Outputs Face Recognition Video plugins are 'bounding box' scorers - the output of a video-processing bounding box scorer is a class, a corresponding score, an associated start and end timestamp denoting when the bounding box is valid, and 4 points associated with this class, time, and score that attempt to localize the detected class (in this case the name of the enrolled face) within the video frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) (<start_seconds>, <end_seconds>) Where the bounding box itself is defined by the four coordinates grouped in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) And the timestamps are the final two grouped numbers. An example output could look like this: test-videos/input_video.mp4 face 0.9974257349967957 (154, 78, 657, 745) (978.31, 978.84) Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon) Compatibility OLIVE 5.3+ Limitations Due to the intensity of resources required for processing videos, this plugin has a few limitations or behaviors that need to be considered. Large Video Files When a video file is opened and decoded into individual frames in memory, it can expand in size by considerable amounts. Because of this expansion, care should be taken to minimize other overheards when processing video files - such as by submitting video files for scoring via a file path instead of as a serialized buffer whenever possible. Realistic expectations should be held when attempting to process large video files when available memory is limited. Please plan on making a minimum of 16GB of memory available for video processing; ideally more for larger files. Resolution Scaling The current crop of OLIVE video processing plugins do not process video at full resolution - as the video files are opened, they are rescaled to 640 x 480 pixel resolution, and processed at this size. Our internal testing has shown this does not significantly degrade performance with these plugins, but drastically reduces required memory resources and improves our processing capabilities as a result. Note that there is currently no retention of the original aspect ratio, so some files, such as those with a very wide, very square, or portrait-orientation aspect ratio may not be processed exactly as expected due to scaling to 640 x 480 exactly. Frame Rate (vs Temporal Resolution) Processing every individual video frame at the videos native frame rate is enormously expensive. To avoid this resource cost and improve the processing speed and reduce the resource requirements of running these plugins, plugins currently process 4 frames per second. This limits the precision of the start and end timestamps for face regions, and makes it possible, though unlikely, for very quickly appearing/disappearing faces to be missed. Comments Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Face Detection Video (FDV)"},{"location":"plugins/fdv-pyEmbed-v1.html#fdv-pyembed-v1-face-detection-video","text":"","title":"fdv-pyEmbed-v1 (Face Detection Video)"},{"location":"plugins/fdv-pyEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/fdv-pyEmbed-v1.html#description","text":"Face Detection Video plugins process an input video and attempt to localize in both space and time one or more enrolled faces within the video frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to belong to one of the enrolled targets, and an associated start and end time region. Unlike Face Recognition plugins, a Face Detection plugin is only looking for faces generically - it doesn't care about the identity of the face or faces, and will report any faces detected.","title":"Description"},{"location":"plugins/fdv-pyEmbed-v1.html#domains","text":"multi-v1 A general purpose video processing domain.","title":"Domains"},{"location":"plugins/fdv-pyEmbed-v1.html#inputs","text":"An video file to process.","title":"Inputs"},{"location":"plugins/fdv-pyEmbed-v1.html#outputs","text":"Face Recognition Video plugins are 'bounding box' scorers - the output of a video-processing bounding box scorer is a class, a corresponding score, an associated start and end timestamp denoting when the bounding box is valid, and 4 points associated with this class, time, and score that attempt to localize the detected class (in this case the name of the enrolled face) within the video frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) (<start_seconds>, <end_seconds>) Where the bounding box itself is defined by the four coordinates grouped in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) And the timestamps are the final two grouped numbers. An example output could look like this: test-videos/input_video.mp4 face 0.9974257349967957 (154, 78, 657, 745) (978.31, 978.84)","title":"Outputs"},{"location":"plugins/fdv-pyEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon)","title":"Functionality (Traits)"},{"location":"plugins/fdv-pyEmbed-v1.html#compatibility","text":"OLIVE 5.3+","title":"Compatibility"},{"location":"plugins/fdv-pyEmbed-v1.html#limitations","text":"Due to the intensity of resources required for processing videos, this plugin has a few limitations or behaviors that need to be considered.","title":"Limitations"},{"location":"plugins/fdv-pyEmbed-v1.html#large-video-files","text":"When a video file is opened and decoded into individual frames in memory, it can expand in size by considerable amounts. Because of this expansion, care should be taken to minimize other overheards when processing video files - such as by submitting video files for scoring via a file path instead of as a serialized buffer whenever possible. Realistic expectations should be held when attempting to process large video files when available memory is limited. Please plan on making a minimum of 16GB of memory available for video processing; ideally more for larger files.","title":"Large Video Files"},{"location":"plugins/fdv-pyEmbed-v1.html#resolution-scaling","text":"The current crop of OLIVE video processing plugins do not process video at full resolution - as the video files are opened, they are rescaled to 640 x 480 pixel resolution, and processed at this size. Our internal testing has shown this does not significantly degrade performance with these plugins, but drastically reduces required memory resources and improves our processing capabilities as a result. Note that there is currently no retention of the original aspect ratio, so some files, such as those with a very wide, very square, or portrait-orientation aspect ratio may not be processed exactly as expected due to scaling to 640 x 480 exactly.","title":"Resolution Scaling"},{"location":"plugins/fdv-pyEmbed-v1.html#frame-rate-vs-temporal-resolution","text":"Processing every individual video frame at the videos native frame rate is enormously expensive. To avoid this resource cost and improve the processing speed and reduce the resource requirements of running these plugins, plugins currently process 4 frames per second. This limits the precision of the start and end timestamps for face regions, and makes it possible, though unlikely, for very quickly appearing/disappearing faces to be missed.","title":"Frame Rate (vs Temporal Resolution)"},{"location":"plugins/fdv-pyEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/fdv-pyEmbed-v1.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/fdv.html","text":"redirect: plugins/fdv-pyEmbed-v1.md","title":"Fdv"},{"location":"plugins/fdv.html#redirect-pluginsfdv-pyembed-v1md","text":"","title":"redirect: plugins/fdv-pyEmbed-v1.md"},{"location":"plugins/for.html","text":"redirect: plugins/for-forensic-v1.md","title":"For"},{"location":"plugins/for.html#redirect-pluginsfor-forensic-v1md","text":"","title":"redirect: plugins/for-forensic-v1.md"},{"location":"plugins/fri-pyEmbed-v1.html","text":"fri-pyEmbed-v1 (Face Recognition Image) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 Description Face Recognition Image plugins process an input image and attempt to localize one or more enrolled faces within the frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be the face of one of the enrolled faces. Unlike Face Detection plugins, a Face Recognition plugin is only looking for the faces of enrolled persons of interest. If faces are detected that the system is not confident belong to one of the enrolled targets, they will not be reported. Domains multi-v1 A general purpose image processing domain. Inputs An image file to process. Enrollments Face Recognition plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's representation - in this case, an image of a person's face. A new enrollment is created with the first class modification, which consists of essentially sending the system an image sample from a person of interest. This enrollment can be augmented with subsequent class modification requests by adding more images with the same class label. Note that currently only images can be used for face enrollment requests; it is not yet possible to enroll faces via video. Outputs Face Recognition Image plugins are 'bounding box' scorers - the output of a bounding box scorer is a class, a corresponding score, and 4 points associated with this class and score that attempt to localize the detected class (in this case a face) within the image frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) Where the bounding box itself is defined by the four coordinates in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) An example output could look like this: input_image.png Marcus 0.5474257 (154, 78, 657, 745) Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon) Compatibility OLIVE 5.3+ Limitations Comments Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Face Recognition Image (FRI)"},{"location":"plugins/fri-pyEmbed-v1.html#fri-pyembed-v1-face-recognition-image","text":"","title":"fri-pyEmbed-v1 (Face Recognition Image)"},{"location":"plugins/fri-pyEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/fri-pyEmbed-v1.html#description","text":"Face Recognition Image plugins process an input image and attempt to localize one or more enrolled faces within the frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be the face of one of the enrolled faces. Unlike Face Detection plugins, a Face Recognition plugin is only looking for the faces of enrolled persons of interest. If faces are detected that the system is not confident belong to one of the enrolled targets, they will not be reported.","title":"Description"},{"location":"plugins/fri-pyEmbed-v1.html#domains","text":"multi-v1 A general purpose image processing domain.","title":"Domains"},{"location":"plugins/fri-pyEmbed-v1.html#inputs","text":"An image file to process.","title":"Inputs"},{"location":"plugins/fri-pyEmbed-v1.html#enrollments","text":"Face Recognition plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's representation - in this case, an image of a person's face. A new enrollment is created with the first class modification, which consists of essentially sending the system an image sample from a person of interest. This enrollment can be augmented with subsequent class modification requests by adding more images with the same class label. Note that currently only images can be used for face enrollment requests; it is not yet possible to enroll faces via video.","title":"Enrollments"},{"location":"plugins/fri-pyEmbed-v1.html#outputs","text":"Face Recognition Image plugins are 'bounding box' scorers - the output of a bounding box scorer is a class, a corresponding score, and 4 points associated with this class and score that attempt to localize the detected class (in this case a face) within the image frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) Where the bounding box itself is defined by the four coordinates in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) An example output could look like this: input_image.png Marcus 0.5474257 (154, 78, 657, 745)","title":"Outputs"},{"location":"plugins/fri-pyEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon)","title":"Functionality (Traits)"},{"location":"plugins/fri-pyEmbed-v1.html#compatibility","text":"OLIVE 5.3+","title":"Compatibility"},{"location":"plugins/fri-pyEmbed-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/fri-pyEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/fri-pyEmbed-v1.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/fri.html","text":"redirect: plugins/fri-pyEmbed-v1.md","title":"Fri"},{"location":"plugins/fri.html#redirect-pluginsfri-pyembed-v1md","text":"","title":"redirect: plugins/fri-pyEmbed-v1.md"},{"location":"plugins/frv-pyEmbed-v1.html","text":"fdv-pyEmbed-v1 (Face Recognition Video) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 Description Face Recognition Video plugins process an input video and attempt to localize in both space and time one or more faces within the video frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be a face, and an associated start and end time region. Unlike Face Detection plugins, a Face Recognition plugin is only looking for the faces of enrolled persons of interest. If faces are detected that the system is not confident belong to one of the enrolled targets, they will not be reported. Domains multi-v1 A general purpose video processing domain. Inputs An video file to process. Enrollments Face Recognition plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's representation - in this case, an image of a person's face. A new enrollment is created with the first class modification, which consists of essentially sending the system an image sample from a person of interest. This enrollment can be augmented with subsequent class modification requests by adding more images with the same class label. Note that currently only images can be used for face enrollment requests; it is not yet possible to enroll faces via video. Outputs Face Recognition Video plugins are 'bounding box' scorers - the output of a video-processing bounding box scorer is a class, a corresponding score, an associated start and end timestamp denoting when the bounding box is valid, and 4 points associated with this class, time, and score that attempt to localize the detected class (in this case a face) within the video frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) (<start_seconds>, <end_seconds>) Where the bounding box itself is defined by the four coordinates grouped in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) And the timestamps are the final two grouped numbers. An example output could look like this: test-videos/input_video.mp4 Marcus 0.5474257349967957 (154, 78, 657, 745) (978.31, 978.84) Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon) Compatibility OLIVE 5.3+ Limitations Due to the intensity of resources required for processing videos, this plugin has a few limitations or behaviors that need to be considered. Large Video Files When a video file is opened and decoded into individual frames in memory, it can expand in size by considerable amounts. Because of this expansion, care should be taken to minimize other overheards when processing video files - such as by submitting video files for scoring via a file path instead of as a serialized buffer whenever possible. Realistic expectations should be held when attempting to process large video files when available memory is limited. Please plan on making a minimum of 16GB of memory available for video processing; ideally more for larger files. Resolution Scaling The current crop of OLIVE video processing plugins do not process video at full resolution - as the video files are opened, they are rescaled to 640 x 480 pixel resolution, and processed at this size. Our internal testing has shown this does not significantly degrade performance with these plugins, but drastically reduces required memory resources and improves our processing capabilities as a result. Note that there is currently no retention of the original aspect ratio, so some files, such as those with a very wide, very square, or portrait-orientation aspect ratio may not be processed exactly as expected due to scaling to 640 x 480 exactly. Frame Rate (vs Temporal Resolution) Processing every individual video frame at the videos native frame rate is enormously expensive. To avoid this resource cost and improve the processing speed and reduce the resource requirements of running these plugins, plugins currently process 4 frames per second. This limits the precision of the start and end timestamps for face regions, and makes it possible, though unlikely, for very quickly appearing/disappearing faces to be missed. Comments Global Options This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Face Recognition Video (FRV)"},{"location":"plugins/frv-pyEmbed-v1.html#fdv-pyembed-v1-face-recognition-video","text":"","title":"fdv-pyEmbed-v1 (Face Recognition Video)"},{"location":"plugins/frv-pyEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/frv-pyEmbed-v1.html#description","text":"Face Recognition Video plugins process an input video and attempt to localize in both space and time one or more faces within the video frame. If they are detected, a 'bounding box' highlighting the face is output, along with an associated confidence score informing how likely this box is to be a face, and an associated start and end time region. Unlike Face Detection plugins, a Face Recognition plugin is only looking for the faces of enrolled persons of interest. If faces are detected that the system is not confident belong to one of the enrolled targets, they will not be reported.","title":"Description"},{"location":"plugins/frv-pyEmbed-v1.html#domains","text":"multi-v1 A general purpose video processing domain.","title":"Domains"},{"location":"plugins/frv-pyEmbed-v1.html#inputs","text":"An video file to process.","title":"Inputs"},{"location":"plugins/frv-pyEmbed-v1.html#enrollments","text":"Face Recognition plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's representation - in this case, an image of a person's face. A new enrollment is created with the first class modification, which consists of essentially sending the system an image sample from a person of interest. This enrollment can be augmented with subsequent class modification requests by adding more images with the same class label. Note that currently only images can be used for face enrollment requests; it is not yet possible to enroll faces via video.","title":"Enrollments"},{"location":"plugins/frv-pyEmbed-v1.html#outputs","text":"Face Recognition Video plugins are 'bounding box' scorers - the output of a video-processing bounding box scorer is a class, a corresponding score, an associated start and end timestamp denoting when the bounding box is valid, and 4 points associated with this class, time, and score that attempt to localize the detected class (in this case a face) within the video frame. That output looks like this: <file> <class> <score> (<x1>, <y1>, <x2>, <y2>) (<start_seconds>, <end_seconds>) Where the bounding box itself is defined by the four coordinates grouped in parentheses: (Upper Left: x1, y1 | Lower Right: x2, y2) And the timestamps are the final two grouped numbers. An example output could look like this: test-videos/input_video.mp4 Marcus 0.5474257349967957 (154, 78, 657, 745) (978.31, 978.84)","title":"Outputs"},{"location":"plugins/frv-pyEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. BOUNDING_BOX_SCORER (NOTE: Coming Soon) \u2013 Score all submitted images or videos, returning labeled bounding box regions within the image frames, or labeled bounding box regions with an associated start and end time region if scoring video files. BoundingBoxScorerRequest (NOTE: Coming Soon)","title":"Functionality (Traits)"},{"location":"plugins/frv-pyEmbed-v1.html#compatibility","text":"OLIVE 5.3+","title":"Compatibility"},{"location":"plugins/frv-pyEmbed-v1.html#limitations","text":"Due to the intensity of resources required for processing videos, this plugin has a few limitations or behaviors that need to be considered.","title":"Limitations"},{"location":"plugins/frv-pyEmbed-v1.html#large-video-files","text":"When a video file is opened and decoded into individual frames in memory, it can expand in size by considerable amounts. Because of this expansion, care should be taken to minimize other overheards when processing video files - such as by submitting video files for scoring via a file path instead of as a serialized buffer whenever possible. Realistic expectations should be held when attempting to process large video files when available memory is limited. Please plan on making a minimum of 16GB of memory available for video processing; ideally more for larger files.","title":"Large Video Files"},{"location":"plugins/frv-pyEmbed-v1.html#resolution-scaling","text":"The current crop of OLIVE video processing plugins do not process video at full resolution - as the video files are opened, they are rescaled to 640 x 480 pixel resolution, and processed at this size. Our internal testing has shown this does not significantly degrade performance with these plugins, but drastically reduces required memory resources and improves our processing capabilities as a result. Note that there is currently no retention of the original aspect ratio, so some files, such as those with a very wide, very square, or portrait-orientation aspect ratio may not be processed exactly as expected due to scaling to 640 x 480 exactly.","title":"Resolution Scaling"},{"location":"plugins/frv-pyEmbed-v1.html#frame-rate-vs-temporal-resolution","text":"Processing every individual video frame at the videos native frame rate is enormously expensive. To avoid this resource cost and improve the processing speed and reduce the resource requirements of running these plugins, plugins currently process 4 frames per second. This limits the precision of the start and end timestamps for face regions, and makes it possible, though unlikely, for very quickly appearing/disappearing faces to be missed.","title":"Frame Rate (vs Temporal Resolution)"},{"location":"plugins/frv-pyEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/frv-pyEmbed-v1.html#global-options","text":"This plugin does not currently have user-configurable options, though it is possible for some performance tweaks and configuration changes to be made. If you find this plugin to not perform adequately for your data conditions, or have a specific use case, please get in touch with SRI to discuss how the plugin can be tuned for optimal performance on your data.","title":"Global Options"},{"location":"plugins/frv.html","text":"redirect: plugins/frv-pyEmbed-v1.md","title":"Frv"},{"location":"plugins/frv.html#redirect-pluginsfrv-pyembed-v1md","text":"","title":"redirect: plugins/frv-pyEmbed-v1.md"},{"location":"plugins/gdd-embedplda-v1.html","text":"gdd-embedplda-v1 (Gender Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0 Description Gender Detection plugins will detect and label the gender of the speaker for regions of speech in a submitted audio segment. This is in contrast to Gender Identification (GID) plugins, which label the entire segment with a single gender. So, unlike Gender Identification (GID), GDD is capable of handling audio where multiple speakers of a different gender are speaking, and will provide timestamp region labels to point to label male and female regions. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Inputs Audio file or buffer and an optional identifier. Outputs GDD plugins return a list of regions with a score for the detected gender within that region. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 Female 5.40590000 input-audio.wav 43.500 77.500 Female 5.29558277 input-audio.wav 77.500 78.500 Male 2.63369179 input-audio.wav 78.500 80.500 Male 2.25519705 input-audio.wav 85.500 86.500 Female 2.06612849 input-audio.wav 97.500 98.500 Female 3.74665093 input-audio.wav 98.500 99.500 Male 2.22936487 input-audio.wav 105.500 106.500 Male 2.72254372 input-audio.wav 107.500 108.500 Female 2.60355234 input-audio.wav 108.500 110.500 Female 2.76414633 input-audio.wav 109.500 113.140 Male 2.85003138 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected gender and corresponding score for this gender RegionScorerRequest Compatibility OLIVE 5.2+ Limitations Known or potential limitations of the plugin are outlined below. Minimum Speech Duration The system will only attempt to perform gender detection if the submitted audio segment contains more than 2 seconds of detected speech. Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 1.5 -10.0 to 10.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for gender. 2.0 1.0 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"Gender Detection (GDD)"},{"location":"plugins/gdd-embedplda-v1.html#gdd-embedplda-v1-gender-detection","text":"","title":"gdd-embedplda-v1 (Gender Detection)"},{"location":"plugins/gdd-embedplda-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/gdd-embedplda-v1.html#description","text":"Gender Detection plugins will detect and label the gender of the speaker for regions of speech in a submitted audio segment. This is in contrast to Gender Identification (GID) plugins, which label the entire segment with a single gender. So, unlike Gender Identification (GID), GDD is capable of handling audio where multiple speakers of a different gender are speaking, and will provide timestamp region labels to point to label male and female regions.","title":"Description"},{"location":"plugins/gdd-embedplda-v1.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB.","title":"Domains"},{"location":"plugins/gdd-embedplda-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/gdd-embedplda-v1.html#outputs","text":"GDD plugins return a list of regions with a score for the detected gender within that region. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 Female 5.40590000 input-audio.wav 43.500 77.500 Female 5.29558277 input-audio.wav 77.500 78.500 Male 2.63369179 input-audio.wav 78.500 80.500 Male 2.25519705 input-audio.wav 85.500 86.500 Female 2.06612849 input-audio.wav 97.500 98.500 Female 3.74665093 input-audio.wav 98.500 99.500 Male 2.22936487 input-audio.wav 105.500 106.500 Male 2.72254372 input-audio.wav 107.500 108.500 Female 2.60355234 input-audio.wav 108.500 110.500 Female 2.76414633 input-audio.wav 109.500 113.140 Male 2.85003138","title":"Outputs"},{"location":"plugins/gdd-embedplda-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected gender and corresponding score for this gender RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/gdd-embedplda-v1.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/gdd-embedplda-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/gdd-embedplda-v1.html#minimum-speech-duration","text":"The system will only attempt to perform gender detection if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/gdd-embedplda-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/gdd-embedplda-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 1.5 -10.0 to 10.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for gender. 2.0 1.0 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction 1.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/gdd.html","text":"redirect: plugins/gdd-embedplda-v1.md","title":"Gdd"},{"location":"plugins/gdd.html#redirect-pluginsgdd-embedplda-v1md","text":"","title":"redirect: plugins/gdd-embedplda-v1.md"},{"location":"plugins/gid-embedplda-v1.html","text":"gid-embedplda-v1 (Gender Identification) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, shares models with gdd-embedplda-v1 released with 5.4.0 v1.0.1 Code streamlining and minor bug fixes, released with 5.5.0 Description Gender Identification (GID) plugins attempt to identify the gender of the talker in the audio submitted to the plugin for scoring. The goal is to distinguish between male and female speakers as a quick triage of incoming data, or to serve as a front-end for other plugins that may have a gender-specific workflow. This plugin is powered by the PLDA Embeddings models originally released with gdd-embedplda-v1 . Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Inputs Audio file or buffer and an optional identifier. Outputs Gender ID plugins report a score for each gender, in the format shown below. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. input-audio.wav Female -3.212653 input-audio.wav Male 5.40590000 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each gender. GlobalScorerRequest Compatibility OLIVE 5.4+ Limitations Labeling Granularity GID plugins assume that an audio segment contains only a single gender and may be scored as a unit. If a segment contains multiple speakers who may or may not be of a different gender, the entire segment will still be scored as a unit. Ensuring that audio submitted for scoring consists of a single speaker is important for reliable results. Age All current Gender ID plugins are trained only on adult male and female speech. Speech from children will likely be confused with female speech, or be otherwise volatile. Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"gid-embedplda-v1 (Gender Identification)"},{"location":"plugins/gid-embedplda-v1.html#gid-embedplda-v1-gender-identification","text":"","title":"gid-embedplda-v1 (Gender Identification)"},{"location":"plugins/gid-embedplda-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, shares models with gdd-embedplda-v1 released with 5.4.0 v1.0.1 Code streamlining and minor bug fixes, released with 5.5.0","title":"Version Changelog"},{"location":"plugins/gid-embedplda-v1.html#description","text":"Gender Identification (GID) plugins attempt to identify the gender of the talker in the audio submitted to the plugin for scoring. The goal is to distinguish between male and female speakers as a quick triage of incoming data, or to serve as a front-end for other plugins that may have a gender-specific workflow. This plugin is powered by the PLDA Embeddings models originally released with gdd-embedplda-v1 .","title":"Description"},{"location":"plugins/gid-embedplda-v1.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB.","title":"Domains"},{"location":"plugins/gid-embedplda-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/gid-embedplda-v1.html#outputs","text":"Gender ID plugins report a score for each gender, in the format shown below. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. input-audio.wav Female -3.212653 input-audio.wav Male 5.40590000","title":"Outputs"},{"location":"plugins/gid-embedplda-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each gender. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/gid-embedplda-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/gid-embedplda-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/gid-embedplda-v1.html#labeling-granularity","text":"GID plugins assume that an audio segment contains only a single gender and may be scored as a unit. If a segment contains multiple speakers who may or may not be of a different gender, the entire segment will still be scored as a unit. Ensuring that audio submitted for scoring consists of a single speaker is important for reliable results.","title":"Labeling Granularity"},{"location":"plugins/gid-embedplda-v1.html#age","text":"All current Gender ID plugins are trained only on adult male and female speech. Speech from children will likely be confused with female speech, or be otherwise volatile.","title":"Age"},{"location":"plugins/gid-embedplda-v1.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/gid-gb-v2.html","text":"gid-gb-v2 (Gender Identification) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0+, released with 5.2.0 Description Gender Identification (GID) plugins attempt to identify the gender of the talker in the audio submitted to the plugin for scoring. The goal is to distinguish between male and female speakers as a quick triage of incoming data, or to serve as a front-end for other plugins that may have a gender-specific workflow. This plugin is powered by a Gaussian backend. Domains clean-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Inputs Audio file or buffer and an optional identifier. Outputs Gender ID plugins report a score for each gender, in the format shown below. The plugins created so far feature score scaling, meaning that the scores returned for male and female are re-scaled so that their values sum to 1.0, in order to facilitate legibility. input-audio.wav f 0.1 input-audio.wav m 0.9 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each gender. GlobalScorerRequest Compatibility OLIVE 5.2+ Limitations Labeling Granularity GID plugins assume that an audio segment contains only a single gender and may be scored as a unit. If a segment contains multiple speakers who may or may not be of a different gender, the entire segment will still be scored as a unit. Ensuring that audio submitted for scoring consists of a single speaker is important for reliable results. Age All current Gender ID plugins are trained only on adult male and female speech. Speech from children will likely be confused with female speech, or be otherwise volatile. Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Gender Identification (GID)"},{"location":"plugins/gid-gb-v2.html#gid-gb-v2-gender-identification","text":"","title":"gid-gb-v2 (Gender Identification)"},{"location":"plugins/gid-gb-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0+, released with 5.2.0","title":"Version Changelog"},{"location":"plugins/gid-gb-v2.html#description","text":"Gender Identification (GID) plugins attempt to identify the gender of the talker in the audio submitted to the plugin for scoring. The goal is to distinguish between male and female speakers as a quick triage of incoming data, or to serve as a front-end for other plugins that may have a gender-specific workflow. This plugin is powered by a Gaussian backend.","title":"Description"},{"location":"plugins/gid-gb-v2.html#domains","text":"clean-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB.","title":"Domains"},{"location":"plugins/gid-gb-v2.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/gid-gb-v2.html#outputs","text":"Gender ID plugins report a score for each gender, in the format shown below. The plugins created so far feature score scaling, meaning that the scores returned for male and female are re-scaled so that their values sum to 1.0, in order to facilitate legibility. input-audio.wav f 0.1 input-audio.wav m 0.9","title":"Outputs"},{"location":"plugins/gid-gb-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each gender. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/gid-gb-v2.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/gid-gb-v2.html#limitations","text":"","title":"Limitations"},{"location":"plugins/gid-gb-v2.html#labeling-granularity","text":"GID plugins assume that an audio segment contains only a single gender and may be scored as a unit. If a segment contains multiple speakers who may or may not be of a different gender, the entire segment will still be scored as a unit. Ensuring that audio submitted for scoring consists of a single speaker is important for reliable results.","title":"Labeling Granularity"},{"location":"plugins/gid-gb-v2.html#age","text":"All current Gender ID plugins are trained only on adult male and female speech. Speech from children will likely be confused with female speech, or be otherwise volatile.","title":"Age"},{"location":"plugins/gid-gb-v2.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/gid.html","text":"redirect: plugins/gid-gb-v2.md","title":"Gid"},{"location":"plugins/gid.html#redirect-pluginsgid-gb-v2md","text":"","title":"redirect: plugins/gid-gb-v2.md"},{"location":"plugins/kws.html","text":"Keyword Spotting (KWS) Released Plugins All of our current QBE plugins match the description on this page. Refer to the list below for the currently supported Speaker Detection plugins. Legacy Plugins (OLIVE 4.x Compatible) OLIVE Version Plugin Description OLIVE 4.12+ kws-dynapy-v1 DynaPy-based keyword spotting plugin improving the overall KWS system infrastructure and performance. Description Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by text input from the user at score time. It is based on automatic speech recognition (ASR) technology, featuring speech-to-text transcription and language modeling. This means that every KWS domain is language dependent, and that keywords can only be detected if they exist in the underlying ASR system's dictionary, making out-of-vocabulary keywords a potential problem. Inputs An audio file or buffer and a list of desired keywords or keyphrases to detect. Outputs When one or more of the enrolled keywords has been detected in the submitted audio, KWS returns a region or list of timestamped regions, each with a score for the keyword that has been detected. Enrollments KWS plugins do not support enrollments; instead, the set of classes the plugin is searching for is provided as a list of text keywords with each scoring request. Note that each domain is language dependent, and that a word or phrase in a language other than the one the domain is trained in is likely to be out-of-vocabulary. This means the word or phrase will be difficult or impossible to recall. For details on how to set or pass these keywords, please refer to the appropriate sections within the OLIVE CLI User Guide or OLIVE API Documentation. Limitations As was previously mentioned, traditional KWS relies on an underlying ASR system, making each KWS domain completely language dependent. This places several real restrictions on the users. First, this means that for keywords to be detectable, the words must be part of the ASR system's dictionary. This may make it difficult to find some keywords or phrases, like names, brands, slang or other colloquialisms, if they are out-of-vocabulary. This also makes it more difficult to deal with speakers or situations that may involve code switching. This reliance on ASR also makes KWS plugins quite heavy with respect to resource requirements, and also quite slow compared to other plugin types. An additional limitation stemming from the language dependence of the system is that if a user would like to detect keywords in a new language that isn't currently offered by SRI, a new domain for that language would need to be created, which requires a large amount of transcribed audio in that language. Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Keyword Spotting (KWS)"},{"location":"plugins/kws.html#keyword-spotting-kws","text":"","title":"Keyword Spotting (KWS)"},{"location":"plugins/kws.html#released-plugins","text":"All of our current QBE plugins match the description on this page. Refer to the list below for the currently supported Speaker Detection plugins.","title":"Released Plugins"},{"location":"plugins/kws.html#legacy-plugins-olive-4x-compatible","text":"OLIVE Version Plugin Description OLIVE 4.12+ kws-dynapy-v1 DynaPy-based keyword spotting plugin improving the overall KWS system infrastructure and performance.","title":"Legacy Plugins (OLIVE 4.x Compatible)"},{"location":"plugins/kws.html#description","text":"Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by text input from the user at score time. It is based on automatic speech recognition (ASR) technology, featuring speech-to-text transcription and language modeling. This means that every KWS domain is language dependent, and that keywords can only be detected if they exist in the underlying ASR system's dictionary, making out-of-vocabulary keywords a potential problem.","title":"Description"},{"location":"plugins/kws.html#inputs","text":"An audio file or buffer and a list of desired keywords or keyphrases to detect.","title":"Inputs"},{"location":"plugins/kws.html#outputs","text":"When one or more of the enrolled keywords has been detected in the submitted audio, KWS returns a region or list of timestamped regions, each with a score for the keyword that has been detected.","title":"Outputs"},{"location":"plugins/kws.html#enrollments","text":"KWS plugins do not support enrollments; instead, the set of classes the plugin is searching for is provided as a list of text keywords with each scoring request. Note that each domain is language dependent, and that a word or phrase in a language other than the one the domain is trained in is likely to be out-of-vocabulary. This means the word or phrase will be difficult or impossible to recall. For details on how to set or pass these keywords, please refer to the appropriate sections within the OLIVE CLI User Guide or OLIVE API Documentation.","title":"Enrollments"},{"location":"plugins/kws.html#limitations","text":"As was previously mentioned, traditional KWS relies on an underlying ASR system, making each KWS domain completely language dependent. This places several real restrictions on the users. First, this means that for keywords to be detectable, the words must be part of the ASR system's dictionary. This may make it difficult to find some keywords or phrases, like names, brands, slang or other colloquialisms, if they are out-of-vocabulary. This also makes it more difficult to deal with speakers or situations that may involve code switching. This reliance on ASR also makes KWS plugins quite heavy with respect to resource requirements, and also quite slow compared to other plugin types. An additional limitation stemming from the language dependence of the system is that if a user would like to detect keywords in a new language that isn't currently offered by SRI, a new domain for that language would need to be created, which requires a large amount of transcribed audio in that language.","title":"Limitations"},{"location":"plugins/kws.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/ldd-embedplda-v1.html","text":"ldd-embedplda-v1 (Language Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0, shares language models and classes with lid-embedplda-v3 v1.0.1 Minor update to add an error message if user tries to unenroll one of the default classes. Released with OLIVE 5.3.0 Description Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one or more of the enrolled languages is found. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 French 11.40590000 input-audio.wav 43.500 77.500 French 11.29558277 input-audio.wav 77.500 78.500 Modern Standard Arabic 2.63369179 input-audio.wav 78.500 80.500 Iraqi Arabic 2.25519705 input-audio.wav 85.500 86.500 French 2.06612849 input-audio.wav 97.500 98.500 French 3.74665093 input-audio.wav 98.500 99.500 Mandarin Chinese 2.22936487 input-audio.wav 105.500 106.500 Spanish 2.72254372 input-audio.wav 107.500 108.500 French 2.60355234 input-audio.wav 108.500 110.500 French 2.76414633 input-audio.wav 109.500 113.140 English 2.85003138 input-audio.wav 113.760 116.260 French 2.50716114 input-audio.wav 120.260 140.260 Korean 14.93032360 input-audio.wav 143.260 157.260 Korean 12.62243176 input-audio.wav 158.260 161.260 Mandarin Chinese 3.24917603 input-audio.wav 161.260 162.260 French 2.73345900 input-audio.wav 165.260 177.260 Korean 12.32051945 input-audio.wav 178.260 180.260 Iraqi Arabic 2.41706276 input-audio.wav 186.320 188.820 Iraqi Arabic 2.85040617 input-audio.wav 193.820 194.820 Spanish 2.21501803 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech and false alarms. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Comments Language/Dialect Detection Granularity LDD plugins attempt to distinguish dialects (ie., Tunisian Arabic and Egyptian Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editting domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Segmentation By Classification Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to win_sec min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 max_class_per_frame Determines whether the plugin is forced to output a proposed language for each speech region, even if the top scoring language is below the detection threshold True True or False max_class_per_file Determines the maximum number of languages the plugin will consider possible to detect in a single file/scoring request. This parameter helps control short, spurious false detections. If the likely number of languages is known a priori, setting this value close to that can help boost performance. If it is set too low, however, you may hinder the plugin's performance, since it may become impossible for the plugin to be correct. If there are 3 languages being spoken in a file, for example, and this is set to 2, then it is impossible for the plugin to detect and output all three languages. 3 2 - 6 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0 Additional Option Notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels","title":"Language Detection (LDD)"},{"location":"plugins/ldd-embedplda-v1.html#ldd-embedplda-v1-language-detection","text":"","title":"ldd-embedplda-v1 (Language Detection)"},{"location":"plugins/ldd-embedplda-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0, shares language models and classes with lid-embedplda-v3 v1.0.1 Minor update to add an error message if user tries to unenroll one of the default classes. Released with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/ldd-embedplda-v1.html#description","text":"Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one or more of the enrolled languages is found. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API.","title":"Description"},{"location":"plugins/ldd-embedplda-v1.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/ldd-embedplda-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/ldd-embedplda-v1.html#outputs","text":"LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 French 11.40590000 input-audio.wav 43.500 77.500 French 11.29558277 input-audio.wav 77.500 78.500 Modern Standard Arabic 2.63369179 input-audio.wav 78.500 80.500 Iraqi Arabic 2.25519705 input-audio.wav 85.500 86.500 French 2.06612849 input-audio.wav 97.500 98.500 French 3.74665093 input-audio.wav 98.500 99.500 Mandarin Chinese 2.22936487 input-audio.wav 105.500 106.500 Spanish 2.72254372 input-audio.wav 107.500 108.500 French 2.60355234 input-audio.wav 108.500 110.500 French 2.76414633 input-audio.wav 109.500 113.140 English 2.85003138 input-audio.wav 113.760 116.260 French 2.50716114 input-audio.wav 120.260 140.260 Korean 14.93032360 input-audio.wav 143.260 157.260 Korean 12.62243176 input-audio.wav 158.260 161.260 Mandarin Chinese 3.24917603 input-audio.wav 161.260 162.260 French 2.73345900 input-audio.wav 165.260 177.260 Korean 12.32051945 input-audio.wav 178.260 180.260 Iraqi Arabic 2.41706276 input-audio.wav 186.320 188.820 Iraqi Arabic 2.85040617 input-audio.wav 193.820 194.820 Spanish 2.21501803","title":"Outputs"},{"location":"plugins/ldd-embedplda-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/ldd-embedplda-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/ldd-embedplda-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/ldd-embedplda-v1.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/ldd-embedplda-v1.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/ldd-embedplda-v1.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech and false alarms. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/ldd-embedplda-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/ldd-embedplda-v1.html#languagedialect-detection-granularity","text":"LDD plugins attempt to distinguish dialects (ie., Tunisian Arabic and Egyptian Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/ldd-embedplda-v1.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/ldd-embedplda-v1.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese","title":"Supported Languages"},{"location":"plugins/ldd-embedplda-v1.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language.","title":"Enrollments"},{"location":"plugins/ldd-embedplda-v1.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editting domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line.","title":"Configuring Languages"},{"location":"plugins/ldd-embedplda-v1.html#segmentation-by-classification","text":"Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/ldd-embedplda-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to win_sec min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 max_class_per_frame Determines whether the plugin is forced to output a proposed language for each speech region, even if the top scoring language is below the detection threshold True True or False max_class_per_file Determines the maximum number of languages the plugin will consider possible to detect in a single file/scoring request. This parameter helps control short, spurious false detections. If the likely number of languages is known a priori, setting this value close to that can help boost performance. If it is set too low, however, you may hinder the plugin's performance, since it may become impossible for the plugin to be correct. If there are 3 languages being spoken in a file, for example, and this is set to 2, then it is impossible for the plugin to detect and output all three languages. 3 2 - 6 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/ldd-embedplda-v1.html#additional-option-notes","text":"","title":"Additional Option Notes"},{"location":"plugins/ldd-embedplda-v1.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/ldd-embedplda-v1.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels","title":"win_sec and step_sec"},{"location":"plugins/ldd-embedpldaSmolive-v1.html","text":"ldd-embedpldaSmolive-v1 (Language Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of ldd-embedplda-v1 but features model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate. Description This plugin is based heavily on its predecessor, ldd-embedplda-v1, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one or more of the enrolled languages is found. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API. Domains multi-int8-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 French 11.40590000 input-audio.wav 43.500 77.500 French 11.29558277 input-audio.wav 77.500 78.500 Modern Standard Arabic 2.63369179 input-audio.wav 78.500 80.500 Iraqi Arabic 2.25519705 input-audio.wav 85.500 86.500 French 2.06612849 input-audio.wav 97.500 98.500 French 3.74665093 input-audio.wav 98.500 99.500 Mandarin Chinese 2.22936487 input-audio.wav 105.500 106.500 Spanish 2.72254372 input-audio.wav 107.500 108.500 French 2.60355234 input-audio.wav 108.500 110.500 French 2.76414633 input-audio.wav 109.500 113.140 English 2.85003138 input-audio.wav 113.760 116.260 French 2.50716114 input-audio.wav 120.260 140.260 Korean 14.93032360 input-audio.wav 143.260 157.260 Korean 12.62243176 input-audio.wav 158.260 161.260 Mandarin Chinese 3.24917603 input-audio.wav 161.260 162.260 French 2.73345900 input-audio.wav 165.260 177.260 Korean 12.32051945 input-audio.wav 178.260 180.260 Iraqi Arabic 2.41706276 input-audio.wav 186.320 188.820 Iraqi Arabic 2.85040617 input-audio.wav 193.820 194.820 Spanish 2.21501803 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.4+ Limitations Known or potential limitations of the plugin are outlined below. Quantized Model Hardware Compatibility There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech and false alarms. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Comments Language/Dialect Detection Granularity LDD plugins attempt to distinguish dialects (ie., Tunisian Arabic and Egyptian Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editting domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Segmentation By Classification Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to win_sec min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 max_class_per_frame Determines whether the plugin is forced to output a proposed language for each speech region, even if the top scoring language is below the detection threshold True True or False max_class_per_file Determines the maximum number of languages the plugin will consider possible to detect in a single file/scoring request. This parameter helps control short, spurious false detections. If the likely number of languages is known a priori, setting this value close to that can help boost performance. If it is set too low, however, you may hinder the plugin's performance, since it may become impossible for the plugin to be correct. If there are 3 languages being spoken in a file, for example, and this is set to 2, then it is impossible for the plugin to detect and output all three languages. 3 2 - 6 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0 Additional Option Notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels","title":"Lanugage Detection (LDD) Low Resource"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#ldd-embedpldasmolive-v1-language-detection","text":"","title":"ldd-embedpldaSmolive-v1 (Language Detection)"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of ldd-embedplda-v1 but features model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate.","title":"Version Changelog"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#description","text":"This plugin is based heavily on its predecessor, ldd-embedplda-v1, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one or more of the enrolled languages is found. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API.","title":"Description"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#domains","text":"multi-int8-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#outputs","text":"LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than the default threshold of \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 French 11.40590000 input-audio.wav 43.500 77.500 French 11.29558277 input-audio.wav 77.500 78.500 Modern Standard Arabic 2.63369179 input-audio.wav 78.500 80.500 Iraqi Arabic 2.25519705 input-audio.wav 85.500 86.500 French 2.06612849 input-audio.wav 97.500 98.500 French 3.74665093 input-audio.wav 98.500 99.500 Mandarin Chinese 2.22936487 input-audio.wav 105.500 106.500 Spanish 2.72254372 input-audio.wav 107.500 108.500 French 2.60355234 input-audio.wav 108.500 110.500 French 2.76414633 input-audio.wav 109.500 113.140 English 2.85003138 input-audio.wav 113.760 116.260 French 2.50716114 input-audio.wav 120.260 140.260 Korean 14.93032360 input-audio.wav 143.260 157.260 Korean 12.62243176 input-audio.wav 158.260 161.260 Mandarin Chinese 3.24917603 input-audio.wav 161.260 162.260 French 2.73345900 input-audio.wav 165.260 177.260 Korean 12.32051945 input-audio.wav 178.260 180.260 Iraqi Arabic 2.41706276 input-audio.wav 186.320 188.820 Iraqi Arabic 2.85040617 input-audio.wav 193.820 194.820 Spanish 2.21501803","title":"Outputs"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#quantized-model-hardware-compatibility","text":"There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function.","title":"Quantized Model Hardware Compatibility"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech and false alarms. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#languagedialect-detection-granularity","text":"LDD plugins attempt to distinguish dialects (ie., Tunisian Arabic and Egyptian Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese","title":"Supported Languages"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language.","title":"Enrollments"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editting domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line.","title":"Configuring Languages"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#segmentation-by-classification","text":"Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to win_sec min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 max_class_per_frame Determines whether the plugin is forced to output a proposed language for each speech region, even if the top scoring language is below the detection threshold True True or False max_class_per_file Determines the maximum number of languages the plugin will consider possible to detect in a single file/scoring request. This parameter helps control short, spurious false detections. If the likely number of languages is known a priori, setting this value close to that can help boost performance. If it is set too low, however, you may hinder the plugin's performance, since it may become impossible for the plugin to be correct. If there are 3 languages being spoken in a file, for example, and this is set to 2, then it is impossible for the plugin to detect and output all three languages. 3 2 - 6 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#additional-option-notes","text":"","title":"Additional Option Notes"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/ldd-embedpldaSmolive-v1.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels","title":"win_sec and step_sec"},{"location":"plugins/ldd-sbcEmbed-v1.html","text":"ldd-sbcEmbed-v1 (Language Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one of the enrolled languages is found. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. The table below displays the list of currently available languages and dialects from SRI. Individual plugin pages will cover the languages actually available in each plugin and/or domain. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 10 languages configured (optionally configurable to up to 63 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 fre 11.40590000 input-audio.wav 43.500 77.500 fre 11.29558277 input-audio.wav 77.500 78.500 arz 2.63369179 input-audio.wav 78.500 80.500 arb 2.25519705 input-audio.wav 85.500 86.500 arz 2.06612849 input-audio.wav 97.500 98.500 fas 3.74665093 input-audio.wav 98.500 99.500 arz 2.22936487 input-audio.wav 105.500 106.500 spa 2.72254372 input-audio.wav 107.500 108.500 fas 2.60355234 input-audio.wav 108.500 110.500 eng 2.76414633 input-audio.wav 109.500 113.140 eng 2.85003138 input-audio.wav 113.760 116.260 arz 2.50716114 input-audio.wav 120.260 140.260 kor 14.93032360 input-audio.wav 143.260 157.260 kor 12.62243176 input-audio.wav 158.260 161.260 cmn 3.24917603 input-audio.wav 161.260 162.260 jpn 2.73345900 input-audio.wav 165.260 177.260 kor 12.32051945 input-audio.wav 178.260 180.260 arz 2.41706276 input-audio.wav 186.320 188.820 arz 2.85040617 input-audio.wav 193.820 194.820 spa 2.21501803 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Comments Language/Dialect Detection Granularity LID plugins that are capable of dialect detection typically include functionality to fall back to the base language class in the case of limited confidence. This is typically done by outputting scores for all dialects (i.e. ara-arz, ara-apc, and ara-arb) as well as the base language (i.e. ara). Note that any language with dialect information does not have the base class enrolled, but this is determined from the maximum of the dialect detectors for the base language available within the plugin (whether exposed or not). In the case that a dialect score is sufficiently high, the base language score will be set to 0.001 lower than the highest-scoring dialect, and otherwise the base class is set to 0.001 higher than the highest-scoring dialect score. In this way, labelling the audio sample based on the maximum scoring will indicate a specific dialect if confident, and otherwise the base language. This default mode is defined as BASEAPPEND. There are two alternate modes available that can optionally be set: BASEAPPEND - Default behavior, described above. BASEONLY \u2013 Output only base language scores formed by the maximum of the dialect-specific scores for a given base language. STANDARD \u2013 Output scores based on enrolled classes without producing a base language summarization for dialect-compatible detectors. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Language Name amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French jpn Japanese kor Korean pus Pashto rus Russian spa Spanish tgl Tagolog tha Thai tur Turkish urd Urdu vie Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Language Name alb Albanian amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic aze Azerbaijani bel Belorussian ben Bengali bos Bosnian bul Bulgarian cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French geo Georgian ger German gre Greek hau Hausa hrv Croatian ind Indonesian ita Italian jpn Japanese khm Khmer kor Korean mac Macedonian mya Burmese nde Ndebele orm Oromo pan Punjabi pol Polish por Portuguese prs Dari pus Pashto ron Romanian rus Russian sna Shona som Somali spa Spanish srp Serbian swa Swahili tam Tamil tgl Tagalog tha Thai tib Tibetan tir Tigrinya tur Turkish ukr Ukranian urd Urdu uzb Uzbek vie Vietnamese Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut), and becomes usable when sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can augment these existing languages using their own data by enrolling audio with the same label as an existing language. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder using the command line interface and calling $ ./configure_languages.py to get all languages or $ ./configure_languages.py lang1,lang2,\u2026,langN for a subset of available languages. Please note that running ./configure_languages.py without any arguments should be done with extreme care. This will enable all languages and dialects in the domain; including those that were included solely for their utility in score calibration, that may not have enough training data to create a model that acts as a reliable detector. Enabling all languages may adversely affect the plugin\u2019s performance. Some plugins may support adjusting the language detection granularity discussed above, though this is for advanced users only. An example of changing this setting using the configure_langauges,py script is $ ./configure_languages.py lang1,lang2,...,langN BASEONLY Where the options for this setting are discussed above, if supported. This will be discussed on the individual plugin page if supported. Segmentation By Classification Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 1.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all languages scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring language (value= True ). False True or False Additional Option Notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels output_only_highest_scoring_detected_language The boolean output_only_highest_scoring_detected_language parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_language is set to False, the plugin will report all the languages above the threshold for a given segment. However, if output_only_highest_scoring_detected_language is set as True, the plugin will report only the language with the maximum score for a given segment even when multiple languages have scores above the threshold. An example of this behavior distance follows. If we have a segment (S) with scores for three different languages previously enrolled, S lang1 10.8 S lang2 8.2 S lang3 3.1 and the threshold is 5.0, then with output_only_highest_scoring_detected_language = True, the system reports: S lang1 10.8 However, with output_only_highest_scoring_detected_language = False, the system reports: S lang1 10.8 S lang2 8.2 The default behavior of this plugin is to have this parameter set to False and to report all language detections over the detection threshold for each region.","title":"ldd-sbcEmbed-v1 (Language Detection)"},{"location":"plugins/ldd-sbcEmbed-v1.html#ldd-sbcembed-v1-language-detection","text":"","title":"ldd-sbcEmbed-v1 (Language Detection)"},{"location":"plugins/ldd-sbcEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/ldd-sbcEmbed-v1.html#description","text":"Language Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled languages are detected being spoken. This is in contrast to Language Identification (LID) plugins, which label the entire segment with a single language. So, unlike Language Identification (LID), LDD is capable of handling audio where multiple languages are being spoken, and will provide timestamp region labels to point to the locations when speech from one of the enrolled languages is found. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. The table below displays the list of currently available languages and dialects from SRI. Individual plugin pages will cover the languages actually available in each plugin and/or domain. The goal of language detection is to find and label the regions in an audio file where languages of interest are being spoken. This capability is designed to be used in files where two or more languages are present. For files where it is certain that only one language will be present, either because it is collected this way or because a human has segmented the file, language recognition (LID) plugins should be used. This release of language detection is based on \"segmentation-by-classification\", an approach in which the enrolled languages are detected using a sliding and overlapping window over the file. The plugin is based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration - it shares underlying language models and architecture with the lid-embedplda-v2 plugin. The LDD plugin was created for clean telephone or microphone data. This plugin has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API.","title":"Description"},{"location":"plugins/ldd-sbcEmbed-v1.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 10 languages configured (optionally configurable to up to 63 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/ldd-sbcEmbed-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/ldd-sbcEmbed-v1.html#outputs","text":"LDD plugins return a list of regions with a score for each detected language. The starting and stopping boundaries are denoted in seconds. As with LID, scores are log-likelihood ratios, where a score greater than \"0\" is considered to be a detection. An example output excerpt: input-audio.wav 0.000 41.500 fre 11.40590000 input-audio.wav 43.500 77.500 fre 11.29558277 input-audio.wav 77.500 78.500 arz 2.63369179 input-audio.wav 78.500 80.500 arb 2.25519705 input-audio.wav 85.500 86.500 arz 2.06612849 input-audio.wav 97.500 98.500 fas 3.74665093 input-audio.wav 98.500 99.500 arz 2.22936487 input-audio.wav 105.500 106.500 spa 2.72254372 input-audio.wav 107.500 108.500 fas 2.60355234 input-audio.wav 108.500 110.500 eng 2.76414633 input-audio.wav 109.500 113.140 eng 2.85003138 input-audio.wav 113.760 116.260 arz 2.50716114 input-audio.wav 120.260 140.260 kor 14.93032360 input-audio.wav 143.260 157.260 kor 12.62243176 input-audio.wav 158.260 161.260 cmn 3.24917603 input-audio.wav 161.260 162.260 jpn 2.73345900 input-audio.wav 165.260 177.260 kor 12.32051945 input-audio.wav 178.260 180.260 arz 2.41706276 input-audio.wav 186.320 188.820 arz 2.85040617 input-audio.wav 193.820 194.820 spa 2.21501803","title":"Outputs"},{"location":"plugins/ldd-sbcEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio where each region includes a detected language of interest and corresponding score for this language RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/ldd-sbcEmbed-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/ldd-sbcEmbed-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/ldd-sbcEmbed-v1.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/ldd-sbcEmbed-v1.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/ldd-sbcEmbed-v1.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length (win_sec) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling language regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower language labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/ldd-sbcEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/ldd-sbcEmbed-v1.html#languagedialect-detection-granularity","text":"LID plugins that are capable of dialect detection typically include functionality to fall back to the base language class in the case of limited confidence. This is typically done by outputting scores for all dialects (i.e. ara-arz, ara-apc, and ara-arb) as well as the base language (i.e. ara). Note that any language with dialect information does not have the base class enrolled, but this is determined from the maximum of the dialect detectors for the base language available within the plugin (whether exposed or not). In the case that a dialect score is sufficiently high, the base language score will be set to 0.001 lower than the highest-scoring dialect, and otherwise the base class is set to 0.001 higher than the highest-scoring dialect score. In this way, labelling the audio sample based on the maximum scoring will indicate a specific dialect if confident, and otherwise the base language. This default mode is defined as BASEAPPEND. There are two alternate modes available that can optionally be set: BASEAPPEND - Default behavior, described above. BASEONLY \u2013 Output only base language scores formed by the maximum of the dialect-specific scores for a given base language. STANDARD \u2013 Output scores based on enrolled classes without producing a base language summarization for dialect-compatible detectors.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/ldd-sbcEmbed-v1.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Language Name amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French jpn Japanese kor Korean pus Pashto rus Russian spa Spanish tgl Tagolog tha Thai tur Turkish urd Urdu vie Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/ldd-sbcEmbed-v1.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Language Name alb Albanian amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic aze Azerbaijani bel Belorussian ben Bengali bos Bosnian bul Bulgarian cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French geo Georgian ger German gre Greek hau Hausa hrv Croatian ind Indonesian ita Italian jpn Japanese khm Khmer kor Korean mac Macedonian mya Burmese nde Ndebele orm Oromo pan Punjabi pol Polish por Portuguese prs Dari pus Pashto ron Romanian rus Russian sna Shona som Somali spa Spanish srp Serbian swa Swahili tam Tamil tgl Tagalog tha Thai tib Tibetan tir Tigrinya tur Turkish ukr Ukranian urd Urdu uzb Uzbek vie Vietnamese","title":"Supported Languages"},{"location":"plugins/ldd-sbcEmbed-v1.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut), and becomes usable when sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can augment these existing languages using their own data by enrolling audio with the same label as an existing language.","title":"Enrollments"},{"location":"plugins/ldd-sbcEmbed-v1.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder using the command line interface and calling $ ./configure_languages.py to get all languages or $ ./configure_languages.py lang1,lang2,\u2026,langN for a subset of available languages. Please note that running ./configure_languages.py without any arguments should be done with extreme care. This will enable all languages and dialects in the domain; including those that were included solely for their utility in score calibration, that may not have enough training data to create a model that acts as a reliable detector. Enabling all languages may adversely affect the plugin\u2019s performance. Some plugins may support adjusting the language detection granularity discussed above, though this is for advanced users only. An example of changing this setting using the configure_langauges,py script is $ ./configure_languages.py lang1,lang2,...,langN BASEONLY Where the options for this setting are discussed above, if supported. This will be discussed on the individual plugin page if supported.","title":"Configuring Languages"},{"location":"plugins/ldd-sbcEmbed-v1.html#segmentation-by-classification","text":"Live, multi-language conversational speech is a very challenging domain due to its high variability and conditions. Rather than exhaustively segment a file to identify pure regions with a single language , SBC scans through the file quickly using target language embeddings to find regions that are likely to be from a language of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection. This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech, default 2 seconds) is then processed to determine the likelihood of containing a language of interest. Speech regions of up to X seconds (configurable as win_sec, default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/win_sec and step size/step_sec) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/ldd-sbcEmbed-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py. Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by language recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 1.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled languages. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all languages scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring language (value= True ). False True or False","title":"Global Options"},{"location":"plugins/ldd-sbcEmbed-v1.html#additional-option-notes","text":"","title":"Additional Option Notes"},{"location":"plugins/ldd-sbcEmbed-v1.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled languages. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct languages. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the language-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/ldd-sbcEmbed-v1.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between languages, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting language boundary labels","title":"win_sec and step_sec"},{"location":"plugins/ldd-sbcEmbed-v1.html#output_only_highest_scoring_detected_language","text":"The boolean output_only_highest_scoring_detected_language parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_language is set to False, the plugin will report all the languages above the threshold for a given segment. However, if output_only_highest_scoring_detected_language is set as True, the plugin will report only the language with the maximum score for a given segment even when multiple languages have scores above the threshold. An example of this behavior distance follows. If we have a segment (S) with scores for three different languages previously enrolled, S lang1 10.8 S lang2 8.2 S lang3 3.1 and the threshold is 5.0, then with output_only_highest_scoring_detected_language = True, the system reports: S lang1 10.8 However, with output_only_highest_scoring_detected_language = False, the system reports: S lang1 10.8 S lang2 8.2 The default behavior of this plugin is to have this parameter set to False and to report all language detections over the detection threshold for each region.","title":"output_only_highest_scoring_detected_language"},{"location":"plugins/ldd.html","text":"redirect: plugins/ldd-embedplda-v1.md","title":"Ldd"},{"location":"plugins/ldd.html#redirect-pluginsldd-embedplda-v1md","text":"","title":"redirect: plugins/ldd-embedplda-v1.md"},{"location":"plugins/lid-embedplda-v2.html","text":"lid-embedplda-v2 (Language Identification) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description LID plugins detect one or more language or dialect classes in an audio segment as a global score. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration. This plug-in has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 10 languages configured (optionally configurable to up to 63 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav amh -19.9012123573 input-audio.wav ara -15.8882738579 input-audio.wav cmn -15.5530382622 input-audio.wav eng -14.1870705116 input-audio.wav fas -17.3224474419 input-audio.wav fre 10.1847232353 input-audio.wav hau -15.1134468544 input-audio.wav jpn -21.0655495155 input-audio.wav kor -18.3601671684 input-audio.wav pus -16.2738787163 input-audio.wav rus -10.4046117294 input-audio.wav spa -18.1588427055 input-audio.wav tur -14.0825478065 input-audio.wav urd -20.4127785194 input-audio.wav vie -18.552107476 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Comments Language/Dialect Detection Granularity LID plugins that are capable of dialect detection typically include functionality to fall back to the base language class in the case of limited confidence. This is typically done by outputting scores for all dialects (i.e. ara-arz, ara-apc, and ara-arb) as well as the base language (i.e. ara). Note that any language with dialect information does not have the base class enrolled, but this is determined from the maximum of the dialect detectors for the base language available within the plugin (whether exposed or not). In the case that a dialect score is sufficiently high, the base language score will be set to 0.001 lower than the highest-scoring dialect, and otherwise the base class is set to 0.001 higher than the highest-scoring dialect score. In this way, labelling the audio sample based on the maximum scoring will indicate a specific dialect if confident, and otherwise the base language. This default mode is defined as BASEAPPEND. There are two alternate modes available that can optionally be set: BASEAPPEND - Default behavior, described above. BASEONLY \u2013 Output only base language scores formed by the maximum of the dialect-specific scores for a given base language. STANDARD \u2013 Output scores based on enrolled classes without producing a base language summarization for dialect-compatible detectors. Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut), and becomes usable when sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can augment these existing languages using their own data by enrolling audio with the same label as an existing language. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder using the command line interface and calling $ ./configure_languages.py to get all languages or $ ./configure_languages.py lang1,lang2,\u2026,langN for a subset of available languages. Please note that running ./configure_languages.py without any arguments should be done with extreme care. This will enable all languages and dialects in the domain; including those that were included solely for their utility in score calibration, that may not have enough training data to create a model that acts as a reliable detector. Enabling all languages may adversely affect the plugin\u2019s performance. This plugin supports adjusting the language detection granularity discussed above, though this is for advanced users only. An example of changing this setting using the configure_langauges,py script is $ ./configure_languages.py lang1,lang2,...,langN BASEONLY Where the options for this setting are discussed above, if supported. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Language Name amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French jpn Japanese kor Korean pus Pashto rus Russian spa Spanish tgl Tagolog tha Thai tur Turkish urd Urdu vie Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Language Name alb Albanian amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic aze Azerbaijani bel Belorussian ben Bengali bos Bosnian bul Bulgarian cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French geo Georgian ger German gre Greek hau Hausa hrv Croatian ind Indonesian ita Italian jpn Japanese khm Khmer kor Korean mac Macedonian mya Burmese nde Ndebele orm Oromo pan Punjabi pol Polish por Portuguese prs Dari pus Pashto ron Romanian rus Russian sna Shona som Somali spa Spanish srp Serbian swa Swahili tam Tamil tgl Tagalog tha Thai tib Tibetan tir Tigrinya tur Turkish ukr Ukranian urd Urdu uzb Uzbek vie Vietnamese Global Options This plugin does not feature user-configurable option parameters. It does, however, offer configurable language models and language-reporting granularity. For details, refer here .","title":"lid-embedplda-v2 (Language Identification)"},{"location":"plugins/lid-embedplda-v2.html#lid-embedplda-v2-language-identification","text":"","title":"lid-embedplda-v2 (Language Identification)"},{"location":"plugins/lid-embedplda-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/lid-embedplda-v2.html#description","text":"LID plugins detect one or more language or dialect classes in an audio segment as a global score. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and duration-aware calibration. This plug-in has been reconfigured to allow enrollment and addition of new classes. Unsupervised adaptation through target mean normalization, and supervised PLDA and calibration updates from enrollments have been implemented via the update function. These updates must be invoked by the user via the API.","title":"Description"},{"location":"plugins/lid-embedplda-v2.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 10 languages configured (optionally configurable to up to 63 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/lid-embedplda-v2.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/lid-embedplda-v2.html#outputs","text":"Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav amh -19.9012123573 input-audio.wav ara -15.8882738579 input-audio.wav cmn -15.5530382622 input-audio.wav eng -14.1870705116 input-audio.wav fas -17.3224474419 input-audio.wav fre 10.1847232353 input-audio.wav hau -15.1134468544 input-audio.wav jpn -21.0655495155 input-audio.wav kor -18.3601671684 input-audio.wav pus -16.2738787163 input-audio.wav rus -10.4046117294 input-audio.wav spa -18.1588427055 input-audio.wav tur -14.0825478065 input-audio.wav urd -20.4127785194 input-audio.wav vie -18.552107476","title":"Outputs"},{"location":"plugins/lid-embedplda-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/lid-embedplda-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/lid-embedplda-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile.","title":"Limitations"},{"location":"plugins/lid-embedplda-v2.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/lid-embedplda-v2.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/lid-embedplda-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/lid-embedplda-v2.html#languagedialect-detection-granularity","text":"LID plugins that are capable of dialect detection typically include functionality to fall back to the base language class in the case of limited confidence. This is typically done by outputting scores for all dialects (i.e. ara-arz, ara-apc, and ara-arb) as well as the base language (i.e. ara). Note that any language with dialect information does not have the base class enrolled, but this is determined from the maximum of the dialect detectors for the base language available within the plugin (whether exposed or not). In the case that a dialect score is sufficiently high, the base language score will be set to 0.001 lower than the highest-scoring dialect, and otherwise the base class is set to 0.001 higher than the highest-scoring dialect score. In this way, labelling the audio sample based on the maximum scoring will indicate a specific dialect if confident, and otherwise the base language. This default mode is defined as BASEAPPEND. There are two alternate modes available that can optionally be set: BASEAPPEND - Default behavior, described above. BASEONLY \u2013 Output only base language scores formed by the maximum of the dialect-specific scores for a given base language. STANDARD \u2013 Output scores based on enrolled classes without producing a base language summarization for dialect-compatible detectors.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/lid-embedplda-v2.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut), and becomes usable when sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can augment these existing languages using their own data by enrolling audio with the same label as an existing language.","title":"Enrollments"},{"location":"plugins/lid-embedplda-v2.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder using the command line interface and calling $ ./configure_languages.py to get all languages or $ ./configure_languages.py lang1,lang2,\u2026,langN for a subset of available languages. Please note that running ./configure_languages.py without any arguments should be done with extreme care. This will enable all languages and dialects in the domain; including those that were included solely for their utility in score calibration, that may not have enough training data to create a model that acts as a reliable detector. Enabling all languages may adversely affect the plugin\u2019s performance. This plugin supports adjusting the language detection granularity discussed above, though this is for advanced users only. An example of changing this setting using the configure_langauges,py script is $ ./configure_languages.py lang1,lang2,...,langN BASEONLY Where the options for this setting are discussed above, if supported.","title":"Configuring Languages"},{"location":"plugins/lid-embedplda-v2.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Language Name amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French jpn Japanese kor Korean pus Pashto rus Russian spa Spanish tgl Tagolog tha Thai tur Turkish urd Urdu vie Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/lid-embedplda-v2.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Language Name alb Albanian amh Amharic arz Egyptian Arabic apc North Levantine Arabic arb Modern Standard Arabic aze Azerbaijani bel Belorussian ben Bengali bos Bosnian bul Bulgarian cmn Mandarin Chinese yue Yue Chinese eng English fas Farsi fre French geo Georgian ger German gre Greek hau Hausa hrv Croatian ind Indonesian ita Italian jpn Japanese khm Khmer kor Korean mac Macedonian mya Burmese nde Ndebele orm Oromo pan Punjabi pol Polish por Portuguese prs Dari pus Pashto ron Romanian rus Russian sna Shona som Somali spa Spanish srp Serbian swa Swahili tam Tamil tgl Tagalog tha Thai tib Tibetan tir Tigrinya tur Turkish ukr Ukranian urd Urdu uzb Uzbek vie Vietnamese","title":"Supported Languages"},{"location":"plugins/lid-embedplda-v2.html#global-options","text":"This plugin does not feature user-configurable option parameters. It does, however, offer configurable language models and language-reporting granularity. For details, refer here .","title":"Global Options"},{"location":"plugins/lid-embedplda-v3.html","text":"lid-embedplda-v3 (Language Identification) Version Changelog Plugin Version Change v3.0.0 Initial plugin release, updated from v2.0.0 with new, improved models, released with OLIVE 5.2.0 v3.0.1 Minor update to add an error message if user tries to unenroll one of the default classes. Released with OLIVE 5.3.0 Description LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. This plug-in has been configured to allow enrollment for the augmentation of existing classes and the addition of new classes. The backend and calibration modules are rapidly retrained with the addition of enrollment data. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or replace/augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.2+ Limitations Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Comments Language/Dialect Detection Granularity LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load. Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language. Note that LID enrollments are not independent. This means that the more languages the user enrolls from their domain, the better a given language they enroll will be. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese Global Options This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"lid-embedplda-v3 (Language Identification)"},{"location":"plugins/lid-embedplda-v3.html#lid-embedplda-v3-language-identification","text":"","title":"lid-embedplda-v3 (Language Identification)"},{"location":"plugins/lid-embedplda-v3.html#version-changelog","text":"Plugin Version Change v3.0.0 Initial plugin release, updated from v2.0.0 with new, improved models, released with OLIVE 5.2.0 v3.0.1 Minor update to add an error message if user tries to unenroll one of the default classes. Released with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/lid-embedplda-v3.html#description","text":"LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. This plug-in has been configured to allow enrollment for the augmentation of existing classes and the addition of new classes. The backend and calibration modules are rapidly retrained with the addition of enrollment data.","title":"Description"},{"location":"plugins/lid-embedplda-v3.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/lid-embedplda-v3.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/lid-embedplda-v3.html#outputs","text":"Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102","title":"Outputs"},{"location":"plugins/lid-embedplda-v3.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or replace/augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/lid-embedplda-v3.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/lid-embedplda-v3.html#limitations","text":"Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile.","title":"Limitations"},{"location":"plugins/lid-embedplda-v3.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/lid-embedplda-v3.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/lid-embedplda-v3.html#comments","text":"","title":"Comments"},{"location":"plugins/lid-embedplda-v3.html#languagedialect-detection-granularity","text":"LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/lid-embedplda-v3.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language. Note that LID enrollments are not independent. This means that the more languages the user enrolls from their domain, the better a given language they enroll will be.","title":"Enrollments"},{"location":"plugins/lid-embedplda-v3.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail.","title":"Configuring Languages"},{"location":"plugins/lid-embedplda-v3.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/lid-embedplda-v3.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese","title":"Supported Languages"},{"location":"plugins/lid-embedplda-v3.html#global-options","text":"This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"Global Options"},{"location":"plugins/lid-embedplda-v4.html","text":"lid-embedplda-v4 (Language Identification) Version Changelog Plugin Version Change v4.0.0 Initial plugin release builds off of v3.0.1 and adds GPU support, released with OLIVE 5.5.0 Description LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. This plug-in has been configured to allow enrollment for the augmentation of existing classes and the addition of new classes. The backend and calibration modules are rapidly retrained with the addition of enrollment data. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or replace/augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.2+ Limitations Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Language/Dialect Detection Granularity LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load. Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language. Note that LID enrollments are not independent. This means that the more languages the user enrolls from their domain, the better a given language they enroll will be. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese Global Options This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"Language Identification (LID) GPU"},{"location":"plugins/lid-embedplda-v4.html#lid-embedplda-v4-language-identification","text":"","title":"lid-embedplda-v4 (Language Identification)"},{"location":"plugins/lid-embedplda-v4.html#version-changelog","text":"Plugin Version Change v4.0.0 Initial plugin release builds off of v3.0.1 and adds GPU support, released with OLIVE 5.5.0","title":"Version Changelog"},{"location":"plugins/lid-embedplda-v4.html#description","text":"LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. This plug-in has been configured to allow enrollment for the augmentation of existing classes and the addition of new classes. The backend and calibration modules are rapidly retrained with the addition of enrollment data.","title":"Description"},{"location":"plugins/lid-embedplda-v4.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/lid-embedplda-v4.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/lid-embedplda-v4.html#outputs","text":"Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102","title":"Outputs"},{"location":"plugins/lid-embedplda-v4.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or replace/augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/lid-embedplda-v4.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/lid-embedplda-v4.html#limitations","text":"Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile.","title":"Limitations"},{"location":"plugins/lid-embedplda-v4.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/lid-embedplda-v4.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/lid-embedplda-v4.html#comments","text":"","title":"Comments"},{"location":"plugins/lid-embedplda-v4.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/lid-embedplda-v4.html#languagedialect-detection-granularity","text":"LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/lid-embedplda-v4.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language. Note that LID enrollments are not independent. This means that the more languages the user enrolls from their domain, the better a given language they enroll will be.","title":"Enrollments"},{"location":"plugins/lid-embedplda-v4.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail.","title":"Configuring Languages"},{"location":"plugins/lid-embedplda-v4.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/lid-embedplda-v4.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese","title":"Supported Languages"},{"location":"plugins/lid-embedplda-v4.html#global-options","text":"This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"Global Options"},{"location":"plugins/lid-embedpldaSmolive-v1.html","text":"lid-embedpldaSmolive-v1 (Language Identification) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of lid-embedplda-v3 but features model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate. Description This plugin is based heavily on its predecessor, lid-embedplda-v3, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. This plug-in has been configured to allow enrollment for the augmentation of existing classes and the addition of new classes. The backend and calibration modules are rapidly retrained with the addition of enrollment data. Domains multi-int8-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or replace/augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.4+ Limitations Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile. Quantized Model Hardware Compatibility There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Comments Language/Dialect Detection Granularity LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load. Enrollments Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language. Note that LID enrollments are not independent. This means that the more languages the user enrolls from their domain, the better a given language they enroll will be. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese Global Options This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"Language Identification (LID) Low Resource"},{"location":"plugins/lid-embedpldaSmolive-v1.html#lid-embedpldasmolive-v1-language-identification","text":"","title":"lid-embedpldaSmolive-v1 (Language Identification)"},{"location":"plugins/lid-embedpldaSmolive-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of lid-embedplda-v3 but features model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate.","title":"Version Changelog"},{"location":"plugins/lid-embedpldaSmolive-v1.html#description","text":"This plugin is based heavily on its predecessor, lid-embedplda-v3, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. This plug-in has been configured to allow enrollment for the augmentation of existing classes and the addition of new classes. The backend and calibration modules are rapidly retrained with the addition of enrollment data.","title":"Description"},{"location":"plugins/lid-embedpldaSmolive-v1.html#domains","text":"multi-int8-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to 54 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/lid-embedpldaSmolive-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/lid-embedpldaSmolive-v1.html#outputs","text":"Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102","title":"Outputs"},{"location":"plugins/lid-embedpldaSmolive-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new language models or replace/augment existing language models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/lid-embedpldaSmolive-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/lid-embedpldaSmolive-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile.","title":"Limitations"},{"location":"plugins/lid-embedpldaSmolive-v1.html#quantized-model-hardware-compatibility","text":"There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function.","title":"Quantized Model Hardware Compatibility"},{"location":"plugins/lid-embedpldaSmolive-v1.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/lid-embedpldaSmolive-v1.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/lid-embedpldaSmolive-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/lid-embedpldaSmolive-v1.html#languagedialect-detection-granularity","text":"LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/lid-embedpldaSmolive-v1.html#enrollments","text":"Some recent LID plugins allows class modifications. A class modification is essentially an enrollment capability similar to SID. A new enrollment is created with the first class modification request (sending the system audio with a language label, generally 30 seconds or more per cut). A new language class will provide volatile scores unless sufficient cuts have been provided (approximately 10). In general, 30 minutes from around 30 samples is the minimum amount of data required to produce a reasonable language model. This enrollment can be augmented with subsequent class modification requests by adding more audio from the same language to an existing class, again, like SID or SDD. In addition to user enrolled languages, most LID plugins are supplied with several pre-enrolled languages. Users can replace these existing languages using their own data by enrolling audio with the same label as an existing language. Note that LID enrollments are not independent. This means that the more languages the user enrolls from their domain, the better a given language they enroll will be.","title":"Enrollments"},{"location":"plugins/lid-embedpldaSmolive-v1.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail.","title":"Configuring Languages"},{"location":"plugins/lid-embedpldaSmolive-v1.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Name (cont'd) Amharic Modern Standard Arabic English Mandarin French Pashto Iranian Persian Portuguese Japanese Russian Korean Spanish Levantine Arabic Tagalog Iraqi Arabic Vietnamese","title":"Default Enabled Languages"},{"location":"plugins/lid-embedpldaSmolive-v1.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Name Albanian German Modern Standard Arabic Somali Wu Azerbaijani Greek Macedonian Spanish Belarusian Haitian Mandarin Swahili Bengali Hausa Min Nan Tagalog Bosnian Hindi Ndebele Tamil Bulgarian Indonesian Oromo Thai Burmese Iranian Persian Panjabi Tibetan Cantonese Italian Pashto Tigrinya Cebuano Japanese Portuguese Tunisian Arabic Croatian Khmer Romanian Turkish English Korean Russian Ukranian French Levantine Arabic Serbian Uzbek Georgian Iraqi Arabic Shona Vietnamese","title":"Supported Languages"},{"location":"plugins/lid-embedpldaSmolive-v1.html#global-options","text":"This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"Global Options"},{"location":"plugins/lid-hdplda-v1.html","text":"lid-hdplda-v1 (Language Identification) Version Changelog Plugin Version Change v1.0.0 Initial prototype plugin release, released with OLIVE 5.3.1 v1.0.1 Finalization of models, addition of a language, officially released with OLIVE 5.4.0 v1.0.2 Fixed slight bugs in available language list and duplicate language mappings. Description LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. In contrast to its predecessor, instead of training the PLDA parameters in a generative way, this plugin discriminatively trains the PDLA parameters. In addition, two PLDA models are trained, one to generate scores for clusters of highly related languages, and a second one to generate scores conditional to each cluster. For example, there is a \u201cSpanish cluster\u201d and inside that cluster we have languages as Castilian Spanish, Catalan, Galego etc\u2026 We call this approach Hierarchical Discriminative PLDA, or HDPLDA. Domains multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to over 100 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary. Inputs Audio file or buffer and an optional identifier. Outputs Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest Compatibility OLIVE 5.4+ Limitations Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile. Minimum Speech Duration The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech. Languages of Low Confidence Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Comments Language/Dialect Detection Granularity LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load. Enrollments Some recent LID plugins allow class modifications. Due to the more complex structure of the model training process for the HDPLDA architecture, this plugin does not support user-enrollable or user-augmentable classes. The language model set for this plugin is fixed, though the provided languages can still be enabled or disabled (see below) as desired. Configuring Languages Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail. Note that internally, this plugin uses ISO-639-3 Language Codes to refer to each language. They are translated to English language names before being reported by OLIVE for human consumption, but it's important to know the language code when enabling or disabling a language. Refer to the link above to look up language codes, or see below for a list of the included languages and a mapping of the internal codes to the reported language name. Default Enabled Languages The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Reported Language Name arb, aeb, acm, afb, alv, arz Arabic (dialects merged and reported as Arabic) cmn MandarinChinese eng English fas Persian fra French jpn Japanese kor Korean por Portuguese pus Pushto spa Spanish tgl Tagalog amh Amharic rus Russian vie Vietnamese yue CantoneseChinese Supported Languages The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Reported Language Name abk Abkhazian aeb TunisianArabic acm MesopotamianArabic afb GulfArabic arb ModernStandardArabic alv LevantineArabic arz EgyptianArabic asm Assamese ben Bengali bod Tibetan bul Bulgarian cmn MandarinChinese dan Danish deu German ell Greek eng English eus Basque fas Persian est Estonian fin Finnish gaz WestCentralOromo fra French hat HaitianCreole hau Hausa heb Hebrew hun Hungarian hye Armenian fao Faroese isl Icelandic ita Italian indsun Indonesian/Sundanese jav Javanese jpn Japanese kat Georgian khm Khmer kor Korean lav Latvian lin Lingala lit Lithuanian ltz Luxembourgish guj Gujarati mar Marathi hbs Serbo/Croatian mkd Macedonian mlg Malagasy mlt Maltese mon Mongolian mri Maori mya Burmese nan MinNanChinese afr Afrikaans nld Dutch npi Nepali bre Breton oci Occitan pan Punjabi por Portuguese pus Pushto ron Romanian sin Sinhala ces Czech pol Polish slk Slovak slv Slovenian nde Ndebele sna Shona snd Sindhi som Somali cat Catalan glg Galician spa Spanish sqi Albanian swa Swahili nno NorwegianNynorsk swe Swedish bak Bashkir kaz Kazakh tat Tatar kan Kannada mal Malayalam tam Tamil tel Telugu tgk Tajik ceb Cebuano tgl Tagalog thalao Thai/Lao amh Amharic tir Tigrinya tuk Turkmen aze Azerbaijani tur Turkish bel Belarusian rus Russian ukr Ukrainian urdhin Urdu/Hindi uzb Uzbek vie Vietnamese wuu WuChinese yid Yiddish ymm MaayMaay yor Yoruba yue CantoneseChinese Global Options This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"Language Identification (LID)"},{"location":"plugins/lid-hdplda-v1.html#lid-hdplda-v1-language-identification","text":"","title":"lid-hdplda-v1 (Language Identification)"},{"location":"plugins/lid-hdplda-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial prototype plugin release, released with OLIVE 5.3.1 v1.0.1 Finalization of models, addition of a language, officially released with OLIVE 5.4.0 v1.0.2 Fixed slight bugs in available language list and duplicate language mappings.","title":"Version Changelog"},{"location":"plugins/lid-hdplda-v1.html#description","text":"LID plugins analyze an audio segment to produce a detection score for each of the enabled language or dialect classes for the domain in use. A plugin domain could consist of 50 or more languages and dialects in a single plugin, or as few as one for use cases where the customer is only focused on a single target class. Some plugin domains are solely focused on dialect or sub-language recognition, such as languages of China. Several LID plugins allow users to add new classes or augment existing classes with more data for the class to improve accuracy. Language recognition plugin for clean telephone or microphone data, based on a language embeddings DNN fed with acoustic DNN bottleneck features, and language classification using a PLDA backend and multi-class calibration. In contrast to its predecessor, instead of training the PLDA parameters in a generative way, this plugin discriminatively trains the PDLA parameters. In addition, two PLDA models are trained, one to generate scores for clusters of highly related languages, and a second one to generate scores conditional to each cluster. For example, there is a \u201cSpanish cluster\u201d and inside that cluster we have languages as Castilian Spanish, Catalan, Galego etc\u2026 We call this approach Hierarchical Discriminative PLDA, or HDPLDA.","title":"Description"},{"location":"plugins/lid-hdplda-v1.html#domains","text":"multi-v1 Generic domain for most close talking conditions with signal-to-noise ratio above 10 dB. Currently set up with 16 languages configured (optionally configurable to up to over 100 languages). See below for the currently-configured and available languages. See the configuring languages section for instructions on reconfiguring the available languages if necessary.","title":"Domains"},{"location":"plugins/lid-hdplda-v1.html#inputs","text":"Audio file or buffer and an optional identifier.","title":"Inputs"},{"location":"plugins/lid-hdplda-v1.html#outputs","text":"Generally, a list of scores for all classes in the domain, for the entire segment. As with SAD and SID, scores are generally log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. Plugins may be altered to return only detections, rather than a list of classes and scores, but this is generally done on the client side for sake of flexibility. An example output excerpt: input-audio.wav Amharic -6.73527861 input-audio.wav Arabic -3.31796265 input-audio.wav English 8.22701168 input-audio.wav French -2.98071671 input-audio.wav Iranian Persian -5.55558729 input-audio.wav Japanese -6.01283073 input-audio.wav Korean -5.64162636 input-audio.wav Mandarin -4.81163836 input-audio.wav Portuguese -1.93523705 input-audio.wav Russian -5.60199690 input-audio.wav Spanish -3.70800495 input-audio.wav Tagalog -4.86510944 input-audio.wav Vietnamese -5.10995102","title":"Outputs"},{"location":"plugins/lid-hdplda-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled and enabled languages of interest. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/lid-hdplda-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/lid-hdplda-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below. All current LID plugins assume that an audio segment contains only a single language and may be scored as a unit. If a segment contains multiple languages the entire segment will still be scored as a unit. In many cases, a minimum duration of speech of 2 seconds is required in order to output scores. This value can optionally be overwritten, but scores provided for such short segments will be volatile.","title":"Limitations"},{"location":"plugins/lid-hdplda-v1.html#minimum-speech-duration","text":"The system will only attempt to perform language identification if the submitted audio segment contains more than 2 seconds of detected speech.","title":"Minimum Speech Duration"},{"location":"plugins/lid-hdplda-v1.html#languages-of-low-confidence","text":"Many of the language models that are included and hidden within the domain's data model, disabled by default, do not contain enough data within the model for reliable detection of this language, and are included solely to help with score calibration, and differentiating other languages. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification.","title":"Languages of Low Confidence"},{"location":"plugins/lid-hdplda-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/lid-hdplda-v1.html#languagedialect-detection-granularity","text":"LID plugins attempt to distinguish dialects (ie., Tunisian Arabic and Levantine Arabic) or a base language class (such as Arabic). These can be mapped back to the base language if desired. This requires one change to be enabled. A mapping file 'dialect_language.map' must exist within the domain of the plugin for which mapping is to be performed (eg. domains/multi-v1/dialect_language.map). This file is a tab-delimited, two-column file that lists each mapping for the dialect to the languages as \" \\t \". Example lines include: Levantine Arabic Arabic Tunisian Arabic Arabic In the example above, the output labels of the dialects will be mapped to the same base language 'Arabic'. Note the exception in which mapping is not performed is for user-enrolled languages where it is assumed the user has provided the dialect or language label based on their requirements. Note that we recommend users request these mapping files from SRI, or request the mapping to be performed before delivery of the plugin so that SRI can test and validate the final mapping before delivery. Note also that the system will not allow you to create an enrollment with the same class name that you have languages mapped to . This is to avoid confusing situations where the system isn't sure if it should be considering the original pre-mapped models, or the newly enrolled user model. You must provide a unique name for any new language enrollments, that does not conflict with the dialect_langage.map. If you have already enrolled a conflicting model, and then add a mapping to this same name, the plugin will provide a warning message and intentionally fail to load.","title":"Language/Dialect Detection Granularity"},{"location":"plugins/lid-hdplda-v1.html#enrollments","text":"Some recent LID plugins allow class modifications. Due to the more complex structure of the model training process for the HDPLDA architecture, this plugin does not support user-enrollable or user-augmentable classes. The language model set for this plugin is fixed, though the provided languages can still be enabled or disabled (see below) as desired.","title":"Enrollments"},{"location":"plugins/lid-hdplda-v1.html#configuring-languages","text":"Most LID plugins have the ability to re-configure the languages available in a domain. Configuring languages in the domain can be done by entering the domain directory of interest within the plugin folder and editing domain_config.txt. This file lists the pre-enrolled languages available in the plugin. Disabled languages are indicated by a # at the start of the line. To enable a language, remove the #. To disable a language, add a # at the start of the line. Note that you cannot add languages to this list that are not supported by underlying models. If nonexistent language are added to this file, the plugin will intentionally fail. Note that internally, this plugin uses ISO-639-3 Language Codes to refer to each language. They are translated to English language names before being reported by OLIVE for human consumption, but it's important to know the language code when enabling or disabling a language. Refer to the link above to look up language codes, or see below for a list of the included languages and a mapping of the internal codes to the reported language name.","title":"Configuring Languages"},{"location":"plugins/lid-hdplda-v1.html#default-enabled-languages","text":"The following languages are identified as high-confidence languages, supported by a sufficient amount of training data to make them reliable language detectors. As such, they are enabled by default in the plugin as-delivered, and serve as a general purpose base language set. Language Code Reported Language Name arb, aeb, acm, afb, alv, arz Arabic (dialects merged and reported as Arabic) cmn MandarinChinese eng English fas Persian fra French jpn Japanese kor Korean por Portuguese pus Pushto spa Spanish tgl Tagalog amh Amharic rus Russian vie Vietnamese yue CantoneseChinese","title":"Default Enabled Languages"},{"location":"plugins/lid-hdplda-v1.html#supported-languages","text":"The full list of languages that exist as an enrolled class within this plugin as delivered are provided in the chart below. Note that as mentioned previously, not all of these languages were enrolled with enough data to serve as reliable detectors, but remain in the domain for the benefits to differentiating other languages, and for score calibration. If in doubt regarding whether an enrolled language should be used for detection or not, please reach out to SRI for clarification. Language Code Reported Language Name abk Abkhazian aeb TunisianArabic acm MesopotamianArabic afb GulfArabic arb ModernStandardArabic alv LevantineArabic arz EgyptianArabic asm Assamese ben Bengali bod Tibetan bul Bulgarian cmn MandarinChinese dan Danish deu German ell Greek eng English eus Basque fas Persian est Estonian fin Finnish gaz WestCentralOromo fra French hat HaitianCreole hau Hausa heb Hebrew hun Hungarian hye Armenian fao Faroese isl Icelandic ita Italian indsun Indonesian/Sundanese jav Javanese jpn Japanese kat Georgian khm Khmer kor Korean lav Latvian lin Lingala lit Lithuanian ltz Luxembourgish guj Gujarati mar Marathi hbs Serbo/Croatian mkd Macedonian mlg Malagasy mlt Maltese mon Mongolian mri Maori mya Burmese nan MinNanChinese afr Afrikaans nld Dutch npi Nepali bre Breton oci Occitan pan Punjabi por Portuguese pus Pushto ron Romanian sin Sinhala ces Czech pol Polish slk Slovak slv Slovenian nde Ndebele sna Shona snd Sindhi som Somali cat Catalan glg Galician spa Spanish sqi Albanian swa Swahili nno NorwegianNynorsk swe Swedish bak Bashkir kaz Kazakh tat Tatar kan Kannada mal Malayalam tam Tamil tel Telugu tgk Tajik ceb Cebuano tgl Tagalog thalao Thai/Lao amh Amharic tir Tigrinya tuk Turkmen aze Azerbaijani tur Turkish bel Belarusian rus Russian ukr Ukrainian urdhin Urdu/Hindi uzb Uzbek vie Vietnamese wuu WuChinese yid Yiddish ymm MaayMaay yor Yoruba yue CantoneseChinese","title":"Supported Languages"},{"location":"plugins/lid-hdplda-v1.html#global-options","text":"This plugin offers several basic user-configurable parameters which can be edited directly in plugin_config.py or passed via the API. Note that if changed in the plugin_config.py file, a server running the plugin will need to be restarted in order to use the new parameters, while parameters passed via the API are dynamically updated and do not require a restart of the server. The options available and their default values are described below: Option Name Description Default Expected Range min_speech The minimum amount of detected speech in order to process a file. A higher value will prevent shorter files from being processed with the benefit of more reliable outputs. 2.0 0.5 - 4.0 sad_threshold The threshold used to determine speech for processing. A higher value results in less speech detected, while removing more noise. 1.0 -3.0 - 6.0","title":"Global Options"},{"location":"plugins/lid.html","text":"redirect: plugins/lid-hdplda-v1.md","title":"Lid"},{"location":"plugins/lid.html#redirect-pluginslid-hdplda-v1md","text":"","title":"redirect: plugins/lid-hdplda-v1.md"},{"location":"plugins/qbe-ftdnnSmolive-v1.html","text":"qbe-ftdnnSmolive-v1 (Query by Example Keyword Spotting) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of qbe-tdnn-v5 but features a shift to Factorized TDNN model architecture as well as model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate. Description This plugin is based heavily on its predecessor, qbe-tdnn-v5, with the important distinction that the model has been modified with both quantization and pruning, in addition to the adoption of factorized TDNN models. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Query by Example Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by audio sample enrollments. It has no language model or other constraints that accompany a traditional keyword spotting plugin, so it is language independent. This is a query by example plugin constructed with Factorized TDNN architecture models, with score calibration and test-adaptive merging of examples. It features dynamic time warping to compensate for speed and cadence differences when detecting keywords. This version also removes Kaldi dependency, replacing a poor-performing bottleneck feature extractor with an in-house developed and trained model, and reduces false alarms by filtering overlapping lower-confidence detections. Domains multi-int8-v1 Multi-condition domain meant for general-purpose audio conditions including telephone, broadband microphone, and other noisy situations without too many digital or PTT distortions. Inputs For enrollment, an audio file or buffer with a corresponding keyword or query label. For scoring, an audio buffer or file. Outputs When one or more of the enrolled keywords has been detected in the submitted audio, QBE returns a region or list of timestamped regions (in seconds), each with a score for the keyword that has been detected. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006 Enrollments Query by Example plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new keyword to enroll. A new enrollment is created with the first class modification request, which consists of essentially sending the system an audio sample of a new keyword or key phrase, along with a label for that query. The label is not used at all by the system for detection, and is only a reference for the user to help recall what the query was. This means that it's completely acceptable to enroll a sample where a speaker is saying something like \"buenas noches\", and to label it \"good night - spanish\" or \"good night\" or even \"lorem ipsum\" in the system. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same query label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected keyword and corresponding score for this keyword. RegionScorerRequest RegionScorerStereoRequest CLASS_MODIFIER \u2013 Enroll new keyword models or augment existing keyword models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.4+ Limitations Known or potential limitations of the plugin are outlined below. Quantized Model Hardware Compatibility There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function. Query/Keyword Recognizability The longer and more distinct the enrolled keyword or key phrase is, the better it will be recognized. Shorter keywords, that may occur often in speech (for example, enrolling the word 'a') or sound very similar to other words, may cause false alarms. In general, QBE plugins are language and speaker independent, and enrolled queries should be able to find speech from other talkers as well. However, if the enrolled keyword example is spoken by someone with a particular accent or non-standard pronunciation, it may not generalize with one or few samples. It's possible to enroll multiple samples and/or samples from multiple speakers to maximize the coverage of the query's model. Silence Sensitivity During Enrollment This version of the plugin is known to be very sensitive to including silence when enrolling new keyword queries, especially at the beginning or end of the query. Care should be taken to ensure that the boundaries of the enrollment submissions are as tight to the actual speech as possible. If excessive amounts of silence are included in the enrollment, the system could confuse this silence as part of the query, and the dynamic-time-warping algorithm may cause this to label keyword detections erroneously including large amounts of silence, and may also drastically increase the keyword search time. Future versions of the plugin will address this. Comments Very short keyword queries will be confusable with many other words, since the phonemes they consist of may be common or frequently occur as part of other words, or sound very similar to words or sounds that may commonly occur. The longer and more distinct a keyword is, the lower the likelihood of false alarms. Global Options This plugin does not feature user-configurable parameters.","title":"Query By Example (QBE)"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#qbe-ftdnnsmolive-v1-query-by-example-keyword-spotting","text":"","title":"qbe-ftdnnSmolive-v1 (Query by Example Keyword Spotting)"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of qbe-tdnn-v5 but features a shift to Factorized TDNN model architecture as well as model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate.","title":"Version Changelog"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#description","text":"This plugin is based heavily on its predecessor, qbe-tdnn-v5, with the important distinction that the model has been modified with both quantization and pruning, in addition to the adoption of factorized TDNN models. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Query by Example Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by audio sample enrollments. It has no language model or other constraints that accompany a traditional keyword spotting plugin, so it is language independent. This is a query by example plugin constructed with Factorized TDNN architecture models, with score calibration and test-adaptive merging of examples. It features dynamic time warping to compensate for speed and cadence differences when detecting keywords. This version also removes Kaldi dependency, replacing a poor-performing bottleneck feature extractor with an in-house developed and trained model, and reduces false alarms by filtering overlapping lower-confidence detections.","title":"Description"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#domains","text":"multi-int8-v1 Multi-condition domain meant for general-purpose audio conditions including telephone, broadband microphone, and other noisy situations without too many digital or PTT distortions.","title":"Domains"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding keyword or query label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#outputs","text":"When one or more of the enrolled keywords has been detected in the submitted audio, QBE returns a region or list of timestamped regions (in seconds), each with a score for the keyword that has been detected. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006","title":"Outputs"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#enrollments","text":"Query by Example plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new keyword to enroll. A new enrollment is created with the first class modification request, which consists of essentially sending the system an audio sample of a new keyword or key phrase, along with a label for that query. The label is not used at all by the system for detection, and is only a reference for the user to help recall what the query was. This means that it's completely acceptable to enroll a sample where a speaker is saying something like \"buenas noches\", and to label it \"good night - spanish\" or \"good night\" or even \"lorem ipsum\" in the system. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same query label.","title":"Enrollments"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected keyword and corresponding score for this keyword. RegionScorerRequest RegionScorerStereoRequest CLASS_MODIFIER \u2013 Enroll new keyword models or augment existing keyword models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#quantized-model-hardware-compatibility","text":"There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function.","title":"Quantized Model Hardware Compatibility"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#querykeyword-recognizability","text":"The longer and more distinct the enrolled keyword or key phrase is, the better it will be recognized. Shorter keywords, that may occur often in speech (for example, enrolling the word 'a') or sound very similar to other words, may cause false alarms. In general, QBE plugins are language and speaker independent, and enrolled queries should be able to find speech from other talkers as well. However, if the enrolled keyword example is spoken by someone with a particular accent or non-standard pronunciation, it may not generalize with one or few samples. It's possible to enroll multiple samples and/or samples from multiple speakers to maximize the coverage of the query's model.","title":"Query/Keyword Recognizability"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#silence-sensitivity-during-enrollment","text":"This version of the plugin is known to be very sensitive to including silence when enrolling new keyword queries, especially at the beginning or end of the query. Care should be taken to ensure that the boundaries of the enrollment submissions are as tight to the actual speech as possible. If excessive amounts of silence are included in the enrollment, the system could confuse this silence as part of the query, and the dynamic-time-warping algorithm may cause this to label keyword detections erroneously including large amounts of silence, and may also drastically increase the keyword search time. Future versions of the plugin will address this.","title":"Silence Sensitivity During Enrollment"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#comments","text":"Very short keyword queries will be confusable with many other words, since the phonemes they consist of may be common or frequently occur as part of other words, or sound very similar to words or sounds that may commonly occur. The longer and more distinct a keyword is, the lower the likelihood of false alarms.","title":"Comments"},{"location":"plugins/qbe-ftdnnSmolive-v1.html#global-options","text":"This plugin does not feature user-configurable parameters.","title":"Global Options"},{"location":"plugins/qbe-tdnn-v5.html","text":"qbe-tdnn-v5 (Query by Example Keyword Spotting) Version Changelog Plugin Version Change v5.0.0 Initial plugin release with OLIVE 5.1.0 v5.0.1 Increased threshold to minimize false alarms. Updated for OLIVE 5.1.0 Description Query by Example Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by audio sample enrollments. It has no language model or other constraints that accompany a traditional keyword spotting plugin, so it is language independent. This is a query by example plugin constructed with the latest TDNN architecture models, with score calibration and test-adaptive merging of examples. It features dynamic time warping to compensate for speed and cadence differences when detecting keywords. This version also removes Kaldi dependency, replacing a poor-performing bottleneck feature extractor with an in-house developed and trained model, and reduces false alarms by filtering overlapping lower-confidence detections. Domains multi-v1 Multi-condition domain meant for general-purpose audio conditions including telephone, broadband microphone, and other noisy situations without too many digital or PTT distortions. Inputs For enrollment, an audio file or buffer with a corresponding keyword or query label. For scoring, an audio buffer or file. Outputs When one or more of the enrolled keywords has been detected in the submitted audio, QBE returns a region or list of timestamped regions (in seconds), each with a score for the keyword that has been detected. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006 Enrollments Query by Example plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new keyword to enroll. A new enrollment is created with the first class modification request, which consists of essentially sending the system an audio sample of a new keyword or key phrase, along with a label for that query. The label is not used at all by the system for detection, and is only a reference for the user to help recall what the query was. This means that it's completely acceptable to enroll a sample where a speaker is saying something like \"buenas noches\", and to label it \"good night - spanish\" or \"good night\" or even \"lorem ipsum\" in the system. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same query label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected keyword and corresponding score for this keyword. RegionScorerRequest RegionScorerStereoRequest CLASS_MODIFIER \u2013 Enroll new keyword models or augment existing keyword models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Query/Keyword Recognizability The longer and more distinct the enrolled keyword or key phrase is, the better it will be recognized. Shorter keywords, that may occur often in speech (for example, enrolling the word 'a') or sound very similar to other words, may cause false alarms. In general, QBE plugins are language and speaker independent, and enrolled queries should be able to find speech from other talkers as well. However, if the enrolled keyword example is spoken by someone with a particular accent or non-standard pronunciation, it may not generalize with one or few samples. It's possible to enroll multiple samples and/or samples from multiple speakers to maximize the coverage of the query's model. Silence Sensitivity During Enrollment This version of the plugin is known to be very sensitive to including silence when enrolling new keyword queries, especially at the beginning or end of the query. Care should be taken to ensure that the boundaries of the enrollment submissions are as tight to the actual speech as possible. If excessive amounts of silence are included in the enrollment, the system could confuse this silence as part of the query, and the dynamic-time-warping algorithm may cause this to label keyword detections erroneously including large amounts of silence, and may also drastically increase the keyword search time. Future versions of the plugin will address this. Comments Very short keyword queries will be confusable with many other words, since the phonemes they consist of may be common or frequently occur as part of other words, or sound very similar to words or sounds that may commonly occur. The longer and more distinct a keyword is, the lower the likelihood of false alarms. Global Options This plugin does not feature user-configurable parameters.","title":"qbe-tdnn-v5 (Query by Example Keyword Spotting)"},{"location":"plugins/qbe-tdnn-v5.html#qbe-tdnn-v5-query-by-example-keyword-spotting","text":"","title":"qbe-tdnn-v5 (Query by Example Keyword Spotting)"},{"location":"plugins/qbe-tdnn-v5.html#version-changelog","text":"Plugin Version Change v5.0.0 Initial plugin release with OLIVE 5.1.0 v5.0.1 Increased threshold to minimize false alarms. Updated for OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/qbe-tdnn-v5.html#description","text":"Query by Example Keyword Spotting plugins are designed to allow users to detect and label targeted keywords or keyphrases that are defined by audio sample enrollments. It has no language model or other constraints that accompany a traditional keyword spotting plugin, so it is language independent. This is a query by example plugin constructed with the latest TDNN architecture models, with score calibration and test-adaptive merging of examples. It features dynamic time warping to compensate for speed and cadence differences when detecting keywords. This version also removes Kaldi dependency, replacing a poor-performing bottleneck feature extractor with an in-house developed and trained model, and reduces false alarms by filtering overlapping lower-confidence detections.","title":"Description"},{"location":"plugins/qbe-tdnn-v5.html#domains","text":"multi-v1 Multi-condition domain meant for general-purpose audio conditions including telephone, broadband microphone, and other noisy situations without too many digital or PTT distortions.","title":"Domains"},{"location":"plugins/qbe-tdnn-v5.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding keyword or query label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/qbe-tdnn-v5.html#outputs","text":"When one or more of the enrolled keywords has been detected in the submitted audio, QBE returns a region or list of timestamped regions (in seconds), each with a score for the keyword that has been detected. The output of QBE follows the format of the traditional KWS output exactly: <audio_file_path> <start_time_s> <end_time_s> <keyword_id> <score> Example: /data/qbe/test/testFile1.wav 0.630 1.170 Airplane 4.37324614709 /data/qbe/test/testFile2.wav 0.350 1.010 Watermelon -1.19732598006","title":"Outputs"},{"location":"plugins/qbe-tdnn-v5.html#enrollments","text":"Query by Example plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new keyword to enroll. A new enrollment is created with the first class modification request, which consists of essentially sending the system an audio sample of a new keyword or key phrase, along with a label for that query. The label is not used at all by the system for detection, and is only a reference for the user to help recall what the query was. This means that it's completely acceptable to enroll a sample where a speaker is saying something like \"buenas noches\", and to label it \"good night - spanish\" or \"good night\" or even \"lorem ipsum\" in the system. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same query label.","title":"Enrollments"},{"location":"plugins/qbe-tdnn-v5.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected keyword and corresponding score for this keyword. RegionScorerRequest RegionScorerStereoRequest CLASS_MODIFIER \u2013 Enroll new keyword models or augment existing keyword models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/qbe-tdnn-v5.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/qbe-tdnn-v5.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/qbe-tdnn-v5.html#querykeyword-recognizability","text":"The longer and more distinct the enrolled keyword or key phrase is, the better it will be recognized. Shorter keywords, that may occur often in speech (for example, enrolling the word 'a') or sound very similar to other words, may cause false alarms. In general, QBE plugins are language and speaker independent, and enrolled queries should be able to find speech from other talkers as well. However, if the enrolled keyword example is spoken by someone with a particular accent or non-standard pronunciation, it may not generalize with one or few samples. It's possible to enroll multiple samples and/or samples from multiple speakers to maximize the coverage of the query's model.","title":"Query/Keyword Recognizability"},{"location":"plugins/qbe-tdnn-v5.html#silence-sensitivity-during-enrollment","text":"This version of the plugin is known to be very sensitive to including silence when enrolling new keyword queries, especially at the beginning or end of the query. Care should be taken to ensure that the boundaries of the enrollment submissions are as tight to the actual speech as possible. If excessive amounts of silence are included in the enrollment, the system could confuse this silence as part of the query, and the dynamic-time-warping algorithm may cause this to label keyword detections erroneously including large amounts of silence, and may also drastically increase the keyword search time. Future versions of the plugin will address this.","title":"Silence Sensitivity During Enrollment"},{"location":"plugins/qbe-tdnn-v5.html#comments","text":"Very short keyword queries will be confusable with many other words, since the phonemes they consist of may be common or frequently occur as part of other words, or sound very similar to words or sounds that may commonly occur. The longer and more distinct a keyword is, the lower the likelihood of false alarms.","title":"Comments"},{"location":"plugins/qbe-tdnn-v5.html#global-options","text":"This plugin does not feature user-configurable parameters.","title":"Global Options"},{"location":"plugins/qbe.html","text":"redirect: plugins/qbe-ftdnnSmolive-v1.md","title":"Qbe"},{"location":"plugins/qbe.html#redirect-pluginsqbe-ftdnnsmolive-v1md","text":"","title":"redirect: plugins/qbe-ftdnnSmolive-v1.md"},{"location":"plugins/red-transform-v1.html","text":"red-transform-v1 (Redaction - Voice Transformation) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0 Description The red-transform plugin operates very similarly to the red-tone-v1 plugin, in that it alters the selected regions of the audio passed to it. In the red-tone-v1 plugin, this alteration was replacing it with a 'bleep' tone; in this new plugin the regions of submitted audio are instead passed through voice transformation with the goal of obscuring a speaker's identity. Those with very close knowledge of the original speaker may still be able to identify the transformed speaker through pronunciation, accent, and other unique identifiers that may not be sufficiently disguised by the transformation algorithm(s). Domains Several pre-set domains are available. Ideally, the audio containing the speaker to be obscured is a clean close microphone and users would select domain clean_close-v1. As the input audio quality decreases and/or distance between the speaker and the microphone increases, users will likely move down the domain list to find a balance between obscurement and intelligibility, with degraded_distant-v1. These domains were chosen as likely-useful compromises along the gammut of speed, intelligibility, and identity-obscurement. If you find your use case is typically not served well by either of these domains, and need something somewhere between the two, or maybe even less aggressive than the one included for degraded or distant speech, please get in touch with us and we can configure a new domain to better match your expected data, or to provide additional options to have on the shelf if desired. Moving further down the domain list (e.g. selecting degraded_distant-v1 over clean_close-v1) also lessens robustness to reverse engineering, but also process significantly faster than more aggressive domains. The included domains: clean_close-v1 This domain offers the highest amount of speaker obscurement, but suffers the most with respect to intelligibility in degraded audio conditions (noise, distant mic). It is the most aggressive regarding the transformations performed, is the slowest to process, and is designed for use in very clean audio conditions. degraded_distant-v1 This domain backs off some of the transformation being applied in an attempt to maximize intelligibility in noisy, degraded, and/or distant speech recordings or conditions, while still providing acceptable levels of speaker identity masking. It is significantly faster than the domain above. Inputs For redaction-voice transformation, an audio file and time-annotated regions corresponding to regions of speech to be obscured is required. Outputs The output of red-transform-v1 is an audio file that has been transformed according the domain specifications with the goal of creating audio where the speech is intelligible, but identifying features of the voice that could lead to recognizing the speaker have been obscured. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AUDIO_CONVERTER \u2013 Take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. AudioModificationRequestRequest Compatibility OLIVE 5.2+ Limitations Intelligibility & Audio conditions This plugin is very sensitive to audio conditions and can struggle to produce intelligible speech in degraded or distant microphone conditions. This is why multiple domains were provided, giving users the options of performing less intense transformation in difficult audio conditions where identity will already be partially obscured. Speed This plugin can be quite resource-intensive when it comes to processing speed. There can be significant speed differences between domains (e.g. domain degraded_distant-v1 is much faster than domain clean_close-v1, due to less processing). The clean_close-v1 domain may be near real-time processing speed, depending on hardware used for the transformation; and other domains should increase in speed from there. Usage This plugin was designed to be used in concert with SRI's Nightingale UI, and the specially designed \"Speaker Redaction\" tools within. For instructions on using the Speaker Redaction module, refer to the Speaker Redaction documentation.","title":"Redaction (RED) Transformation"},{"location":"plugins/red-transform-v1.html#red-transform-v1-redaction-voice-transformation","text":"","title":"red-transform-v1 (Redaction - Voice Transformation)"},{"location":"plugins/red-transform-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/red-transform-v1.html#description","text":"The red-transform plugin operates very similarly to the red-tone-v1 plugin, in that it alters the selected regions of the audio passed to it. In the red-tone-v1 plugin, this alteration was replacing it with a 'bleep' tone; in this new plugin the regions of submitted audio are instead passed through voice transformation with the goal of obscuring a speaker's identity. Those with very close knowledge of the original speaker may still be able to identify the transformed speaker through pronunciation, accent, and other unique identifiers that may not be sufficiently disguised by the transformation algorithm(s).","title":"Description"},{"location":"plugins/red-transform-v1.html#domains","text":"Several pre-set domains are available. Ideally, the audio containing the speaker to be obscured is a clean close microphone and users would select domain clean_close-v1. As the input audio quality decreases and/or distance between the speaker and the microphone increases, users will likely move down the domain list to find a balance between obscurement and intelligibility, with degraded_distant-v1. These domains were chosen as likely-useful compromises along the gammut of speed, intelligibility, and identity-obscurement. If you find your use case is typically not served well by either of these domains, and need something somewhere between the two, or maybe even less aggressive than the one included for degraded or distant speech, please get in touch with us and we can configure a new domain to better match your expected data, or to provide additional options to have on the shelf if desired. Moving further down the domain list (e.g. selecting degraded_distant-v1 over clean_close-v1) also lessens robustness to reverse engineering, but also process significantly faster than more aggressive domains. The included domains: clean_close-v1 This domain offers the highest amount of speaker obscurement, but suffers the most with respect to intelligibility in degraded audio conditions (noise, distant mic). It is the most aggressive regarding the transformations performed, is the slowest to process, and is designed for use in very clean audio conditions. degraded_distant-v1 This domain backs off some of the transformation being applied in an attempt to maximize intelligibility in noisy, degraded, and/or distant speech recordings or conditions, while still providing acceptable levels of speaker identity masking. It is significantly faster than the domain above.","title":"Domains"},{"location":"plugins/red-transform-v1.html#inputs","text":"For redaction-voice transformation, an audio file and time-annotated regions corresponding to regions of speech to be obscured is required.","title":"Inputs"},{"location":"plugins/red-transform-v1.html#outputs","text":"The output of red-transform-v1 is an audio file that has been transformed according the domain specifications with the goal of creating audio where the speech is intelligible, but identifying features of the voice that could lead to recognizing the speaker have been obscured.","title":"Outputs"},{"location":"plugins/red-transform-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. AUDIO_CONVERTER \u2013 Take an audio file or buffer, potentially perform some modification(s) on it, and return audio to the requestor, this can be the same audio untouched, a modified version of this audio, or completely different audio, depending on the purpose of the plugin. AudioModificationRequestRequest","title":"Functionality (Traits)"},{"location":"plugins/red-transform-v1.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/red-transform-v1.html#limitations","text":"","title":"Limitations"},{"location":"plugins/red-transform-v1.html#intelligibility-audio-conditions","text":"This plugin is very sensitive to audio conditions and can struggle to produce intelligible speech in degraded or distant microphone conditions. This is why multiple domains were provided, giving users the options of performing less intense transformation in difficult audio conditions where identity will already be partially obscured.","title":"Intelligibility &amp; Audio conditions"},{"location":"plugins/red-transform-v1.html#speed","text":"This plugin can be quite resource-intensive when it comes to processing speed. There can be significant speed differences between domains (e.g. domain degraded_distant-v1 is much faster than domain clean_close-v1, due to less processing). The clean_close-v1 domain may be near real-time processing speed, depending on hardware used for the transformation; and other domains should increase in speed from there.","title":"Speed"},{"location":"plugins/red-transform-v1.html#usage","text":"This plugin was designed to be used in concert with SRI's Nightingale UI, and the specially designed \"Speaker Redaction\" tools within. For instructions on using the Speaker Redaction module, refer to the Speaker Redaction documentation.","title":"Usage"},{"location":"plugins/red.html","text":"redirect: plugins/red-transform-v1.md","title":"Red"},{"location":"plugins/red.html#redirect-pluginsred-transform-v1md","text":"","title":"redirect: plugins/red-transform-v1.md"},{"location":"plugins/sad-dnn-v7.html","text":"sad-dnn-v7 (Speech Activity Detection) Version Changelog Plugin Version Change v7.0.0 Initial plugin release, functionally identical to v6.0.0, but updated to be compatible with OLIVE 5.0.0 v7.0.1 Updated to be compatible with OLIVE 5.1.0, and introduces 'fast-multi-v1' domain v7.0.2 Bug fixes from v7.0.1, released with OLIVE 5.2.0 Description Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring. Domains multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. fast-multi-v1 Multi-condition domain trained on the same data as the multi-v1 model above, but featuring configuration changes that allow it to process much more quickly, with a possible very slight trade off in accuracy in some circumstances. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins. Adaptation Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide . Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions. Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Speech Disclaimer A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced. DTMF False Alarms It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech. Minimum Audio Length A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection. Comments Speech Region Padding When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD. Supervised Adaptation Guidance When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"sad-dnn-v7 (Speech Activity Detection)"},{"location":"plugins/sad-dnn-v7.html#sad-dnn-v7-speech-activity-detection","text":"","title":"sad-dnn-v7 (Speech Activity Detection)"},{"location":"plugins/sad-dnn-v7.html#version-changelog","text":"Plugin Version Change v7.0.0 Initial plugin release, functionally identical to v6.0.0, but updated to be compatible with OLIVE 5.0.0 v7.0.1 Updated to be compatible with OLIVE 5.1.0, and introduces 'fast-multi-v1' domain v7.0.2 Bug fixes from v7.0.1, released with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/sad-dnn-v7.html#description","text":"Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring.","title":"Description"},{"location":"plugins/sad-dnn-v7.html#domains","text":"multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. fast-multi-v1 Multi-condition domain trained on the same data as the multi-v1 model above, but featuring configuration changes that allow it to process much more quickly, with a possible very slight trade off in accuracy in some circumstances.","title":"Domains"},{"location":"plugins/sad-dnn-v7.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/sad-dnn-v7.html#outputs","text":"SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/sad-dnn-v7.html#adaptation","text":"Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide .","title":"Adaptation"},{"location":"plugins/sad-dnn-v7.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/sad-dnn-v7.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sad-dnn-v7.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/sad-dnn-v7.html#speech-disclaimer","text":"A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced.","title":"Speech Disclaimer"},{"location":"plugins/sad-dnn-v7.html#dtmf-false-alarms","text":"It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech.","title":"DTMF False Alarms"},{"location":"plugins/sad-dnn-v7.html#minimum-audio-length","text":"A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection.","title":"Minimum Audio Length"},{"location":"plugins/sad-dnn-v7.html#comments","text":"","title":"Comments"},{"location":"plugins/sad-dnn-v7.html#speech-region-padding","text":"When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD.","title":"Speech Region Padding"},{"location":"plugins/sad-dnn-v7.html#supervised-adaptation-guidance","text":"When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported.","title":"Supervised Adaptation Guidance"},{"location":"plugins/sad-dnn-v7.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/sad-dnn-v8.html","text":"sad-dnn-v8 (Speech Activity Detection) Version Changelog Plugin Version Change v8.0.0 Initial plugin release, functionally identical to v7.0.2, but updated to include GPU support with proper configuration. Tested and released with OLIVE 5.5.0 Description Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring. Domains multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. fast-multi-v1 Multi-condition domain trained on the same data as the multi-v1 model above, but featuring configuration changes that allow it to process much more quickly, with a possible very slight trade off in accuracy in some circumstances. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins. Adaptation Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide . Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions. Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Speech Disclaimer A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced. DTMF False Alarms It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech. Minimum Audio Length A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection. Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Speech Region Padding When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD. Supervised Adaptation Guidance When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Speech Activity Detection (SAD)"},{"location":"plugins/sad-dnn-v8.html#sad-dnn-v8-speech-activity-detection","text":"","title":"sad-dnn-v8 (Speech Activity Detection)"},{"location":"plugins/sad-dnn-v8.html#version-changelog","text":"Plugin Version Change v8.0.0 Initial plugin release, functionally identical to v7.0.2, but updated to include GPU support with proper configuration. Tested and released with OLIVE 5.5.0","title":"Version Changelog"},{"location":"plugins/sad-dnn-v8.html#description","text":"Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring.","title":"Description"},{"location":"plugins/sad-dnn-v8.html#domains","text":"multi-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. fast-multi-v1 Multi-condition domain trained on the same data as the multi-v1 model above, but featuring configuration changes that allow it to process much more quickly, with a possible very slight trade off in accuracy in some circumstances.","title":"Domains"},{"location":"plugins/sad-dnn-v8.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/sad-dnn-v8.html#outputs","text":"SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/sad-dnn-v8.html#adaptation","text":"Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide .","title":"Adaptation"},{"location":"plugins/sad-dnn-v8.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/sad-dnn-v8.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sad-dnn-v8.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/sad-dnn-v8.html#speech-disclaimer","text":"A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced.","title":"Speech Disclaimer"},{"location":"plugins/sad-dnn-v8.html#dtmf-false-alarms","text":"It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech.","title":"DTMF False Alarms"},{"location":"plugins/sad-dnn-v8.html#minimum-audio-length","text":"A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection.","title":"Minimum Audio Length"},{"location":"plugins/sad-dnn-v8.html#comments","text":"","title":"Comments"},{"location":"plugins/sad-dnn-v8.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/sad-dnn-v8.html#speech-region-padding","text":"When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD.","title":"Speech Region Padding"},{"location":"plugins/sad-dnn-v8.html#supervised-adaptation-guidance","text":"When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported.","title":"Supervised Adaptation Guidance"},{"location":"plugins/sad-dnn-v8.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/sad-dnnSmolive-v1.html","text":"sad-dnnSmolive-v1 (Speech Activity Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of sad-dnn-v7 but features model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate. Description This plugin is based heavily on its predecessor, sad-dnn-v7, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring. Domains multi-int8-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins. Adaptation Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide . Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions. Compatibility OLIVE 5.4+ Limitations Any known or potential limitations with this plugin are listed below. Quantized Model Hardware Compatibility There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function. Speech Disclaimer A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced. DTMF False Alarms It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech. Minimum Audio Length A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection. Comments Speech Region Padding When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD. Supervised Adaptation Guidance When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Speech Activity Detection (SAD) Low Resource"},{"location":"plugins/sad-dnnSmolive-v1.html#sad-dnnsmolive-v1-speech-activity-detection","text":"","title":"sad-dnnSmolive-v1 (Speech Activity Detection)"},{"location":"plugins/sad-dnnSmolive-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of sad-dnn-v7 but features model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate.","title":"Version Changelog"},{"location":"plugins/sad-dnnSmolive-v1.html#description","text":"This plugin is based heavily on its predecessor, sad-dnn-v7, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring.","title":"Description"},{"location":"plugins/sad-dnnSmolive-v1.html#domains","text":"multi-int8-v1 Multi-condition domain trained on push-to-talk (PTT), telephony, and distant-microphone speech. Also hardened against speech containing music, to have more robust performance when encountering such conditions.","title":"Domains"},{"location":"plugins/sad-dnnSmolive-v1.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/sad-dnnSmolive-v1.html#outputs","text":"SAD has two possible output formats: frame scores and region scores. Frame scores return log-likelihood ratio (LLR) score of speech vs. non-speech per each 10ms frame of the input audio segment. An LLR of greater than \u201c0\u201d indicates that the likelihood of speech is greater than the likelihood of non-speech and \u201c0\u201d is generally used as the threshold for detecting speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech. Region scores internally post-process frame scores to return speech regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by subtracting a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 second, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of SAD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/sad-dnnSmolive-v1.html#adaptation","text":"Many SAD plugins allow for supervised adaptation. Supervised adaptation is the process of retraining the speech and non-speech SAD models with new labeled audio samples, to make them a better match to the operational environment. If while using SAD it is found that too many segments of speech are being missed, or too many false alarms are appearing (hypothesizing speech in regions without true speech), it is suggested to first try changing the threshold to higher values (to reduce false alarms) or to lower values (to reduce the missed speech segments). If changing the threshold is unsuccessful at increasing the performance of the system, then supervised adaptation may be able to address the performance deficiencies. Please check the individual plugin documentation page to verify if this functionality is available for the plugin you are working with. Supervised adaptation is accomplished by providing the system with three inputs: audio regions, in the form of start and end timestamps, in seconds labels for each region The labels indicate which regions in the audio are speech or non-speech. The system adapts the model using this set of data and labels (\u2018S\u2019 for speech and \u2018NS\u2019 for non-speech) in order to improve performance in target conditions that differ from its original training. Adaptation can substantially improve performance with 6 minutes or more of speech and non-speech region annotations, and performance can improve with as little as one minute. Adaptation durations of less than one minute have not been tested, and therefore results will be uncertain. Inputs to the plug-in should include both S and NS (speech and non-speech, respectively) regions. Inputs do not need to be balanced, but it is preferable that the S and NS regions are of similar durations, since just speech or just non-speech may not provide much performance benefit. For example: 20131213T071501UTC_11020_A.wav S 72.719000 73.046 20131213T071501UTC_11020_A.wav NS 51.923000 53.379000 Note that it is important to provide the proper full or relative path to each audio file in order for it to be used for processing. Note also that when performing any operation with OLIVE as of OLIVE 5.1, regions are required to be in seconds - this is in contrast to previous versions of OLIVE where some operations, such as adaptation through the Enterprise API, used to require milliseconds. For more details about integration of adaptation in the API Integration , or for performing adaptation using the OLIVE command line tools, see the appropriate section in the CLI User Guide .","title":"Adaptation"},{"location":"plugins/sad-dnnSmolive-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest SUPERVISED_ADAPTER - Allow users to perform domain adaptation to enhance performance in new audio conditions by providing the system with new, labeled audio data to learn from. Performance shows substantial improvement with six minutes of annotated speech and non-speech. PreprocessAudioAdaptRequest SupervisedAdaptationRequest This SAD plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SAD will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/sad-dnnSmolive-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/sad-dnnSmolive-v1.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/sad-dnnSmolive-v1.html#quantized-model-hardware-compatibility","text":"There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function.","title":"Quantized Model Hardware Compatibility"},{"location":"plugins/sad-dnnSmolive-v1.html#speech-disclaimer","text":"A SAD plugin detects any and all speech, including singing, whether it is intelligible or not, live or recorded or even machine-produced.","title":"Speech Disclaimer"},{"location":"plugins/sad-dnnSmolive-v1.html#dtmf-false-alarms","text":"It is possible for this plugin to produce false alarms when presented with audio containing DTMF tone signals and certain other signals with a similar structure to speech.","title":"DTMF False Alarms"},{"location":"plugins/sad-dnnSmolive-v1.html#minimum-audio-length","text":"A minimum waveform duration of 0.31 seconds is required to produce a meaningful speech detection.","title":"Minimum Audio Length"},{"location":"plugins/sad-dnnSmolive-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/sad-dnnSmolive-v1.html#speech-region-padding","text":"When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD.","title":"Speech Region Padding"},{"location":"plugins/sad-dnnSmolive-v1.html#supervised-adaptation-guidance","text":"When performing supervised adaptation with this plugin, the user must provide adaptation audio and annotations with both speech and non-speech segments, preferably of similar duration each. Suggested usage for supervised adaptation in this context is when performing in new acoustic environments where SAD is found to be not working as expected - either encountering too many true speech regions that are being missed, or too many non-speech segments being falsely identified as speech. We suggest adapting with a minimum of 6 minutes total, which based on experimental results should provide satisfactory baseline adaptation when encountering stationary background noises, through to a minimum of 30 minutes total if dealing with non-stationary or music-like background environments. As little data as one minute can be used for adaptation and still provide performance improvements. Adaptation durations lower than 60s have not been tested, and a warning will be triggereed if adaptation is performed with fewer than 60s of adaptation annotation regions. A minimum of 3 seconds of annotations must be provided in order for adaptation to be performed. If fewer than 3 seconds are provided, adaptation will halt and an error will be reported.","title":"Supervised Adaptation Guidance"},{"location":"plugins/sad-dnnSmolive-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/sad.html","text":"redirect: plugins/sad-dnn-v7.md","title":"Sad"},{"location":"plugins/sad.html#redirect-pluginssad-dnn-v7md","text":"","title":"redirect: plugins/sad-dnn-v7.md"},{"location":"plugins/sdd-diarizeEmbed-v1.html","text":"sdd-diarizeEmbed-v1 (Speaker Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 Description This plugin bridges the gap between Speaker Detection and Speaker Diarization plugins. As it processes the submitted audio, the diarization stage will segment the submitted audio to determine 'who spoke when' by automatically clustering speech regions that it determines to be attributable to the same speaker, and labels them with class names such as 'unknownspk1', 'unknownspk2', etc. With those regions established, if there are speaker enrollments for different speakers of interest, the plugin will then attempt to determine if any of those \"unknown speaker\" regions belong to one or more of the enrolled speakers of interest, similar to a traditional Speaker Detection plugin. The output then combines these operations, by outputting the enrolled class name for regions where the system is confident that an enrolled speaker is detected, and retaining the \"unknown speaker\" labels where the system isn't confident that the speech comes from an enrolled speaker. Domains telClosetalk-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 unknownspk00 1.4000 /data/sid/audio/file1.wav 13.280 29.960 unknownspk01 1.4000 /data/sid/audio/file1.wav 30.350 32.030 unknownspk00 1.4000 /data/sid/audio/file2.wav 32.310 46.980 Phil 2.5333 /data/sid/audio/file2.wav 47.790 51.120 unknownspk02 1.4000 /data/sid/audio/file2.wav 54.340 55.400 unknownspk00 1.4000 Enrollments Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.3+ Limitations Known or potential limitations of the plugin are outlined below. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Comments Segmentation By Diarization By default, this plugin uses blind speaker diarization to separate individual speakers within a file. This allows us to have much higher resolution when determining speaker boundaries than the previous plugins' \"segmentation by classification\" approach, and also avoids forcing a decision to be made between bounday resolution and processing speed. Under this segmentation scheme, the plugin first attempts to perform speaker diarization on the audio, clustering speakers that it believes are similar together, and labeling those regions appropriately unknownspk00 , unknownspk01 , etc. Once it has hypothesized individual speakear regions within the audio, it will then compare that audio against any enrolled speakers if they exist. If the plugin is confident enough that the speech of one of the \"unknowns\" actually belongs to an enrolled speaker, it updates the label and score for those regions accordingly. Segmentation By Classification (Legacy, Optional) This segmentation method is an optional, legacy method of clustering the speakers found within a file. Due to design considerations, it will always have to compromise between processing speed and diarization resolution. It should only be enabled by experienced users under specific circumstances. Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin performs when operating under SBC takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type. Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False dia_fixed_unknown_speaker_score The placeholder score assigned to unknown speaker regions to maintain region scoring format compatbility. 1.4 -1000 - 10000 enable_diarization Determines whether segmentation by diarization is performed, to use blind diarization to automatically cluster and segment the file into different speakers. If set to false, segmentation by classification is performed is performed instead, using the sliding window approach described above. True True or False enable_diarization_unknown_spk_output Determines whether labels for unknown speakers are output. If set to false, plugin only performs speaker detection and will not attempt to label unknown speakers. True True or False Additional option notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables only come into play if the enable_diarization option is set to false and the plugin is processing via segmentation by classification instead of segmentation by diarization. This is an optional, legacy mode for the plugin and will likely not be encountered. These options can be ignored unless you are intentionally enabling the legacy segmentation method. These options determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels output_only_highest_scoring_detected_speaker The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (/data/sid/audio/file2.wav) with scores for three different speakers previously enrolled, /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 /data/sid/audio/file2.wav 54.340 55.400 Sarah 2.430 and we change the threshold to 3.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: /data/sid/audio/file2.wav 41.130 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 However, with output_only_highest_scoring_detected_speaker = False, the system reports: /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"sdd-diarizeEmbed-v1 (Speaker Detection)"},{"location":"plugins/sdd-diarizeEmbed-v1.html#sdd-diarizeembed-v1-speaker-detection","text":"","title":"sdd-diarizeEmbed-v1 (Speaker Detection)"},{"location":"plugins/sdd-diarizeEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0","title":"Version Changelog"},{"location":"plugins/sdd-diarizeEmbed-v1.html#description","text":"This plugin bridges the gap between Speaker Detection and Speaker Diarization plugins. As it processes the submitted audio, the diarization stage will segment the submitted audio to determine 'who spoke when' by automatically clustering speech regions that it determines to be attributable to the same speaker, and labels them with class names such as 'unknownspk1', 'unknownspk2', etc. With those regions established, if there are speaker enrollments for different speakers of interest, the plugin will then attempt to determine if any of those \"unknown speaker\" regions belong to one or more of the enrolled speakers of interest, similar to a traditional Speaker Detection plugin. The output then combines these operations, by outputting the enrolled class name for regions where the system is confident that an enrolled speaker is detected, and retaining the \"unknown speaker\" labels where the system isn't confident that the speech comes from an enrolled speaker.","title":"Description"},{"location":"plugins/sdd-diarizeEmbed-v1.html#domains","text":"telClosetalk-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations.","title":"Domains"},{"location":"plugins/sdd-diarizeEmbed-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sdd-diarizeEmbed-v1.html#outputs","text":"In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 unknownspk00 1.4000 /data/sid/audio/file1.wav 13.280 29.960 unknownspk01 1.4000 /data/sid/audio/file1.wav 30.350 32.030 unknownspk00 1.4000 /data/sid/audio/file2.wav 32.310 46.980 Phil 2.5333 /data/sid/audio/file2.wav 47.790 51.120 unknownspk02 1.4000 /data/sid/audio/file2.wav 54.340 55.400 unknownspk00 1.4000","title":"Outputs"},{"location":"plugins/sdd-diarizeEmbed-v1.html#enrollments","text":"Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sdd-diarizeEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sdd-diarizeEmbed-v1.html#compatibility","text":"OLIVE 5.3+","title":"Compatibility"},{"location":"plugins/sdd-diarizeEmbed-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sdd-diarizeEmbed-v1.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/sdd-diarizeEmbed-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sdd-diarizeEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/sdd-diarizeEmbed-v1.html#segmentation-by-diarization","text":"By default, this plugin uses blind speaker diarization to separate individual speakers within a file. This allows us to have much higher resolution when determining speaker boundaries than the previous plugins' \"segmentation by classification\" approach, and also avoids forcing a decision to be made between bounday resolution and processing speed. Under this segmentation scheme, the plugin first attempts to perform speaker diarization on the audio, clustering speakers that it believes are similar together, and labeling those regions appropriately unknownspk00 , unknownspk01 , etc. Once it has hypothesized individual speakear regions within the audio, it will then compare that audio against any enrolled speakers if they exist. If the plugin is confident enough that the speech of one of the \"unknowns\" actually belongs to an enrolled speaker, it updates the label and score for those regions accordingly.","title":"Segmentation By Diarization"},{"location":"plugins/sdd-diarizeEmbed-v1.html#segmentation-by-classification-legacy-optional","text":"This segmentation method is an optional, legacy method of clustering the speakers found within a file. Due to design considerations, it will always have to compromise between processing speed and diarization resolution. It should only be enabled by experienced users under specific circumstances. Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin performs when operating under SBC takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification (Legacy, Optional)"},{"location":"plugins/sdd-diarizeEmbed-v1.html#options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False dia_fixed_unknown_speaker_score The placeholder score assigned to unknown speaker regions to maintain region scoring format compatbility. 1.4 -1000 - 10000 enable_diarization Determines whether segmentation by diarization is performed, to use blind diarization to automatically cluster and segment the file into different speakers. If set to false, segmentation by classification is performed is performed instead, using the sliding window approach described above. True True or False enable_diarization_unknown_spk_output Determines whether labels for unknown speakers are output. If set to false, plugin only performs speaker detection and will not attempt to label unknown speakers. True True or False","title":"Options"},{"location":"plugins/sdd-diarizeEmbed-v1.html#additional-option-notes","text":"","title":"Additional option notes"},{"location":"plugins/sdd-diarizeEmbed-v1.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/sdd-diarizeEmbed-v1.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables only come into play if the enable_diarization option is set to false and the plugin is processing via segmentation by classification instead of segmentation by diarization. This is an optional, legacy mode for the plugin and will likely not be encountered. These options can be ignored unless you are intentionally enabling the legacy segmentation method. These options determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"win_sec and step_sec"},{"location":"plugins/sdd-diarizeEmbed-v1.html#output_only_highest_scoring_detected_speaker","text":"The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (/data/sid/audio/file2.wav) with scores for three different speakers previously enrolled, /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 /data/sid/audio/file2.wav 54.340 55.400 Sarah 2.430 and we change the threshold to 3.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: /data/sid/audio/file2.wav 41.130 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 However, with output_only_highest_scoring_detected_speaker = False, the system reports: /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"output_only_highest_scoring_detected_speaker"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html","text":"sdd-diarizeEmbedSmolive-v1 (Speaker Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.0.2 Feature-enhanced plugin, this version adds a float-32 full-resolution model to back off to when loading of the quantized 'smolive' model fails, which can happen on certain hardware that does not fully support the quantized model. Released with OLIVE 5.4.0. Description The model in this plugin has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. This plugin bridges the gap between Speaker Detection and Speaker Diarization plugins. As it processes the submitted audio, the diarization stage will segment the submitted audio to determine 'who spoke when' by automatically clustering speech regions that it determines to be attributable to the same speaker, and labels them with class names such as 'unknownspk1', 'unknownspk2', etc. With those regions established, if there are speaker enrollments for different speakers of interest, the plugin will then attempt to determine if any of those \"unknown speaker\" regions belong to one or more of the enrolled speakers of interest, similar to a traditional Speaker Detection plugin. The output then combines these operations, by outputting the enrolled class name for regions where the system is confident that an enrolled speaker is detected, and retaining the \"unknown speaker\" labels where the system isn't confident that the speech comes from an enrolled speaker. Domains telClosetalk-smart-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 unknownspk00 1.4000 /data/sid/audio/file1.wav 13.280 29.960 unknownspk01 1.4000 /data/sid/audio/file1.wav 30.350 32.030 unknownspk00 1.4000 /data/sid/audio/file2.wav 32.310 46.980 Phil 2.5333 /data/sid/audio/file2.wav 47.790 51.120 unknownspk02 1.4000 /data/sid/audio/file2.wav 54.340 55.400 unknownspk00 1.4000 Enrollments Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.4+ Limitations Known or potential limitations of the plugin are outlined below. Quantized Model Hardware Compatibility There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Comments Segmentation By Diarization By default, this plugin uses blind speaker diarization to separate individual speakers within a file. This allows us to have much higher resolution when determining speaker boundaries than the previous plugins' \"segmentation by classification\" approach, and also avoids forcing a decision to be made between bounday resolution and processing speed. Under this segmentation scheme, the plugin first attempts to perform speaker diarization on the audio, clustering speakers that it believes are similar together, and labeling those regions appropriately unknownspk00 , unknownspk01 , etc. Once it has hypothesized individual speakear regions within the audio, it will then compare that audio against any enrolled speakers if they exist. If the plugin is confident enough that the speech of one of the \"unknowns\" actually belongs to an enrolled speaker, it updates the label and score for those regions accordingly. Segmentation By Classification (Legacy, Optional) This segmentation method is an optional, legacy method of clustering the speakers found within a file. Due to design considerations, it will always have to compromise between processing speed and diarization resolution. It should only be enabled by experienced users under specific circumstances. Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin performs when operating under SBC takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type. Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False dia_fixed_unknown_speaker_score The placeholder score assigned to unknown speaker regions to maintain region scoring format compatbility. 1.4 -1000 - 10000 enable_diarization Determines whether segmentation by diarization is performed, to use blind diarization to automatically cluster and segment the file into different speakers. If set to false, segmentation by classification is performed is performed instead, using the sliding window approach described above. True True or False enable_diarization_unknown_spk_output Determines whether labels for unknown speakers are output. If set to false, plugin only performs speaker detection and will not attempt to label unknown speakers. True True or False Additional option notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables only come into play if the enable_diarization option is set to false and the plugin is processing via segmentation by classification instead of segmentation by diarization. This is an optional, legacy mode for the plugin and will likely not be encountered. These options can be ignored unless you are intentionally enabling the legacy segmentation method. These options determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels output_only_highest_scoring_detected_speaker The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (/data/sid/audio/file2.wav) with scores for three different speakers previously enrolled, /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 /data/sid/audio/file2.wav 54.340 55.400 Sarah 2.430 and we change the threshold to 3.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: /data/sid/audio/file2.wav 41.130 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 However, with output_only_highest_scoring_detected_speaker = False, the system reports: /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"Speaker Detection (SDD) Low Resource"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#sdd-diarizeembedsmolive-v1-speaker-detection","text":"","title":"sdd-diarizeEmbedSmolive-v1 (Speaker Detection)"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin, released with OLIVE 5.3.0 v1.0.2 Feature-enhanced plugin, this version adds a float-32 full-resolution model to back off to when loading of the quantized 'smolive' model fails, which can happen on certain hardware that does not fully support the quantized model. Released with OLIVE 5.4.0.","title":"Version Changelog"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#description","text":"The model in this plugin has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. This plugin bridges the gap between Speaker Detection and Speaker Diarization plugins. As it processes the submitted audio, the diarization stage will segment the submitted audio to determine 'who spoke when' by automatically clustering speech regions that it determines to be attributable to the same speaker, and labels them with class names such as 'unknownspk1', 'unknownspk2', etc. With those regions established, if there are speaker enrollments for different speakers of interest, the plugin will then attempt to determine if any of those \"unknown speaker\" regions belong to one or more of the enrolled speakers of interest, similar to a traditional Speaker Detection plugin. The output then combines these operations, by outputting the enrolled class name for regions where the system is confident that an enrolled speaker is detected, and retaining the \"unknown speaker\" labels where the system isn't confident that the speech comes from an enrolled speaker.","title":"Description"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#domains","text":"telClosetalk-smart-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations.","title":"Domains"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#outputs","text":"In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 unknownspk00 1.4000 /data/sid/audio/file1.wav 13.280 29.960 unknownspk01 1.4000 /data/sid/audio/file1.wav 30.350 32.030 unknownspk00 1.4000 /data/sid/audio/file2.wav 32.310 46.980 Phil 2.5333 /data/sid/audio/file2.wav 47.790 51.120 unknownspk02 1.4000 /data/sid/audio/file2.wav 54.340 55.400 unknownspk00 1.4000","title":"Outputs"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#enrollments","text":"Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#quantized-model-hardware-compatibility","text":"There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function.","title":"Quantized Model Hardware Compatibility"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#segmentation-by-diarization","text":"By default, this plugin uses blind speaker diarization to separate individual speakers within a file. This allows us to have much higher resolution when determining speaker boundaries than the previous plugins' \"segmentation by classification\" approach, and also avoids forcing a decision to be made between bounday resolution and processing speed. Under this segmentation scheme, the plugin first attempts to perform speaker diarization on the audio, clustering speakers that it believes are similar together, and labeling those regions appropriately unknownspk00 , unknownspk01 , etc. Once it has hypothesized individual speakear regions within the audio, it will then compare that audio against any enrolled speakers if they exist. If the plugin is confident enough that the speech of one of the \"unknowns\" actually belongs to an enrolled speaker, it updates the label and score for those regions accordingly.","title":"Segmentation By Diarization"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#segmentation-by-classification-legacy-optional","text":"This segmentation method is an optional, legacy method of clustering the speakers found within a file. Due to design considerations, it will always have to compromise between processing speed and diarization resolution. It should only be enabled by experienced users under specific circumstances. Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin performs when operating under SBC takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification (Legacy, Optional)"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False dia_fixed_unknown_speaker_score The placeholder score assigned to unknown speaker regions to maintain region scoring format compatbility. 1.4 -1000 - 10000 enable_diarization Determines whether segmentation by diarization is performed, to use blind diarization to automatically cluster and segment the file into different speakers. If set to false, segmentation by classification is performed is performed instead, using the sliding window approach described above. True True or False enable_diarization_unknown_spk_output Determines whether labels for unknown speakers are output. If set to false, plugin only performs speaker detection and will not attempt to label unknown speakers. True True or False","title":"Options"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#additional-option-notes","text":"","title":"Additional option notes"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables only come into play if the enable_diarization option is set to false and the plugin is processing via segmentation by classification instead of segmentation by diarization. This is an optional, legacy mode for the plugin and will likely not be encountered. These options can be ignored unless you are intentionally enabling the legacy segmentation method. These options determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"win_sec and step_sec"},{"location":"plugins/sdd-diarizeEmbedSmolive-v1.html#output_only_highest_scoring_detected_speaker","text":"The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (/data/sid/audio/file2.wav) with scores for three different speakers previously enrolled, /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 /data/sid/audio/file2.wav 54.340 55.400 Sarah 2.430 and we change the threshold to 3.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: /data/sid/audio/file2.wav 41.130 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 However, with output_only_highest_scoring_detected_speaker = False, the system reports: /data/sid/audio/file2.wav 32.310 46.980 Phil 3.5333 /data/sid/audio/file2.wav 32.310 41.120 Fred 5.400 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"output_only_highest_scoring_detected_speaker"},{"location":"plugins/sdd-sbcEmbed-v2.html","text":"sdd-sbcEmbed-v2 (Speaker Detection) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 Updated to be compatible with OLIVE 5.1.0 v2.0.3 Bug fixes, released with OLIVE 5.2.0 Description Speaker Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled speakers are detected. Unlike Speaker Identification (SID), SDD is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations of speakers when speech from one of the enrolled speakers is found. The goal of speaker detection is to identify and label regions within an audio file where enrolled target speakers are talking. This capability is designed to be used in files with multiple talkers speaking within the same file. For files where it is certain that only one talker will be present, either because it is collected this way or because a human has segmented the file, speaker recognition (SID) plugins should be used. This release of speaker detection is based on \"segmentation-by-classification\", in which the enrolled speakers are detected using a sliding and overlapping window over the file. This plugin does not do do \"diarization\"; it is only searching for regions where the particular enrolled speakers are talking and ignores all non-target speakers. The plugin is based on a core speaker recognition framework using speaker embeddings with PLDA backend and duration-aware calibration. This plugin improves on prior, deprecated technology based on diarization-and-classification using variational bayes diarization followed by scoring of discrete speaker regions. This approach had several major issues, including high use of computational resources, very long and unpredictable run-times (often 10 times slower than real time) and unpredictable performance. This release is average 30 times faster than the previous release, with much greater robustness for a variety of conditions and highly predictable performance. Domains micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording. telClosetalk-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 speaker1 -0.5348 /data/sid/audio/file1.wav 13.280 29.960 speaker2 3.2122 /data/sid/audio/file1.wav 30.350 32.030 speaker3 -5.5340 /data/sid/audio/file2.wav 32.310 46.980 speaker1 0.5333 /data/sid/audio/file2.wav 47.790 51.120 speaker2 -4.9444 /data/sid/audio/file2.wav 54.340 55.400 speaker3 -2.6564 Enrollments Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Comments Segmentation By Classification Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False Additional option notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels output_only_highest_scoring_detected_speaker The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (S) with scores for three different speakers previously enrolled, S spk1 10.8 S spk2 8.2 S spk3 3.1 and the threshold is 5.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: S spk1 10.8 However, with output_only_highest_scoring_detected_speaker = False, the system reports: S spk1 10.8 S spk2 8.2 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"sdd-sbcEmbed-v2 (Speaker Detection)"},{"location":"plugins/sdd-sbcEmbed-v2.html#sdd-sbcembed-v2-speaker-detection","text":"","title":"sdd-sbcEmbed-v2 (Speaker Detection)"},{"location":"plugins/sdd-sbcEmbed-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 Updated to be compatible with OLIVE 5.1.0 v2.0.3 Bug fixes, released with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/sdd-sbcEmbed-v2.html#description","text":"Speaker Detection plugins will detect and label regions of speech in a submitted audio segment where one or more enrolled speakers are detected. Unlike Speaker Identification (SID), SDD is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations of speakers when speech from one of the enrolled speakers is found. The goal of speaker detection is to identify and label regions within an audio file where enrolled target speakers are talking. This capability is designed to be used in files with multiple talkers speaking within the same file. For files where it is certain that only one talker will be present, either because it is collected this way or because a human has segmented the file, speaker recognition (SID) plugins should be used. This release of speaker detection is based on \"segmentation-by-classification\", in which the enrolled speakers are detected using a sliding and overlapping window over the file. This plugin does not do do \"diarization\"; it is only searching for regions where the particular enrolled speakers are talking and ignores all non-target speakers. The plugin is based on a core speaker recognition framework using speaker embeddings with PLDA backend and duration-aware calibration. This plugin improves on prior, deprecated technology based on diarization-and-classification using variational bayes diarization followed by scoring of discrete speaker regions. This approach had several major issues, including high use of computational resources, very long and unpredictable run-times (often 10 times slower than real time) and unpredictable performance. This release is average 30 times faster than the previous release, with much greater robustness for a variety of conditions and highly predictable performance.","title":"Description"},{"location":"plugins/sdd-sbcEmbed-v2.html#domains","text":"micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording. telClosetalk-v1 Domain focused on close-talking microphones meant to address the audio conditions experienced with telephone conversations.","title":"Domains"},{"location":"plugins/sdd-sbcEmbed-v2.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sdd-sbcEmbed-v2.html#outputs","text":"In the basic case, an SDD plugin returns a list of regions with a score for each detected, enrolled speaker. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SDD plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/sid/audio/file1.wav 8.320 13.110 speaker1 -0.5348 /data/sid/audio/file1.wav 13.280 29.960 speaker2 3.2122 /data/sid/audio/file1.wav 30.350 32.030 speaker3 -5.5340 /data/sid/audio/file2.wav 32.310 46.980 speaker1 0.5333 /data/sid/audio/file2.wav 47.790 51.120 speaker2 -4.9444 /data/sid/audio/file2.wav 54.340 55.400 speaker3 -2.6564","title":"Outputs"},{"location":"plugins/sdd-sbcEmbed-v2.html#enrollments","text":"Speaker Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sdd-sbcEmbed-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sdd-sbcEmbed-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sdd-sbcEmbed-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sdd-sbcEmbed-v2.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/sdd-sbcEmbed-v2.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sdd-sbcEmbed-v2.html#comments","text":"","title":"Comments"},{"location":"plugins/sdd-sbcEmbed-v2.html#segmentation-by-classification","text":"Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/sdd-sbcEmbed-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 output_only_highest_scoring_detected_speaker Determines the output style of the plugin, and whether all speakers scoring over the detection threshold are reported for each applicable speech segment (value= False ), or only the top scoring speaker (value= True ). False True or False","title":"Global Options"},{"location":"plugins/sdd-sbcEmbed-v2.html#additional-option-notes","text":"","title":"Additional option notes"},{"location":"plugins/sdd-sbcEmbed-v2.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/sdd-sbcEmbed-v2.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"win_sec and step_sec"},{"location":"plugins/sdd-sbcEmbed-v2.html#output_only_highest_scoring_detected_speaker","text":"The boolean output_only_highest_scoring_detected_speaker parameter determines the format of the output by the plugin. If output_only_highest_scoring_detected_speaker is set to False , the plugin will report all the speakers above the threshold for a given segment. However, if output_only_highest_scoring_detected_speaker is set as True , the plugin will report only the speaker with the maximum score for a given segment even when multiple speakers have scores above the threshold. An example of this behavior distance follows. If we have a segment (S) with scores for three different speakers previously enrolled, S spk1 10.8 S spk2 8.2 S spk3 3.1 and the threshold is 5.0, then with output_only_highest_scoring_detected_speaker = True, the system reports: S spk1 10.8 However, with output_only_highest_scoring_detected_speaker = False, the system reports: S spk1 10.8 S spk2 8.2 The default behavior of this plugin is to have this parameter set to False and to report all speaker detections over the detection threshold for each region.","title":"output_only_highest_scoring_detected_speaker"},{"location":"plugins/sdd.html","text":"redirect: plugins/sdd-diarizeEmbedSmolive-v1.md","title":"Sdd"},{"location":"plugins/sdd.html#redirect-pluginssdd-diarizeembedsmolive-v1md","text":"","title":"redirect: plugins/sdd-diarizeEmbedSmolive-v1.md"},{"location":"plugins/sed-rmsEnergy-v1.html","text":"sed-rmsEnergy-v1 (Signal Energy Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Signal energy detection (SED) plugin detects the presence of any sound by measuring the energy level of the input signal. In contrast to the speech activity detector (SAD), this plugin can detect a wide range of sound activity, including speech and non-speech activity. Goal is to detect presence of any signal versus silence or low-level background noises (lower than 60 dB). Unlike SAD, this plugin is using signal processing techniques (RMS) to gate out low energy regions, rather than a machine learning model. Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring. Domains default-v1 Default domain meant for general-purpose audio conditions. Inputs An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored. Outputs SED has two possible output formats: frame scores and region scores, depending on which API is used to interface with the plug-in. Frame scoring interface returns a score (log-likelihood ratio) for each 10ms frame of the input audio segment. \u201c0\u201d is generally used as the threshold for detecting sound regions. Region scores internally post-process frame scores to return sound regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: 0.00223 0.00237 0.00250 0.00265 0.00280 0.00291 0.00305 0.00319 0.00332 0.00345 This can be transformed into a region scoring output, either at the client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where sound was detected in the audio. When the plugin is invoked with the region scoring interface, the conversion from frame scores to region scores is done internally by applying a threshold and padding to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions in seconds. An example of output SED region scores: Test.wav 0.50 1.55 sound 0.00000000 Test.wav 1.54 13.94 sound 0.00000000 Test.wav 14.42 17.06 sound 0.00000000 Test.wav 17.73 35.54 sound 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest This SED plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SED will return results only for those specified regions. Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Frame Scoring vs. Region Scoring We recommend using the \u2018region scoring\u2019 interface for most applications, since thresholding and padding the raw frame scores can be complicated and require expert knowledge. Threshold This plugin detects presence of signal based on the rms energy level of the signal, thus it may produce some false detections in heavy noise conditions likely higher than 60 dB. This can be adjusted by changing the threshold value. Minimum Audio Length A minimum waveform duration of 0.31 seconds is required to produce a meaningful detection. False Alarms in heavy noise condition This plugin detects presence of signal based on rms energy level of the signal, thus it may produce some false detections in heavy noise conditions likely higher than 60 dB. This can be adjusted by changing the threshold value. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 0.0 to 0.5","title":"sed-rmsEnergy-v1 (Signal Energy Detection)"},{"location":"plugins/sed-rmsEnergy-v1.html#sed-rmsenergy-v1-signal-energy-detection","text":"","title":"sed-rmsEnergy-v1 (Signal Energy Detection)"},{"location":"plugins/sed-rmsEnergy-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/sed-rmsEnergy-v1.html#description","text":"Signal energy detection (SED) plugin detects the presence of any sound by measuring the energy level of the input signal. In contrast to the speech activity detector (SAD), this plugin can detect a wide range of sound activity, including speech and non-speech activity. Goal is to detect presence of any signal versus silence or low-level background noises (lower than 60 dB). Unlike SAD, this plugin is using signal processing techniques (RMS) to gate out low energy regions, rather than a machine learning model. Speech activity detection (SAD, often referred to as voice activity detection) detects the presence of human vocalizations (speech) for each region or frame. In general, SAD outputs are processed for human listening and contain a very short buffer around each detected region to avoid edge effects when listening to the speech regions. This plugin is a general-purpose DNN-based speech activity detection that is now capable of performing both frame scoring and region scoring.","title":"Description"},{"location":"plugins/sed-rmsEnergy-v1.html#domains","text":"default-v1 Default domain meant for general-purpose audio conditions.","title":"Domains"},{"location":"plugins/sed-rmsEnergy-v1.html#inputs","text":"An audio file or buffer and (optional) identifier and (optional) regions to be scored. If no regions are specific the entire audio file or buffer will be scored.","title":"Inputs"},{"location":"plugins/sed-rmsEnergy-v1.html#outputs","text":"SED has two possible output formats: frame scores and region scores, depending on which API is used to interface with the plug-in. Frame scoring interface returns a score (log-likelihood ratio) for each 10ms frame of the input audio segment. \u201c0\u201d is generally used as the threshold for detecting sound regions. Region scores internally post-process frame scores to return sound regions, with some padding and interpolation. An excerpt example of what frame scores typically look like: 0.00223 0.00237 0.00250 0.00265 0.00280 0.00291 0.00305 0.00319 0.00332 0.00345 This can be transformed into a region scoring output, either at the client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where sound was detected in the audio. When the plugin is invoked with the region scoring interface, the conversion from frame scores to region scores is done internally by applying a threshold and padding to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions in seconds. An example of output SED region scores: Test.wav 0.50 1.55 sound 0.00000000 Test.wav 1.54 13.94 sound 0.00000000 Test.wav 14.42 17.06 sound 0.00000000 Test.wav 17.73 35.54 sound 0.00000000 The final number in this format is a placeholding number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/sed-rmsEnergy-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest This SED plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, SED will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/sed-rmsEnergy-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sed-rmsEnergy-v1.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/sed-rmsEnergy-v1.html#frame-scoring-vs-region-scoring","text":"We recommend using the \u2018region scoring\u2019 interface for most applications, since thresholding and padding the raw frame scores can be complicated and require expert knowledge.","title":"Frame Scoring vs. Region Scoring"},{"location":"plugins/sed-rmsEnergy-v1.html#threshold","text":"This plugin detects presence of signal based on the rms energy level of the signal, thus it may produce some false detections in heavy noise conditions likely higher than 60 dB. This can be adjusted by changing the threshold value.","title":"Threshold"},{"location":"plugins/sed-rmsEnergy-v1.html#minimum-audio-length","text":"A minimum waveform duration of 0.31 seconds is required to produce a meaningful detection.","title":"Minimum Audio Length"},{"location":"plugins/sed-rmsEnergy-v1.html#false-alarms-in-heavy-noise-condition","text":"This plugin detects presence of signal based on rms energy level of the signal, thus it may produce some false detections in heavy noise conditions likely higher than 60 dB. This can be adjusted by changing the threshold value.","title":"False Alarms in heavy noise condition"},{"location":"plugins/sed-rmsEnergy-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_sad_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false speech segments. Reduce the threshold value if there are too many missed speech segments. 0.0 0.0 to 0.5","title":"Global Options"},{"location":"plugins/shl-sbcEmbed-v1.html","text":"shl-sbcEmbed-v1 (Speaker Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 Updated to be compatible with OLIVE 5.1.0 v1.0.2 Bug fixes, released with OLIVE 5.2.0 Description Speaker Highlighting plugins will detect and label regions of speech in a submitted audio segment by searching for more speech within the audio that resembles the speaker in one or more user-provided 'seed' regions. Unlike Speaker Identification (SID), SHL is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations where the desired seed speaker is found. Unlike SID and SDD, Speaker Highlighting is not capable of performing any type of 'enrollment', and can only search for more examples of a speaker from within a given audio segment. There is no persistent speaker information retained for future trials, so a new target speaker seed must be supplied each time a new analysis is requested. Speaker Highlighting is meant to be used as a quick triage tool within a file, often longer files, where finding more or all speech from a given speaker is the goal. This can be done as a front-end for a task such as Speaker Redaction, where you'd like to remove, disguise, or otherwise process sections of a given speaker's voice, or as a 'helper' tool for Speaker Identification or Speaker Detection, to assist in building better speaker enrollment models for those plugins, by quickly finding additional candidate speech to add to an enrollment. Domains micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording. Inputs For scoring, an audio buffer or file, in addition to one or more timestamp regions denoting known locations of the speaker of interest. Outputs Speaker Highlighting returns a list of regions with an associated score for each region in the audio where the speaker is determined as 'detected'. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SHL plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/audio/file1.wav 8.320 13.110 speaker 0.5348 /data/audio/file1.wav 13.280 29.960 speaker 3.2122 /data/audio/file1.wav 30.350 32.030 speaker 5.5340 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Speaker Information Persistence Since Speaker Highlighting has no concept of enrollments, no information is retained between different audio analysis queries, and can only search for more of a given speaker within individual audio buffers or files. It can also only search audio for a single speaker at a time, since it assumes that all provided timestamped regions belong to the same speaker. Just as SID and SDD are sensitive to having \"good\" data provided as enrollment exemplars to perform properly, care must be applied when choosing the timestamp regions of the target speaker to 'seed' the system. If you provide a region that contains speech from multiple speakers, or is too short, or is noisy, or otherwise compromised, the performance of the system will degrade. Labeling Resolution vs. Processing Speed vs. Detection Accuracy Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults. Minimum Speech Duration The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default). Comments Segmentation By Classification Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0 Additional option notes min_speech The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin. win_sec and step_sec The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"Speaker Highlighting (SHL)"},{"location":"plugins/shl-sbcEmbed-v1.html#shl-sbcembed-v1-speaker-detection","text":"","title":"shl-sbcEmbed-v1 (Speaker Detection)"},{"location":"plugins/shl-sbcEmbed-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 Updated to be compatible with OLIVE 5.1.0 v1.0.2 Bug fixes, released with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/shl-sbcEmbed-v1.html#description","text":"Speaker Highlighting plugins will detect and label regions of speech in a submitted audio segment by searching for more speech within the audio that resembles the speaker in one or more user-provided 'seed' regions. Unlike Speaker Identification (SID), SHL is capable of handling audio with multiple talkers, as in a telephone conversation, and will provide timestamp region labels to point to the locations where the desired seed speaker is found. Unlike SID and SDD, Speaker Highlighting is not capable of performing any type of 'enrollment', and can only search for more examples of a speaker from within a given audio segment. There is no persistent speaker information retained for future trials, so a new target speaker seed must be supplied each time a new analysis is requested. Speaker Highlighting is meant to be used as a quick triage tool within a file, often longer files, where finding more or all speech from a given speaker is the goal. This can be done as a front-end for a task such as Speaker Redaction, where you'd like to remove, disguise, or otherwise process sections of a given speaker's voice, or as a 'helper' tool for Speaker Identification or Speaker Detection, to assist in building better speaker enrollment models for those plugins, by quickly finding additional candidate speech to add to an enrollment.","title":"Description"},{"location":"plugins/shl-sbcEmbed-v1.html#domains","text":"micFarfield-v1 Domain optimized for microphones at various non-close distances from the speaker, designed to deal with natural room reverberation and other artifacts resulting from far-field audio recording.","title":"Domains"},{"location":"plugins/shl-sbcEmbed-v1.html#inputs","text":"For scoring, an audio buffer or file, in addition to one or more timestamp regions denoting known locations of the speaker of interest.","title":"Inputs"},{"location":"plugins/shl-sbcEmbed-v1.html#outputs","text":"Speaker Highlighting returns a list of regions with an associated score for each region in the audio where the speaker is determined as 'detected'. Regions are represented in seconds. As with SID, scores are log-likelihood ratios where a score of greater than \u201c0\u201d is considered a detection. The SHL plugins are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Example output: /data/audio/file1.wav 8.320 13.110 speaker 0.5348 /data/audio/file1.wav 13.280 29.960 speaker 3.2122 /data/audio/file1.wav 30.350 32.030 speaker 5.5340","title":"Outputs"},{"location":"plugins/shl-sbcEmbed-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected speaker of interest and corresponding score for this speaker. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/shl-sbcEmbed-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/shl-sbcEmbed-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/shl-sbcEmbed-v1.html#speaker-information-persistence","text":"Since Speaker Highlighting has no concept of enrollments, no information is retained between different audio analysis queries, and can only search for more of a given speaker within individual audio buffers or files. It can also only search audio for a single speaker at a time, since it assumes that all provided timestamped regions belong to the same speaker. Just as SID and SDD are sensitive to having \"good\" data provided as enrollment exemplars to perform properly, care must be applied when choosing the timestamp regions of the target speaker to 'seed' the system. If you provide a region that contains speech from multiple speakers, or is too short, or is noisy, or otherwise compromised, the performance of the system will degrade.","title":"Speaker Information Persistence"},{"location":"plugins/shl-sbcEmbed-v1.html#labeling-resolution-vs-processing-speed-vs-detection-accuracy","text":"Region scoring is performed by first identifying speech regions and then processing the resulting speech regions above a certain length ( win_sec ) with a sliding window. Altering the default parameters for this windowing algorithm will have some impacts and tradeoffs with the plugin's overall performance. Shortening the window and/or step size will allow the plugin to have a finer resolution when labeling speaker regions, by allowing it to make decisions on a smaller scale. The tradeoff made by a shorter window size, though, is that the system will have less maximum speech to make its decisions, resulting in a potentially lower speaker labeling accuracy, particularly affecting the rate of missed speech. A shorter step size will result in more window overlap, and therefore more audio segments that are processed multiple times, causing the processing time of the plugin to increase. These tradeoffs must be managed with care if changing the parameters from their defaults.","title":"Labeling Resolution vs. Processing Speed vs. Detection Accuracy"},{"location":"plugins/shl-sbcEmbed-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker detection on segments of speech that are longer than X seconds (configurable as min_speech , 2 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/shl-sbcEmbed-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/shl-sbcEmbed-v1.html#segmentation-by-classification","text":"Live, multi-talker conversational speech is a very challenging domain due to its high variability and quantity of speech from many speakers across varying conditions. Rather than exhaustively segment a file to identify pure regions with a single talker (the vast majority of whom are not of actual interest), SBC scans through the file quickly using target speaker embeddings to find regions that are likely to be from a speaker of interest, based on the scores for their enrolled model. The approach consists on a sliding window with x-set steps as described in Figure 1. Figure 1: Sliding window approach for Segmentation-by-Classification (SBC) plugin The first step this plugin takes is to mask the audio by performing speech activity detection . This allows some natural segmentation by discovering breaks between speech sections caused by silence, and allows the algorithm to focus on the portions of the audio that actually contain speech. Any speech segment longer than X seconds (configurable as min_speech , default 2 seconds) is then processed to determine the likelihood of containing a speaker of interest. Speech regions of up to X seconds (configurable as win_sec , default 4 seconds) are processed and scored whole, while contiguous segments longer than this are then processed using the sliding window algorithm shown above, whose parameters (window size/ win_sec and step size/ step_sec ) are configurable if you find the defaults not to work well with your data type.","title":"Segmentation By Classification"},{"location":"plugins/shl-sbcEmbed-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range det_threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 10.0 win_sec Length in seconds of the sliding window used to chunk audio into segments that will be scored by speaker recognition. See below for notes on how this will impact the system's performance. 4.0 2.0 to 8.0 step_sec Amount of time in seconds the sliding window will shift each time it steps. See below for important notes about the sliding window algorithm behavior. A generally good rule of thumb to follow for setting this parameter is half of the window size. 2.0 1.0 to 4.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 2.0 1.0 - 4.0","title":"Global Options"},{"location":"plugins/shl-sbcEmbed-v1.html#additional-option-notes","text":"","title":"Additional option notes"},{"location":"plugins/shl-sbcEmbed-v1.html#min_speech","text":"The min_speech parameter determines the minimum amount of contiguous speech in a segment required before OLIVE will analyze it to attempt to detect enrolled speakers. This is limited to contiguous speech since we do not want the system to score audio that may be separated by a substantial amount of non-speech, due to the likelihood of including speech from two distinct talkers. The parameter is a float value in seconds, and is by default set to 2.0 seconds. Any speech segment whose length is shorter than this value will be ignored by the speaker-scoring portion of the plugin.","title":"min_speech"},{"location":"plugins/shl-sbcEmbed-v1.html#win_sec-and-step_sec","text":"The win_sec and step_sec variables determine the length of the window and the step size of the windowing algorithm respectively. Both parameters are represented in seconds. These parameters affect the accuracy, the precision in the boundaries between speakers, and the speed of the approach. Figure 2 shows an example on how the modification of size of the window (W) and the step (S) affect those factors. Figure 2: Example of changing the win_sec and step_sec parameters and how this influences the algorithm speed as well as the precision and accuracy of the resulting speaker boundary labels","title":"win_sec and step_sec"},{"location":"plugins/shl.html","text":"redirect: plugins/shl-sbcEmbed-v1.md","title":"Shl"},{"location":"plugins/shl.html#redirect-pluginsshl-sbcembed-v1md","text":"","title":"redirect: plugins/shl-sbcEmbed-v1.md"},{"location":"plugins/sid-dplda-v2.html","text":"sid-dplda-v2 (Speaker Identification) Version Changelog Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 Updated to be compatible with OLIVE 5.1.0 v2.0.2 Compatibility and bug fixes, released with OLIVE 5.2.0 Description Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. This is a SID plugin leveraging dynamic calibration and discrimination via a DNN-powered DPLDA backend. This plugin accounts for the conditions of the trial to provide superior calibration performance out-of-the-box relative to prior plugins. This plugin features: Discriminative PLDA: SRI-pioneered approach to modeling of speaker variability using a DNN-trained backend and internal calibration defined by conditions of the audio. This approach is considerably faster and more reliable than the prior approach from SRI termed Trial-based Calibration (TBC). Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained. Domains multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Enrollments SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Detection Granularity All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment. Minimum Speech Duration The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default). Comments The plugin will only process files with at least 0.5 seconds of detected speech (configurable). A custom option to output embeddings, the speech regions, and duration of speech is available by setting output_ivs_dump_path=$OUTPUT_PATH where OUTPUT_PATH is a user defined directory to store items. During enrollment and evaluation, if the information for an audio file (based on md5sum) exists in this directory, it is loaded instead of re-computed. The plugin is an audio vectorizer and class exporter/importer. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 20.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.5 0.5 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"sid-dplda-v2 (Speaker Identification)"},{"location":"plugins/sid-dplda-v2.html#sid-dplda-v2-speaker-identification","text":"","title":"sid-dplda-v2 (Speaker Identification)"},{"location":"plugins/sid-dplda-v2.html#version-changelog","text":"Plugin Version Change v2.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.0.0 v2.0.1 Updated to be compatible with OLIVE 5.1.0 v2.0.2 Compatibility and bug fixes, released with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/sid-dplda-v2.html#description","text":"Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. This is a SID plugin leveraging dynamic calibration and discrimination via a DNN-powered DPLDA backend. This plugin accounts for the conditions of the trial to provide superior calibration performance out-of-the-box relative to prior plugins. This plugin features: Discriminative PLDA: SRI-pioneered approach to modeling of speaker variability using a DNN-trained backend and internal calibration defined by conditions of the audio. This approach is considerably faster and more reliable than the prior approach from SRI termed Trial-based Calibration (TBC). Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained.","title":"Description"},{"location":"plugins/sid-dplda-v2.html#domains","text":"multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs.","title":"Domains"},{"location":"plugins/sid-dplda-v2.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sid-dplda-v2.html#outputs","text":"Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Outputs"},{"location":"plugins/sid-dplda-v2.html#enrollments","text":"SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sid-dplda-v2.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sid-dplda-v2.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sid-dplda-v2.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sid-dplda-v2.html#detection-granularity","text":"All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment.","title":"Detection Granularity"},{"location":"plugins/sid-dplda-v2.html#minimum-speech-duration","text":"The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sid-dplda-v2.html#comments","text":"The plugin will only process files with at least 0.5 seconds of detected speech (configurable). A custom option to output embeddings, the speech regions, and duration of speech is available by setting output_ivs_dump_path=$OUTPUT_PATH where OUTPUT_PATH is a user defined directory to store items. During enrollment and evaluation, if the information for an audio file (based on md5sum) exists in this directory, it is loaded instead of re-computed. The plugin is an audio vectorizer and class exporter/importer.","title":"Comments"},{"location":"plugins/sid-dplda-v2.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 20.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.5 0.5 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/sid-dplda-v3.html","text":"sid-dplda-v3 (Speaker Identification) Version Changelog Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v2.0.2, but updated to be include GPU support with proper configuration. Tested and released with OLIVE 5.5.0 Description Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. This is a SID plugin leveraging dynamic calibration and discrimination via a DNN-powered DPLDA backend. This plugin accounts for the conditions of the trial to provide superior calibration performance out-of-the-box relative to prior plugins. This plugin features: Discriminative PLDA: SRI-pioneered approach to modeling of speaker variability using a DNN-trained backend and internal calibration defined by conditions of the audio. This approach is considerably faster and more reliable than the prior approach from SRI termed Trial-based Calibration (TBC). Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained. Domains multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Enrollments SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Detection Granularity All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment. Minimum Speech Duration The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default). Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Minimum Speech The plugin will only process files with at least 0.5 seconds of detected speech (configurable). Advanced (Experimental) Usage for Exporting Embeddings A custom option to output embeddings, the speech regions, and duration of speech is available by setting output_ivs_dump_path=$OUTPUT_PATH where OUTPUT_PATH is a user defined directory to store items. During enrollment and evaluation, if the information for an audio file (based on md5sum) exists in this directory, it is loaded instead of re-computed. The plugin is an audio vectorizer and class exporter/importer. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 20.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.5 0.5 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Speaker Identification (SID)"},{"location":"plugins/sid-dplda-v3.html#sid-dplda-v3-speaker-identification","text":"","title":"sid-dplda-v3 (Speaker Identification)"},{"location":"plugins/sid-dplda-v3.html#version-changelog","text":"Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v2.0.2, but updated to be include GPU support with proper configuration. Tested and released with OLIVE 5.5.0","title":"Version Changelog"},{"location":"plugins/sid-dplda-v3.html#description","text":"Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. This is a SID plugin leveraging dynamic calibration and discrimination via a DNN-powered DPLDA backend. This plugin accounts for the conditions of the trial to provide superior calibration performance out-of-the-box relative to prior plugins. This plugin features: Discriminative PLDA: SRI-pioneered approach to modeling of speaker variability using a DNN-trained backend and internal calibration defined by conditions of the audio. This approach is considerably faster and more reliable than the prior approach from SRI termed Trial-based Calibration (TBC). Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained.","title":"Description"},{"location":"plugins/sid-dplda-v3.html#domains","text":"multi-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs.","title":"Domains"},{"location":"plugins/sid-dplda-v3.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sid-dplda-v3.html#outputs","text":"Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Outputs"},{"location":"plugins/sid-dplda-v3.html#enrollments","text":"SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sid-dplda-v3.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sid-dplda-v3.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sid-dplda-v3.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sid-dplda-v3.html#detection-granularity","text":"All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment.","title":"Detection Granularity"},{"location":"plugins/sid-dplda-v3.html#minimum-speech-duration","text":"The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sid-dplda-v3.html#comments","text":"","title":"Comments"},{"location":"plugins/sid-dplda-v3.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/sid-dplda-v3.html#minimum-speech","text":"The plugin will only process files with at least 0.5 seconds of detected speech (configurable).","title":"Minimum Speech"},{"location":"plugins/sid-dplda-v3.html#advanced-experimental-usage-for-exporting-embeddings","text":"A custom option to output embeddings, the speech regions, and duration of speech is available by setting output_ivs_dump_path=$OUTPUT_PATH where OUTPUT_PATH is a user defined directory to store items. During enrollment and evaluation, if the information for an audio file (based on md5sum) exists in this directory, it is loaded instead of re-computed. The plugin is an audio vectorizer and class exporter/importer.","title":"Advanced (Experimental) Usage for Exporting Embeddings"},{"location":"plugins/sid-dplda-v3.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output, but of higher reliability. 0.0 -10.0 to 20.0 min_speech The minimum length that a speech segment must contain in order to be scored/analyzed for the presence of enrolled speakers. 0.5 0.5 - 4.0 sad_threshold SAD threshold for determining the audio to be used in meteadata extraction -2.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/sid-embed-v6.html","text":"sid-embed-v6 (Speaker Identification) Version Changelog Plugin Version Change v6.0.0 Initial plugin release, functionally identical to v5.0.0, but updated to be compatible with OLIVE 5.0.0 v6.0.1 Updated to be compatible with OLIVE 5.1.0 v6.0.2 Compatibility and bug fixes, released with OLIVE 5.2.0 Description Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. The release of the sid-embed-v6 plugin provides a new level of performance and automatic adaptation to new domains with a focus on cross-language and non-English speaker trials. This plugin features: Joint PLDA: SRI-pioneered approach to modeling of speaker land language variability jointly in the PLDA modeling space. Dynamic Mean Normalization: A novel approach to automatically adapting a single parameter (mean of the embedding space) based on the enrollment data available from the domain. This innovation provides a major improvement to performance in new domains while counteracting calibration mismatch prior to PLDA modeling. This simplifies the calibration process enabling the use of a single calibration model across domains. By default this option is turned off. Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained. Domains multicond-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. multilang-v1 A domain specifically optimized for cross-languages comparison trials and trained with a preponderance of non-English data, for enhanced performance on non-English data. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Enrollments SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations Known or potential limitations of the plugin are outlined below. Detection Granularity All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment. Minimum Speech Duration The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default). Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range sad_threshold SAD threshold for determining the audio to be used in metadata extraction 1.0 -5.0 - 6.0","title":"sid-embed-v6  (Speaker Identification)"},{"location":"plugins/sid-embed-v6.html#sid-embed-v6-speaker-identification","text":"","title":"sid-embed-v6  (Speaker Identification)"},{"location":"plugins/sid-embed-v6.html#version-changelog","text":"Plugin Version Change v6.0.0 Initial plugin release, functionally identical to v5.0.0, but updated to be compatible with OLIVE 5.0.0 v6.0.1 Updated to be compatible with OLIVE 5.1.0 v6.0.2 Compatibility and bug fixes, released with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/sid-embed-v6.html#description","text":"Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. The release of the sid-embed-v6 plugin provides a new level of performance and automatic adaptation to new domains with a focus on cross-language and non-English speaker trials. This plugin features: Joint PLDA: SRI-pioneered approach to modeling of speaker land language variability jointly in the PLDA modeling space. Dynamic Mean Normalization: A novel approach to automatically adapting a single parameter (mean of the embedding space) based on the enrollment data available from the domain. This innovation provides a major improvement to performance in new domains while counteracting calibration mismatch prior to PLDA modeling. This simplifies the calibration process enabling the use of a single calibration model across domains. By default this option is turned off. Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained.","title":"Description"},{"location":"plugins/sid-embed-v6.html#domains","text":"multicond-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. multilang-v1 A domain specifically optimized for cross-languages comparison trials and trained with a preponderance of non-English data, for enhanced performance on non-English data.","title":"Domains"},{"location":"plugins/sid-embed-v6.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sid-embed-v6.html#outputs","text":"Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Outputs"},{"location":"plugins/sid-embed-v6.html#enrollments","text":"SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sid-embed-v6.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sid-embed-v6.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/sid-embed-v6.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sid-embed-v6.html#detection-granularity","text":"All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment.","title":"Detection Granularity"},{"location":"plugins/sid-embed-v6.html#minimum-speech-duration","text":"The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sid-embed-v6.html#comments","text":"","title":"Comments"},{"location":"plugins/sid-embed-v6.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range sad_threshold SAD threshold for determining the audio to be used in metadata extraction 1.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/sid-embedSmolive-v1.html","text":"sid-embedSmolive-v1 (Speaker Identification) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of sid-embed-v6 but features model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate. Description This plugin is based heavily on its predecessor, sid-embed-v6, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. The release of the sid-embed-v6 plugin provides a new level of performance and automatic adaptation to new domains with a focus on cross-language and non-English speaker trials. This plugin features: Joint PLDA: SRI-pioneered approach to modeling of speaker land language variability jointly in the PLDA modeling space. Dynamic Mean Normalization: A novel approach to automatically adapting a single parameter (mean of the embedding space) based on the enrollment data available from the domain. This innovation provides a major improvement to performance in new domains while counteracting calibration mismatch prior to PLDA modeling. This simplifies the calibration process enabling the use of a single calibration model across domains. By default this option is turned off. Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained. Domains multicond-prun-int8-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs. Inputs For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file. Outputs Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564 Enrollments SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.4+ Limitations Known or potential limitations of the plugin are outlined below. Quantized Model Hardware Compatibility There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function. Detection Granularity All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment. Minimum Speech Duration The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default). Comments Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range sad_threshold SAD threshold for determining the audio to be used in metadata extraction 1.0 -5.0 - 6.0","title":"Speaker Identification (SID) Low Resource"},{"location":"plugins/sid-embedSmolive-v1.html#sid-embedsmolive-v1-speaker-identification","text":"","title":"sid-embedSmolive-v1  (Speaker Identification)"},{"location":"plugins/sid-embedSmolive-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 - This plugin is based off of sid-embed-v6 but features model quantization and pruning in addition to a full-resolution model for hardware compatibility when the pruned model cannot operate.","title":"Version Changelog"},{"location":"plugins/sid-embedSmolive-v1.html#description","text":"This plugin is based heavily on its predecessor, sid-embed-v6, with the important distinction that the model has been modified with both quantization and pruning. Quantized models perform computations at a reduced bit resolution (integer8 in our case) than the standard floating point precision allowing for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. Further details can be found at pytorch Pruning aims to make neural network models smaller by removing a subset of the nodes that have minimal impact on the performance. The pruned neural network is fine-tuned on the target task to minimize the performance loss. The result is a much lighter-weight and often faster-performing model that sacrifices little-to-no accuracy. Speaker Identification plugins score a submitted segment of audio against one or more enrolled speakers with the goal of determining whether the speech in the segment in question was produced by one of the enrolled speakers. The release of the sid-embed-v6 plugin provides a new level of performance and automatic adaptation to new domains with a focus on cross-language and non-English speaker trials. This plugin features: Joint PLDA: SRI-pioneered approach to modeling of speaker land language variability jointly in the PLDA modeling space. Dynamic Mean Normalization: A novel approach to automatically adapting a single parameter (mean of the embedding space) based on the enrollment data available from the domain. This innovation provides a major improvement to performance in new domains while counteracting calibration mismatch prior to PLDA modeling. This simplifies the calibration process enabling the use of a single calibration model across domains. By default this option is turned off. Multi-bandwidth embeddings: The new embeddings DNN leverages the information in the 8-16kHz bandwidth to provided improved accuracy in audio files above 8kHz. No options are required by the user to define a bandwidth of choice as all audio is resampled to 16kHz prior to processing. The upsampling of 8kHz to 16kHz is suitable for this plugin due to the manner in which it was trained.","title":"Description"},{"location":"plugins/sid-embedSmolive-v1.html#domains","text":"multicond-prun-int8-v1 Multi-condition domain tested heavily on telephone and microphone conditions, multiple languages, distances, and varying background noises and codecs.","title":"Domains"},{"location":"plugins/sid-embedSmolive-v1.html#inputs","text":"For enrollment, an audio file or buffer with a corresponding speaker identifier/label. For scoring, an audio buffer or file.","title":"Inputs"},{"location":"plugins/sid-embedSmolive-v1.html#outputs","text":"Generally, a list of scores, one for each of the speakers enrolled in the domain, for the entire segment. As with SAD and LID, scores are log-likelihood ratios where a score of greater than 0 is considered a detection. SID plugins, in particular, due to their association with forensics are generally calibrated or use dynamic calibration to ensure valid log-likelihood ratios to facilitate detection. Plugins may be altered to return only detections, rather than a list of enrollees and scores, but this is generally done on the client side for the sake of flexibility. Example output: /data/sid/audio/file1.wav speaker1 -0.5348 /data/sid/audio/file1.wav speaker2 3.2122 /data/sid/audio/file1.wav speaker3 -5.5340 /data/sid/audio/file2.wav speaker1 0.5333 /data/sid/audio/file2.wav speaker2 -4.9444 /data/sid/audio/file2.wav speaker3 -2.6564","title":"Outputs"},{"location":"plugins/sid-embedSmolive-v1.html#enrollments","text":"SID plugins allow for class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new speaker. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample from a speaker, generally 5 seconds or more, along with a label for that speaker. This enrollment can be augmented with subsequent class modification requests by adding more audio with the same speaker label.","title":"Enrollments"},{"location":"plugins/sid-embedSmolive-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning a single score for the entire audio segment for each of the enrolled speakers of interest. GlobalScorerRequest CLASS_MODIFIER \u2013 Enroll new speaker models or augment existing speaker models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/sid-embedSmolive-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/sid-embedSmolive-v1.html#limitations","text":"Known or potential limitations of the plugin are outlined below.","title":"Limitations"},{"location":"plugins/sid-embedSmolive-v1.html#quantized-model-hardware-compatibility","text":"There are certain host/hardware requirements for the quantized models to be able to run; namely support for avx2. To avoid the situation where a lack of this support would cause the plugin to become nonfuntional, a full-bit (float32) model has been included that will be loaded and used in the rare case that the quantized model fails to load. This will use more memory, but allow the plugin to function.","title":"Quantized Model Hardware Compatibility"},{"location":"plugins/sid-embedSmolive-v1.html#detection-granularity","text":"All current SID plugins assume that an audio segment contains only a single speaker and may be scored as a single unit. If a given segment contains multiple speakers, the entire segment will still be scored as a unit. Speaker detection (SDD) represents another plugin type with the goal of locating known speakers, but that does not have this assumption, and will instead attempt to locate and label regions consisting of individual speakers within the audio segment.","title":"Detection Granularity"},{"location":"plugins/sid-embedSmolive-v1.html#minimum-speech-duration","text":"The system will only attempt to perform speaker identification if the submitted audio segment contains more than X seconds of detected speech (configurable as min_speech , 0.5 seconds by default).","title":"Minimum Speech Duration"},{"location":"plugins/sid-embedSmolive-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/sid-embedSmolive-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range sad_threshold SAD threshold for determining the audio to be used in metadata extraction 1.0 -5.0 - 6.0","title":"Global Options"},{"location":"plugins/sid.html","text":"redirect: plugins/sid-dplda-v2.md","title":"Sid"},{"location":"plugins/sid.html#redirect-pluginssid-dplda-v2md","text":"","title":"redirect: plugins/sid-dplda-v2.md"},{"location":"plugins/tmt-neural-v1.html","text":"tmt-neural-v1 (Text Machine Translation) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 v1.1.0 Updated plugin, adds GPU support, and including minor bug fixes and additional domains. Tested and released with OLIVE 5.5.0 v1.1.1 Updated to add Iraqi Arabic to English as an available domain. Tested and released with OLIVE 5.5.1 Description Text Machine Translation plugins perform translation of text from one language to another, typically from one language, specified by the domain, to English. All TMT domains are language-dependent, and each one is meant to work only with a single, specific language. TMT plugins may have some limitations or special processing considerations depending on the language(s) involved and their native alphabets, as well as the training data used to train the underlying models. This is the first neural Text Machine Translation plugin released for the OLIVE architecture. It offers neural-model based translation of text, using a pure C++ neural machine translation toolkit as its base. Each domain provides translation from one language into English, with the three source languages currently available being Mandarin Chinese, Russian, and Spanish. The input and output formats match those shown in the examples below. The goal of this plugin is to ingest text in one language, defined by the specified domain, and translate it to another language. In both currently available domains, the destination language is English. An example input string, in Spanish: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras Note that some punctuation and special characters will be stripped from the input during preprocessing. An example output string translation of the above example, as provided by the spa-eng-generic-v2 domain: manual of photography to learn everything essential about unk cameras Words that the system does not recognize or can't translate will be marked up with an unk tag, as can be seen in the example output below. Note that all system output will be lowercase for case sensitive languages, and apart from the unk tag, all output will be devoid of punctuation. Domains (Supported Languages) irq-eng-nmt-v1 Translates Iraqi Arabic text into English text. cmn-eng-nmt-v1 Translates Mandarin text into English text. rus-eng-nmt-v1 Translates Russian text into English text. spa-eng-nmt-v3 Translates Spanish text into English text. ukr-eng-nmt-v3 Translates Ukrainian text into English text. eng-cmn-nmt-v1 Translates English text into Mandarin text. eng-rus-nmt-v1 Translates English text into Russian text. eng-spa-nmt-v3 Translates English text into Spanish text. Inputs For scoring, a text string or text-populated file is required. There is no verification performed by OLIVE or by TMT plugins that the text passed as input is actually in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Note that output may fail or be very confusing if the input language does not match the domain's capabilities. An example input string: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras There is a bit of preprocessing of input that occurs before a string is sent to translation. Thus, all input is lower-cased and frequent punctuation marks such as commas, exclamation marks or question marks are stripped from the string. However, no spelling error correction of any kind is performed. Outputs The output format for TMT plugins is simply text. Words that the system does not recognize or can't translate will be marked up with an unk tag, as can be seen in the example output below. Note that all system output will be lowercase, and apart from the unk tag, all output will be devoid of punctuation. An example output string translation of the input example: manual of photography to learn everything essential about unk cameras Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. TextTransformer \u2013 Plugin accepts and analyzes text string inputs, and outputs a new text string as output. In the case of TMT plugins, a text string in the source language should be provided as input, with the expectation that the output will be a text string translated to the desired destination language. TextTransformRequest TextTransformResult Compatibility OLIVE 5.4+ Limitations As the debut neural TMT plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. Like ASR, TMT plugins are language dependent and also largely text domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the translation models underlying each domain. Several factors contribute to what might limit the vocabulary of a translation model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, TMT plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. The size-on-disk of each domain can be quite large depending on the technology of the individual plugin, and some technologies/plugins may have additional constraints. See the individual plugin detail pages for more information. Multi-Language Memory Constraints Each domain currently uses a significant amount of memory once the translation models have been loaded by using the plugin. Without substantial memory resources available on the machine hosting OLIVE, it's possible to quickly run out of available memory and run into strange behavior and/or failures - especially when running TMT alongside other memory-intensive plugins like ASR. Language Dependence Each domain of this TMT plugin is language specific, and is only capable of translating text from one single language to one other. There is no filter or any sort of verification performed by OLIVE to ensure that the text passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input text if the source language is unknown, though it is often easy to spot mismatched input languages by the high number of output words that appears with the \" unk \" tag. Spelling Errors Note that there is no spell-checking or other types of spelling related pre-processing that occurs on the input data. Therefore any spelling mistakes in the input are likely to cause the system to output unk tags. Out of Vocabulary (OOV) Words, Names The individual words that the plugin is capable of recognizing and translating is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be translated by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary or to the translation model. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications. Input Expectations This plugin has been trained to translate a single sentence at a time - to maximize the performance of this plugin, each scoring request input should be a single sentence. When longer segments, like paragraphs, pages, or even whole files are input, performance may be less than optimal, but the plugin will 'split' the data into 25-word chunks that it treats as sentences to minimize the negative impact. This is less than ideal, but performs much better on the whole than treating larger inputs as a single sentence and leaving them as-is. Comments GPU Support Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only. Global Options This plugin does not expose any options to the user.","title":"Text Machine Translation (TMT)"},{"location":"plugins/tmt-neural-v1.html#tmt-neural-v1-text-machine-translation","text":"","title":"tmt-neural-v1 (Text Machine Translation)"},{"location":"plugins/tmt-neural-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.4.0 v1.1.0 Updated plugin, adds GPU support, and including minor bug fixes and additional domains. Tested and released with OLIVE 5.5.0 v1.1.1 Updated to add Iraqi Arabic to English as an available domain. Tested and released with OLIVE 5.5.1","title":"Version Changelog"},{"location":"plugins/tmt-neural-v1.html#description","text":"Text Machine Translation plugins perform translation of text from one language to another, typically from one language, specified by the domain, to English. All TMT domains are language-dependent, and each one is meant to work only with a single, specific language. TMT plugins may have some limitations or special processing considerations depending on the language(s) involved and their native alphabets, as well as the training data used to train the underlying models. This is the first neural Text Machine Translation plugin released for the OLIVE architecture. It offers neural-model based translation of text, using a pure C++ neural machine translation toolkit as its base. Each domain provides translation from one language into English, with the three source languages currently available being Mandarin Chinese, Russian, and Spanish. The input and output formats match those shown in the examples below. The goal of this plugin is to ingest text in one language, defined by the specified domain, and translate it to another language. In both currently available domains, the destination language is English. An example input string, in Spanish: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras Note that some punctuation and special characters will be stripped from the input during preprocessing. An example output string translation of the above example, as provided by the spa-eng-generic-v2 domain: manual of photography to learn everything essential about unk cameras Words that the system does not recognize or can't translate will be marked up with an unk tag, as can be seen in the example output below. Note that all system output will be lowercase for case sensitive languages, and apart from the unk tag, all output will be devoid of punctuation.","title":"Description"},{"location":"plugins/tmt-neural-v1.html#domains-supported-languages","text":"irq-eng-nmt-v1 Translates Iraqi Arabic text into English text. cmn-eng-nmt-v1 Translates Mandarin text into English text. rus-eng-nmt-v1 Translates Russian text into English text. spa-eng-nmt-v3 Translates Spanish text into English text. ukr-eng-nmt-v3 Translates Ukrainian text into English text. eng-cmn-nmt-v1 Translates English text into Mandarin text. eng-rus-nmt-v1 Translates English text into Russian text. eng-spa-nmt-v3 Translates English text into Spanish text.","title":"Domains (Supported Languages)"},{"location":"plugins/tmt-neural-v1.html#inputs","text":"For scoring, a text string or text-populated file is required. There is no verification performed by OLIVE or by TMT plugins that the text passed as input is actually in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Note that output may fail or be very confusing if the input language does not match the domain's capabilities. An example input string: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras There is a bit of preprocessing of input that occurs before a string is sent to translation. Thus, all input is lower-cased and frequent punctuation marks such as commas, exclamation marks or question marks are stripped from the string. However, no spelling error correction of any kind is performed.","title":"Inputs"},{"location":"plugins/tmt-neural-v1.html#outputs","text":"The output format for TMT plugins is simply text. Words that the system does not recognize or can't translate will be marked up with an unk tag, as can be seen in the example output below. Note that all system output will be lowercase, and apart from the unk tag, all output will be devoid of punctuation. An example output string translation of the input example: manual of photography to learn everything essential about unk cameras","title":"Outputs"},{"location":"plugins/tmt-neural-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. TextTransformer \u2013 Plugin accepts and analyzes text string inputs, and outputs a new text string as output. In the case of TMT plugins, a text string in the source language should be provided as input, with the expectation that the output will be a text string translated to the desired destination language. TextTransformRequest TextTransformResult","title":"Functionality (Traits)"},{"location":"plugins/tmt-neural-v1.html#compatibility","text":"OLIVE 5.4+","title":"Compatibility"},{"location":"plugins/tmt-neural-v1.html#limitations","text":"As the debut neural TMT plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. Like ASR, TMT plugins are language dependent and also largely text domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the translation models underlying each domain. Several factors contribute to what might limit the vocabulary of a translation model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, TMT plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. The size-on-disk of each domain can be quite large depending on the technology of the individual plugin, and some technologies/plugins may have additional constraints. See the individual plugin detail pages for more information.","title":"Limitations"},{"location":"plugins/tmt-neural-v1.html#multi-language-memory-constraints","text":"Each domain currently uses a significant amount of memory once the translation models have been loaded by using the plugin. Without substantial memory resources available on the machine hosting OLIVE, it's possible to quickly run out of available memory and run into strange behavior and/or failures - especially when running TMT alongside other memory-intensive plugins like ASR.","title":"Multi-Language Memory Constraints"},{"location":"plugins/tmt-neural-v1.html#language-dependence","text":"Each domain of this TMT plugin is language specific, and is only capable of translating text from one single language to one other. There is no filter or any sort of verification performed by OLIVE to ensure that the text passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input text if the source language is unknown, though it is often easy to spot mismatched input languages by the high number of output words that appears with the \" unk \" tag.","title":"Language Dependence"},{"location":"plugins/tmt-neural-v1.html#spelling-errors","text":"Note that there is no spell-checking or other types of spelling related pre-processing that occurs on the input data. Therefore any spelling mistakes in the input are likely to cause the system to output unk tags.","title":"Spelling Errors"},{"location":"plugins/tmt-neural-v1.html#out-of-vocabulary-oov-words-names","text":"The individual words that the plugin is capable of recognizing and translating is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be translated by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary or to the translation model. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications.","title":"Out of Vocabulary (OOV) Words, Names"},{"location":"plugins/tmt-neural-v1.html#input-expectations","text":"This plugin has been trained to translate a single sentence at a time - to maximize the performance of this plugin, each scoring request input should be a single sentence. When longer segments, like paragraphs, pages, or even whole files are input, performance may be less than optimal, but the plugin will 'split' the data into 25-word chunks that it treats as sentences to minimize the negative impact. This is less than ideal, but performs much better on the whole than treating larger inputs as a single sentence and leaving them as-is.","title":"Input Expectations"},{"location":"plugins/tmt-neural-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/tmt-neural-v1.html#gpu-support","text":"Please refer to the OLIVE GPU Installation and Support documentation page for instructions on how to enable and configure GPU capability in supported plugins. By default this plugin will run on CPU only.","title":"GPU Support"},{"location":"plugins/tmt-neural-v1.html#global-options","text":"This plugin does not expose any options to the user.","title":"Global Options"},{"location":"plugins/tmt-statistical-v1.html","text":"tmt-statistical-v1 (Text Machine Translation) Version Changelog Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0 Description Text Machine Translation plugins perform translation of text from one language to another, typically from one language, specified by the domain, to English. All TMT domains are language-dependent, and each one is meant to work only with a single, specific language. TMT plugins may have some limitations or special processing considerations depending on the language(s) involved and their native alphabets, as well as the training data used to train the underlying models. This is the first Text Machine Translation plugin released for the OLIVE architecture. It offers statistical-model based translation of text, using SRI's SRInterp engine. Each domain provides translation from one language into English, with the two source languages currently available being Spanish and French. The input and output formats match those shown in the examples below. The goal of this plugin is to ingest text in one language, defined by the specified domain, and translate it to another language. In both currently available domains, the destination language is English. An example input string, in Spanish: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras Note that some punctuation and special characters will be stripped from the input during preprocessing. An example output string translation of the above example, as provided by the spa-eng-generic-v2 domain: manual of photography to learn everything essential about __UNKNOWN : fujifilm cameras Words that the system does not recognize or can't translate will be marked up with an __UNKNOWN: tag, as can be seen in the example output below. Note that all system output will be lowercase for case sensitive languages, and apart from the __UNKNOWN: tag, all output will be devoid of punctuation. Domains spa-eng-generic-v2 Translates Spanish text into English text. Trained mostly on OpenSubtitles data. fre-eng-generic-v1 Translates French text into English text. Trained mostly on OpenSubtitles data. Inputs For scoring, a text string or text-populated file is required. There is no verification performed by OLIVE or by TMT plugins that the text passed as input is actually in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Note that output may fail or be very confusing if the input language does not match the domain's capabilities. An example input string: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras There is a bit of preprocessing of input that occurs before a string is sent to translation. Thus, all input is lower-cased and frequent punctuation marks such as commas, exclamation marks or question marks are stripped from the string. However, no spelling error correction of any kind is performed. Outputs The output format for TMT plugins is simply text. Words that the system does not recognize or can't translate will be marked up with an __UNKNOWN: tag, as can be seen in the example output below. Note that all system output will be lowercase, and apart from the __UNKNOWN: tag, all output will be devoid of punctuation. An example output string translation of the input example: manual of photography to learn everything essential about __UNKNOWN : fujifilm cameras Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. TextTransformer \u2013 Plugin accepts and analyzes text string inputs, and outputs a new text string as output. In the case of TMT plugins, a text string in the source language should be provided as input, with the expectation that the output will be a text string translated to the desired destination language. TextTransformRequest TextTransformResult Compatibility OLIVE 5.1+ Limitations As the debut TMT plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. Like ASR, TMT plugins are language dependent and also largely text domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the translation models underlying each domain. Several factors contribute to what might limit the vocabulary of a translation model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, TMT plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. The size-on-disk of each domain can be quite large depending on the technology of the individual plugin, and some technologies/plugins may have additional constraints. See the individual plugin detail pages for more information. Multi-Job Threading This plugin is currently not capable of multi-threading or parallel processing. If using this plugin with an OLIVE server, the server must be run in single-worker mode, by specifying a maximum number of jobs of 1: scenicserver -j 1 or scenicserver --workers 1 Alternatively, if the connected client only submits jobs synchronously, waiting for the completion and response of each job before submitting additional queries, problems will be avoided. Due to this limitation, of performing machine translation with this plugin using the CLI tools (localanalyze), if a multi-line input file is provided, this same stipulation of maximizing the active workers to '1' must be used to ensure that the line order of the output text file matches the line order of the input file. Language Dependence Each domain of this TMT plugin is language specific, and is only capable of translating text from one single language to one other. There is no filter or any sort of verification performed by OLIVE to ensure that the text passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input text if the source language is unknown, though it is often easy to spot mismatched input languages by the high number of output words that appears with the \" __UNKNOWN: \" tag. Spelling Errors Note that there is no spell-checking or other types of spelling related pre-processing that occurs on the input data. Therefore any spelling mistakes in the input are likely to cause the system to output __UNKNOWN: tags. Out of Vocabulary (OOV) Words, Names The individual words that the plugin is capable of recognizing and translating is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be translated by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary or to the translation model. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications. Input Limit There is a maximum limit on the input that a single request/query can have. This may depend on input resources available and number of total characters, but currently seems to be roughly 600 words. Extra large inputs that may approach or exceed this number should be split into multiple job submissions. Resources (disk space) Because it is based on statistical MT, this plugin's performance generally directly corresponds to the size of the models it uses, as these models grow the system is exposed to more and more data to learn from. As a result, the included models are very large. Please ensure you have adequate disk space available before attempting to use this plugin. Future TMT plugins will be based on a neural MT architecture that will not have such extreme model size requirements. Comments Global Options This plugin does not expose any options to the user.","title":"tmt-statistical-v1 (Text Machine Translation)"},{"location":"plugins/tmt-statistical-v1.html#tmt-statistical-v1-text-machine-translation","text":"","title":"tmt-statistical-v1 (Text Machine Translation)"},{"location":"plugins/tmt-statistical-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release with OLIVE 5.0.0 v1.0.1 (latest) Updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/tmt-statistical-v1.html#description","text":"Text Machine Translation plugins perform translation of text from one language to another, typically from one language, specified by the domain, to English. All TMT domains are language-dependent, and each one is meant to work only with a single, specific language. TMT plugins may have some limitations or special processing considerations depending on the language(s) involved and their native alphabets, as well as the training data used to train the underlying models. This is the first Text Machine Translation plugin released for the OLIVE architecture. It offers statistical-model based translation of text, using SRI's SRInterp engine. Each domain provides translation from one language into English, with the two source languages currently available being Spanish and French. The input and output formats match those shown in the examples below. The goal of this plugin is to ingest text in one language, defined by the specified domain, and translate it to another language. In both currently available domains, the destination language is English. An example input string, in Spanish: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras Note that some punctuation and special characters will be stripped from the input during preprocessing. An example output string translation of the above example, as provided by the spa-eng-generic-v2 domain: manual of photography to learn everything essential about __UNKNOWN : fujifilm cameras Words that the system does not recognize or can't translate will be marked up with an __UNKNOWN: tag, as can be seen in the example output below. Note that all system output will be lowercase for case sensitive languages, and apart from the __UNKNOWN: tag, all output will be devoid of punctuation.","title":"Description"},{"location":"plugins/tmt-statistical-v1.html#domains","text":"spa-eng-generic-v2 Translates Spanish text into English text. Trained mostly on OpenSubtitles data. fre-eng-generic-v1 Translates French text into English text. Trained mostly on OpenSubtitles data.","title":"Domains"},{"location":"plugins/tmt-statistical-v1.html#inputs","text":"For scoring, a text string or text-populated file is required. There is no verification performed by OLIVE or by TMT plugins that the text passed as input is actually in the language that the domain is capable of recognizing. The burden lies on the user to manually or automatically screen this audio before attempting to recognize. Note that output may fail or be very confusing if the input language does not match the domain's capabilities. An example input string: manual de fotograf\u00eda para aprender todo lo esencial sobre fujifilm c\u00e1maras There is a bit of preprocessing of input that occurs before a string is sent to translation. Thus, all input is lower-cased and frequent punctuation marks such as commas, exclamation marks or question marks are stripped from the string. However, no spelling error correction of any kind is performed.","title":"Inputs"},{"location":"plugins/tmt-statistical-v1.html#outputs","text":"The output format for TMT plugins is simply text. Words that the system does not recognize or can't translate will be marked up with an __UNKNOWN: tag, as can be seen in the example output below. Note that all system output will be lowercase, and apart from the __UNKNOWN: tag, all output will be devoid of punctuation. An example output string translation of the input example: manual of photography to learn everything essential about __UNKNOWN : fujifilm cameras","title":"Outputs"},{"location":"plugins/tmt-statistical-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. TextTransformer \u2013 Plugin accepts and analyzes text string inputs, and outputs a new text string as output. In the case of TMT plugins, a text string in the source language should be provided as input, with the expectation that the output will be a text string translated to the desired destination language. TextTransformRequest TextTransformResult","title":"Functionality (Traits)"},{"location":"plugins/tmt-statistical-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/tmt-statistical-v1.html#limitations","text":"As the debut TMT plugin release for OLIVE, there are several known limitations that will impact the usage of this plugin. Like ASR, TMT plugins are language dependent and also largely text domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the translation models underlying each domain. Several factors contribute to what might limit the vocabulary of a translation model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts). Regarding performance, TMT plugins are especially sensitive to the tradeoffs between accuracy performance and realistic resource requirement and runtime constraints. The size-on-disk of each domain can be quite large depending on the technology of the individual plugin, and some technologies/plugins may have additional constraints. See the individual plugin detail pages for more information.","title":"Limitations"},{"location":"plugins/tmt-statistical-v1.html#multi-job-threading","text":"This plugin is currently not capable of multi-threading or parallel processing. If using this plugin with an OLIVE server, the server must be run in single-worker mode, by specifying a maximum number of jobs of 1: scenicserver -j 1 or scenicserver --workers 1 Alternatively, if the connected client only submits jobs synchronously, waiting for the completion and response of each job before submitting additional queries, problems will be avoided. Due to this limitation, of performing machine translation with this plugin using the CLI tools (localanalyze), if a multi-line input file is provided, this same stipulation of maximizing the active workers to '1' must be used to ensure that the line order of the output text file matches the line order of the input file.","title":"Multi-Job Threading"},{"location":"plugins/tmt-statistical-v1.html#language-dependence","text":"Each domain of this TMT plugin is language specific, and is only capable of translating text from one single language to one other. There is no filter or any sort of verification performed by OLIVE to ensure that the text passed to this domain is indeed of the correct language - this burden lies on the users to either manually or automatically triage input text if the source language is unknown, though it is often easy to spot mismatched input languages by the high number of output words that appears with the \" __UNKNOWN: \" tag.","title":"Language Dependence"},{"location":"plugins/tmt-statistical-v1.html#spelling-errors","text":"Note that there is no spell-checking or other types of spelling related pre-processing that occurs on the input data. Therefore any spelling mistakes in the input are likely to cause the system to output __UNKNOWN: tags.","title":"Spelling Errors"},{"location":"plugins/tmt-statistical-v1.html#out-of-vocabulary-oov-words-names","text":"The individual words that the plugin is capable of recognizing and translating is determined by the vocabulary that the corresponding language model was trained with. This means that some uncommon or unofficial words, like slang or other types of colloquial speech, as well as names or places, may not be possible to be translated by a plugin out-of-the-box. Several factors contribute to what might limit the vocabulary of a language model, including the age of the text data used during development, the source or domain of this data (such as broadcast news transcript versus social media type posts), or pruning the vocabulary to increase processing speed and/or reduce memory requirements. At this time there is no provision in OLIVE for adding words or names to the vocabulary or to the translation model. Some of the models underlying this plugin support vocabulary addition like this, so it may be possible for this feature to be supported in the future with OLIVE modifications.","title":"Out of Vocabulary (OOV) Words, Names"},{"location":"plugins/tmt-statistical-v1.html#input-limit","text":"There is a maximum limit on the input that a single request/query can have. This may depend on input resources available and number of total characters, but currently seems to be roughly 600 words. Extra large inputs that may approach or exceed this number should be split into multiple job submissions.","title":"Input Limit"},{"location":"plugins/tmt-statistical-v1.html#resources-disk-space","text":"Because it is based on statistical MT, this plugin's performance generally directly corresponds to the size of the models it uses, as these models grow the system is exposed to more and more data to learn from. As a result, the included models are very large. Please ensure you have adequate disk space available before attempting to use this plugin. Future TMT plugins will be based on a neural MT architecture that will not have such extreme model size requirements.","title":"Resources (disk space)"},{"location":"plugins/tmt-statistical-v1.html#comments","text":"","title":"Comments"},{"location":"plugins/tmt-statistical-v1.html#global-options","text":"This plugin does not expose any options to the user.","title":"Global Options"},{"location":"plugins/tmt.html","text":"redirect: plugins/tmt-neural-v1.md","title":"Tmt"},{"location":"plugins/tmt.html#redirect-pluginstmt-neural-v1md","text":"","title":"redirect: plugins/tmt-neural-v1.md"},{"location":"plugins/tpd-dynapy-v3.html","text":"tpd-dynapy-v3 (Topic Detection) Version Changelog Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v2.0.0, but updated to be compatible with OLIVE 5.1.0 Description Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic detection framework using ASR-derived BERT embeddings, with PLDA backend scoring and multi-class calibration. This plugin improves on prior topic detection technology using updated automatic speech recognition (ASR) models, and BERT embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided. Domains eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-cts-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech. Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language. Comments The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below. Base Topic Data eng-cts-v1 BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 ACTIVITIES CHILDREN COMPUTERS-INTERNET-TECHNOLOGY-SOCIAL_MEDIA EDUCATION-CLASS-EXAMS-HOMEWORK-SCHOOLS ENTERTAINMENT-CONCERTS-SHOWS-FESTIVALS FOOD-DRINK-COOKING-DIET-RESTAURANT-GROCERY_SHOPPING FRIENDS-RELATIVES GET-TOGETHER HEALTH-ILLNESS-INJURY-TREATMENT-INSURANCE HOME-RESIDENCE-NEIGHBORHOOD-HOTEL-REAL_ESTATE LIFE-PHILOSOPHY-RELATIONSHIPS SPEECH_COLLECTION_PROJECT SPORTS-GAMES-HOBBIES-LEISURE TRANSPORTATION-VEHICLES TRAVEL WORK-PROFESSION cmn-cts-v1 ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_tpd_dynapy_v2_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 45.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"tpd-dynapy-v3 (Topic Detection)"},{"location":"plugins/tpd-dynapy-v3.html#tpd-dynapy-v3-topic-detection","text":"","title":"tpd-dynapy-v3 (Topic Detection)"},{"location":"plugins/tpd-dynapy-v3.html#version-changelog","text":"Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v2.0.0, but updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/tpd-dynapy-v3.html#description","text":"Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic detection framework using ASR-derived BERT embeddings, with PLDA backend scoring and multi-class calibration. This plugin improves on prior topic detection technology using updated automatic speech recognition (ASR) models, and BERT embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided.","title":"Description"},{"location":"plugins/tpd-dynapy-v3.html#domains","text":"eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-cts-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection.","title":"Domains"},{"location":"plugins/tpd-dynapy-v3.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-dynapy-v3.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-dynapy-v3.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s).","title":"Enrollments"},{"location":"plugins/tpd-dynapy-v3.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-dynapy-v3.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/tpd-dynapy-v3.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-dynapy-v3.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-dynapy-v3.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-dynapy-v3.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech.","title":"Minimum Speech Duration"},{"location":"plugins/tpd-dynapy-v3.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language.","title":"Language Dependence"},{"location":"plugins/tpd-dynapy-v3.html#comments","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below.","title":"Comments"},{"location":"plugins/tpd-dynapy-v3.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-dynapy-v3.html#eng-cts-v1","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1"},{"location":"plugins/tpd-dynapy-v3.html#rus-cts-v1","text":"ACTIVITIES CHILDREN COMPUTERS-INTERNET-TECHNOLOGY-SOCIAL_MEDIA EDUCATION-CLASS-EXAMS-HOMEWORK-SCHOOLS ENTERTAINMENT-CONCERTS-SHOWS-FESTIVALS FOOD-DRINK-COOKING-DIET-RESTAURANT-GROCERY_SHOPPING FRIENDS-RELATIVES GET-TOGETHER HEALTH-ILLNESS-INJURY-TREATMENT-INSURANCE HOME-RESIDENCE-NEIGHBORHOOD-HOTEL-REAL_ESTATE LIFE-PHILOSOPHY-RELATIONSHIPS SPEECH_COLLECTION_PROJECT SPORTS-GAMES-HOBBIES-LEISURE TRANSPORTATION-VEHICLES TRAVEL WORK-PROFESSION","title":"rus-cts-v1"},{"location":"plugins/tpd-dynapy-v3.html#cmn-cts-v1","text":"ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK","title":"cmn-cts-v1"},{"location":"plugins/tpd-dynapy-v3.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_tpd_dynapy_v2_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 45.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/tpd-dynapy-v4.html","text":"tpd-dynapy-v4 (Topic Detection) Version Changelog Plugin Version Change v4.0.0 Initial plugin release, tested to be compatible with OLIVE 5.1.0 Description Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic embeddings framework with PLDA backend scoring and multi-class calibration. This plugin improves on prior TPD technology using updated ASR models, and XLM-RoBERTa word embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided. Domains eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-cts-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech. Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language. Comments or Usage Notes The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below. Base Topic Data eng-cts-v1 BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK cmn-cts-v1 ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_tpd_dynapy_v2_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"tpd-dynapy-v4 (Topic Detection)"},{"location":"plugins/tpd-dynapy-v4.html#tpd-dynapy-v4-topic-detection","text":"","title":"tpd-dynapy-v4 (Topic Detection)"},{"location":"plugins/tpd-dynapy-v4.html#version-changelog","text":"Plugin Version Change v4.0.0 Initial plugin release, tested to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/tpd-dynapy-v4.html#description","text":"Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic embeddings framework with PLDA backend scoring and multi-class calibration. This plugin improves on prior TPD technology using updated ASR models, and XLM-RoBERTa word embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided.","title":"Description"},{"location":"plugins/tpd-dynapy-v4.html#domains","text":"eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-cts-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection.","title":"Domains"},{"location":"plugins/tpd-dynapy-v4.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-dynapy-v4.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-dynapy-v4.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic.","title":"Enrollments"},{"location":"plugins/tpd-dynapy-v4.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-dynapy-v4.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/tpd-dynapy-v4.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-dynapy-v4.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-dynapy-v4.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-dynapy-v4.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech.","title":"Minimum Speech Duration"},{"location":"plugins/tpd-dynapy-v4.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language.","title":"Language Dependence"},{"location":"plugins/tpd-dynapy-v4.html#comments-or-usage-notes","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below.","title":"Comments or Usage Notes"},{"location":"plugins/tpd-dynapy-v4.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-dynapy-v4.html#eng-cts-v1","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1"},{"location":"plugins/tpd-dynapy-v4.html#rus-cts-v1","text":"ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK","title":"rus-cts-v1"},{"location":"plugins/tpd-dynapy-v4.html#cmn-cts-v1","text":"ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK","title":"cmn-cts-v1"},{"location":"plugins/tpd-dynapy-v4.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_tpd_dynapy_v2_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/tpd-dynapy-v5.html","text":"tpd-dynapy-v5 (Topic Detection) Version Changelog Plugin Version Change v5.0.0 Initial plugin release, tested to be compatible with OLIVE 5.2.0 v5.0.1 Released with OLIVE 5.3.0, minor parameter change to improve output stability Description Topic Detection (TPD) plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic embeddings framework with PLDA backend scoring and multi-class calibration. This plugin improves on prior TPD technology using updated ASR models, and XLM-RoBERTa word embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided. Domains This plugin includes two Mandarin Chinese domains - they are both using monolingual word embedding extractors with the difference being the size of their respective embedding sizes, 768 for cmn-tel-v1 vs. 1024 for cmn-tel-large-v1. The base domain is significantly faster and uses significantly less memory during processing than the 'large' domain. The larger word embeddings used by the 'large' domain afforded slightly better TPD performance compared to the standard model in SRI's pilot experiments, and may have the potential to improve performance more in user data. eng-tel-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-tel-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-large-v1 Same as above, except this one uses higher-dimensional word embeddings. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.2+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech. Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language. Comments or Usage Notes The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below. Base Topic Data eng-cts-v1 BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK cmn-cts-v1 ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Topic Detection (TPD) ASR-based"},{"location":"plugins/tpd-dynapy-v5.html#tpd-dynapy-v5-topic-detection","text":"","title":"tpd-dynapy-v5 (Topic Detection)"},{"location":"plugins/tpd-dynapy-v5.html#version-changelog","text":"Plugin Version Change v5.0.0 Initial plugin release, tested to be compatible with OLIVE 5.2.0 v5.0.1 Released with OLIVE 5.3.0, minor parameter change to improve output stability","title":"Version Changelog"},{"location":"plugins/tpd-dynapy-v5.html#description","text":"Topic Detection (TPD) plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin is a topic embeddings framework with PLDA backend scoring and multi-class calibration. This plugin improves on prior TPD technology using updated ASR models, and XLM-RoBERTa word embeddings. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided.","title":"Description"},{"location":"plugins/tpd-dynapy-v5.html#domains","text":"This plugin includes two Mandarin Chinese domains - they are both using monolingual word embedding extractors with the difference being the size of their respective embedding sizes, 768 for cmn-tel-v1 vs. 1024 for cmn-tel-large-v1. The base domain is significantly faster and uses significantly less memory during processing than the 'large' domain. The larger word embeddings used by the 'large' domain afforded slightly better TPD performance compared to the standard model in SRI's pilot experiments, and may have the potential to improve performance more in user data. eng-tel-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-tel-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-large-v1 Same as above, except this one uses higher-dimensional word embeddings.","title":"Domains"},{"location":"plugins/tpd-dynapy-v5.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-dynapy-v5.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-dynapy-v5.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic.","title":"Enrollments"},{"location":"plugins/tpd-dynapy-v5.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-dynapy-v5.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/tpd-dynapy-v5.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-dynapy-v5.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-dynapy-v5.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-dynapy-v5.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech.","title":"Minimum Speech Duration"},{"location":"plugins/tpd-dynapy-v5.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language.","title":"Language Dependence"},{"location":"plugins/tpd-dynapy-v5.html#comments-or-usage-notes","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below.","title":"Comments or Usage Notes"},{"location":"plugins/tpd-dynapy-v5.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-dynapy-v5.html#eng-cts-v1","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1"},{"location":"plugins/tpd-dynapy-v5.html#rus-cts-v1","text":"ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK","title":"rus-cts-v1"},{"location":"plugins/tpd-dynapy-v5.html#cmn-cts-v1","text":"ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK","title":"cmn-cts-v1"},{"location":"plugins/tpd-dynapy-v5.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/tpd-embed-v3.html","text":"tpd-embed-v3 (Topic Detection) Version Changelog Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.1.0 Description Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin features a topic embeddings framework based on phone-level acoustic representations rather than words. This version of the plugin uses both a built-in set of topics for multi-class calibration as well as any other enrolled classes/topics. This makes the plugin more accurate, due to the leverage of other topics in the target domains, but also creates a dependency between enrolled topics. This plugin improves on prior TPD technology in being 20x faster, having a very small footprint of 89M on disk and using less than 1G in RAM, while offering similar performance. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). There are no pre-enrolled classes for detection, user must add annotated topic data to the system (enrollment). TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. Domains eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.1+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain, since this plugin is using the other topics to improve its performance. If a given topic has few samples (such as 10), it will not perform as well as if it had 50 samples (ideal). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate against each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Topic Embeddings Trade-offs Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. This is a newer TPD plugin that is based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. Comments The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 base topics for each domain used to improve calibration when limited topics have been enrolled. Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Base Topic Data eng-cts-v1 Domain BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 Domain CHILDREN EDUCATION-CLASS-EXAMS-HOMEWORK-SCHOOLS FOOD-DRINK-COOKING-DIET-RESTAURANT-GROCERY_SHOPPING FRIENDS-RELATIVES GET-TOGETHER-FUTURE LIFE-PHILOSOPHY-RELATIONSHIPS SPORTS-GAMES-HOBBIES-LEISURE TRAVEL-FUTURE TRAVEL-PAST WORK-PROFESSION Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 45.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"tpd-embed-v3 (Topic Detection)"},{"location":"plugins/tpd-embed-v3.html#tpd-embed-v3-topic-detection","text":"","title":"tpd-embed-v3 (Topic Detection)"},{"location":"plugins/tpd-embed-v3.html#version-changelog","text":"Plugin Version Change v3.0.0 Initial plugin release, functionally identical to v1.0.0, but updated to be compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/tpd-embed-v3.html#description","text":"Topic Detection plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin features a topic embeddings framework based on phone-level acoustic representations rather than words. This version of the plugin uses both a built-in set of topics for multi-class calibration as well as any other enrolled classes/topics. This makes the plugin more accurate, due to the leverage of other topics in the target domains, but also creates a dependency between enrolled topics. This plugin improves on prior TPD technology in being 20x faster, having a very small footprint of 89M on disk and using less than 1G in RAM, while offering similar performance. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). There are no pre-enrolled classes for detection, user must add annotated topic data to the system (enrollment). TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations.","title":"Description"},{"location":"plugins/tpd-embed-v3.html#domains","text":"eng-cts-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-cts-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection.","title":"Domains"},{"location":"plugins/tpd-embed-v3.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-embed-v3.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-embed-v3.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. While the previous release of TPD also required non-topic data to provide negative examples, the latest acoustic-based systems do not. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s).","title":"Enrollments"},{"location":"plugins/tpd-embed-v3.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-embed-v3.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/tpd-embed-v3.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-embed-v3.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-embed-v3.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain, since this plugin is using the other topics to improve its performance. If a given topic has few samples (such as 10), it will not perform as well as if it had 50 samples (ideal). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate against each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-embed-v3.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ).","title":"Minimum Speech Duration"},{"location":"plugins/tpd-embed-v3.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2).","title":"Language Dependence"},{"location":"plugins/tpd-embed-v3.html#topic-embeddings-trade-offs","text":"Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. This is a newer TPD plugin that is based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach.","title":"Topic Embeddings Trade-offs"},{"location":"plugins/tpd-embed-v3.html#comments","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 base topics for each domain used to improve calibration when limited topics have been enrolled. Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below.","title":"Comments"},{"location":"plugins/tpd-embed-v3.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-embed-v3.html#eng-cts-v1-domain","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1 Domain"},{"location":"plugins/tpd-embed-v3.html#rus-cts-v1-domain","text":"CHILDREN EDUCATION-CLASS-EXAMS-HOMEWORK-SCHOOLS FOOD-DRINK-COOKING-DIET-RESTAURANT-GROCERY_SHOPPING FRIENDS-RELATIVES GET-TOGETHER-FUTURE LIFE-PHILOSOPHY-RELATIONSHIPS SPORTS-GAMES-HOBBIES-LEISURE TRAVEL-FUTURE TRAVEL-PAST WORK-PROFESSION","title":"rus-cts-v1 Domain"},{"location":"plugins/tpd-embed-v3.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 45.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/tpd-fusion-v1.html","text":"tpd-fusion-v1 (Topic Detection) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, tested to be compatible with OLIVE 5.2.0 v1.0.1 Released with OLIVE 5.3.0, minor parameter change to improve output stability Description Topic Detection (TPD) plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin features a bimodal TPD framework that combines the word (XLM-RoBERTa) and acoustic topic embeddings before the PLDA backend scoring and multi-class calibration. The word and acoustic embeddings are extracted separately using the embedding extractors of the unimodal TPD plugins, tpd-dynapy and tpd-embed, respectively and fused by applying linear discriminant analysis before applying the PLDA scoring. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided. Domains This plugin includes two Mandarin Chinese domains - they are both using monolingual word embedding extractors with the difference being the size of their respective embedding sizes, 768 for cmn-tel-v1 vs. 1024 for cmn-tel-large-v1. The base domain is significantly faster and uses significantly less memory during processing than the 'large' domain. The larger word embeddings used by the 'large' domain afforded slightly better TPD performance compared to the standard model in SRI's pilot experiments, and may have the potential to improve performance more in user data. eng-tel-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-tel-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-large-v1 Same as above, except this one uses higher-dimensional word embeddings. Inputs For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration. Outputs TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237 Enrollments Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest Compatibility OLIVE 5.2+ Limitations There are four main limitations that will impact the usage of this plugin. Labeling Resolution Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds. Low Enrollment Data While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other. Minimum Speech Duration The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech. Language Dependence TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language. Comments or Usage Notes The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below. Base Topic Data eng-cts-v1 BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING rus-cts-v1 ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK cmn-cts-v1 ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK Global Options The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Topic Detection (TPD) ASR/Acoustic Fusion"},{"location":"plugins/tpd-fusion-v1.html#tpd-fusion-v1-topic-detection","text":"","title":"tpd-fusion-v1 (Topic Detection)"},{"location":"plugins/tpd-fusion-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, tested to be compatible with OLIVE 5.2.0 v1.0.1 Released with OLIVE 5.3.0, minor parameter change to improve output stability","title":"Version Changelog"},{"location":"plugins/tpd-fusion-v1.html#description","text":"Topic Detection (TPD) plugins detect and label regions within a submitted audio segment where one or more enrolled topics are discussed. TPD plugins are capable of handling multiple topics within a single audio segment, and require topics of interest to be enrolled into the system by the end user. This plugin features a bimodal TPD framework that combines the word (XLM-RoBERTa) and acoustic topic embeddings before the PLDA backend scoring and multi-class calibration. The word and acoustic embeddings are extracted separately using the embedding extractors of the unimodal TPD plugins, tpd-dynapy and tpd-embed, respectively and fused by applying linear discriminant analysis before applying the PLDA scoring. The discrimination of the system is improved when adding more enrollment data for each topic, or adding new topics - that is, topics are NOT independent. All domains are tailored toward conversational telephone speech, but may be suitable for other domains (lightly tested). Before a topic can be detected by the TPD plugin, the topic of interest must be enrolled into the system by providing annotated audio examples from the desired topic. TPD requires substantial enrollment data per topic and is very unlikely to perform well with less than ten examples from ten separate conversations. There are several pre-enrolled background classes for each domain. These background topics give the system a sort of baseline for calibration and comparison when attempting to detect topics from the pool of target user-enrolled topics. Take care when enrolling new topics for detection. If one of these 'background' topics is similar to a topic the user is interested in, the identical topic name should be used when enrolling examples of this topic, so the system knows to replace the similar 'background' topic with the topic of interest, using the enrollment audio provided.","title":"Description"},{"location":"plugins/tpd-fusion-v1.html#domains","text":"This plugin includes two Mandarin Chinese domains - they are both using monolingual word embedding extractors with the difference being the size of their respective embedding sizes, 768 for cmn-tel-v1 vs. 1024 for cmn-tel-large-v1. The base domain is significantly faster and uses significantly less memory during processing than the 'large' domain. The larger word embeddings used by the 'large' domain afforded slightly better TPD performance compared to the standard model in SRI's pilot experiments, and may have the potential to improve performance more in user data. eng-tel-v1 English domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. rus-tel-v1 Russian domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-v1 Mandarin Chinese domain focused on conversational telephony speech. There are no pre-enrolled classes for detection. cmn-tel-large-v1 Same as above, except this one uses higher-dimensional word embeddings.","title":"Domains"},{"location":"plugins/tpd-fusion-v1.html#inputs","text":"For enrollment, an audio file or buffer and time-annotated regions corresponding to a given topic identifier/label are required. For scoring, an audio buffer or file. IMPORTANT: TPD requires substantial enrollment data per class to function properly, or error rates will be very high. The system begins to perform well with ten or more examples of the topic, from different conversations. These topics should be annotated to include only the parts of the conversation that are \"on-topic\". Each conversational example should be at least 30 seconds or more in duration.","title":"Inputs"},{"location":"plugins/tpd-fusion-v1.html#outputs","text":"TPD is a region scorer, and as such will return a list of detections consisting of timestamp regions in seconds, each with an accompanying score, and a previously-enrolled topic name that this score belongs to, when a topic has been detected in the input audio. It's possible to have overlapping topics, to have more than one topic detected within the same audio file/segment, and to have more than one detected region with the same topic being discussed. /data/eng/jRVUhtd_O1M.wav 352.380 424.550 WINE_MAKING 2.60474730 /data/eng/jRVUhtd_O1M.wav 431.180 442.280 WINE_MAKING 2.52452803 /data/eng/mueQ8-wABmg.wav 48.670 131.130 FISHING 5.71391582 /data/eng/mueQ8-wABmg.wav 142.250 167.950 FISHING 9.55547237 /data/eng/mueQ8-wABmg.wav 172.220 182.370 FISHING 9.55547237","title":"Outputs"},{"location":"plugins/tpd-fusion-v1.html#enrollments","text":"Topic Detection plugins allow class modifications. A class modification is essentially the capability to enroll a class with sample(s) of a class's speech - in this case, a new topic. Adding additional examples to existing topics proceeds exactly like other enrollments for plugins like speaker detection, etc. TPD enrollment requires an audio sample with regions that are labeled for the desired topic(s). If no annotations for time offsets are provided it is assumed that the entire files is on-topic. A new enrollment is created with the first class modification, which consists of essentially sending the system an audio sample with a topic annotation. This enrollment can be augmented with subsequent class modification requests by adding more audio with region labels for the same topic(s). Note that you should never enroll the same audio in two or more different topics, even if the topics are closely related. Each recording should only ever have one label - audio should never be reused in enrollment unless it is first deleted from an existing topic.","title":"Enrollments"},{"location":"plugins/tpd-fusion-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected topic of interest and corresponding score for this topic. RegionScorerRequest RegionScorerStereoRequest? CLASS_MODIFIER \u2013 Enroll new topic models or augment existing topic models with additional data. ClassModificationRequest ClassRemovalRequest","title":"Functionality (Traits)"},{"location":"plugins/tpd-fusion-v1.html#compatibility","text":"OLIVE 5.2+","title":"Compatibility"},{"location":"plugins/tpd-fusion-v1.html#limitations","text":"There are four main limitations that will impact the usage of this plugin.","title":"Limitations"},{"location":"plugins/tpd-fusion-v1.html#labeling-resolution","text":"Region scoring is performed using a sliding window, if a large window is used (60 seconds) and a single topic exists in that window, the system is robust. If multiple topics exist in a single window or a smaller window (20 seconds) is used, the results will be noisier. There is a balance to be found between resolution of topic change and robustness. Testing on a dataset known to have short time-span topics (say 20 seconds) using a 60 second window will not perform as well as matching the window to the expected topic duration of 20 seconds.","title":"Labeling Resolution"},{"location":"plugins/tpd-fusion-v1.html#low-enrollment-data","text":"While it is possible to enroll a topic on a single file, the system thrives on ample enrollment data per topic and benefits from having multiple topics from the target domain. If a given topic has few samples (such as 10), it will not perform as good as if it had 50 samples (recommended). If one topic is enrolled, the system will not perform as well as when 10 or 20 topics are enrolled for the target domain. This is because the enrolled topics actively discriminate each other.","title":"Low Enrollment Data"},{"location":"plugins/tpd-fusion-v1.html#minimum-speech-duration","text":"The system will not attempt to perform topic detection unless 10 seconds of speech of more is found in the file (configurable as min_speech ). This minimum amount of speech must also occur in a relatively short timespan, with no more than a 4 second gap between islands of speech.","title":"Minimum Speech Duration"},{"location":"plugins/tpd-fusion-v1.html#language-dependence","text":"TPD is language dependent and also largely domain dependent. The domains that a plugin can effectively cover are largely determined by the data used to train the speech recognition system (for text-based plug-ins like tpd-dynapy-v1) or acoustic embeddings extractor (as with tpd-embed-v1/v2). Older TPD plugins that are based on ASR technology can also be very slow and heavy on resource utilization, making them cumbersome to use. Newer TPD plugins are based on a topic embeddings framework that is much easier to port to new languages since it is based on more language-independent technology. It is also significantly faster than the ASR-based approach. Topic embeddings plugins do have trade-offs, though; as a newer and less mature technology, the topic detection accuracy often does not yet match the traditional ASR approach. It is up to the user to ensure that the audio being passed in to this plugin is of the appropriate language. Each domain is capable of recognizing audio in a single, specific language, and has no capability of detecting or rejecting if input audio does not fit this language.","title":"Language Dependence"},{"location":"plugins/tpd-fusion-v1.html#comments-or-usage-notes","text":"The system retrains the discriminative space on loading or after any enrollment is added/removed. For this purpose, some base data is included in the plugin with enrollment data used to complement the space. This results in topic detection enrollments being dependent on each other (i.e., unenroll topic A and topic B scores will change for a previously run file). Similarly, there are 10 or 16 base topics for each domain used to improve calibration when limited topics have been enrolled.Data from these topics is not used if the user enrolls a topic of the same label. It is therefore important for the user to match the naming of these topics if they choose to enroll a very similar or the same topic in order to prevent competition of the same/similar topic. The topics for each domain are listed in a file 'model.classes' within each domain and listed below. Additionally, the min_speech requirement of 10.0 seconds requires that speech segments must make up 10.0 seconds of continuous speech whereby they are deemed continuous when no more than 4 seconds of silence exists between segments. These base-topics are listed below.","title":"Comments or Usage Notes"},{"location":"plugins/tpd-fusion-v1.html#base-topic-data","text":"","title":"Base Topic Data"},{"location":"plugins/tpd-fusion-v1.html#eng-cts-v1","text":"BUYING_A_CAR CAPITAL_PUNISHMENT DRUG_TESTING EXERCISE_AND_FITNESS FAMILY_FINANCE JOB_BENEFITS NEWS_MEDIA PETS PUBLIC_EDUCATION RECYCLING","title":"eng-cts-v1"},{"location":"plugins/tpd-fusion-v1.html#rus-cts-v1","text":"ACTIVITIES BIRTHDAY-WISHES BOOKS BUYING CHILDREN COMPUTER EDUCATION ENTERTAINMENT FOOD FRIENDS GET-TOGETHER HEALTH HOLIDAY HOME-MAINTENANCE IMMIGRATION LANGUAGE LIFE LOCATION MARRIAGE MOVING MUSIC PERFORMANCE PERSONAL PETS POLITICS PROJECT REAL-ESTATE SPEECH-COLLECTION SPORTS TRANSPORTATION TRAVEL TV WEATHER WORK","title":"rus-cts-v1"},{"location":"plugins/tpd-fusion-v1.html#cmn-cts-v1","text":"ACTIVITIES BUYING CITIZENSHIP COLLECTION CORRESPONDENCE FINANCES FRIENDS HEALTH HOME LIFE LOCATION PERSONAL SCHOOL TRANSPORT TRAVEL WORK","title":"cmn-cts-v1"},{"location":"plugins/tpd-fusion-v1.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file: plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. 0.0 -10.0 to 10.0 window Size of sliding window over speech: Each window is assumed to contain a single topic. Longer is more robust at the cost of resolution. 60.0 10.0 to 120.0 min_speech Required amount of speech in file to enroll and perform topic detection. 10.0 2.0 to 30.0","title":"Global Options"},{"location":"plugins/tpd-new.html","text":"redirect: plugins/tpd-fusion-v1.md","title":"Tpd new"},{"location":"plugins/tpd-new.html#redirect-pluginstpd-fusion-v1md","text":"","title":"redirect: plugins/tpd-fusion-v1.md"},{"location":"plugins/tpd-trad.html","text":"redirect: plugins/tpd-dynapy-v5.md","title":"Tpd trad"},{"location":"plugins/tpd-trad.html#redirect-pluginstpd-dynapy-v5md","text":"","title":"redirect: plugins/tpd-dynapy-v5.md"},{"location":"plugins/voi-speakingStyle-v1.html","text":"voi-speakingStyle-v1 (Voice Characterization - Speaking Style) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 Description Speaking Style (voi-speakingStyle) plugins detect different types of speaking styles in a given audio segment. These speaking styles are \"conversation\" or \"oration\". Domains multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Inputs An audio file or buffer to be scored. Outputs In the basic case, Speaking Style plugins returns the top speaking style score for each input audio file, labeled with \"conversation\" or \"oration\". The 'score' field is log-likelihood ratio ranging 0 to 1. input-audio1.wav conversation 0.94832635 input-audio2.wav oration 0.99265623 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning labeled number of speakers within the submitted audio. GlobalScorerRequest Compatibility OLIVE 5.1+ Limitations All current voi-speakingStyle plugins assume that an audio segment contains single type of speaking. A minimum duration of speech of 5 seconds is required in order to output scores. This value can optionally be overwritten by the user, but scores provided for such short segments will be less reliable.","title":"voi-speakingStyle-v1 (Voice Characterization - Speaking Style)"},{"location":"plugins/voi-speakingStyle-v1.html#voi-speakingstyle-v1-voice-characterization-speaking-style","text":"","title":"voi-speakingStyle-v1 (Voice Characterization - Speaking Style)"},{"location":"plugins/voi-speakingStyle-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/voi-speakingStyle-v1.html#description","text":"Speaking Style (voi-speakingStyle) plugins detect different types of speaking styles in a given audio segment. These speaking styles are \"conversation\" or \"oration\".","title":"Description"},{"location":"plugins/voi-speakingStyle-v1.html#domains","text":"multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/voi-speakingStyle-v1.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/voi-speakingStyle-v1.html#outputs","text":"In the basic case, Speaking Style plugins returns the top speaking style score for each input audio file, labeled with \"conversation\" or \"oration\". The 'score' field is log-likelihood ratio ranging 0 to 1. input-audio1.wav conversation 0.94832635 input-audio2.wav oration 0.99265623","title":"Outputs"},{"location":"plugins/voi-speakingStyle-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. GLOBAL_SCORER \u2013 Score all submitted audio, returning labeled number of speakers within the submitted audio. GlobalScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/voi-speakingStyle-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/voi-speakingStyle-v1.html#limitations","text":"All current voi-speakingStyle plugins assume that an audio segment contains single type of speaking. A minimum duration of speech of 5 seconds is required in order to output scores. This value can optionally be overwritten by the user, but scores provided for such short segments will be less reliable.","title":"Limitations"},{"location":"plugins/voi-vocalEffort-v1.html","text":"voi-vocalEffort-v1 (Voice Characterization - VocalEffort) Version Changelog Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 v1.0.1 Various bug fixes and stability improvements, compatible with OLIVE 5.1.0 Description Vocal Effort (voi-vocalEffort) plugins detects level of vocal effort in an audio segment. The range of vocal effort includes \"whisper\", \"neutral\", or \"shout\". Domains multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Inputs An audio file or buffer to be scored. Outputs In the basic case, Vocal Effort plugins returns a list of regions labeled with \"whisper\", \"neutral\", or \"shout\". Regions are represented in seconds. The 'score' field is log-likelihood ratio ranging 0 to 1. input-audio1.wav 2.0 10.0 neutral 1.0 input-audio2.wav 2.0 11.0 whisper 0.97211498 input-audio3.wav 2.0 13.0 shout 0.99872446 input-audio4.wav 2.0 9.0 neutral 1.0 input-audio5.wav 2.0 9.0 neutral 1.0 Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected level of vocal effort and corresponding score for this level. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations All current voi-speakingStyle plugins assume that an audio segment contains single type of speaking. A minimum duration of speech of 3 seconds is required in order to output scores. This value can optionally be overwritten by the user, but scores provided for such short segments will be less reliable.","title":"voi-vocalEffort-v1 (Voice Characterization - VocalEffort)"},{"location":"plugins/voi-vocalEffort-v1.html#voi-vocaleffort-v1-voice-characterization-vocaleffort","text":"","title":"voi-vocalEffort-v1 (Voice Characterization - VocalEffort)"},{"location":"plugins/voi-vocalEffort-v1.html#version-changelog","text":"Plugin Version Change v1.0.0 Initial plugin release, compatible with OLIVE 5.1.0 v1.0.1 Various bug fixes and stability improvements, compatible with OLIVE 5.1.0","title":"Version Changelog"},{"location":"plugins/voi-vocalEffort-v1.html#description","text":"Vocal Effort (voi-vocalEffort) plugins detects level of vocal effort in an audio segment. The range of vocal effort includes \"whisper\", \"neutral\", or \"shout\".","title":"Description"},{"location":"plugins/voi-vocalEffort-v1.html#domains","text":"multi-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/voi-vocalEffort-v1.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/voi-vocalEffort-v1.html#outputs","text":"In the basic case, Vocal Effort plugins returns a list of regions labeled with \"whisper\", \"neutral\", or \"shout\". Regions are represented in seconds. The 'score' field is log-likelihood ratio ranging 0 to 1. input-audio1.wav 2.0 10.0 neutral 1.0 input-audio2.wav 2.0 11.0 whisper 0.97211498 input-audio3.wav 2.0 13.0 shout 0.99872446 input-audio4.wav 2.0 9.0 neutral 1.0 input-audio5.wav 2.0 9.0 neutral 1.0","title":"Outputs"},{"location":"plugins/voi-vocalEffort-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected level of vocal effort and corresponding score for this level. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/voi-vocalEffort-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/voi-vocalEffort-v1.html#limitations","text":"All current voi-speakingStyle plugins assume that an audio segment contains single type of speaking. A minimum duration of speech of 3 seconds is required in order to output scores. This value can optionally be overwritten by the user, but scores provided for such short segments will be less reliable.","title":"Limitations"},{"location":"plugins/voi-voiceCharacterization-v1.html","text":"Voice Characterization (VOI) - voi-voiceCharacterization-v1.0.0 Description Voice Characterization (VOI) plugins will detect and label regions of speech in a submitted audio segment where one or more classes, types, or artifacts of speech are detected. The actual classes being detected will depend on the domain being used. This plugin has domains with two goals: Speaking Style The speakingStyle domain will detect and label regions of speech where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. Vocal Effort In both cases, the plugin will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech is none of these styles and its score is below threshold, the plugins detect none of these. No options are provided for users. Speaking Style (voi-speakingStyle) plugins will detect and label regions of speech in a submitted audio segment where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. The plugins will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech is none of these styles and its score is below threshold, the plugins detect none of these. No options are provided for users. Vocal Effort (voi-vocalEffort) plugins will detect and label regions of speech in a submitted audio segment where one or more levels of vocal effort (low, normal, high, scream, shout) are detected. The plugins will provide timestamp region labels to point to the locations when one of these levels is found. When the input speech is none of these levels and its score is below threshold, the plugins detect none of these. No options are provided for users Inputs An audio file or buffer to be scored. Outputs Speaking Style plugins returns a list of regions labelled with \"dialogue\", \"oration\", \"read\" with scores. Regions are represented in seconds. The 'score' field is the posterior probability ranging 0 to 1. All scores 0.0 above will be detected as output by default. input-audio1.wav 2.00 23.18 dialogue 0.94832635 input-audio2.wav 2.00 29.99 read 0.99265623 input-audio3.wav 2.00 27.78 oration 0.98858994 Vocal Effort plugins returns a list of regions labelled with \"low\", \"normal\", \"high\", \"scream\", \"shout\" with scores. Regions are represented in seconds. The 'score' field is the posterior probability ranging 0 to 1. All scores 0.0 above will be detected as output. input-audio1.wav 0.50 10.20 low 1.0 input-audio2.wav 0.50 10.62 normal 0.97211498 input-audio3.wav 0.50 12.98 high 0.99872446 input-audio4.wav 0.50 9.97 shout 1.0 input-audio5.wav 0.50 9.17 scream 1.0 Limitations A minimum duration of speech of 5 seconds is required in order to output scores. If the duration of speech is lesser than 5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. A minimum duration of speech of 1.5 seconds is required in order to output scores. If the duration of speech is lesser than 1.5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. Interface For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide . voi-speakingStyle-v1 (Speaking Style) Description Speaking Style (voi-speakingStyle) plugins will detect and label regions of speech in a submitted audio segment where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. The plugins will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech contains none of these styles and its score is below the threshold, the plugins detect none of these. No options are provided for users. Domains default-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. default-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected type of speaking style and corresponding score for this type. RegionScorerRequest Compatibility OLIVE 5.1+ Limitations A minimum duration of speech of 5 seconds is required in order to output scores. If the duration of speech is lesser than 5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. A minimum duration of speech of 1.5 seconds is required in order to output scores. If the duration of speech is lesser than 1.5 seconds or no speech is found in a given audio segment, the plugins will raise insufficient speech messages.","title":"Voice Characterization (VOI) - voi-voiceCharacterization-v1.0.0"},{"location":"plugins/voi-voiceCharacterization-v1.html#voice-characterization-voi-voi-voicecharacterization-v100","text":"","title":"Voice Characterization (VOI) - voi-voiceCharacterization-v1.0.0"},{"location":"plugins/voi-voiceCharacterization-v1.html#description","text":"Voice Characterization (VOI) plugins will detect and label regions of speech in a submitted audio segment where one or more classes, types, or artifacts of speech are detected. The actual classes being detected will depend on the domain being used. This plugin has domains with two goals: Speaking Style The speakingStyle domain will detect and label regions of speech where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. Vocal Effort In both cases, the plugin will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech is none of these styles and its score is below threshold, the plugins detect none of these. No options are provided for users. Speaking Style (voi-speakingStyle) plugins will detect and label regions of speech in a submitted audio segment where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. The plugins will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech is none of these styles and its score is below threshold, the plugins detect none of these. No options are provided for users. Vocal Effort (voi-vocalEffort) plugins will detect and label regions of speech in a submitted audio segment where one or more levels of vocal effort (low, normal, high, scream, shout) are detected. The plugins will provide timestamp region labels to point to the locations when one of these levels is found. When the input speech is none of these levels and its score is below threshold, the plugins detect none of these. No options are provided for users","title":"Description"},{"location":"plugins/voi-voiceCharacterization-v1.html#inputs","text":"An audio file or buffer to be scored.","title":"Inputs"},{"location":"plugins/voi-voiceCharacterization-v1.html#outputs","text":"Speaking Style plugins returns a list of regions labelled with \"dialogue\", \"oration\", \"read\" with scores. Regions are represented in seconds. The 'score' field is the posterior probability ranging 0 to 1. All scores 0.0 above will be detected as output by default. input-audio1.wav 2.00 23.18 dialogue 0.94832635 input-audio2.wav 2.00 29.99 read 0.99265623 input-audio3.wav 2.00 27.78 oration 0.98858994 Vocal Effort plugins returns a list of regions labelled with \"low\", \"normal\", \"high\", \"scream\", \"shout\" with scores. Regions are represented in seconds. The 'score' field is the posterior probability ranging 0 to 1. All scores 0.0 above will be detected as output. input-audio1.wav 0.50 10.20 low 1.0 input-audio2.wav 0.50 10.62 normal 0.97211498 input-audio3.wav 0.50 12.98 high 0.99872446 input-audio4.wav 0.50 9.97 shout 1.0 input-audio5.wav 0.50 9.17 scream 1.0","title":"Outputs"},{"location":"plugins/voi-voiceCharacterization-v1.html#limitations","text":"A minimum duration of speech of 5 seconds is required in order to output scores. If the duration of speech is lesser than 5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. A minimum duration of speech of 1.5 seconds is required in order to output scores. If the duration of speech is lesser than 1.5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message.","title":"Limitations"},{"location":"plugins/voi-voiceCharacterization-v1.html#interface","text":"For command line interface use see the appropriate section of the OLIVE CLI User Guide . For API usage see the appropriate section of the OLIVE Application Programming Interface Guide .","title":"Interface"},{"location":"plugins/voi-voiceCharacterization-v1.html#voi-speakingstyle-v1-speaking-style","text":"","title":"voi-speakingStyle-v1 (Speaking Style)"},{"location":"plugins/voi-voiceCharacterization-v1.html#description_1","text":"Speaking Style (voi-speakingStyle) plugins will detect and label regions of speech in a submitted audio segment where one or more of 3 speaking styles (dialogue/conversation, oration, read) are detected being spoken. The plugins will provide timestamp region labels to point to the locations when one of these styles is found. When the input speech contains none of these styles and its score is below the threshold, the plugins detect none of these. No options are provided for users.","title":"Description"},{"location":"plugins/voi-voiceCharacterization-v1.html#domains","text":"default-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise. default-v1 A generic domain trained for close-talking audio conditions (telephony close talking microphone) and various conditions such as distant speech, compressed speech and background noise.","title":"Domains"},{"location":"plugins/voi-voiceCharacterization-v1.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to go to additional implementation details below. REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region includes a detected type of speaking style and corresponding score for this type. RegionScorerRequest","title":"Functionality (Traits)"},{"location":"plugins/voi-voiceCharacterization-v1.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/voi-voiceCharacterization-v1.html#limitations_1","text":"A minimum duration of speech of 5 seconds is required in order to output scores. If the duration of speech is lesser than 5 seconds or no speech is found in a given audio segment, the plugins will raise an insufficient speech message. A minimum duration of speech of 1.5 seconds is required in order to output scores. If the duration of speech is lesser than 1.5 seconds or no speech is found in a given audio segment, the plugins will raise insufficient speech messages.","title":"Limitations"},{"location":"plugins/vtd-dnn-v7.html","text":"vtd-dnn-v7 (Voice Type Discrimination) Version Changelog Plugin Version Change v7.0.1 Initial plugin release, this plugin shares a codebase with sad-dnn-v7.0.1, with only the models and parameters configured for live speech detection. Released with OLIVE 5.1.0 v7.0.2 Bug fixes from v7.0.1, released with OLIVE 5.2.0 Description Voice type discrimination (VTD) plugins are designed to detect the presence of speech coming from a live human talker. The goal of VTD is to be able to distinguish not only live-produced, human speech from silence or noise, but also from speech being played over an electronic speaker, such as from a television or phone. When live speech is detected, it is labeled with the timestamps corresponding to its location in the audio. Like SAD, this plug-in may be used either as a frame scorer or region scorer. Domains vtd-v1 Domain designed to detect live speech indoors, differentiating within room live-speech from background distractors like TV, radio, door sound, telephone ringing, traffic etc. Inputs An audio file or buffer and optional identifier and/or optional regions. Outputs The current VTD plugin is capable of performing both frame scoring and region scoring . For frame scoring, typically, a log-likelihood ratio (LLR) score of live-speech vs. non-speech/non-live-speech per each 10ms frame of the input audio segment is output (i.e. 100 audio frames per second). An LLR of greater than \u201c0\u201d indicates that the likelihood of live-speech is greater than the likelihood of non-speech or non-live-speech and \u201c0\u201d is generally used as the threshold for detecting these live-speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech or non-live-speech. VTD plugins may also post-process frame scores to return speech regions, though this is often done on the client-side for flexibility. An excerpt example of what this typically looks like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by adding a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 seconds, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of VTD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a place holder number to retain formatting compatibility with other region-scoring type plugins. Adaptation VTD does not currently support adaptation. Functionality ( Traits ) The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest FrameScorerStereoRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest RegionScorerStereoRequest? This plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, VTD will return results only for those specified regions. Compatibility OLIVE 5.1+ Limitations Any known or potential limitations with this plugin are listed below. Speech Intelligibility The VTD plugin detects any and all live speech, whether it is intelligible or not. Live-speech Detection Difficulties It is especially difficult to detect live-speech or differentiate live-speech from pre-recorded or electronic-speaker-produced speech 1) when the microphone is placed very close to distractor sources like TV, radio, etc., 2) when these distractors are played at an unusually high volume or 3) in cases where the microphone is very distant from the source and the signal is weak. Minimum Audio Length A minimum waveform duration of 0.3 seconds is suggested to produce a meaningful live-speech detection. Comments Speech Region Padding When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD. Global Options The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false live-speech segments. Reduce the threshold value if there are too many missed live-speech segments. 0.0 -4.0 to 4.0","title":"Voice Type Discrimination (VTD)"},{"location":"plugins/vtd-dnn-v7.html#vtd-dnn-v7-voice-type-discrimination","text":"","title":"vtd-dnn-v7 (Voice Type Discrimination)"},{"location":"plugins/vtd-dnn-v7.html#version-changelog","text":"Plugin Version Change v7.0.1 Initial plugin release, this plugin shares a codebase with sad-dnn-v7.0.1, with only the models and parameters configured for live speech detection. Released with OLIVE 5.1.0 v7.0.2 Bug fixes from v7.0.1, released with OLIVE 5.2.0","title":"Version Changelog"},{"location":"plugins/vtd-dnn-v7.html#description","text":"Voice type discrimination (VTD) plugins are designed to detect the presence of speech coming from a live human talker. The goal of VTD is to be able to distinguish not only live-produced, human speech from silence or noise, but also from speech being played over an electronic speaker, such as from a television or phone. When live speech is detected, it is labeled with the timestamps corresponding to its location in the audio. Like SAD, this plug-in may be used either as a frame scorer or region scorer.","title":"Description"},{"location":"plugins/vtd-dnn-v7.html#domains","text":"vtd-v1 Domain designed to detect live speech indoors, differentiating within room live-speech from background distractors like TV, radio, door sound, telephone ringing, traffic etc.","title":"Domains"},{"location":"plugins/vtd-dnn-v7.html#inputs","text":"An audio file or buffer and optional identifier and/or optional regions.","title":"Inputs"},{"location":"plugins/vtd-dnn-v7.html#outputs","text":"The current VTD plugin is capable of performing both frame scoring and region scoring . For frame scoring, typically, a log-likelihood ratio (LLR) score of live-speech vs. non-speech/non-live-speech per each 10ms frame of the input audio segment is output (i.e. 100 audio frames per second). An LLR of greater than \u201c0\u201d indicates that the likelihood of live-speech is greater than the likelihood of non-speech or non-live-speech and \u201c0\u201d is generally used as the threshold for detecting these live-speech regions. A score of \u201c0\u201d indicated that speech is equally likely as non-speech or non-live-speech. VTD plugins may also post-process frame scores to return speech regions, though this is often done on the client-side for flexibility. An excerpt example of what this typically looks like: -0.68944 -0.47805 -0.27453 -0.07032 0.13456 0.34013 0.53357 0.97258 1.10885 This can be transformed into a region scoring output, either client-side, or by requesting region scores from the plugin. Region scores output by the plugin will provide the timestamp boundaries, in seconds, of the locations where live speech was detected in the audio. When the plugin is asked for region scores, the conversion from frame scores to region scores is done internally by applying a threshold to the scores. The contiguous frames that are above a threshold are converted to timestamps representing the start and end of each of these regions. These regions are then extended in duration by adding a padding value (typically of 0.5s or lower) to the region time start and adding that same padding value to the region time end. For example, if a region starts at 10 seconds and ends at 20 seconds, with a 1-second padding value, the new 'padded' region will range from 9 to 21 seconds. If after region-padding, two regions overlap in time, they will be merged into a single extended region. An example of VTD region scores: test_1.wav 10.159 13.219 speech 0.00000000 test_1.wav 149.290 177.110 speech 0.00000000 test_1.wav 188.810 218.849 speech 0.00000000 The final number in this format is a place holder number to retain formatting compatibility with other region-scoring type plugins.","title":"Outputs"},{"location":"plugins/vtd-dnn-v7.html#adaptation","text":"VTD does not currently support adaptation.","title":"Adaptation"},{"location":"plugins/vtd-dnn-v7.html#functionality-traits","text":"The functions of this plugin are defined by its Traits and implemented API messages. A list of these Traits is below, along with the corresponding API messages for each. Click the message name below to be brought to additional implementation details below. FRAME_SCORER - Score all submitted audio, returning a score corresponding to each 10ms frame of the file, representing the likelihood that each respective frame contains a detection. FrameScorerRequest FrameScorerStereoRequest REGION_SCORER \u2013 Score all submitted audio, returning labeled regions within the submitted audio, where each region represents timestamp boundaries where speech was detected. RegionScorerRequest RegionScorerStereoRequest? This plugin is capable of accepting Annotation Regions as part of the Audio message that each of the above messages include. When these optional Annotation Regions are provided, VTD will return results only for those specified regions.","title":"Functionality (Traits)"},{"location":"plugins/vtd-dnn-v7.html#compatibility","text":"OLIVE 5.1+","title":"Compatibility"},{"location":"plugins/vtd-dnn-v7.html#limitations","text":"Any known or potential limitations with this plugin are listed below.","title":"Limitations"},{"location":"plugins/vtd-dnn-v7.html#speech-intelligibility","text":"The VTD plugin detects any and all live speech, whether it is intelligible or not.","title":"Speech Intelligibility"},{"location":"plugins/vtd-dnn-v7.html#live-speech-detection-difficulties","text":"It is especially difficult to detect live-speech or differentiate live-speech from pre-recorded or electronic-speaker-produced speech 1) when the microphone is placed very close to distractor sources like TV, radio, etc., 2) when these distractors are played at an unusually high volume or 3) in cases where the microphone is very distant from the source and the signal is weak.","title":"Live-speech Detection Difficulties"},{"location":"plugins/vtd-dnn-v7.html#minimum-audio-length","text":"A minimum waveform duration of 0.3 seconds is suggested to produce a meaningful live-speech detection.","title":"Minimum Audio Length"},{"location":"plugins/vtd-dnn-v7.html#comments","text":"","title":"Comments"},{"location":"plugins/vtd-dnn-v7.html#speech-region-padding","text":"When performing region scoring with this plugin, a 0.3 second padding is added to the start and end of each detected speech region, in order to reduce edge-effects for human listening, and to otherwise smooth the output regions of SAD.","title":"Speech Region Padding"},{"location":"plugins/vtd-dnn-v7.html#global-options","text":"The following options are available to this plugin, adjustable in the plugin's configuration file; plugin_config.py . Option Name Description Default Expected Range threshold Detection threshold: Higher value results in less detections being output. Increase the threshold value to reduce the number and duration of false live-speech segments. Reduce the threshold value if there are too many missed live-speech segments. 0.0 -4.0 to 4.0","title":"Global Options"},{"location":"plugins/vtd.html","text":"redirect: plugins/vtd-dnn-v7.md","title":"Vtd"},{"location":"plugins/vtd.html#redirect-pluginsvtd-dnn-v7md","text":"","title":"redirect: plugins/vtd-dnn-v7.md"}]}; var search = { index: new Promise(resolve => setTimeout(() => resolve(local_index), 0)) }